<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><meta name="author" content="Fy J"><meta name="renderer" content="webkit"><meta name="copyright" content="Fy J"><meta name="keywords" content="MoyangSensei"><meta name="description" content="null"><meta name="Cache-Control" content="no-cache"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>论文研读：SurroundOcc · MoYang</title><link rel="stylesheet" href="/css/style.css?v=2018.7.9"><link rel="stylesheet" href="/css/animation.css?v=2018.7.9"><link rel="icon" href="/img/assets/Moyangico.ico"><link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.6"><!-- scripts--><script>(function( w ){
  "use strict";
  // rel=preload support test
  if( !w.loadCSS ){
    w.loadCSS = function(){};
  }
  // define on the loadCSS obj
  var rp = loadCSS.relpreload = {};
  // rel=preload feature support test
  // runs once and returns a function for compat purposes
  rp.support = (function(){
    var ret;
    try {
      ret = w.document.createElement( "link" ).relList.supports( "preload" );
    } catch (e) {
      ret = false;
    }
    return function(){
      return ret;
    };
  })();

  // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
  // then change that media back to its intended value on load
  rp.bindMediaToggle = function( link ){
    // remember existing media attr for ultimate state, or default to 'all'
    var finalMedia = link.media || "all";

    function enableStylesheet(){
      link.media = finalMedia;
    }

    // bind load handlers to enable media
    if( link.addEventListener ){
      link.addEventListener( "load", enableStylesheet );
    } else if( link.attachEvent ){
      link.attachEvent( "onload", enableStylesheet );
    }

    // Set rel and non-applicable media type to start an async request
    // note: timeout allows this to happen async to let rendering continue in IE
    setTimeout(function(){
      link.rel = "stylesheet";
      link.media = "only x";
    });
    // also enable media after 3 seconds,
    // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
    setTimeout( enableStylesheet, 3000 );
  };

  // loop through link elements in DOM
  rp.poly = function(){
    // double check this to prevent external calls from running
    if( rp.support() ){
      return;
    }
    var links = w.document.getElementsByTagName( "link" );
    for( var i = 0; i < links.length; i++ ){
      var link = links[ i ];
      // qualify links to those with rel=preload and as=style attrs
      if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
        // prevent rerunning on link
        link.setAttribute( "data-loadcss", true );
        // bind listeners to toggle media back
        rp.bindMediaToggle( link );
      }
    }
  };

  // if unsupported, run the polyfill
  if( !rp.support() ){
    // run once at least
    rp.poly();

    // rerun poly on an interval until onload
    var run = w.setInterval( rp.poly, 500 );
    if( w.addEventListener ){
      w.addEventListener( "load", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    } else if( w.attachEvent ){
      w.attachEvent( "onload", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    }
  }


  // commonjs
  if( typeof exports !== "undefined" ){
    exports.loadCSS = loadCSS;
  }
  else {
    w.loadCSS = loadCSS;
  }
}( typeof global !== "undefined" ? global : this ) );</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" defer></script><script src="/js/main.js?v=2018.7.9" defer></script><!-- fancybox--><link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script><!-- busuanzi--><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="MoyangSensei" type="application/atom+xml">
</head><body><section class="profile-close" id="cxo-profile"><div class="profile-avatar"><i class="fa fa-caret-left"></i><img src="/img/assets/cat.png"></div><!--.profile-saying
  i.fa.fa-comment
  .saying--><div class="cxo-profile-inner"><div class="profile-name">Fy J</div><div class="profile-signature">CS专业扫雷学深造学者互联网冲浪一级选手</div><div class="friends"><div>FRIENDS</div><span><a href="//hnjia00.github.io" target="_black">jhn</a></span></div><div class="read-progress"></div></div></section><header id="cxo-intro" style="height: 70vh;background-image: url(/img/intro/index-bg.png);"><nav id="cxo-intro-nav"><section><div class="intro-nav-title"><a href="/">MoYang's Blog</a></div><div class="intro-nav-label-box"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div><i class="fa fa-bars intro-nav-menu"><div class="intro-nav-drop"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div></i><div class="clear"></div></section></nav><h1 class="post-title">论文研读：SurroundOcc</h1><div class="post-intros"><div class="post-intro-meta"><span class="post-intro-time"><i class="post-intro-calendar fa fa-calendar"></i><span>09-07-2023 14:14:16</span></span><span class="post-intro-tags"><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="论文研读"> 论文研读</a><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="3D目标检测"> 3D目标检测</a></span></div><div class="post-intro-read"><span> Word count: <span class="post-count">3.1k</span> | Reading time: <span class="post-count">11</span>min</span></div></div></header><article class="cxo-up" id="cxo-content-outer"><section id="cxo-content-inner"><article class="article-entry" id="post"><blockquote>
<p>原创文章，转载、引用请注明出处！</p>
</blockquote>
<hr>
<h1 id="文章信息"><a href="#文章信息" class="headerlink" title="文章信息"></a>文章信息</h1><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/3DOD-SurroundOcc/1.png" title="Optional title"></p>
<blockquote>
<p>SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving<br>ICCV 2023</p>
</blockquote>
<h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>文章认为过往方法存在的缺陷为：</p>
<ul>
<li><p>Although multi-camera 3D object detection plays an im- portant role [29, 28, 33, 21] in vision-based 3D perception, it is easy to suffer from the <strong>long-tail problem</strong> and difficult to recognize all classes of objects in the real world.</p>
</li>
<li><p>However, depth maps only predict the nearest occupied point in each optical ray and are <strong>unable to recover the occluded parts of the 3D scene</strong>.</p>
</li>
</ul>
<blockquote>
<p>在实际的视觉相关任务中，数据都存在长尾分布，少量类别占据了绝大多少样本，大量的类别仅有少量的样本，称为tail部分。<br>介绍部分认为传统方法的缺陷主要在于它们只关注于3D物体检测，而没有提供对3D场景的更全面的感知。深度图仅能预测每条光线中最近的有占据点，无法恢复3D场景中被遮挡的部分。</p>
</blockquote>
<p>本文提出一种<strong>使用多相机图像预测自动驾驶中周围场景的三维占用的方法</strong>。与现有方法专注于三维目标检测不同，该方法旨在提供对三维场景的更全面的感知。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/3DOD-SurroundOcc/2.png" title="Optional title"></p>
<p>该方法首先从每个图像中提取多尺度特征，并使用空间2D-3D注意力将它们提升到三维体积空间。然后，应用三维卷积逐渐上采样体积特征，并在多个级别上施加监督。通过设计一种流程来融合动态物体和静态场景的多帧激光雷达扫描，生成密集的占用地面真实值，从而避免昂贵的注释。</p>
<p>在nuScenes和SemanticKITTI数据集上的实验证明了该方法的优越性。</p>
<p>文章主要贡献：</p>
<ul>
<li><p>利用多帧的LiDAR点云构建稠密occupancy数据集；</p>
</li>
<li><p>设计occupancy预测的网络。</p>
</li>
</ul>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/3DOD-SurroundOcc/3.png" title="Optional title"></p>
<ul>
<li><p>Multi-camera images：给定一组周围的N摄像头图像；</p>
</li>
<li><p>Multi-camera features：使用骨干网络（文中给出的例子是ResNet-101）提取N个摄像头在M个级别的多尺度特征X；</p>
</li>
<li><p>Multi-scale feature aggreation：对于每个级别的特征，通过空间交叉注意力来融合多摄像头特征；</p>
</li>
<li><p>3D volume features：利用3D conv对多尺度体积特征进行上采样（upsample）；</p>
</li>
<li><p>Occupancy prediction：生成最终的预测，每个级别的预测由所生成的Dense Occupancy GD进行监督。</p>
</li>
</ul>
<h2 id="2D-3D-Spatial-Attention"><a href="#2D-3D-Spatial-Attention" class="headerlink" title="2D-3D Spatial Attention"></a>2D-3D Spatial Attention</h2><p>许多3D场景重建方法把多视角的2D特征集成（重新投影）到已知姿态的3D空间中。网格特征是通过简单地将该网格中所有2D特征求平均来计算得到的。这类方法假设不同的视角对于3D体积的贡献是相等的，并不符合实际，尤其在some views are occluded or blurred的情况下更不合理。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/3DOD-SurroundOcc/4.png" title="Optional title"></p>
<p>2D-3D空间注意力层将三维参考点投影到二维视图中，图2中的cross-view attention。来自deformable attention[71,29]。cross-view attention的输出就是deformable attention对采样特征的加权和。</p>
<p>deformable attention机制的表示：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/3DOD-SurroundOcc/5.png" title="Optional title"></p>
<p>Fp和Qp表示输出特征和3D volume query的第p个元素。qp是查询点的相应三维位置，P 是三维到二维投影函数。Vhit表示3D查询点的被命中视图。该层使用的是只使用三维参考点命中的视图。然后在这些投影的二维位置周围采样二维特征。</p>
<p>cross-view attention是2D-3D的初始层，2D-3D输出是一个3D volume特征。</p>
<blockquote>
<p>[71] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In ICLR, 2020.<br>[29] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong- hao Sima, Tong Lu, Qiao Yu, and Jifeng Dai. Bevformer: Learning bird’s-eye-view representation from multi-camera images via spatiotemporal transformers. In ECCV, 2022.</p>
</blockquote>
<blockquote>
<p>3D-&gt;2D-&gt;3D，第一步由backbone+cross-view attention完成，第二步由3D conv完成。</p>
</blockquote>
<h2 id="Multi-scale-Occupancy-Prediction"><a href="#Multi-scale-Occupancy-Prediction" class="headerlink" title="Multi-scale Occupancy Prediction"></a>Multi-scale Occupancy Prediction</h2><p>构造了一个U-Net结构，使用3D volume的前一个level的feature来构造当前level：给定多尺度的2D特征，采用不同数量的2D-3D空间注意力层来提取多尺度的3D特征，然后使用3D deconv对第j-1级的3D体积特征Yj-1进行上采样，并与Fj进行融合。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/3DOD-SurroundOcc/6.png" title="Optional title"></p>
<p>所有的level都是有监督训练。</p>
<h1 id="Dense-Occupancy-Ground-Truth"><a href="#Dense-Occupancy-Ground-Truth" class="headerlink" title="Dense Occupancy Ground Truth"></a>Dense Occupancy Ground Truth</h1><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/3DOD-SurroundOcc/7.png" title="Optional title"></p>
<p>只由稀疏LiDAR点组成的网络无法预测足够密集的占据情况。因此，有必要生成密集的占据标签。设计了一个流程，利用现有的3D检测和3D语义分割标签来生成密集的占据真值，而不需要额外的人工注释：分别拼接动态物体和静态场景的多帧LiDAR，采用Poisson重建来填补空洞，并对所得到的网格进行体素化，以获得密集的体积占据情况。</p>
<blockquote>
<p>这里提出的motivation是：一种直观的方法是直接将多帧LiDAR点序列转换为统一的坐标系统，然后将串联的密集点进行体素化。然而，这种直接的解决方案只适用于完全静态的场景，忽略了移动物体。此外，多帧点云并不足够密集，仍然存在许多空洞，这将导致错误的占据标签。</p>
</blockquote>
<h2 id="Multi-frame-Point-Cloud-Stitching"><a href="#Multi-frame-Point-Cloud-Stitching" class="headerlink" title="Multi-frame Point Cloud Stitching"></a>Multi-frame Point Cloud Stitching</h2><p>对于每一帧，根据3D边界框标签，先从LiDAR点云中切割出可移动的物体，以便可以得到静态场景和可移动物体的3D点云。将它们的坐标通过已知的校准矩阵和自动驾驶位置转换到世界坐标系中。当前帧的占用标签利用了整个序列的LiDAR点云。</p>
<blockquote>
<p>生成GT的时候用到了过去帧和未来帧，预测的时候仅使用过去帧作为监督。</p>
</blockquote>
<h2 id="Densifying-with-Poisson-Reconstruction"><a href="#Densifying-with-Poisson-Reconstruction" class="headerlink" title="Densifying with Poisson Reconstruction"></a>Densifying with Poisson Reconstruction</h2><p>LiDAR中点的分布不均匀。</p>
<p>先根据局部领域的空间分布计算法向量。然后进行Poisson表面重建：输入是带有法向量的点云，输出是一个三角网格（triangular mesh）。</p>
<p>得到的网格用均匀分布的顶点V填充了点云的空洞，这样可以进一步将网格转换为稠密体素Vd。</p>
<h2 id="Semantic-Labeling-with-NN-Algorithm"><a href="#Semantic-Labeling-with-NN-Algorithm" class="headerlink" title="Semantic Labeling with NN Algorithm"></a>Semantic Labeling with NN Algorithm</h2><p>位置重建只能应用于三维空间，而不能应用于语义空间。提出利用最近邻算法来搜索每个体素的最近语义标签：首先将带有语义的P进行体素化，对于Vd中每个占用的体素，使用NN在Vs中搜索最近的体素，并将语义标签分配给它。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h2><ul>
<li><p>在nuScenes数据集上进行了多摄像机实验，使用训练集来训练模型，并使用验证集进行评估，测试集缺少3D语义和3D检测标签无法用；占据预测范围设置为X、Y轴为[-50m, 50m]，Z轴为[-5m, 3m]。主干网络使用Resnet-101-DCN，权重初始化使用FCOS3D中的内容。2D-3D空间注意力层的数量设置为1、3、6。</p>
</li>
<li><p>在SemanticKITTI数据集上对单目语义场景补全进行了实验。采用预训练的EfficientNetB7作为主干网络。2D-3D空间注意力层的数量设置为1、3、8。</p>
</li>
<li><p>在8个RTX 3090上进行实验。</p>
</li>
</ul>
<h2 id="3D-Semantic-Occupancy-Prediction"><a href="#3D-Semantic-Occupancy-Prediction" class="headerlink" title="3D Semantic Occupancy Prediction"></a>3D Semantic Occupancy Prediction</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/3DOD-SurroundOcc/8.png" title="Optional title"></p>
<p>对于MonoScene [8]，我们将多摄像头占用预测拼接在一起，并使用0.5m的体素大小对其进行体素化，这与我们的设置相同。对于BEVFormer [29]，添加了一个3D分割头来预测语义占用情况。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/3DOD-SurroundOcc/9.png" title="Optional title"></p>
<p>图6展示了雨天和夜晚的可视化结果。LiDAR传感器在雨天中受到影响，很容易丢失点云数据。通过多帧的聚合和泊松重建，我们能够大幅增加点云的密度并提供强有力的监督，这对于这些具有挑战性的场景至关重要。此外，我们使用了颜色抖动增强，增加了对亮度变化的鲁棒性。我们还需要指出的是，部分车辆的背侧是LiDAR传感器无法观测到的。令人惊讶的是，我们的模型可以根据周围的信息预测出完整的物体形状。</p>
<blockquote>
<p>图6左绿色：街边的bicycle<br>图6左橘色：街边的ped<br>图6右红色：路上的车被完整预测</p>
</blockquote>
<p>在SemanticKITTI数据集上进行了单目3D语义场景补全，虽然方法并不是为单目感知设计的，但仍然sota。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/3DOD-SurroundOcc/10.png" title="Optional title"></p>
<blockquote>
<p>在3D自动驾驶中，”monocular perception”指的是使用单眼相机进行感知和理解环境的过程。这种感知方法主要通过分析单幅图像来推断环境的三维信息，而不依赖于额外的深度传感器或多摄像头系统。在monocular perception中，通过对图像中的物体进行分析和识别，以及通过对视觉特征、几何信息和语义信息进行推断，从而实现对周围环境的感知和理解。</p>
</blockquote>
<h2 id="3D-Scene-Reconstruction"><a href="#3D-Scene-Reconstruction" class="headerlink" title="3D Scene Reconstruction"></a>3D Scene Reconstruction</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/3DOD-SurroundOcc/11.png" title="Optional title"></p>
<p>进一步评估了在不使用多类语义标签的情况下的3D重建性能。按照[37, 6]的方法，运行TSDF融合[13, 38]来将多摄像头的深度融合为点云。融合的是相同时间戳的多摄像头深度图而不是多帧深度，因此不需要特别处理可移动物体。仍然sota。</p>
<h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/3DOD-SurroundOcc/12.png" title="Optional title"></p>
<p>三个消融实验分别从spatial attention、multi-scale structure+multi-scale supervision（这里说的就是U-Net结构的整体设计）以及occupancy labels的构建进行验证，证明文中所有设置都是合理的。</p>
<h2 id="效率"><a href="#效率" class="headerlink" title="效率"></a>效率</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/3DOD-SurroundOcc/13.png" title="Optional title"></p>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>主页：weiyithu.github.io/SurroundOcc/</p>
<p>文章：arxiv.org/pdf/2303.09551</p>
<p>code：<a target="_blank" rel="noopener" href="https://github.com/weiyithu/SurroundOcc">https://github.com/weiyithu/SurroundOcc</a></p>
<p>data：<a target="_blank" rel="noopener" href="https://cloud.tsinghua.edu.cn/d/8dcb547238144d08a0bb/">https://cloud.tsinghua.edu.cn/d/8dcb547238144d08a0bb/</a></p>
<p>作者在github的issue中提到的一些关键信息：</p>
<ul>
<li>[issues 61] Q：Monoscene（table1、table3内其中一个对比模型）在nuscene和semantickitti上的表现差异巨大。A：对于monoscene，我们将每个视角的结果都投影到三维voxel空间，每个voxel最终的label是这个voxel中所有点label的众数。在实验中我们发现，由于每个视角的pattern不同，monoscene不能很好地同时处理多个视角的结果，这可能是效果较差的原因。以及多视角occupancy结果并不容易直接通过后处理的方式fuse</li>
<li>[issues 59] Q：Occ真值的生成过程十分缓慢。A：使用Intel(R) Xeon(R) Gold 6145 CPU，RTX 2080 ti GPU，大概十四天。开了5个进程，多开进程来进行加速，每个进程负责不同的序列。如果觉得太慢的话，lidar点云本身就较为稠密的话，可以不用泊松重建，即去掉–to_mesh,会加速很多。或者试试对整个场景而不是逐帧进行泊松重建，即–whole_scene_to_mesh，也会加速。</li>
<li>[issues 2] Q：ask about the total training time on 8 3090s. A：It will take about 2.5 days, which is similar to BEVFormer’s training time.</li>
<li>[issues 15] 提供了一份training log文件。</li>
</ul>
<h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>从github大家的评价、星标数和在公众号上作者自己写的简介上可以推定这是一篇比较优秀的文章。看完之后觉得最亮眼的还是数据生成这一步，文中提供的网络+数据生成方法全部都sota。且个人认为不管对比实验的结果如何，这样安排内容也很合理很有说服力。但是问题61中提到了某一对比实验用他们给的数据生成方法会有较大的误差，这也许是黑点，自造GT确实比较难弄。</p>
<p>因为是头一次精读自动驾驶+3D目标检测，仍有疑问的地方：</p>
<ul>
<li>2D-3D Spatial Attention中的deformable attention，叙述方式是qkv这一套东西，但是文中有一句却说使用的不是代价高的Transformer，而是3D conv？deformable attention被引的两个文章也许还需要读。</li>
<li>使用泊松重建空洞+NN为空洞分配新标签这一套做法是做数据的常见套路还是新思路？如果是常见套路，那么是否有其他组合方式？</li>
<li>文中提到了对比模型为了适应新的数据生成方式而做的改动，需要通过代码进一步了解。</li>
<li>代码是真的难，从整体结构而言比之前的仅构造model的代码复杂得多，还得熟悉。</li>
</ul>
</article><!-- lincense--><div class="post-paginator"><a class="nextSlogan" href="/2023/07/04/%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97%EF%BC%9A2023-bjtu-%E8%AE%A1%E9%99%A2-cs%EF%BC%88%E4%BA%8C%EF%BC%89/" title="博士申请指南：2023+bjtu+计院+cs（二）"><span>NextPost ></span><br><span class="nextTitle">博士申请指南：2023+bjtu+计院+cs（二）</span></a><div class="clear"></div></div><div id="comment"></div></section></article><footer id="cxo-footer-outer"><div id="cxo-footer-inner"><p class="footer-container"><span>Site by </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span> | theme </span><a target="_blank" rel="noopener" href="https://github.com/Longlongyu/hexo-theme-Cxo"><span>Cxo</span></a></p><i class="fa fa-user"> </i><span id="busuanzi_value_site_uv"></span><span> | </span><i class="fa fa-eye"> </i><span id="busuanzi_value_site_pv"></span></div></footer><!-- catelog--><div class="toc-wrapper" style="top: 70vh;"><div class="toc-catalog"><i class="fa fa-list"> </i><span>CATALOG</span></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%87%E7%AB%A0%E4%BF%A1%E6%81%AF"><span class="toc-number">1.</span> <span class="toc-text">文章信息</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AB%8B%E6%84%8F"><span class="toc-number">2.</span> <span class="toc-text">立意</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84"><span class="toc-number">3.1.</span> <span class="toc-text">整体架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2D-3D-Spatial-Attention"><span class="toc-number">3.2.</span> <span class="toc-text">2D-3D Spatial Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-scale-Occupancy-Prediction"><span class="toc-number">3.3.</span> <span class="toc-text">Multi-scale Occupancy Prediction</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Dense-Occupancy-Ground-Truth"><span class="toc-number">4.</span> <span class="toc-text">Dense Occupancy Ground Truth</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-frame-Point-Cloud-Stitching"><span class="toc-number">4.1.</span> <span class="toc-text">Multi-frame Point Cloud Stitching</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Densifying-with-Poisson-Reconstruction"><span class="toc-number">4.2.</span> <span class="toc-text">Densifying with Poisson Reconstruction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Semantic-Labeling-with-NN-Algorithm"><span class="toc-number">4.3.</span> <span class="toc-text">Semantic Labeling with NN Algorithm</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">5.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%86%E8%8A%82"><span class="toc-number">5.1.</span> <span class="toc-text">实验细节</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3D-Semantic-Occupancy-Prediction"><span class="toc-number">5.2.</span> <span class="toc-text">3D Semantic Occupancy Prediction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3D-Scene-Reconstruction"><span class="toc-number">5.3.</span> <span class="toc-text">3D Scene Reconstruction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="toc-number">5.4.</span> <span class="toc-text">消融实验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%88%E7%8E%87"><span class="toc-number">5.5.</span> <span class="toc-text">效率</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">6.</span> <span class="toc-text">其他</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%84%9F%E6%83%B3"><span class="toc-number">7.</span> <span class="toc-text">感想</span></a></li></ol></div><!-- top--><i class="fa fa-arrow-up close" id="go-up" aria-hidden="true"></i></body></html>