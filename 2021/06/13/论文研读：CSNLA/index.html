<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><meta name="author" content="Fy J"><meta name="renderer" content="webkit"><meta name="copyright" content="Fy J"><meta name="keywords" content="MoyangSensei"><meta name="description" content="null"><meta name="Cache-Control" content="no-cache"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>论文研读：CSNLA · MoYang</title><link rel="stylesheet" href="/css/style.css?v=2018.7.9"><link rel="stylesheet" href="/css/animation.css?v=2018.7.9"><link rel="icon" href="/img/assets/Moyangico.ico"><link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.6"><!-- scripts--><script>(function( w ){
  "use strict";
  // rel=preload support test
  if( !w.loadCSS ){
    w.loadCSS = function(){};
  }
  // define on the loadCSS obj
  var rp = loadCSS.relpreload = {};
  // rel=preload feature support test
  // runs once and returns a function for compat purposes
  rp.support = (function(){
    var ret;
    try {
      ret = w.document.createElement( "link" ).relList.supports( "preload" );
    } catch (e) {
      ret = false;
    }
    return function(){
      return ret;
    };
  })();

  // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
  // then change that media back to its intended value on load
  rp.bindMediaToggle = function( link ){
    // remember existing media attr for ultimate state, or default to 'all'
    var finalMedia = link.media || "all";

    function enableStylesheet(){
      link.media = finalMedia;
    }

    // bind load handlers to enable media
    if( link.addEventListener ){
      link.addEventListener( "load", enableStylesheet );
    } else if( link.attachEvent ){
      link.attachEvent( "onload", enableStylesheet );
    }

    // Set rel and non-applicable media type to start an async request
    // note: timeout allows this to happen async to let rendering continue in IE
    setTimeout(function(){
      link.rel = "stylesheet";
      link.media = "only x";
    });
    // also enable media after 3 seconds,
    // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
    setTimeout( enableStylesheet, 3000 );
  };

  // loop through link elements in DOM
  rp.poly = function(){
    // double check this to prevent external calls from running
    if( rp.support() ){
      return;
    }
    var links = w.document.getElementsByTagName( "link" );
    for( var i = 0; i < links.length; i++ ){
      var link = links[ i ];
      // qualify links to those with rel=preload and as=style attrs
      if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
        // prevent rerunning on link
        link.setAttribute( "data-loadcss", true );
        // bind listeners to toggle media back
        rp.bindMediaToggle( link );
      }
    }
  };

  // if unsupported, run the polyfill
  if( !rp.support() ){
    // run once at least
    rp.poly();

    // rerun poly on an interval until onload
    var run = w.setInterval( rp.poly, 500 );
    if( w.addEventListener ){
      w.addEventListener( "load", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    } else if( w.attachEvent ){
      w.attachEvent( "onload", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    }
  }


  // commonjs
  if( typeof exports !== "undefined" ){
    exports.loadCSS = loadCSS;
  }
  else {
    w.loadCSS = loadCSS;
  }
}( typeof global !== "undefined" ? global : this ) );</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" defer></script><script src="/js/main.js?v=2018.7.9" defer></script><!-- fancybox--><link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script><!-- busuanzi--><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="MoyangSensei" type="application/atom+xml">
</head><body><section class="profile-close" id="cxo-profile"><div class="profile-avatar"><i class="fa fa-caret-left"></i><img src="/img/assets/cat.png"></div><!--.profile-saying
  i.fa.fa-comment
  .saying--><div class="cxo-profile-inner"><div class="profile-name">Fy J</div><div class="profile-signature">CS专业扫雷学深造学者互联网冲浪一级选手</div><div class="friends"><div>FRIENDS</div><span><a href="//hnjia00.github.io" target="_black">jhn</a></span></div><div class="read-progress"></div></div></section><header id="cxo-intro" style="height: 70vh;background-image: url(/img/intro/index-bg.png);"><nav id="cxo-intro-nav"><section><div class="intro-nav-title"><a href="/">MoYang's Blog</a></div><div class="intro-nav-label-box"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div><i class="fa fa-bars intro-nav-menu"><div class="intro-nav-drop"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div></i><div class="clear"></div></section></nav><h1 class="post-title">论文研读：CSNLA</h1><div class="post-intros"><div class="post-intro-meta"><span class="post-intro-time"><i class="post-intro-calendar fa fa-calendar"></i><span>06-13-2021 15:46:31</span></span><span class="post-intro-tags"><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="论文研读"> 论文研读</a><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="图像超分辨率"> 图像超分辨率</a></span></div><div class="post-intro-read"><span> Word count: <span class="post-count">1.1k</span> | Reading time: <span class="post-count">4</span>min</span></div></div></header><article class="cxo-up" id="cxo-content-outer"><section id="cxo-content-inner"><article class="article-entry" id="post"><blockquote>
<p>原创文章，转载、引用请注明出处！</p>
</blockquote>
<hr>
<blockquote>
<p>Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining</p>
</blockquote>
<blockquote>
<p>CVPR 2020</p>
</blockquote>
<h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>文章指出，传统SISR方法着眼于LR图像，主要集中在局部先验和非局部先验的匹配和重建上。特别是基于局部先验（双线性、双三次插值）的方法，仅仅通过相邻像素的加权和来重建像素。</p>
<p><strong>自然图像中广泛存在跨尺度的块相似性。</strong></p>
<p>基于上述两点，<strong>为了突破局部限制，考虑基于非局部均值滤波的方法：即整个LR图像上全局搜索相似的块。</strong>也就是说，除了非局部像素对像素的匹配外，像素还可以与较大的图像块进行匹配。</p>
<p><strong>对于SISR任务来说，由于自然的跨尺度特征对应关系，可以直接从LR图像中搜索高频细节。</strong></p>
<p>论文主要有以下三点贡献：</p>
<ul>
<li><p>提出了用于SISR任务的第一个跨尺度非局部（CS-NL）注意力模块，计算图像内部的像素到块以及块到块的相似性。</p>
</li>
<li><p>提出了一个强大的SEM单元，在单元内部，通过结合<code>局部</code>、<code>尺度内非局部</code>和<code>跨尺度非局部</code>特征相关性，尽可能挖掘更多的的先验信息。</p>
</li>
<li><p>该网络在多个图像基准数据集上达到了最佳性能。</p>
</li>
</ul>
<h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/1.png" title="Optional title"></p>
<h2 id="In-Scale-Non-Local-IS-NL-Attention"><a href="#In-Scale-Non-Local-IS-NL-Attention" class="headerlink" title="In-Scale Non-Local (IS-NL) Attention"></a>In-Scale Non-Local (IS-NL) Attention</h2><p>非局部注意可以通过从整体图像中总结相关特征来探索自我样本。形式上，给定图像特征映射X，非局部注意定义为</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/2.png" title="Optional title"></p>
<p>其中，<code>(i，j)、(g，h)和(u，v)</code>是X的坐标对，<code>ψ(·)</code>是特征变换函数，<code>φ(·,·)</code> 是用来衡量相似性的相关函数，定义为</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/3.png" title="Optional title"></p>
<p>其中，<code>θ(·)、δ(·)</code>是特征变换。</p>
<h2 id="Cross-Scale-Non-Local-CS-NL-Attention"><a href="#Cross-Scale-Non-Local-CS-NL-Attention" class="headerlink" title="Cross-Scale Non-Local (CS-NL) Attention"></a>Cross-Scale Non-Local (CS-NL) Attention</h2><p><strong>CS-NL注意是建立在IS-NL注意的基础上的，主要做法是：在根据比例<code>s</code>下采样过的图像中寻找特征Y=X中的候选特征。下采样操作是双线性插值。</strong></p>
<p>这样做的原因是，由于空间维度的差异，使用公共相似性度量直接将像素与patch匹配是不可行的。因此，只需对特征进行降采样，将patch表示为像素，并测量其距离（affinity）。</p>
<blockquote>
<p>The reason to do so is because directly matching pixels with patches using common similarity measurement is infeasible due to spatial dimension difference. So we simply downsample the features to represent the patch as pixel and measure the affinity.</p>
</blockquote>
<p>IS-NL公式扩展到CS-NL版本：设缩放比例为<code>s</code>为了计算像素和patch的相似性，首先需要从<code>X(w×h)</code>到<code>Y(w/s×h/s)</code>进行下采样，找到X和Y的pixel-wise相似性，最后使用相应的<code>s×s patch</code>重建X，输出是<code>Z(sh×sh)</code>。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/4.png" title="Optional title"></p>
<p>CS-NL attention module的流程：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/5.png" title="Optional title"></p>
<h2 id="Patch-Based-Cross-Scale-Non-Local-Attention"><a href="#Patch-Based-Cross-Scale-Non-Local-Attention" class="headerlink" title="Patch-Based Cross-Scale Non-Local Attention"></a>Patch-Based Cross-Scale Non-Local Attention</h2><p>CS-NL attention 的距离（affinity）度量可能存在问题。</p>
<blockquote>
<p>First, high-level features are robust to transformations and distortions, that is rotated/distorted low-level patches may yield same high-level features.<br>Besides, adjacent target regions are generated in a non-overlapping fashion, possibly creating discontinuous region boundaries artifacts.</p>
</blockquote>
<p>给出新的公式：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/6.png" title="Optional title"></p>
<p>p×p给出了非1的patch大小，当p=1的时候就回归了CS-NL。</p>
<blockquote>
<p>解释原理是上一个公式，代码里用的是这一个公式。</p>
</blockquote>
<h2 id="Self-Exemplars-Mining-SEM-Cell"><a href="#Self-Exemplars-Mining-SEM-Cell" class="headerlink" title="Self-Exemplars Mining (SEM) Cell"></a>Self-Exemplars Mining (SEM) Cell</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/7.png" title="Optional title"></p>
<h3 id="Multi-Branch-Exemplars"><a href="#Multi-Branch-Exemplars" class="headerlink" title="Multi-Branch Exemplars"></a>Multi-Branch Exemplars</h3><p>Local Branch不对Li-1做任何操作，直接搬过来；Cross-Scale NL Attention前面提过；In-Scale NL Attention计算的是特征图内像素间的非局部相似性。</p>
<h3 id="Mutual-Projected-Fusion"><a href="#Mutual-Projected-Fusion" class="headerlink" title="Mutual-Projected Fusion"></a>Mutual-Projected Fusion</h3><p>将三部分的特征进行融合：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/8.png" title="Optional title"></p>
<blockquote>
<p>式(5)-(8)</p>
</blockquote>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><ul>
<li><p>训练集：DIV2K（800 images）</p>
</li>
<li><p>测试集：Set5, Set14, B100, Urban100, and Manga109</p>
</li>
<li><p>SEM：12</p>
</li>
<li><p>p = 3</p>
</li>
</ul>
<h2 id="定量试验"><a href="#定量试验" class="headerlink" title="定量试验"></a>定量试验</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/9.png" title="Optional title"></p>
<h2 id="定性试验"><a href="#定性试验" class="headerlink" title="定性试验"></a>定性试验</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/10.png" title="Optional title"></p>
<h2 id="branch效果对比"><a href="#branch效果对比" class="headerlink" title="branch效果对比"></a>branch效果对比</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/11.png" title="Optional title"></p>
<h2 id="branch融合方式对比"><a href="#branch融合方式对比" class="headerlink" title="branch融合方式对比"></a>branch融合方式对比</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/12.png" title="Optional title"></p>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul>
<li><p>文章：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2006.01424.pdf">https://arxiv.org/pdf/2006.01424.pdf</a></p>
</li>
<li><p>code：<a target="_blank" rel="noopener" href="https://github.com/SHI-Labs/Cross-ScaleNon-Local-Attention">https://github.com/SHI-Labs/Cross-ScaleNon-Local-Attention</a></p>
</li>
</ul>
<h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>从文章的结果来看，用相似块来做SR的这个思路是非常成功的，也符合自然的想法，合理是非常合理的总之。</p>
<p>实验做的很细，但感觉少了一个非常重要的内容，就是SEM的数量问题。p的大小是做了实验的，上面没放。</p>
<p>最终的对比结果里，用了注意力机制的都是效果不错的，还是要多关注Attention。</p>
</article><!-- lincense--><div class="post-paginator"><a class="prevSlogan" href="/2021/06/21/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AIGNN/" title="论文研读：IGNN"><span>< PreviousPost</span><br><span class="prevTitle">论文研读：IGNN</span></a><a class="nextSlogan" href="/2021/05/28/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AAdderNet:AdderSR/" title="论文研读：AdderNet/AdderSR"><span>NextPost ></span><br><span class="nextTitle">论文研读：AdderNet/AdderSR</span></a><div class="clear"></div></div><div id="comment"></div></section></article><footer id="cxo-footer-outer"><div id="cxo-footer-inner"><p class="footer-container"><span>Site by </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span> | theme </span><a target="_blank" rel="noopener" href="https://github.com/Longlongyu/hexo-theme-Cxo"><span>Cxo</span></a></p><i class="fa fa-user"> </i><span id="busuanzi_value_site_uv"></span><span> | </span><i class="fa fa-eye"> </i><span id="busuanzi_value_site_pv"></span></div></footer><!-- catelog--><div class="toc-wrapper" style="top: 70vh;"><div class="toc-catalog"><i class="fa fa-list"> </i><span>CATALOG</span></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AB%8B%E6%84%8F"><span class="toc-number">1.</span> <span class="toc-text">立意</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text">模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#In-Scale-Non-Local-IS-NL-Attention"><span class="toc-number">2.1.</span> <span class="toc-text">In-Scale Non-Local (IS-NL) Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cross-Scale-Non-Local-CS-NL-Attention"><span class="toc-number">2.2.</span> <span class="toc-text">Cross-Scale Non-Local (CS-NL) Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Patch-Based-Cross-Scale-Non-Local-Attention"><span class="toc-number">2.3.</span> <span class="toc-text">Patch-Based Cross-Scale Non-Local Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Self-Exemplars-Mining-SEM-Cell"><span class="toc-number">2.4.</span> <span class="toc-text">Self-Exemplars Mining (SEM) Cell</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-Branch-Exemplars"><span class="toc-number">2.4.1.</span> <span class="toc-text">Multi-Branch Exemplars</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mutual-Projected-Fusion"><span class="toc-number">2.4.2.</span> <span class="toc-text">Mutual-Projected Fusion</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">3.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E8%AE%BE%E7%BD%AE"><span class="toc-number">3.1.</span> <span class="toc-text">实验设置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E9%87%8F%E8%AF%95%E9%AA%8C"><span class="toc-number">3.2.</span> <span class="toc-text">定量试验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E6%80%A7%E8%AF%95%E9%AA%8C"><span class="toc-number">3.3.</span> <span class="toc-text">定性试验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#branch%E6%95%88%E6%9E%9C%E5%AF%B9%E6%AF%94"><span class="toc-number">3.4.</span> <span class="toc-text">branch效果对比</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#branch%E8%9E%8D%E5%90%88%E6%96%B9%E5%BC%8F%E5%AF%B9%E6%AF%94"><span class="toc-number">3.5.</span> <span class="toc-text">branch融合方式对比</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">4.</span> <span class="toc-text">其他</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%84%9F%E6%83%B3"><span class="toc-number">5.</span> <span class="toc-text">感想</span></a></li></ol></div><!-- top--><i class="fa fa-arrow-up close" id="go-up" aria-hidden="true"></i></body></html>