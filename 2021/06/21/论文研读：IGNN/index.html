<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><meta name="author" content="Fy J"><meta name="renderer" content="webkit"><meta name="copyright" content="Fy J"><meta name="keywords" content="MoyangSensei"><meta name="description" content="null"><meta name="Cache-Control" content="no-cache"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>论文研读：IGNN · MoYang</title><link rel="stylesheet" href="/css/style.css?v=2018.7.9"><link rel="stylesheet" href="/css/animation.css?v=2018.7.9"><link rel="icon" href="/img/assets/Moyangico.ico"><link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.6"><!-- scripts--><script>(function( w ){
  "use strict";
  // rel=preload support test
  if( !w.loadCSS ){
    w.loadCSS = function(){};
  }
  // define on the loadCSS obj
  var rp = loadCSS.relpreload = {};
  // rel=preload feature support test
  // runs once and returns a function for compat purposes
  rp.support = (function(){
    var ret;
    try {
      ret = w.document.createElement( "link" ).relList.supports( "preload" );
    } catch (e) {
      ret = false;
    }
    return function(){
      return ret;
    };
  })();

  // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
  // then change that media back to its intended value on load
  rp.bindMediaToggle = function( link ){
    // remember existing media attr for ultimate state, or default to 'all'
    var finalMedia = link.media || "all";

    function enableStylesheet(){
      link.media = finalMedia;
    }

    // bind load handlers to enable media
    if( link.addEventListener ){
      link.addEventListener( "load", enableStylesheet );
    } else if( link.attachEvent ){
      link.attachEvent( "onload", enableStylesheet );
    }

    // Set rel and non-applicable media type to start an async request
    // note: timeout allows this to happen async to let rendering continue in IE
    setTimeout(function(){
      link.rel = "stylesheet";
      link.media = "only x";
    });
    // also enable media after 3 seconds,
    // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
    setTimeout( enableStylesheet, 3000 );
  };

  // loop through link elements in DOM
  rp.poly = function(){
    // double check this to prevent external calls from running
    if( rp.support() ){
      return;
    }
    var links = w.document.getElementsByTagName( "link" );
    for( var i = 0; i < links.length; i++ ){
      var link = links[ i ];
      // qualify links to those with rel=preload and as=style attrs
      if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
        // prevent rerunning on link
        link.setAttribute( "data-loadcss", true );
        // bind listeners to toggle media back
        rp.bindMediaToggle( link );
      }
    }
  };

  // if unsupported, run the polyfill
  if( !rp.support() ){
    // run once at least
    rp.poly();

    // rerun poly on an interval until onload
    var run = w.setInterval( rp.poly, 500 );
    if( w.addEventListener ){
      w.addEventListener( "load", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    } else if( w.attachEvent ){
      w.attachEvent( "onload", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    }
  }


  // commonjs
  if( typeof exports !== "undefined" ){
    exports.loadCSS = loadCSS;
  }
  else {
    w.loadCSS = loadCSS;
  }
}( typeof global !== "undefined" ? global : this ) );</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" defer></script><script src="/js/main.js?v=2018.7.9" defer></script><!-- fancybox--><link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script><!-- busuanzi--><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="MoyangSensei" type="application/atom+xml">
</head><body><section class="profile-close" id="cxo-profile"><div class="profile-avatar"><i class="fa fa-caret-left"></i><img src="/img/assets/cat.png"></div><!--.profile-saying
  i.fa.fa-comment
  .saying--><div class="cxo-profile-inner"><div class="profile-name">Fy J</div><div class="profile-signature">CS专业扫雷学深造学者互联网冲浪一级选手</div><div class="friends"><div>FRIENDS</div><span><a href="//hnjia00.github.io" target="_black">jhn</a></span></div><div class="read-progress"></div></div></section><header id="cxo-intro" style="height: 70vh;background-image: url(/img/intro/index-bg.png);"><nav id="cxo-intro-nav"><section><div class="intro-nav-title"><a href="/">MoYang's Blog</a></div><div class="intro-nav-label-box"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div><i class="fa fa-bars intro-nav-menu"><div class="intro-nav-drop"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div></i><div class="clear"></div></section></nav><h1 class="post-title">论文研读：IGNN</h1><div class="post-intros"><div class="post-intro-meta"><span class="post-intro-time"><i class="post-intro-calendar fa fa-calendar"></i><span>06-21-2021 16:40:41</span></span><span class="post-intro-tags"><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="论文研读"> 论文研读</a><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="图像超分辨率"> 图像超分辨率</a></span></div><div class="post-intro-read"><span> Word count: <span class="post-count">1.4k</span> | Reading time: <span class="post-count">5</span>min</span></div></div></header><article class="cxo-up" id="cxo-content-outer"><section id="cxo-content-inner"><article class="article-entry" id="post"><blockquote>
<p>原创文章，转载、引用请注明出处！</p>
</blockquote>
<hr>
<blockquote>
<p>Cross-Scale Internal Graph Neural Network for Image Super-Resolution</p>
</blockquote>
<blockquote>
<p>Nips 2020</p>
</blockquote>
<h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>和CS-NL那篇相似，都是利用一张图片中的相似patch做超分。</p>
<p>解决了两个问题：</p>
<ul>
<li><p>怎么准确地找到这些相似的patch？</p>
</li>
<li><p>怎么合理地融合这些patch？</p>
</li>
</ul>
<h1 id="Non-local"><a href="#Non-local" class="headerlink" title="Non-local"></a>Non-local</h1><h2 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h2><p>Non-local在图像重建中用的很多。许多经典的方法如非局部均值和BM3D，将相似的patch聚集起来进行图像去噪。</p>
<p>相似patch聚集的过程可表述为：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-IGNN/1.png" title="Optional title"></p>
<p>其中，<code>Xi</code>和<code>Yi</code>是第i个位置（聚集中心）的输入和输出patch；<code>Xj</code>是第i个位置的相邻特征patch集<code>Si</code>中包含的第j个邻居；<code>Q(·)</code>将输入X变换到另一个特征空间；<code>C(·,·)</code>根据Xj和Xi的相关性得到的权重，越相关权重越大；<code>δi(X)</code>给前面的和项做归一化。</p>
<p><strong>将patch和权重看作图结构的顶点和边，那么上面的过程可以看作是一个GNN。</strong></p>
<blockquote>
<p>The above aggregation can be treated as a GNN if we treat the feature patches and weighted connections as vertices and edges respectively. The non-local neural networks actually model a fully-connected self-similarity graph. They estimate the aggregation weights between the query item Xi and all the spatially nearby patches Xj in a d × d window (or within the whole features). To reduce the memory and computational costs introduced by the above dense connection, some k-nearest neighbor based networks, e.g., GCDN and N3Net, only consider k (k ≪ d2) most similar feature patches for aggregation and treat them as the neighbors in Si for every query Xi. For all the above mentioned non-local methods, the aggregated neighboring patches are all in the same scale of the query and no HR information is incorporated, thus leading to a limited performance improvement for SISR.</p>
</blockquote>
<h2 id="一般化图像是否泛用？"><a href="#一般化图像是否泛用？" class="headerlink" title="一般化图像是否泛用？"></a>一般化图像是否泛用？</h2><p>论文中给出的图片是比较特殊的一类。实际拍摄图像中往往不会特别多重复的图像块（一栋大楼上都是一模一样的窗户，但拍大楼的图片只是极狭窄的题材）。</p>
<p><strong>真实算法中考虑的图像块非常小，往往只是很小的重复纹理repetitive texture、边edge或角corner，所以Non-local patch复现的性质对于任何一般图片都是成立的。</strong></p>
<blockquote>
<p>Zontak and Irani, Internal statistics of a single natural image<br>CVPR 2011</p>
</blockquote>
<h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-IGNN/2.png" title="Optional title"></p>
<h2 id="Graph-Construction"><a href="#Graph-Construction" class="headerlink" title="Graph Construction"></a>Graph Construction</h2><p>这部分的过程如下：</p>
<ul>
<li><p>对LR下采样（指向左边的黑色箭头）：先使用双三次将输入的LR图像<code>IL</code>下采样，倍率为<code>s</code>，表示为<code>IL↓s</code>。s的取值所需的上采样的倍率。且文章指出对于上采样，s=2比s=4好。</p>
</li>
<li><p>特征提取：用VGG19对<code>IL</code>和<code>IL↓s</code>进行特征提取得到对应特征图。</p>
</li>
<li><p>查找相似patch：对于<code>EL</code>中要搜索的一个patch，<strong>在下采样特征图<code>EL↓s</code>中以MSE作为metric，搜索到k个相似patch，然后按位置对应回EL（图中红色虚线Vertex Mapping）</strong>，这样对于一个搜索patch就得到了k个EL中的相似patch。</p>
</li>
</ul>
<p>要搜索的patch和得到的k个相似patch是图的节点，搜索patch和相似patch的差是图的边，逐个patch进行搜索完成，图就构建完毕了。</p>
<p>根据对图片的统计验证，图像的相似patch很多。<strong>也就是说Graph Construction这部分如果按照最朴素的想法来做的话，计算量会很大。</strong></p>
<p><strong>设定k和s就是为了保证模型速度。</strong></p>
<ul>
<li><p>s：选定在IL下采样s倍的特征图上做搜索，而不是IL本身对应的特征图中去搜索；</p>
</li>
<li><p>k：只选k个相似的patch，减少后面聚合的计算量。</p>
</li>
</ul>
<h2 id="Patch-Aggregation"><a href="#Patch-Aggregation" class="headerlink" title="Patch Aggregation"></a>Patch Aggregation</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-IGNN/3.png" title="Optional title"></p>
<p>这部分是对前面获得的相似patch信息的应用。本质和一开始列的那个non-local聚合公式是一样的，都是加权，只不过形式复杂一点：这里是对于要搜索patch的对应k个相似patch进行加权，权重就是前面说到的图结构的边，将边值送至网络的输出取对数作为权。</p>
<h2 id="跳转连接"><a href="#跳转连接" class="headerlink" title="跳转连接"></a>跳转连接</h2><p>通过跨不同规模的跳转连接，聚合HR功能中的丰富HR信息↑s在网络中直接从中间位置传递到后面位置。这种机制允许HR信息帮助网络生成更详细的输出。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><ul>
<li><p>骨干网络：为了证明GraphAgg模块的有效性，选择了<code>EDSR</code>作为骨干网络，它包含32个ResBlock。所提出的GraphAgg模块在<code>IGNN中只使用一次</code>，并且<code>插入在第16个ResBlock之后</code>。</p>
</li>
<li><p>图构造：使用VGG19的前三层和固定的预训练参数来嵌入图像IL和IL↓s到EL和EL↓s。</p>
</li>
<li><p>训练集：800 high-quality (2K resolution) images from DIV2K dataset</p>
</li>
<li><p>测试集：Set5, Set14, BSD100, Urban100 and Manga109 in three upscaling factors: ×2, ×3 and ×4. </p>
</li>
</ul>
<h2 id="定量实验"><a href="#定量实验" class="headerlink" title="定量实验"></a>定量实验</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-IGNN/4.png" title="Optional title"></p>
<h2 id="定性试验"><a href="#定性试验" class="headerlink" title="定性试验"></a>定性试验</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-IGNN/5.png" title="Optional title"></p>
<h2 id="window-size-d-和-neighbor-k-的取值"><a href="#window-size-d-和-neighbor-k-的取值" class="headerlink" title="window size d 和 neighbor k 的取值"></a>window size d 和 neighbor k 的取值</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-IGNN/6.png" title="Optional title"></p>
<blockquote>
<p>windos size 实验中固定为60 × 60。</p>
</blockquote>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul>
<li><p>文章：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2006.16673.pdf">https://arxiv.org/pdf/2006.16673.pdf</a></p>
</li>
<li><p>code：<a target="_blank" rel="noopener" href="https://github.com/sczhou/IGNN">https://github.com/sczhou/IGNN</a></p>
</li>
</ul>
<h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>看这篇是因为跟CS-NL那篇的思想相似，想做一对儿文章来讲。但实际上看的顺序反了，应该先看这篇。</p>
<p>IGNN这篇对于我来说，最大的好处就是<strong>解释清楚了Non-local的前世今生</strong>。这一点CS-NL没太解释也没太重视。</p>
<p>思路方面的评价是一样的，Non-local这个思路的有效性之前就说过。<strong>这类通过对LR-HR exemplars的挖掘，可以在一定程度上缓解超分任务中ill-posed的问题。</strong></p>
<p>以及还是比较在意效率问题，这点文章没提，估计也是效率方面差，扬长避短了。虽然在Graph Construction那边做了限制，但以图结构的复杂度，还是平方级别的计算量增长。这一点好像避不开，那么后面的优化就需要多做一做了。</p>
<p>结构方面，这一篇做的就不如CS-NL的解释性好了。</p>
</article><!-- lincense--><div class="post-paginator"><a class="nextSlogan" href="/2021/06/13/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ACSNLA/" title="论文研读：CSNLA"><span>NextPost ></span><br><span class="nextTitle">论文研读：CSNLA</span></a><div class="clear"></div></div><div id="comment"></div></section></article><footer id="cxo-footer-outer"><div id="cxo-footer-inner"><p class="footer-container"><span>Site by </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span> | theme </span><a target="_blank" rel="noopener" href="https://github.com/Longlongyu/hexo-theme-Cxo"><span>Cxo</span></a></p><i class="fa fa-user"> </i><span id="busuanzi_value_site_uv"></span><span> | </span><i class="fa fa-eye"> </i><span id="busuanzi_value_site_pv"></span></div></footer><!-- catelog--><div class="toc-wrapper" style="top: 70vh;"><div class="toc-catalog"><i class="fa fa-list"> </i><span>CATALOG</span></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AB%8B%E6%84%8F"><span class="toc-number">1.</span> <span class="toc-text">立意</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Non-local"><span class="toc-number">2.</span> <span class="toc-text">Non-local</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%9D%E6%83%B3"><span class="toc-number">2.1.</span> <span class="toc-text">思想</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E8%88%AC%E5%8C%96%E5%9B%BE%E5%83%8F%E6%98%AF%E5%90%A6%E6%B3%9B%E7%94%A8%EF%BC%9F"><span class="toc-number">2.2.</span> <span class="toc-text">一般化图像是否泛用？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.</span> <span class="toc-text">模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Graph-Construction"><span class="toc-number">3.1.</span> <span class="toc-text">Graph Construction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Patch-Aggregation"><span class="toc-number">3.2.</span> <span class="toc-text">Patch Aggregation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B7%B3%E8%BD%AC%E8%BF%9E%E6%8E%A5"><span class="toc-number">3.3.</span> <span class="toc-text">跳转连接</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">4.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E8%AE%BE%E7%BD%AE"><span class="toc-number">4.1.</span> <span class="toc-text">实验设置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E9%87%8F%E5%AE%9E%E9%AA%8C"><span class="toc-number">4.2.</span> <span class="toc-text">定量实验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E6%80%A7%E8%AF%95%E9%AA%8C"><span class="toc-number">4.3.</span> <span class="toc-text">定性试验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#window-size-d-%E5%92%8C-neighbor-k-%E7%9A%84%E5%8F%96%E5%80%BC"><span class="toc-number">4.4.</span> <span class="toc-text">window size d 和 neighbor k 的取值</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">5.</span> <span class="toc-text">其他</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%84%9F%E6%83%B3"><span class="toc-number">6.</span> <span class="toc-text">感想</span></a></li></ol></div><!-- top--><i class="fa fa-arrow-up close" id="go-up" aria-hidden="true"></i></body></html>