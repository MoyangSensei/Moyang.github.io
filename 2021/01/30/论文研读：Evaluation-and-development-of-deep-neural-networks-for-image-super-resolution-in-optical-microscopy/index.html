<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><meta name="author" content="Fy J"><meta name="renderer" content="webkit"><meta name="copyright" content="Fy J"><meta name="keywords" content="MoyangSensei"><meta name="description" content="null"><meta name="Cache-Control" content="no-cache"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>论文研读：Evaluation and development of deep neural networks for image super-resolution in optical microscopy · MoYang</title><link rel="stylesheet" href="/css/style.css?v=2018.7.9"><link rel="stylesheet" href="/css/animation.css?v=2018.7.9"><link rel="icon" href="/img/assets/Moyangico.ico"><link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.6"><!-- scripts--><script>(function( w ){
  "use strict";
  // rel=preload support test
  if( !w.loadCSS ){
    w.loadCSS = function(){};
  }
  // define on the loadCSS obj
  var rp = loadCSS.relpreload = {};
  // rel=preload feature support test
  // runs once and returns a function for compat purposes
  rp.support = (function(){
    var ret;
    try {
      ret = w.document.createElement( "link" ).relList.supports( "preload" );
    } catch (e) {
      ret = false;
    }
    return function(){
      return ret;
    };
  })();

  // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
  // then change that media back to its intended value on load
  rp.bindMediaToggle = function( link ){
    // remember existing media attr for ultimate state, or default to 'all'
    var finalMedia = link.media || "all";

    function enableStylesheet(){
      link.media = finalMedia;
    }

    // bind load handlers to enable media
    if( link.addEventListener ){
      link.addEventListener( "load", enableStylesheet );
    } else if( link.attachEvent ){
      link.attachEvent( "onload", enableStylesheet );
    }

    // Set rel and non-applicable media type to start an async request
    // note: timeout allows this to happen async to let rendering continue in IE
    setTimeout(function(){
      link.rel = "stylesheet";
      link.media = "only x";
    });
    // also enable media after 3 seconds,
    // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
    setTimeout( enableStylesheet, 3000 );
  };

  // loop through link elements in DOM
  rp.poly = function(){
    // double check this to prevent external calls from running
    if( rp.support() ){
      return;
    }
    var links = w.document.getElementsByTagName( "link" );
    for( var i = 0; i < links.length; i++ ){
      var link = links[ i ];
      // qualify links to those with rel=preload and as=style attrs
      if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
        // prevent rerunning on link
        link.setAttribute( "data-loadcss", true );
        // bind listeners to toggle media back
        rp.bindMediaToggle( link );
      }
    }
  };

  // if unsupported, run the polyfill
  if( !rp.support() ){
    // run once at least
    rp.poly();

    // rerun poly on an interval until onload
    var run = w.setInterval( rp.poly, 500 );
    if( w.addEventListener ){
      w.addEventListener( "load", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    } else if( w.attachEvent ){
      w.attachEvent( "onload", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    }
  }


  // commonjs
  if( typeof exports !== "undefined" ){
    exports.loadCSS = loadCSS;
  }
  else {
    w.loadCSS = loadCSS;
  }
}( typeof global !== "undefined" ? global : this ) );</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" defer></script><script src="/js/main.js?v=2018.7.9" defer></script><!-- fancybox--><link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script><!-- busuanzi--><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="MoyangSensei" type="application/atom+xml">
</head><body><section class="profile-close" id="cxo-profile"><div class="profile-avatar"><i class="fa fa-caret-left"></i><img src="/img/assets/cat.png"></div><!--.profile-saying
  i.fa.fa-comment
  .saying--><div class="cxo-profile-inner"><div class="profile-name">Fy J</div><div class="profile-signature">CS专业扫雷学深造学者互联网冲浪一级选手</div><div class="friends"><div>FRIENDS</div><span><a href="//hnjia00.github.io" target="_black">jhn</a></span></div><div class="read-progress"></div></div></section><header id="cxo-intro" style="height: 70vh;background-image: url(/img/intro/index-bg.png);"><nav id="cxo-intro-nav"><section><div class="intro-nav-title"><a href="/">MoYang's Blog</a></div><div class="intro-nav-label-box"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div><i class="fa fa-bars intro-nav-menu"><div class="intro-nav-drop"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div></i><div class="clear"></div></section></nav><h1 class="post-title">论文研读：Evaluation and development of deep neural networks for image super-resolution in optical microscopy</h1><div class="post-intros"><div class="post-intro-meta"><span class="post-intro-time"><i class="post-intro-calendar fa fa-calendar"></i><span>01-30-2021 19:35:46</span></span><span class="post-intro-tags"><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="论文研读"> 论文研读</a><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="图像超分辨率"> 图像超分辨率</a></span></div><div class="post-intro-read"><span> Word count: <span class="post-count">5.8k</span> | Reading time: <span class="post-count">21</span>min</span></div></div></header><article class="cxo-up" id="cxo-content-outer"><section id="cxo-content-inner"><article class="article-entry" id="post"><blockquote>
<p>原创文章，转载、引用请注明出处！</p>
</blockquote>
<blockquote>
<p>2021，加油。</p>
</blockquote>
<hr>
<blockquote>
<p>2021.1.21 Nature</p>
</blockquote>
<h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>文章做了三项工作：</p>
<ul>
<li><p>数据集BioSR：近年来，基于深度学习的SISR模型被应用于提高科学显微镜图像的分辨率。与SISR在增强宏观真实照片纹理方面的应用相比，用于科学分析的超分辨率显微图像对推断出的纳米尺度结构提出了更高的精度和可量化性要求。然而，基于深度学习的超分辨率（DLSR）图像所传递的信息在多大程度上可以用于定量分析，以及在什么条件下DLSR方法优于传统的超分辨率显微镜，仍然是个未知数。在这里，使用自制的多模结构照明显微镜（SIM）系统，该系统集成了全内反射荧光（TIRF-SIM）、掠入射（GI-SIM）和非线性SIM（方法），在输入LR图像的信噪比（SNR）水平的宽范围内获得匹配良好的LR–SR图像对，观察到的生物结构的复杂性和期望的放大因子。这个数据集被命名为BioSR。</p>
</li>
<li><p>DFCAN及其衍生的生成对抗训练策略DFGAN：DLSR网络的训练可以看作是一个提取高维特征的过程，这些特征连接了LR和SR图像空间。众所周知，输入LR图像的功率谱被限制在衍射限制频率以下，因此推测，利用傅里叶域中不同特征的频率内容差异，而不是空间域中的结构差异，可能使DLSR网络能够学习高频信息的分层表示更加精确和有效。受深度剩余通道注意网络（RCAN）中空间域通道注意机制的启发，开发了DFCAN及其派生的生成性对抗网络（GAN）训练策略，称为DFGAN。</p>
</li>
<li><p>证明DFCAN的Fourier域聚焦能够在低信噪比条件下实现SIM图像的鲁棒重建。在多色活体细胞成像实验中，证明df可以在10倍的时间内获得与SIM相当的图像质量，揭示了线粒体嵴和类核细胞的详细结构以及细胞器和细胞骨架的相互作用动力学。</p>
</li>
</ul>
<blockquote>
<p>Deep neural networks have enabled astonishing transformations from low-resolution (LR) to super-resolved images. However, whether, and under what imaging conditions, such deep-learning models outperform super-resolution (SR) microscopy is poorly explored. Here, using multimodality structured illumination microscopy (SIM), we first provide an extensive dataset of LR–SR image pairs and evaluate the deep-learning SR models in terms of structural complexity, signal-to-noise ratio and upscaling fac- tor. Second, we devise the deep Fourier channel attention network (DFCAN), which leverages the frequency content difference across distinct features to learn precise hierarchical representations of high-frequency information about diverse biological structures. Third, we show that DFCAN’s Fourier domain focalization enables robust reconstruction of SIM images under low signal-to-noise ratio conditions. We demonstrate that DFCAN achieves comparable image quality to SIM over a tenfold longer duration in multicolor live-cell imaging experiments, which reveal the detailed structures of mitochondrial cristae and nucleoids and the interaction dynamics of organelles and cytoskeleton.</p>
</blockquote>
<h1 id="BioSR"><a href="#BioSR" class="headerlink" title="BioSR"></a>BioSR</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>使用multimodality structured illumination microscopy（SIM，多模结构照明显微镜），获取了如下类型的数据集，这些数据集代表了结构复杂性的增加：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/1.jpg" title="Optional title"></p>
<ul>
<li><p>clathrin-coated pits（CCPs，网格蛋白包被坑/网格蛋白小窝）</p>
</li>
<li><p>endoplasmic reticulum（ER，内质网）</p>
</li>
<li><p>microtubules（MTs，微管）</p>
</li>
<li><p>F-actin filaments（F-肌动蛋白丝）</p>
</li>
</ul>
<p>对于每种类型的样本，在10个不断上升的激发光强度（excitation light intensity）水平上获得了大约50组原始SIM图像。且在最高激发水平下，确保所有原始图像的SNR（signal-to-noise ratio，信噪比）足够高，以重建高质量的SIM图像。</p>
<p>每一组原始SIM图像被平均化为WF（diffraction-limited wide-field，衍射限制宽场）图像。</p>
<p><strong>WF图像用作DLSR网络的输入LR图像，而SIM图像用作评估DLSR方法在特定成像条件下是否优于传统SR显微镜的参考。</strong></p>
<blockquote>
<p>WF图像是如何做出来的？</p>
</blockquote>
<blockquote>
<p>SIM的结构和工作原理在Method部分给出，比较专业。</p>
<blockquote>
<p>将405 nm（LBX-405-300，Oxxius）、488 nm（Genesis MX SLM，相干）和560 nm（2RU-VFL-P-500-560，MPB通信）的三束激光组合在一起，然后通过声光可调谐滤波器（AOTF，AOTFnC-400.650，AA Quanta Tech）。在特定的成像模式下，AOTF可以灵活地控制所需激光束的曝光顺序和曝光时间。<br>将9幅TIRF-SIM或GI-SIM原始图像和25幅非线性SIM原始图像分别重建为分辨率提高2倍和3倍的SR图像。为了减小中低荧光强度下SIM图像的重建伪影，采用截止频率与重建光学传递函数（OTF）边界匹配的高斯切趾函数（Gaussian apodization function）来抑制高频噪声。</p>
</blockquote>
</blockquote>
<h2 id="采集数据细节"><a href="#采集数据细节" class="headerlink" title="采集数据细节"></a>采集数据细节</h2><p>对于每种类型的标本和每种成像方式，从至少50个不同的ROI（regions-of-interest，感兴趣区域）获取原始数据集，其中35个ROI的数据集用于训练，而其他15个ROI的数据集用于生成评估矩阵。</p>
<blockquote>
<p>数据分布。</p>
</blockquote>
<p>对于每个感兴趣区，获得了9组N phase × M orientation的原始图像，曝光时间不变，但激发光强度增加，其中N和M分别为TIRF-SIM和GI-SIM的3组和非线性SIM的5组。</p>
<p>将每组N×M原始图像平均为衍射受限的WF图像，然后对其进行掩模（masked）以计算其每像素的平均光子计数。</p>
<p>将不同荧光水平的原始SIM图像和WF图像作为DLSR网络的输入LR图像。同时，将每组N×M的原始图像重建成与相应WF图像具有相同荧光水平的SIM图像，作为评价该荧光水平下DLSR图像质量的参考。</p>
<blockquote>
<p>DLSR的输入和输出。</p>
</blockquote>
<p>此外，在相同的ROI中，最终提高了激发强度和曝光时间（通常为120W 平方cm 10ms）以达到平均光子数&gt;1200的高荧光水平，并独立获得三组N×M原始图像。将得到的三幅超高信噪比的SIM图像平均为GT-SIM图像，保证了图像的高质量。</p>
<blockquote>
<p>在最高激发水平下，确保所有原始图像的SNR（signal-to-noise ratio，信噪比）足够高，以重建高质量的SIM图像。</p>
</blockquote>
<p>在转染后16-36小时将细胞固定以获得CCPs、MTs和F-肌动蛋白的数据。然而，发现目前的化学固定方法导致的内质网标记蛋白calnexin明显聚集，这显著改变了内质网的形态，因此从活细胞（live cells）中获得了内质网数据。</p>
<blockquote>
<p>内质网使用了live cells获取数据。</p>
</blockquote>
<p>仅使用肌动蛋白细胞骨架结构评估了3倍放大的DLSR成像性能。</p>
<blockquote>
<p>只有F-actin filaments做了3×，其他都是2×。</p>
</blockquote>
<h2 id="在BioSR上评估几种典型DLSR方法"><a href="#在BioSR上评估几种典型DLSR方法" class="headerlink" title="在BioSR上评估几种典型DLSR方法"></a>在BioSR上评估几种典型DLSR方法</h2><p>选择了四个具有代表性的DLSR模型，其中包括：<strong>SRCNN、EDSR、Pix2Pix、CM（cross-modality，跨模态）GAN。</strong></p>
<h3 id="不同荧光程度图像下SR方法的效果"><a href="#不同荧光程度图像下SR方法的效果" class="headerlink" title="不同荧光程度图像下SR方法的效果"></a>不同荧光程度图像下SR方法的效果</h3><p>对于所有的图像，文章以“荧光程度（fluorescence）”作为区分标准，将图像（或者一张图像中的不同部分）分为低荧光、中荧光和高荧光三种。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/2.png" title="Optional title"></p>
<blockquote>
<p>图1的a、b，分别是高、低荧光水平下的DLSR结果。</p>
</blockquote>
<p>文章计算了SISR图像和GT（SIM）的像素级绝对差（MAE，平均绝对误差）：<strong>中等或相对较高的荧光信号电平（这是活细胞SIM成像的典型值）足以允许常规SIM通常优于当前的DLSR方法。</strong>相比之下，在低荧光的成像条件下，SISR图像显示出比传统SIM图像更少的残余差异，因为数据驱动的DLSR方法通常有利于从生物结构中分离噪声。</p>
<blockquote>
<p>这里的意思就是说，在常规的活细胞SIM成像情况下，中等荧光水平以上的成像条件得到的图像已经足够用了，DLSR方法做出的结果并不特别接近GT。</p>
</blockquote>
<p>然而，如果期望的上缩放因子增加到3×，则SISR图像（图1a、b的底行）将包含太多错误或伪影，使得人们无法信任所推断的精细结构。</p>
<blockquote>
<p>3×的DLSR的结果会更差。</p>
</blockquote>
<h3 id="不同SR方法的性能"><a href="#不同SR方法的性能" class="headerlink" title="不同SR方法的性能"></a>不同SR方法的性能</h3><p>为了定量评估不同SR方法的性能，综合了标准化均方根误差（NRMSE）、多尺度结构相似性指数（MS-SSIM）和分辨率三个指标，以测量评估矩阵中每个成像条件下SR图像的质量。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/3.png" title="Optional title"></p>
<p>由于科学研究通常需要以牺牲荧光光子为代价的高真实性和可量化的SR图像，因此首先在评估矩阵的空间中确定了常规SIM的适用区域（图1c中用黑色虚线勾勒），其中常规SIM图像的三个矩阵与之接近GT。常规SIM的适用范围主要是中、高荧光区，符合活体细胞SIM成像实验的实际情况。</p>
<p>此外，利用这三个指标来评估四个DLSR模型，根据评估结果，确定了使用DLSR模型的优先区域，其中SISR图像的三种方法与传统SIM图像的方法相当或更好（图1c和补充说明4中以绿色列出）。显然，优先级区域越大，DLSR模型的性能越好。</p>
<p>然而，<strong>所有四种DLSR模型的优先区域都相对较小，并且集中在低荧光和低结构复杂性的区域，这些区域很少与常规SIM的适用区域重叠（图1c）</strong>。这些数据表明，硬件SR显微镜，例如SIM，比最新的DLSR模型更有效地利用增加的荧光来产生高保真SR信息，这可能阻碍DLSR模型在实际实验中的广泛应用。</p>
<h1 id="DFCAN和DFGAN"><a href="#DFCAN和DFGAN" class="headerlink" title="DFCAN和DFGAN"></a>DFCAN和DFGAN</h1><p>在学习了上述DLSR模型的特性之后，设计了DFCAN和DFGAN。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/4.png" title="Optional title"></p>
<h2 id="DFCAN"><a href="#DFCAN" class="headerlink" title="DFCAN"></a>DFCAN</h2><p>从卷积层和GELU开始。GELU的输出后面是四个相同的RG（residual groups，残差组），每个RG由四个FCAB（Fourier channel attention blocks傅立叶通道注意块）和一个跳跃连接组成。RG的操作表示为如下。x表示RG的输入特征映射。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/5.png" title="Optional title"></p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/6.png" title="Optional title"></p>
<p>在每个FCAB中，特征图按如下方式按通道重新缩放：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/7.png" title="Optional title"></p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/8.png" title="Optional title"></p>
<p>FFT（·）表示快速傅里叶变换，γ用于增强图像质量。</p>
<p>平均池化层，使得φ（y）的每个分量都可以解释为每个特征图的代表值。</p>
<p>WD和WU分别是下向和上向权值，这两种权值均由网络中的1×1卷积层实现。</p>
<p>f（·）和δ（·）分别是sigmoid激活函数和ReLU激活函数。它们共同产生了一种选通机制，可以自适应地计算最终的重缩放因子。</p>
<p>最后一个RG的输出被送入由GELU激活函数激活的卷积层。然后使用像素混洗层（Pixel Shuffle）、卷积层和sigmoid激活层将图像放大到与GT图像相同的大小（以适应推断的高频信息）。</p>
<p>输出为单色灰度SR图像。</p>
<p><strong>该网络的损失函数定义为MSE损耗和SSIM损耗的组合。</strong></p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/9.png" title="Optional title"></p>
<p>其中，Y^定义为DFCAN的输出，Y定义为相应的GT，（w，h）定义为输出图像的像素大小。<strong>λ是用于平衡SSIM和MSE相对贡献的标量权重，文中大多数情况下设置为0.1。</strong></p>
<p><strong>MSE损失保证了像素级的精度，均衡了预测的动态范围，SSIM损失增强了输出的结构相似性。</strong></p>
<h2 id="DFGAN"><a href="#DFGAN" class="headerlink" title="DFGAN"></a>DFGAN</h2><p>DFGAN是基于cGAN（条件GAN）框架构建的。</p>
<p>在DFGAN中，更深层次的DFCAN充当G，G以低分辨率荧光图像作为输入，其输出是放大的SR图像。D基于传统的CNN架构，由12个卷积层组成，每个卷积层的输出由leaky因子α=0.1的LeakyReLU激活函数激活。</p>
<p>分别定义了G和D的损失函数。G的损失函数L_G/D由两项组成：SR误差，用于惩罚G输出和GT图像之间的差异；判别误差，与D计算的概率有关。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/10.png" title="Optional title"></p>
<p>其中X是输入的低分辨率图像，Y是SR目标图像。β、γ和λ是用来平衡相应项的标量加权因子，根据经验将其设置为β=0.1，γ=1，λ=0.1。</p>
<blockquote>
<p>最终结果对上面三个加权因子不是很敏感。</p>
</blockquote>
<p>D的损失函数定义为二元交叉熵：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/11.png" title="Optional title"></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="FCA和SCA"><a href="#FCA和SCA" class="headerlink" title="FCA和SCA"></a>FCA和SCA</h3><p>DFCAN和DFGAN的设计利用了<strong>Fourier域中不同特征映射的功率谱特性。</strong></p>
<blockquote>
<p>何为“Fourier域中不同特征映射的功率谱特性”？</p>
</blockquote>
<p>在每个残差块中，FCA（Fourier channel attention，傅立叶信道注意）机制（图2a）使得网络能够根据其功率谱中包含的所有频率分量的综合贡献自适应地重新缩放每个特征图。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/12.png" title="Optional title"></p>
<p>相比之下，SCA（spatial channel attention，空间通道注意）机制仅利用特征映射的平均强度（相当于零频率（即直流分量））来计算重缩放因子。</p>
<blockquote>
<p>FCA的初衷是什么？凭空想出来的吗？还是有借鉴有曾今的论文做过这类理论？<br>为什么要和SCA进行比较？</p>
</blockquote>
<h4 id="FCA和SCA的性能比较"><a href="#FCA和SCA的性能比较" class="headerlink" title="FCA和SCA的性能比较"></a>FCA和SCA的性能比较</h4><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/13.png" title="Optional title"></p>
<p>为了比较FCA机制与SCA的性能，将DFCAN中的FCA替换为SCA，从而形成了深空间通道注意网络（DSCAN）。此外，通过去除DFCAN中的FCA，构造了一个改进的ResNet进行比较。</p>
<p><strong>结果显示，DFCAN比其他两个网络实现更快的收敛和更低的验证NRMSE。</strong></p>
<h4 id="FCA是否可以应用于其他网络结构？"><a href="#FCA是否可以应用于其他网络结构？" class="headerlink" title="FCA是否可以应用于其他网络结构？"></a>FCA是否可以应用于其他网络结构？</h4><p>为了进一步验证FCA机制是否可以普遍应用于其他类型的神经网络结构，在两个广泛使用的网络上实现了FCA：U-net18和DenseNet19。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/14.png" title="Optional title"></p>
<p>用三个下采样块和三个上采样块构造了U网络，并将两个机制引入到U网络中，生成的网络分别命名为U-net-FCA和U-net-SCA。</p>
<p>DenseNet由三个dense blocks和两个过渡层交织而成。每个dense blocks由8个密集连接的ReLU-Conv模块组成，每个过渡层由一个ReLU激活层和1×1卷积层组成。在每个dense blocks的末尾实现了FCA和SCA模块，分别生成DenseNet FCA和DenseNet SCA网络。</p>
<p>所有这些基于U-net或DenseNet的模型都是用管状结构的模拟数据进行训练的。<strong>结果显示，基于FCA的模型比基于SCA的相应模型更精确地推断出交错管状结构的精细结构。</strong></p>
<p>上述两部分实验的数据结果：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/15.png" title="Optional title"></p>
<h3 id="基于NRMSE、MS-SSIM和分辨率的DLSR模型性能评估"><a href="#基于NRMSE、MS-SSIM和分辨率的DLSR模型性能评估" class="headerlink" title="基于NRMSE、MS-SSIM和分辨率的DLSR模型性能评估"></a>基于NRMSE、MS-SSIM和分辨率的DLSR模型性能评估</h3><p>为了清楚地说明所有被评估的DLSR模型的性能差异，将它们分为两类：非GAN和GAN基模型。</p>
<p>对于每种类型的样品，分别绘制了同一类别模型的NRMSE、MS-SSIM和分辨率作为荧光强度的函数。<strong>结果表明，无论是DFCAN还是DFGAN的NRMSE都比同类的DLSR模型小。且在NRMSE和MS-SSIM指标方面，只有达到相对较高的荧光强度，传统的SIM才能超过DFCAN和DFGAN。</strong></p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/16.png" title="Optional title"></p>
<blockquote>
<p>这里只放了NRMSE，其他指标的图示在补充图内。</p>
</blockquote>
<p>因此，相对于其他DLSR模型，DFCAN和DFGAN都提供了扩大的优先级区域。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/17.png" title="Optional title"></p>
<blockquote>
<p>这里是与前面的一张评估矩阵进行相比，绿色部分的面积扩大了。</p>
</blockquote>
<h3 id="DFCAN-DFGAN在SISR的2倍放大活细胞成像任务表现"><a href="#DFCAN-DFGAN在SISR的2倍放大活细胞成像任务表现" class="headerlink" title="DFCAN/DFGAN在SISR的2倍放大活细胞成像任务表现"></a>DFCAN/DFGAN在SISR的2倍放大活细胞成像任务表现</h3><p><strong>线粒体是高度动态的细胞器，线粒体动力学对于维持线粒体功能和细胞质量控制非常重要。</strong>然而，实现线粒体动力学的长时移SR成像仍然具有挑战性，当前SR成像技术需要高光照强度或长曝光时间来获取多个原始图像，这些原始图像容易引起线粒体的光毒性。</p>
<p>由于DFCAN采集单个WF图像所需的荧光比传统SR方法少得多，DFCAN-SISR允许长延时SR活细胞成像超过1000帧，与传统SIM或STED显微镜相比，成像持续时间延长了约10倍。</p>
<blockquote>
<p>生物相关，这部分实验与线粒体的内部结构有关。实验不放了，看不懂。</p>
</blockquote>
<h3 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h3><blockquote>
<p>直接从原文摘过来的。</p>
</blockquote>
<ul>
<li><p>图像预处理：对于每种类型的标本和每种成像方式，总共获得了约50组WF（512×512像素）和GT-SIM（1024×1024像素）或GT-NLSIM（1536×1536像素）图像。每一组都有9个不断上升的荧光水平。为了生成训练数据集，选取了35组原始数据（补充表5），采用随机裁剪、水平/垂直翻转和旋转变换等方法进一步丰富训练数据集，最终生成了20000对WF（128×128像素）和GT-SIM（256×256像素）图像，即2，每个荧光水平200对。对于每种类型的DLSR网络，用属于同一类型样品的所有荧光水平的数据训练一个专用模型。为了生成测试数据集，将剩余的15组数据扩充为WF（256×256像素）和GT-SIM（512×512像素）数据集，然后根据平均光子计数（即特定细胞的表达水平）将这些配对图像分为25到600的16个荧光水平。每一个荧光水平都保证有100多个图像。</p>
</li>
<li><p>训练：在一台计算机工作站上进行的，该工作站配备了3.20 GHz的Xeon（R）Gold 6134 CPU（Intel）和两块RTX 2080Ti图形处理卡（NVIDIA），其中包含python v.3.6、Tensorflow v.1.11.0和Keras v.2.2.4。在训练过程中，根据网络规模使用Adam优化器和2到6个批量大小。以CCPs的训练过程为例，对SRCNN、EDSR和DFCAN的非GAN方法，随机初始化网络，以1×10−4的典型起始学习率训练模型。<strong>最后的SRCNN、EDSR、DFCAN和RCAN模型分别经过约70000、150000、200000和500000次小批量迭代训练，时间分别为10、120、24和120h。对于基于GAN的Pix2Pix、CMGAN和DFGAN方法，还随机初始化了网络，训练了具有典型启动的生成模型和判别模型学习率分别为2×10−5和1×10−4。最后的Pix2Pix、CMGAN和DFGAN模型分别进行了约50000、90000和80000次小批量迭代，每次迭代时间分别为20、120和80h。在不同网络的训练过程期间的验证NRMSE的代表图示于补充图13中。通过迁移学习和混合精度训练可以缩短训练时间。一旦网络被训练，所有这些DLSR模型通常需要不到1s的时间来重建1024×1024像素的SR图像。</strong></p>
</li>
</ul>
<h3 id="统计和复现性"><a href="#统计和复现性" class="headerlink" title="统计和复现性"></a>统计和复现性</h3><blockquote>
<p>直接从原文摘过来的。</p>
</blockquote>
<p><strong>每个DLSR模型用相同的训练数据和超参数独立训练三次，然后采用NRMSE最低的模型进行进一步的评估。</strong></p>
<p>1a、b、3a、5a–d和扩展数据图2重复了120个测试图像，用于每种类型的样本和放大因子，都获得了相似的结果。</p>
<p>评估矩阵或曲线中的所有数据均来自100多幅图像的测试。</p>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><ul>
<li><p>BioSR，它由广泛的LR–SR图像对组成，涵盖了广泛的信噪比水平、结构复杂性和放大因子。</p>
</li>
<li><p>非GAN模型更适用于低至中等荧光成像条件，以产生具有良好可量化性的SR图像。然而，如果GAN模型能够提供与传统SIM（例如DFGAN）相当的NRMSE和MS-SSIM，那么它将是首选的DLSR模型，特别是对于高结构复杂性的样本。</p>
</li>
<li><p>无论采用哪种DLSR模型，NRMSE和MS-SSIM对荧光强度的评估函数都很快接近渐近稳定，但即使接近无限荧光，也不能达到理想值。相比之下，随着荧光强度接近GT成像水平，传统SIM图像的度量越来越接近deal值。因此，这一局限性表明了用纯计算方法完全取代SR显微镜的巨大挑战。所以，将深度学习算法巧妙地集成到显微镜硬件开发中的整体设计可能是下一代SR显微镜的一种有前途的方法。</p>
</li>
</ul>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul>
<li><p>文章：<a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41592-020-01048-5#data-availability">https://www.nature.com/articles/s41592-020-01048-5#data-availability</a></p>
</li>
<li><p>code：<a target="_blank" rel="noopener" href="https://github.com/qc17-THU/DL-SR">https://github.com/qc17-THU/DL-SR</a></p>
</li>
<li><p>数据集（BioSR）：<a target="_blank" rel="noopener" href="https://figshare.com/articles/dataset/BioSR/13264793">https://figshare.com/articles/dataset/BioSR/13264793</a></p>
</li>
</ul>
<h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>文章内容太多了，一时半会都不能理解它到底在推什么东西，到底是数据集，还是算法，还是思想，还是硬件。但是非常有借鉴意义，得多读几遍。</p>
<hr>
<h1 id="GELU"><a href="#GELU" class="headerlink" title="GELU"></a>GELU</h1><blockquote>
<p>Gaussian Error Linear Units (GELUs)<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.08415">https://arxiv.org/abs/1606.08415</a></p>
</blockquote>
<p>高斯误差线性单元。</p>
<p>GELU非线性的实现是对神经网络的输入进行随机正则化的变化，为输入匹配一个或0或1的随机值。与ReLU的不同，GELU为其按照输入的magnitude（等级）为inputs加权值的；ReLUs是根据inputs的sign（正负）来gate（加门限）的。</p>
<p>论文实验证明GELU在多项计算机视觉，自然语言处理，语音任务上效果优于ReLU，ELU。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/18.png" title="Optional title"></p>
<h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><h2 id="MAE"><a href="#MAE" class="headerlink" title="MAE"></a>MAE</h2><p>MAE（Mean Square Error,平均绝对误差），所有单个观测值与算术平均值的偏差的绝对值的平均。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/19.png" title="Optional title"></p>
<h2 id="MSE"><a href="#MSE" class="headerlink" title="MSE"></a>MSE</h2><p>见《基础：图像超分辨率》。</p>
<h2 id="PSNR"><a href="#PSNR" class="headerlink" title="PSNR"></a>PSNR</h2><p>见《基础：图像超分辨率》。</p>
<h2 id="SSIM"><a href="#SSIM" class="headerlink" title="SSIM"></a>SSIM</h2><p>见《基础：图像超分辨率》。</p>
<h2 id="RMSE"><a href="#RMSE" class="headerlink" title="RMSE"></a>RMSE</h2><p>RMSE（Root Mean Square Error，均方根误差），观测值与真值偏差的平方和与观测次数m比值的平方根。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/20.png" title="Optional title"></p>
<blockquote>
<p>假如有2000次观测，即m=2000，对于某一次（第i次）观测来说，y值是真实值，而h(x)是观测值，对所有m次观测的的偏差取平方后相加，得到的值再除以m，然后再开根号，就得到RMSE了。</p>
</blockquote>
<p>RMSE对偏差做了一次平方（相比于MAE），这样，如果误差的离散度高，也就是说，如果最大偏差值大的话，RMSE就放大了。</p>
<h2 id="NRMSE"><a href="#NRMSE" class="headerlink" title="NRMSE"></a>NRMSE</h2><p>NRMSE（Normalized Root Mean Square Error，归一化均方根误差）</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/21.png" title="Optional title"></p>
<p>从数值角度，NRMSE就是将RMSE的值变成(0,1)之间。</p>
<h2 id="MS-SSIM"><a href="#MS-SSIM" class="headerlink" title="MS-SSIM"></a>MS-SSIM</h2><p>单尺度SSIM需要在特定的配置下才能表现良好，而MSSIM对不同分辨率的图像都能保持性能稳定。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/22.png" title="Optional title"></p>
<p>MSSIM的流程如上图所示。将参考图像和失真图像作为输入，然后分别依次迭代的使用低通滤波器和1/2降采样。假设原始图像为Scale 1，最高尺度为Scale M经过M-1次迭代得到。对于第j个尺度，只计算对比度c(x,y)和结构相似度s(x,y)。仅在Scale M计算亮度相似度l(x,y)。如上图所示。最终的SSIM是将各个尺度的结果连接起来：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/23.png" title="Optional title"></p>
<p>SSIM不能很好的反应人类真实的视觉感受。</p>
<p>有人认为，这种方法之所以比SSIM好，是因为人类一般不是使用最清晰的图像，而是用低通分辨率处理之后的图像，所以把不同分辨率的图像纳入到评价参数中更符合人的使用习惯。</p>
</article><!-- lincense--><div class="post-paginator"><a class="prevSlogan" href="/2021/04/03/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ADegradation-Model-Learning-for-Real-World-Single-Image-Super-resolution/" title="论文研读：Degradation Model Learning for Real-World Single Image Super-resolution"><span>< PreviousPost</span><br><span class="prevTitle">论文研读：Degradation Model Learning for Real-World Single Image Super-resolution</span></a><a class="nextSlogan" href="/2020/12/23/Docker%EF%BC%9A%E9%85%8D%E7%BD%AE%E3%80%81%E5%9F%BA%E7%A1%80%E3%80%81%E5%BA%94%E7%94%A8%E6%A0%88%E3%80%81%E7%A7%81%E6%9C%89%E4%BA%91/" title="Docker：配置、基础、应用栈、私有云"><span>NextPost ></span><br><span class="nextTitle">Docker：配置、基础、应用栈、私有云</span></a><div class="clear"></div></div><div id="comment"></div></section></article><footer id="cxo-footer-outer"><div id="cxo-footer-inner"><p class="footer-container"><span>Site by </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span> | theme </span><a target="_blank" rel="noopener" href="https://github.com/Longlongyu/hexo-theme-Cxo"><span>Cxo</span></a></p><i class="fa fa-user"> </i><span id="busuanzi_value_site_uv"></span><span> | </span><i class="fa fa-eye"> </i><span id="busuanzi_value_site_pv"></span></div></footer><!-- catelog--><div class="toc-wrapper" style="top: 70vh;"><div class="toc-catalog"><i class="fa fa-list"> </i><span>CATALOG</span></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AB%8B%E6%84%8F"><span class="toc-number">1.</span> <span class="toc-text">立意</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#BioSR"><span class="toc-number">2.</span> <span class="toc-text">BioSR</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-number">2.1.</span> <span class="toc-text">原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%87%E9%9B%86%E6%95%B0%E6%8D%AE%E7%BB%86%E8%8A%82"><span class="toc-number">2.2.</span> <span class="toc-text">采集数据细节</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9C%A8BioSR%E4%B8%8A%E8%AF%84%E4%BC%B0%E5%87%A0%E7%A7%8D%E5%85%B8%E5%9E%8BDLSR%E6%96%B9%E6%B3%95"><span class="toc-number">2.3.</span> <span class="toc-text">在BioSR上评估几种典型DLSR方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E8%8D%A7%E5%85%89%E7%A8%8B%E5%BA%A6%E5%9B%BE%E5%83%8F%E4%B8%8BSR%E6%96%B9%E6%B3%95%E7%9A%84%E6%95%88%E6%9E%9C"><span class="toc-number">2.3.1.</span> <span class="toc-text">不同荧光程度图像下SR方法的效果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E5%90%8CSR%E6%96%B9%E6%B3%95%E7%9A%84%E6%80%A7%E8%83%BD"><span class="toc-number">2.3.2.</span> <span class="toc-text">不同SR方法的性能</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DFCAN%E5%92%8CDFGAN"><span class="toc-number">3.</span> <span class="toc-text">DFCAN和DFGAN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#DFCAN"><span class="toc-number">3.1.</span> <span class="toc-text">DFCAN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DFGAN"><span class="toc-number">3.2.</span> <span class="toc-text">DFGAN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">3.3.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#FCA%E5%92%8CSCA"><span class="toc-number">3.3.1.</span> <span class="toc-text">FCA和SCA</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#FCA%E5%92%8CSCA%E7%9A%84%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83"><span class="toc-number">3.3.1.1.</span> <span class="toc-text">FCA和SCA的性能比较</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#FCA%E6%98%AF%E5%90%A6%E5%8F%AF%E4%BB%A5%E5%BA%94%E7%94%A8%E4%BA%8E%E5%85%B6%E4%BB%96%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%9F"><span class="toc-number">3.3.1.2.</span> <span class="toc-text">FCA是否可以应用于其他网络结构？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8ENRMSE%E3%80%81MS-SSIM%E5%92%8C%E5%88%86%E8%BE%A8%E7%8E%87%E7%9A%84DLSR%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0"><span class="toc-number">3.3.2.</span> <span class="toc-text">基于NRMSE、MS-SSIM和分辨率的DLSR模型性能评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DFCAN-DFGAN%E5%9C%A8SISR%E7%9A%842%E5%80%8D%E6%94%BE%E5%A4%A7%E6%B4%BB%E7%BB%86%E8%83%9E%E6%88%90%E5%83%8F%E4%BB%BB%E5%8A%A1%E8%A1%A8%E7%8E%B0"><span class="toc-number">3.3.3.</span> <span class="toc-text">DFCAN&#x2F;DFGAN在SISR的2倍放大活细胞成像任务表现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%86%E8%8A%82"><span class="toc-number">3.3.4.</span> <span class="toc-text">实验细节</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E5%92%8C%E5%A4%8D%E7%8E%B0%E6%80%A7"><span class="toc-number">3.3.5.</span> <span class="toc-text">统计和复现性</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">4.</span> <span class="toc-text">结论</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">5.</span> <span class="toc-text">其他</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%84%9F%E6%83%B3"><span class="toc-number">6.</span> <span class="toc-text">感想</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#GELU"><span class="toc-number">7.</span> <span class="toc-text">GELU</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-number">8.</span> <span class="toc-text">评价指标</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#MAE"><span class="toc-number">8.1.</span> <span class="toc-text">MAE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MSE"><span class="toc-number">8.2.</span> <span class="toc-text">MSE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PSNR"><span class="toc-number">8.3.</span> <span class="toc-text">PSNR</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SSIM"><span class="toc-number">8.4.</span> <span class="toc-text">SSIM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RMSE"><span class="toc-number">8.5.</span> <span class="toc-text">RMSE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NRMSE"><span class="toc-number">8.6.</span> <span class="toc-text">NRMSE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MS-SSIM"><span class="toc-number">8.7.</span> <span class="toc-text">MS-SSIM</span></a></li></ol></li></ol></div><!-- top--><i class="fa fa-arrow-up close" id="go-up" aria-hidden="true"></i></body></html>