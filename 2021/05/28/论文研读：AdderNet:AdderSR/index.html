<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><meta name="author" content="Fy J"><meta name="renderer" content="webkit"><meta name="copyright" content="Fy J"><meta name="keywords" content="MoyangSensei"><meta name="description" content="null"><meta name="Cache-Control" content="no-cache"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>论文研读：AdderNet/AdderSR · MoYang</title><link rel="stylesheet" href="/css/style.css?v=2018.7.9"><link rel="stylesheet" href="/css/animation.css?v=2018.7.9"><link rel="icon" href="/img/assets/Moyangico.ico"><link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.6"><!-- scripts--><script>(function( w ){
  "use strict";
  // rel=preload support test
  if( !w.loadCSS ){
    w.loadCSS = function(){};
  }
  // define on the loadCSS obj
  var rp = loadCSS.relpreload = {};
  // rel=preload feature support test
  // runs once and returns a function for compat purposes
  rp.support = (function(){
    var ret;
    try {
      ret = w.document.createElement( "link" ).relList.supports( "preload" );
    } catch (e) {
      ret = false;
    }
    return function(){
      return ret;
    };
  })();

  // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
  // then change that media back to its intended value on load
  rp.bindMediaToggle = function( link ){
    // remember existing media attr for ultimate state, or default to 'all'
    var finalMedia = link.media || "all";

    function enableStylesheet(){
      link.media = finalMedia;
    }

    // bind load handlers to enable media
    if( link.addEventListener ){
      link.addEventListener( "load", enableStylesheet );
    } else if( link.attachEvent ){
      link.attachEvent( "onload", enableStylesheet );
    }

    // Set rel and non-applicable media type to start an async request
    // note: timeout allows this to happen async to let rendering continue in IE
    setTimeout(function(){
      link.rel = "stylesheet";
      link.media = "only x";
    });
    // also enable media after 3 seconds,
    // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
    setTimeout( enableStylesheet, 3000 );
  };

  // loop through link elements in DOM
  rp.poly = function(){
    // double check this to prevent external calls from running
    if( rp.support() ){
      return;
    }
    var links = w.document.getElementsByTagName( "link" );
    for( var i = 0; i < links.length; i++ ){
      var link = links[ i ];
      // qualify links to those with rel=preload and as=style attrs
      if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
        // prevent rerunning on link
        link.setAttribute( "data-loadcss", true );
        // bind listeners to toggle media back
        rp.bindMediaToggle( link );
      }
    }
  };

  // if unsupported, run the polyfill
  if( !rp.support() ){
    // run once at least
    rp.poly();

    // rerun poly on an interval until onload
    var run = w.setInterval( rp.poly, 500 );
    if( w.addEventListener ){
      w.addEventListener( "load", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    } else if( w.attachEvent ){
      w.attachEvent( "onload", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    }
  }


  // commonjs
  if( typeof exports !== "undefined" ){
    exports.loadCSS = loadCSS;
  }
  else {
    w.loadCSS = loadCSS;
  }
}( typeof global !== "undefined" ? global : this ) );</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" defer></script><script src="/js/main.js?v=2018.7.9" defer></script><!-- fancybox--><link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script><!-- busuanzi--><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="MoyangSensei" type="application/atom+xml">
</head><body><section class="profile-close" id="cxo-profile"><div class="profile-avatar"><i class="fa fa-caret-left"></i><img src="/img/assets/cat.png"></div><!--.profile-saying
  i.fa.fa-comment
  .saying--><div class="cxo-profile-inner"><div class="profile-name">Fy J</div><div class="profile-signature">CS专业扫雷学深造学者互联网冲浪一级选手</div><div class="friends"><div>FRIENDS</div><span><a href="//hnjia00.github.io" target="_black">jhn</a></span></div><div class="read-progress"></div></div></section><header id="cxo-intro" style="height: 70vh;background-image: url(/img/intro/index-bg.png);"><nav id="cxo-intro-nav"><section><div class="intro-nav-title"><a href="/">MoYang's Blog</a></div><div class="intro-nav-label-box"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div><i class="fa fa-bars intro-nav-menu"><div class="intro-nav-drop"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div></i><div class="clear"></div></section></nav><h1 class="post-title">论文研读：AdderNet/AdderSR</h1><div class="post-intros"><div class="post-intro-meta"><span class="post-intro-time"><i class="post-intro-calendar fa fa-calendar"></i><span>05-28-2021 14:47:01</span></span><span class="post-intro-tags"><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="论文研读"> 论文研读</a><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="图像超分辨率"> 图像超分辨率</a></span></div><div class="post-intro-read"><span> Word count: <span class="post-count">2.9k</span> | Reading time: <span class="post-count">11</span>min</span></div></div></header><article class="cxo-up" id="cxo-content-outer"><section id="cxo-content-inner"><article class="article-entry" id="post"><blockquote>
<p>原创文章，转载、引用请注明出处！</p>
</blockquote>
<hr>
<h1 id="AdderNet"><a href="#AdderNet" class="headerlink" title="AdderNet"></a>AdderNet</h1><blockquote>
<p>AdderNet: Do We Really Need Multiplications in Deep Learning?</p>
</blockquote>
<blockquote>
<p>CVPR 2019</p>
</blockquote>
<h2 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h2><p>在深度学习算法中，卷积操作被广泛的用于度量输入特征和卷积滤波器之间的相似性。</p>
<p>GPU+深度卷积神经网络，大大加快了计算机视觉领域的发展。但GPU有着显而易见的缺陷，主要是物理因素所导致的。第一个是功耗问题，再一个是体积问题。这两个问题导致了现有的多数深度学习方法只能运行在装有大型GPU的机器上。人们希望这些算法能在小巧的移动设备上实现。因此，目的就是降低算法的运行成本，使得在硬件配置不够的情况下也能高效的运行这些深度算法。</p>
<p><strong>在正向推理过程中，深层神经网络的计算大多是浮值权和浮值激活的乘法运算。显然，乘法运算比加法运算要慢。</strong></p>
<p><strong>本文提出的AdderNet，就是在放弃卷积运算的同时，利用l1距离，最大限度地描述和利用加法，逼近卷积操作的精度。为了保证模板的充分更新和网络的收敛性，设计了一种改进的梯度正则化反向传播算法。</strong></p>
<h2 id="数学描述"><a href="#数学描述" class="headerlink" title="数学描述"></a>数学描述</h2><h3 id="Adder-Network"><a href="#Adder-Network" class="headerlink" title="Adder Network"></a>Adder Network</h3><p>对于CNN中的卷积运算，假定输入特征<code>X</code>，filter表示为<code>F</code>，卷积后输出的是二者的相似性度量，表述如下面公式：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/1.png" title="Optional title"></p>
<p>其中，<code>d</code>为kernel size，<code>c_in</code>和<code>c_out</code>分别为输入、输出通道数；卷积核<code>F</code>的大小则为<code>d × d × c_in × c_out</code>；H和W为输入特征的长和宽，输入特征<code>X</code>的大小为<code>H × W × c_in</code>。</p>
<p>以及，<code>S(·,·)</code>是相似性度量，也就是距离度量函数。如果将互相关（cross-correlation）作为距离的度量，即<code>S(x,y)= x × y</code>，这时式（1）成为卷积运算。式（1）还可以表示当d=1时计算完全连接的层。</p>
<p>还有许多其他度量来测量滤波器和输入特征之间的距离，但这些度量大多涉及乘法。</p>
<p>于是，希望在式（1）中用加法来代替乘法。<strong>L1距离仅涉及到两个向量差的绝对值</strong>，不包含乘法。因此，通过计算滤波器和输入特征之间的L1距离，式（1）可以重新表示为：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/2.png" title="Optional title"></p>
<p>不论是使用互相关，还是L1距离，都可以完成相似性度量，但二者的输出结果还是有一些差别的：通过卷积核完成输入特征图谱的加权和计算，结果可正可负；<strong>但adder filter输出的结果恒为负，为此作者引入了batch normalization将结果归一化到一定范围区间内，从而保证传统CNN使用的激活函数在此依旧可以正常使用。</strong></p>
<blockquote>
<p>用式（3）计算的特征都是负数，因此在特征计算后，采用BN层来归一化，之后再用激活函数提高特征非线性。</p>
</blockquote>
<h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><p>神经网络利用<strong>反向传播</strong>来计算滤波器的梯度和<strong>随机梯度下降</strong>来更新参数。</p>
<p>在CNN中，输出特征Y相对于滤波器F的偏导数被计算为：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/3.png" title="Optional title"></p>
<p>在AdderNets中，Y相对于F的偏导数是：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/4.png" title="Optional title"></p>
<blockquote>
<p>sgn(·)是阶跃函数，取-1、0、1。</p>
</blockquote>
<p>式（4）中等号右边的是<code>signSGD</code>，<strong>signSGD被证明的缺陷是几乎不会选择到最陡的方向，而且随着维度增加效果会更差。</strong></p>
<p>因此AdderNet使用如下公式进行梯度更新：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/5.png" title="Optional title"></p>
<p>此外，如果使用全精度梯度的更新方法，由于涉及到前层的梯度值，很容易导致梯度爆炸。这里使用<code>HardTanh</code>将输出限定在[-1,1]范围内：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/6.png" title="Optional title"></p>
<blockquote>
<p>梯度计算也好了，接下来的问题是：加法层的方差会很大。</p>
</blockquote>
<h3 id="Adaptive-Learning-Rate-Scaling"><a href="#Adaptive-Learning-Rate-Scaling" class="headerlink" title="Adaptive Learning Rate Scaling"></a>Adaptive Learning Rate Scaling</h3><p>在传统的CNN中，假设权值和输入特征是独立的，服从正态分布，输出的方差大致可以估计为：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/7.png" title="Optional title"></p>
<p>对于AdderNet，输出的方差可以近似为：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/8.png" title="Optional title"></p>
<p><strong>实际上，权重var[F]的方差通常非常小（10^−3～10^-4）。因此，式（9）中的加法运算会比式（8）带来更大的输出方差。</strong></p>
<blockquote>
<p>式（8）:var[X] × var[F]<br>式（9）:var[X] + var[F]</p>
</blockquote>
<p>AdderNet的输出具有较大方差，在更新时根据常规的链式法则会导致梯度比常规CNN更小，从而导致参数更新过慢。</p>
<p>这里，最容易想到的解决方法就是：使用大的learning rate。这样虽然可以提高学习速度，但是文中又指出，<strong>不同层其权重梯度变化很大</strong>：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/9.png" title="Optional title"></p>
<p><strong>这表明每一层的参数需要不同。</strong>因此作者给<strong>每一个adder layer</strong>设计了不同学习率：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/10.png" title="Optional title"></p>
<p>其中，<code>γ</code>是整个神经网络的全局学习速率，<code>∆L(Fl)</code>是层L中过滤器的梯度，<code>αl</code>是其相应的局部学习率。</p>
<blockquote>
<p>输入是经过BN的，也就是说在AdderNet中，filters的值也是经过了归一化。</p>
</blockquote>
<p>局部学习率<code>αl</code>定义为：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/11.png" title="Optional title"></p>
<p>其中，<code>η</code>是一个控制adder filters学习率的超参数，<code>k</code>是Fl中参数的个数。</p>
<h3 id="Adder-Network的前馈和反传流程"><a href="#Adder-Network的前馈和反传流程" class="headerlink" title="Adder Network的前馈和反传流程"></a>Adder Network的前馈和反传流程</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/12.png" title="Optional title"></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h3><ul>
<li><p>NVIDIA Tesla V100 GPU：2017年中发布、16/32GB（文中没提用的是什么显存的版本）</p>
</li>
<li><p>PyTorch</p>
</li>
</ul>
<h3 id="MINST-LeNet-5-BN-AddNN-CNN"><a href="#MINST-LeNet-5-BN-AddNN-CNN" class="headerlink" title="MINST + LeNet-5-BN + AddNN/CNN"></a>MINST + LeNet-5-BN + AddNN/CNN</h3><p>结果：</p>
<ul>
<li><p>CNN的准确率为99.4%∼435K乘法。</p>
</li>
<li><p><strong>通过将卷积运算中的乘法替换为加法运算，该加法网的准确率达到99.4%∼870K加法，几乎没有乘法。</strong></p>
</li>
</ul>
<p>训练设置：</p>
<ul>
<li>图像大小：调整为32×32；</li>
<li>优化：Nesterov加速梯度法（NAG）；</li>
<li>weight decay/momentum：5×10^−4/0.9；</li>
<li>初始lr为0.1、cosine learning rate decay（余弦学习率衰减）；</li>
<li><strong>用加法器滤波器代替LeNet-5-BN中的卷积滤波器、将全连接层中的乘法替换为减法；</strong></li>
<li>50个epochs的训练。</li>
</ul>
<h3 id="CIFAR-10-100-VGG-ResNet-BNN-AddNN-CNN"><a href="#CIFAR-10-100-VGG-ResNet-BNN-AddNN-CNN" class="headerlink" title="CIFAR-10/100 + VGG/ResNet + BNN/AddNN/CNN"></a>CIFAR-10/100 + VGG/ResNet + BNN/AddNN/CNN</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/13.png" title="Optional title"></p>
<p>结果：</p>
<ul>
<li><p>对于VGG，AddNN与CNNs（CIFAR-10为93.80%，CIFAR-100为72.73%）的结果基本相同（CIFAR-10为93.72%，CIFAR-100为72.64%）；BNN的模型尺寸比AdderNet和CNN小得多，精度也低得多（CIFAR-10为89.80%，CIFAR-100为65.41%）；</p>
</li>
<li><p>对于ResNet-20，CNNs的精确度最高（CIFAR10为92.25%，CIFAR-100为68.14%），但乘法次数较多（41.17M），AddNN少了0.5个点左右（CIFAR-10和CIFAR-100中的无乘法运算精度分别为91.84%和67.60%）。</p>
</li>
</ul>
<blockquote>
<p>由于二进制神经网络BNN可以使用XNOR运算来代替乘法，所以还比较了BNN的结果。</p>
<blockquote>
<p>XNOR：同或。（1 XNOR 1 = 0 XNOR 0 = 1，1 XNOR 0 = 0 XNOR 1 = 0）</p>
</blockquote>
</blockquote>
<p>训练设置：</p>
<ul>
<li>图像大小：32×32；</li>
<li>与何凯明大佬2016年残差那篇相同的DA和处理方法；</li>
<li>初始lr为0.1、后续遵循多项式学习率时间表；</li>
<li>bs=256；</li>
<li>BNN中，将第一层和最后一层设置为全精度卷积层；</li>
<li>400个epochs的训练。</li>
</ul>
<h3 id="ImageNet-ResNet-BNN-AddNN-CNN"><a href="#ImageNet-ResNet-BNN-AddNN-CNN" class="headerlink" title="ImageNet + ResNet + BNN/AddNN/CNN"></a>ImageNet + ResNet + BNN/AddNN/CNN</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/14.png" title="Optional title"></p>
<h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><ul>
<li><p>文章：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.13200.pdf">https://arxiv.org/pdf/1912.13200.pdf</a></p>
</li>
<li><p>code：<a target="_blank" rel="noopener" href="https://github.com/huawei-noah/AdderNet">https://github.com/huawei-noah/AdderNet</a></p>
</li>
</ul>
<h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>首先，文章思路很不错。相似的思路之前也提过，这一篇的内容做的挺多的，不管是数学方面的论证还是实验部分做的都还不错，比较清晰。</p>
<p>以及看了知乎上这篇文章的团队内成员的答复和一些网友的评论，整体的评价也还不错，去年6月份的时候，大部分都还在求硬件加速之类的，官方也说会给出预训练模型，接下来的一篇文章也是继承了这个工作，说明这个工作持续的在做。这是比较nice的。</p>
<p>缺陷也有，最主要的一点就是<strong>为什么最后的实验不去对比GPU下的速度？</strong>显得有点文不对题，故意隐瞒，不是特别的convincing。以及实验主要做的是图像分类的问题，但也没提修改了卷积操作之后，在别的领域的应用，也没特别说明只是针对图像分类问题做的。</p>
<p>最后，有一点没看懂，就是做的是简化网络，从GPU转移到CPU，但是最终部署的时候用的还是GPU，且没给CPU的主频信息等，包括文章中没有特别提到的硬件方面的东西，包括cuda编程等。这些都是没说清楚的点，也许不重要？但是这要不说清楚，代码都不怎么看得懂，哪有心思跑它。</p>
<hr>
<h1 id="AdderSR"><a href="#AdderSR" class="headerlink" title="AdderSR"></a>AdderSR</h1><blockquote>
<p>AdderSR: Towards Energy Efficient Image Super-Resolution</p>
</blockquote>
<blockquote>
<p>CVPR 2021</p>
</blockquote>
<h2 id="立意-1"><a href="#立意-1" class="headerlink" title="立意"></a>立意</h2><p>AdderNet用于SISR问题中会出现两个问题：</p>
<ul>
<li><p>加法器不能很容易的学习身份映射（identity mapping）；</p>
</li>
<li><p>加法器不能实现高通滤波。</p>
</li>
</ul>
<blockquote>
<p>Specifically, the adder operation cannot easily learn the identity mapping, which is essential for image processing tasks. In addition, the functionality of high-pass filters cannot be ensured by AdderNets.</p>
</blockquote>
<p>换言之，在SISR中，AdderNet需要保证两个重要特性：<strong>各卷积层输入输出特征的相似性和高频信息的细节增强</strong>。</p>
<blockquote>
<p>In SISR, there are two important properties that should be ensured by adder neural networks: the similarity between the input and output features of each convolutional layer, and the enhancement of details w.r.t. high frequency information.</p>
</blockquote>
<p>VDSR中输入图像“蝴蝶”的不同层的输出特征映射。<strong>任何两个相邻层之间的差异在纹理和颜色信息上都非常相似。高频信息的细节随着深度的增加而增强。</strong>这两个重要性质需要加法神经网络来保证。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/15.png" title="Optional title"></p>
<p>这篇文章分别设计了模块去解决了上述问题，最终使AdderNet的SISR模型逼近CNN性能。</p>
<blockquote>
<p>To maximally excavate the potential for exploiting AdderNets to establish SISR models, we first analyze the theoretical difficulties for applying the additions into SISR tasks. Specifically, input and output features in any two neighbor layers in SISR models are very close with the similar global texture and color information as shown in Figure 2. However, the identify mapping cannot be learned by a one-layer adder network. Thus, we suggest to insert self-shortcuts and formulate new adder models for the SISR task. Moreover, we find that the high-pass filter is also hard to approximate by adder units. We then develop a learnable power activation. By exploiting these two techniques, we replace the conventional convolution filters in modern SISR networks by adder filters and establish AdderSR models accordingly. </p>
</blockquote>
<h2 id="数学描述-1"><a href="#数学描述-1" class="headerlink" title="数学描述"></a>数学描述</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>一般的SISR任务的目标函数可以表示为：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/16.png" title="Optional title"></p>
<p>其中，其中<code>Iy</code>是观测数据，即低分，<code>Ix</code>是期望的高分辨率图像，<code>R(·)</code>表示所使用的先验信息，例如平滑和加性噪声，<code>λ</code>是权重。</p>
<p>式（2）又提了一遍加法器模型，就是上一篇的式（2）。</p>
<blockquote>
<p>本篇的式（2）跟上一篇的式（2）有一点点不一样，就是求和号的角标都是从1开始的，上一篇是从0；以及换了两个字母表示。不过应该不是什么问题。</p>
</blockquote>
<h3 id="Learning-Identity-Mapping-using-AdderNet"><a href="#Learning-Identity-Mapping-using-AdderNet" class="headerlink" title="Learning Identity Mapping using AdderNet"></a>Learning Identity Mapping using AdderNet</h3><blockquote>
<p>一上来，这个恒等映射的作用就没太看懂，查只能查到何大佬那篇文章，但也没有引，不知道说的是不是一个事情。</p>
</blockquote>
<h3 id="Learnable-Power-Activation"><a href="#Learnable-Power-Activation" class="headerlink" title="Learnable Power Activation"></a>Learnable Power Activation</h3><h2 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h2><hr>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="卷积、互相关、自相关"><a href="#卷积、互相关、自相关" class="headerlink" title="卷积、互相关、自相关"></a>卷积、互相关、自相关</h2><blockquote>
<p>卷积、互相关与自相关-知乎专栏<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/62292503">https://zhuanlan.zhihu.com/p/62292503</a></p>
</blockquote>
<h2 id="L1距离"><a href="#L1距离" class="headerlink" title="L1距离"></a>L1距离</h2><h2 id="signSGD"><a href="#signSGD" class="headerlink" title="signSGD"></a>signSGD</h2><h2 id="HardTanh"><a href="#HardTanh" class="headerlink" title="HardTanh"></a>HardTanh</h2><p>HardTanh比Tanh计算开销小，缺点是和dying relu类似，部分参数可能不更新。</p>
<h2 id="Identity-Mapping"><a href="#Identity-Mapping" class="headerlink" title="Identity Mapping"></a>Identity Mapping</h2><blockquote>
<p>Identity Mappings in Deep Residual Networks</p>
</blockquote>
<blockquote>
<p>2016 ECCV</p>
</blockquote>
<p>这个关键词何凯明大佬为了残差网络而提到的。这篇文章的核心是：分析了残差模块的传播方式，能过解释为什么使用identity mapping作为跳跃连接和加和的激活项，能使得前向和反向的信号能直接在模块之间传播。</p>
</article><!-- lincense--><div class="post-paginator"><a class="nextSlogan" href="/2021/05/28/Ubuntu16-04%E8%99%9A%E6%8B%9F%E6%9C%BA-Hadoop%EF%BC%9A%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F/" title="Ubuntu16.04虚拟机+Hadoop：伪分布式"><span>NextPost ></span><br><span class="nextTitle">Ubuntu16.04虚拟机+Hadoop：伪分布式</span></a><div class="clear"></div></div><div id="comment"></div></section></article><footer id="cxo-footer-outer"><div id="cxo-footer-inner"><p class="footer-container"><span>Site by </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span> | theme </span><a target="_blank" rel="noopener" href="https://github.com/Longlongyu/hexo-theme-Cxo"><span>Cxo</span></a></p><i class="fa fa-user"> </i><span id="busuanzi_value_site_uv"></span><span> | </span><i class="fa fa-eye"> </i><span id="busuanzi_value_site_pv"></span></div></footer><!-- catelog--><div class="toc-wrapper" style="top: 70vh;"><div class="toc-catalog"><i class="fa fa-list"> </i><span>CATALOG</span></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#AdderNet"><span class="toc-number">1.</span> <span class="toc-text">AdderNet</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AB%8B%E6%84%8F"><span class="toc-number">1.1.</span> <span class="toc-text">立意</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E6%8F%8F%E8%BF%B0"><span class="toc-number">1.2.</span> <span class="toc-text">数学描述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Adder-Network"><span class="toc-number">1.2.1.</span> <span class="toc-text">Adder Network</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Optimization"><span class="toc-number">1.2.2.</span> <span class="toc-text">Optimization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adaptive-Learning-Rate-Scaling"><span class="toc-number">1.2.3.</span> <span class="toc-text">Adaptive Learning Rate Scaling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adder-Network%E7%9A%84%E5%89%8D%E9%A6%88%E5%92%8C%E5%8F%8D%E4%BC%A0%E6%B5%81%E7%A8%8B"><span class="toc-number">1.2.4.</span> <span class="toc-text">Adder Network的前馈和反传流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">1.3.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83"><span class="toc-number">1.3.1.</span> <span class="toc-text">实验环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MINST-LeNet-5-BN-AddNN-CNN"><span class="toc-number">1.3.2.</span> <span class="toc-text">MINST + LeNet-5-BN + AddNN&#x2F;CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CIFAR-10-100-VGG-ResNet-BNN-AddNN-CNN"><span class="toc-number">1.3.3.</span> <span class="toc-text">CIFAR-10&#x2F;100 + VGG&#x2F;ResNet + BNN&#x2F;AddNN&#x2F;CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ImageNet-ResNet-BNN-AddNN-CNN"><span class="toc-number">1.3.4.</span> <span class="toc-text">ImageNet + ResNet + BNN&#x2F;AddNN&#x2F;CNN</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E5%AE%83"><span class="toc-number">1.4.</span> <span class="toc-text">其它</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%84%9F%E6%83%B3"><span class="toc-number">1.5.</span> <span class="toc-text">感想</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#AdderSR"><span class="toc-number">2.</span> <span class="toc-text">AdderSR</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AB%8B%E6%84%8F-1"><span class="toc-number">2.1.</span> <span class="toc-text">立意</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E6%8F%8F%E8%BF%B0-1"><span class="toc-number">2.2.</span> <span class="toc-text">数学描述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">2.2.1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Learning-Identity-Mapping-using-AdderNet"><span class="toc-number">2.2.2.</span> <span class="toc-text">Learning Identity Mapping using AdderNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Learnable-Power-Activation"><span class="toc-number">2.2.3.</span> <span class="toc-text">Learnable Power Activation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C-1"><span class="toc-number">2.3.</span> <span class="toc-text">实验</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">3.</span> <span class="toc-text">其他</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E3%80%81%E4%BA%92%E7%9B%B8%E5%85%B3%E3%80%81%E8%87%AA%E7%9B%B8%E5%85%B3"><span class="toc-number">3.1.</span> <span class="toc-text">卷积、互相关、自相关</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#L1%E8%B7%9D%E7%A6%BB"><span class="toc-number">3.2.</span> <span class="toc-text">L1距离</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#signSGD"><span class="toc-number">3.3.</span> <span class="toc-text">signSGD</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HardTanh"><span class="toc-number">3.4.</span> <span class="toc-text">HardTanh</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Identity-Mapping"><span class="toc-number">3.5.</span> <span class="toc-text">Identity Mapping</span></a></li></ol></li></ol></div><!-- top--><i class="fa fa-arrow-up close" id="go-up" aria-hidden="true"></i></body></html>