<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>MoyangSensei</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2020-12-05T01:50:53.489Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Fy J</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文研读：SISR-DRN</title>
    <link href="http://example.com/2020/12/04/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASISR-DRN/"/>
    <id>http://example.com/2020/12/04/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASISR-DRN/</id>
    <published>2020-12-04T12:41:23.000Z</published>
    <updated>2020-12-05T01:50:53.489Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Closed-loop Matters: Dual Regression Networks for Single Image Super-Resolution<br>2020 cvpr</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>文章提出，目前超分辨率算法存在两个明显的问题：</p><ul><li>从LR图像到HR图像通常是一个ill-posed的反问题，<strong>存在无数可能的HR图像通过降采样得到同一张LR图像</strong>。解空间过大，从而很难去找到一个合适的解。</li></ul><blockquote><p>First, learning the mapping from LR to HR images is typ- ically an ill-posed problem since there exist infinitely many HR images that can be downscaled to obtain the same LR image [36]. Thus, the space of the possible functions that map LR to HR images becomes extremely large. As a result, the learning performance can be limited since learning a good solution in such a large space is very hard.</p></blockquote><ul><li>真实场景应用中，<strong>成对的LR-HR图像往往无法获得，因此对应图像降采样方式也往往未知</strong>。因此对于更普遍的情形，现有的SR模型经常会引起适应性问题，从而导致性能不佳。</li></ul><blockquote><p>Second, it is hard to obtain a promising SR model when the paired data are unavailable [43, 54]. Note that most SR methods rely on the paired training data, i.e., HR images with their Bicubic-degraded LR counterparts. However, the paired data may be unavailable and the unpaired data often dominate the real-world applications.</p></blockquote><p>论文针对这两个主要的问题进行改进，提出了对偶回归策略，通过引入对LR图像额外的约束，从而减小解空间的大小。</p><p>也就是说，模型除了学习LR到HR图像的映射外，还学习了额外的对偶回归映射，用于估计下采样内核并重建LR图像，从而形成一个闭环以提供额外的监督。特别地是，由于对偶回归策略并不依赖HR图像，因此可以直接从LR图像中进行学习。因此，可以很好地使得SR模型适应真实世界图像。</p><blockquote><p>作者的意思就是针对LR到HR解空间大的问题，作者通过设计一个反向的一个网络，实现HR到LR的映射，以此来制约和平衡主网络（也就是LR到HR映射的网络）的训练，怎么平衡的可以通过后面作者给的损失函数看出来。而为了解决HR和LR成对训练的依赖问题，作者通过在训练集中加入不成对的LR图像，那肯定有人要问了，那LR图像没有对应的HR图像那怎么训练呢？这个问题也是能通过后面作者给的损失函数来解决。</p></blockquote><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p>网络的整体结构如下，<strong>分为P和D两部分。</strong>：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/1.png" title="Optional title"></p><p>模型中黑色箭头所指部分，为DRN模型的Primary网络，而红色箭头所指部分，则对应Dual Regression 网络。Primary网络实现了从LR到HR映射，并且生成多尺度的SR图像：x1 SR，x2 SR。Dual Regression网络则是HR到LR映射，生成多尺度的LR图像：x2 LR，x1 LR。</p><blockquote><p>D网络中优化的损失函数不止一个，通过上图可以发现对于最后结果为4x的图像，反向进行下采样可以下采样成2x和1x的。而作者在P网络的设计中一开始Input图像（LR通过插值上采样后的）在输入时也经历了两个阶段就是下采样成2x和1x的，所以这就和D网络对应了起来。P网络的2x和D网络的2x图像形成一对，并进行损失函数优化。1x图像也是如此。如果最后的结果是8x的图像，就多一个4x的P网络和D网络的成对优化。</p></blockquote><h2 id="成对数据"><a href="#成对数据" class="headerlink" title="成对数据"></a>成对数据</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/2.png" title="Optional title"></p><p>针对配对的训练数据，将SR问题公式化为涉及两个回归任务的对偶回归模型。损失函数如下图所示，包含两部分，一个是P网络的损失，一个是D网络的损失：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/3.png" title="Optional title"></p><p>其中，lamda为对偶回归Loss的权重参数,推荐设置为0.1。每个loss均可以用L1或L2loss，本文中使用的是L1 loss。</p><h2 id="不成对数据"><a href="#不成对数据" class="headerlink" title="不成对数据"></a>不成对数据</h2><p>对于没有成对数据集的情况下，DRN采用了半监督学习，引入部分的成对数据集用于训练，DRN损失函数为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/4.png" title="Optional title"></p><p>首先，为了保证SR的质量，这里需要添加一部分成对的合成数据（简单双三次三采样退化）。训练时选取m个无标签数据和n个合成数据。</p><p>其次，1sp表示数据为合成数据时取1，无标签数据时取0。通过这个参数来控制训练LR没有对应HR图像的情况下的训练损失函数，通过后面加上lamda权重的D网络损失函数，来平衡P网络的训练。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="有监督部分"><a href="#有监督部分" class="headerlink" title="有监督部分"></a>有监督部分</h2><p>有监督部分和目前大部分模型一样，使用的是常见的Data pair数据集DIV2K和Flick2K作为训练集，测试集为常见的SET5, SET14, BSDS100,URBAN100 和MANGA109。模型设置上，作者设置了两个不同参数量的模型，DRN-S（小模型）和DRN-L（大模型）。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/5.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/6.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/7.png" title="Optional title"></p><h2 id="半监督部分"><a href="#半监督部分" class="headerlink" title="半监督部分"></a>半监督部分</h2><p>半监督部分，data pair部分使用了DIV2K的训练集，而对于data unpiar部分，则划分成两个实验：</p><h3 id="Comparison-on-Unpaired-Synthetic-Data"><a href="#Comparison-on-Unpaired-Synthetic-Data" class="headerlink" title="Comparison on Unpaired Synthetic Data"></a>Comparison on Unpaired Synthetic Data</h3><p>data unpair则是在ImageNet中随机选取了3K张图像，并对图片使用Nearest和BD两种不同的方式采样出LR图像，将LR用于半监督训练。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/8.png" title="Optional title"></p><h3 id="Comparison-on-Unpaired-Real-world-Data"><a href="#Comparison-on-Unpaired-Real-world-Data" class="headerlink" title="Comparison on Unpaired Real-world Data"></a>Comparison on Unpaired Real-world Data</h3><p>data unpair则是来自于未知退化核的YouTube视频的3K张原始帧。由于没有GT，只能通过感性的方式进行不同方法的下恢复效果的观测。</p><h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><h3 id="对偶回归学习是否有效？lamba设置为多少合适？"><a href="#对偶回归学习是否有效？lamba设置为多少合适？" class="headerlink" title="对偶回归学习是否有效？lamba设置为多少合适？"></a>对偶回归学习是否有效？lamba设置为多少合适？</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/9.png" title="Optional title"></p><h3 id="半监督学习中，data-pair数据和unpair数据比例如何划分？"><a href="#半监督学习中，data-pair数据和unpair数据比例如何划分？" class="headerlink" title="半监督学习中，data pair数据和unpair数据比例如何划分？"></a>半监督学习中，data pair数据和unpair数据比例如何划分？</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/10.png" title="Optional title"></p><h1 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h1><p>来自原文叙述的主要贡献：</p><ul><li><p>We develop a dual regression scheme by introducing an additional constraint such that the mappings can form a closed-loop and LR images can be reconstructed to enhance the performance of SR models. Moreover, we also theoretically analyze the generalization ability of the proposed scheme, which further confirms its superiority to existing methods.</p></li><li><p>We study a <code>more general super-resolution case</code> where there is no corresponding HR data w.r.t. the real-world LR data. With the proposed dual regression scheme, deep models can be <code>easily adapted to real-world data</code>, e.g., raw video frames from YouTube.</p></li><li><p>Extensive experiments on both the SR tasks with paired training data and unpaired real-world data demonstrate the effectiveness of the <code>proposed dual regression scheme</code> in image super-resolution.</p></li></ul><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>文章：<br><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Closed-Loop_Matters_Dual_Regression_Networks_for_Single_Image_Super-Resolution_CVPR_2020_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Closed-Loop_Matters_Dual_Regression_Networks_for_Single_Image_Super-Resolution_CVPR_2020_paper.pdf</a></p><p>code：<a href="https://github.com/guoyongcs/DRN">https://github.com/guoyongcs/DRN</a></p><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><ul><li><p>思路非常的新颖且实用。最大的亮点，在于利用对偶任务的特点，将真实LR的半监督引入，为目前SISR普遍存在的泛化性问题提供了一种新的解决思路。即当前更多的工作是如何去盲估计出一个未知的退化核，这种盲估计通常是非常难的。而这种半监督的学习，则需要将真实的LR接纳到模型中来进行学习，因此这种半监督的方式，在真实场景下可能更多需要Online learning。</p></li><li><p>推测这种学习范式对其他的low level vision tasks应该都是同样适用的。</p></li></ul><hr><h1 id="对偶学习"><a href="#对偶学习" class="headerlink" title="对偶学习"></a>对偶学习</h1><blockquote><p>Dual Learning for Machine Translation<br>NIPS 2016</p></blockquote><blockquote><p><a href="https://www.msra.cn/zh-cn/news/features/dual-learning-20161207">https://www.msra.cn/zh-cn/news/features/dual-learning-20161207</a></p></blockquote><h2 id="立意-1"><a href="#立意-1" class="headerlink" title="立意"></a>立意</h2><p>深度学习之所以能够取得巨大的成功，一个非常重要的因素就是大数据，特别是大规模的带标签的数据。但在很多任务中，没办法收集到大规模的标注数据。为了使深度学习能够取得更广泛的成功，需要降低其对大规模标注数据的依赖性。</p><p><strong>为了解决这个问题，我们提出了一种新的学习范式，把它称作对偶学习。</strong> </p><h2 id="做法"><a href="#做法" class="headerlink" title="做法"></a>做法</h2><p>很多人工智能的应用涉及两个互为对偶的任务，例如机器翻译中从中文到英文翻译和从英文到中文的翻译互为对偶、语音处理中语音识别和语音合成互为对偶、图像理解中基于图像生成文本和基于文本生成图像互为对偶、问答系统中回答问题和生成问题互为对偶，以及在搜索引擎中给检索词查找相关的网页和给网页生成关键词互为对偶。这些互为对偶的人工智能任务可以形成一个闭环，使从没有标注的数据中进行学习成为可能。</p><p>对偶学习的最关键一点在于，给定一个原始任务模型，其对偶任务的模型可以给其提供反馈；同样的，给定一个对偶任务的模型，其原始任务的模型也可以给该对偶任务的模型提供反馈；从而这两个互为对偶的任务可以相互提供反馈，相互学习、相互提高。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/11.png" title="Optional title"></p><p>考虑一个对偶翻译游戏，里面有两个玩家小明和爱丽丝。小明只能讲中文，爱丽丝只会讲英文，他们两个人一起希望能够提高英文到中文的翻译模型f和中文到英文的翻译模型g。给定一个英文的句子x，爱丽丝首先通过f把这个句子翻译成中文句子y1，然后把这个中文的句子发给小明。因为没有标注，所以小明不知道正确的翻译是什么，但是小明可以知道，这个中文的句子是不是语法正确、符不符合中文的语言模型，这些信息都能帮助小明大概判断翻译模型f是不是做的好。然后小明再把这个中文的句子y1通过翻译模型g翻译成一个新的英文句子x1，并发给爱丽丝。通过比较x和x1是不是相似，爱丽丝就能够知道翻译模型f和g是不是做得好，尽管x只是一个没有标注的句子。因此，通过这样一个对偶游戏的过程，我们能够从没有标注的数据上获得反馈，从而知道如何提高机器学习模型。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>认为对偶学习是一个全新的学习范式。</p><ul><li><p>监督学习（supervised learning）只能从标注的数据进行学习，只涉及一个学习任务；而对偶学习涉及至少两个学习任务，可以从未标注的数据进行学习。</p></li><li><p>半监督学习（semi-supervised learning）尽管可以对未标注的样本生成伪标签，但无法知道这些伪标签的好坏，而对偶学习通过对偶游戏生成的反馈（例如对偶翻译中x和x1的相似性）能知道中间过程产生的伪标签（y1）的好坏，因而可以更有效地利用未标注的数据。我们甚至可以说，对偶学习在某种程度上是把未标注的数据当作带标签的数据来使用。</p></li><li><p>对偶学习和多任务学习（multi-task learning）也不相同。尽管多任务学习也是同时学习多个任务共的模型，但这些任务必须共享相同的输入空间，而对偶学习对输入空间没有要求，只要这些任务能形成一个闭环系统即可。</p></li><li><p>对偶学习和迁移学习（transfer learning）也很不一样。迁移学习用一个或多个相关的任务来辅助主要任务的学习，而在对偶学习中，多个任务是相互帮助、相互提高，并没有主次之分。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Closed-loop Matters: Dual Regression Networks for Single Image Super</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：SISR数据增广的一种新思路</title>
    <link href="http://example.com/2020/12/04/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASISR%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%B9%BF%E7%9A%84%E4%B8%80%E7%A7%8D%E6%96%B0%E6%80%9D%E8%B7%AF/"/>
    <id>http://example.com/2020/12/04/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASISR%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%B9%BF%E7%9A%84%E4%B8%80%E7%A7%8D%E6%96%B0%E6%80%9D%E8%B7%AF/</id>
    <published>2020-12-04T12:40:23.000Z</published>
    <updated>2020-12-05T01:30:12.472Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Rethinking Data Augmentation for Image Super-resolution: A Comprehensive Analysis and a New Strategy<br>2020 cvpr</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>数据扩充（DA）是在不增加计算成本的情况下提高模型性能的最实用的方法之一。<strong>但现在许多数据增强的方法也都是用于High-level的cv任务，并不适合图像超分辨率任务。</strong></p><p>像超分辨率这样的图像恢复研究，非常依赖于合成数据集。最常见的做法是通过模拟系统退化函数（bicubic），可以增加训练样本的数量。但这样的做法欠缺考虑：由于模拟数据分布与实际数据分布之间存在差距，在模拟数据集上训练的模型在实际环境中并没有表现出最佳性能。</p><p>这方面的相关工作很少有人做，[24]首先做了这个事情，但不过也就是通过旋转和翻转。也有部分论文提到过这个观点，但是要么就是给出的例子太少，要么就是使用的baseline比较老旧（SRCNN级别的这种）。总之这方面研究还比较稀少。</p><blockquote><p>[24]Seven ways to improve example-based single image super resolution.<br>2016cvpr</p></blockquote><p>为了更好地理解低水平视觉中的DA方法，对最初为高级视觉任务开发的各种DA方法的效果进行了全面分析（第2节）。首先根据方法的应用领域将现有的增强技术分为两类：像素域和特征域。当直接应用于SISR时，发现有些方法会损害图像恢复结果，甚至阻碍训练，特别是当一种方法在很大程度上导致相邻像素之间的空间信息丢失或混淆时。</p><p>在分析的基础上，提出了CutBlur。<strong>具体做法是：剪切模糊剪切并粘贴LR图像补丁到其对应的真实HR图像补丁，</strong>反之也一样。CurBlur的关键在于让模型不仅知道如何超分，同时也知道哪里需要超分。通过这样的方法，模型能够自适应地去决定图像多大程度上去应用超分而不是盲目地对所有像素进行超分。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DA-NewStrategy/1.png" title="Optional title"></p><h1 id="CurBlur"><a href="#CurBlur" class="headerlink" title="CurBlur"></a>CurBlur</h1><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>假设给定LR图像xlr(w<em>h</em>c)和xhr(sw<em>sh</em>c)图像块 [公式] ，其中 [公式] 是放大倍数。由于 CurBlur需要匹配LR图像和HR图像的分辨率，所以会LR图像进行s倍的双三次插值。CurBlur然后就会生成成对的训练样本lr-&gt;hr和hr-&gt;lr ：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DA-NewStrategy/2.png" title="Optional title"></p><p>其中，M为二值Mask，用于决定哪里需要被替换掉。</p><p>源码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cutblur</span>(<span class="params">im1, im2, prob=<span class="number">1.0</span>, alpha=<span class="number">1.0</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> im1.size() != im2.size():</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;im1 and im2 have to be the same resolution.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> alpha &lt;= <span class="number">0</span> <span class="keyword">or</span> np.random.rand(<span class="number">1</span>) &gt;= prob:</span><br><span class="line">        <span class="keyword">return</span> im1, im2</span><br><span class="line"></span><br><span class="line">    cut_ratio = np.random.randn() * <span class="number">0.01</span> + alpha</span><br><span class="line"></span><br><span class="line">    h, w = im2.size(<span class="number">2</span>), im2.size(<span class="number">3</span>)</span><br><span class="line">    ch, cw = np.int(h*cut_ratio), np.int(w*cut_ratio)</span><br><span class="line">    cy = np.random.randint(<span class="number">0</span>, h-ch+<span class="number">1</span>)</span><br><span class="line">    cx = np.random.randint(<span class="number">0</span>, w-cw+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply CutBlur to inside or outside</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> np.random.random() &gt; <span class="number">0.5</span>:</span><br><span class="line">        im2[..., cy:cy+ch, cx:cx+cw] = im1[..., cy:cy+ch, cx:cx+cw]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        im2_aug = im1.clone()</span><br><span class="line">        im2_aug[..., cy:cy+ch, cx:cx+cw] = im2[..., cy:cy+ch, cx:cx+cw]</span><br><span class="line">        im2 = im2_aug</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> im1, im2</span><br></pre></td></tr></table></figure><h2 id="为什么CutBlur适用于SR？"><a href="#为什么CutBlur适用于SR？" class="headerlink" title="为什么CutBlur适用于SR？"></a>为什么CutBlur适用于SR？</h2><p>从之前不同DA方法对比，可以看到，图像内容信息的急剧变化，图像块的混叠，或者是丢失像素的相关性都能够损害SR的性能。因此，用于SR的良好的DA方法不应存在不符合实际的模式或信息丢失，并且应该为SR模型良好的正则。</p><p>CutBlur能够满足这样的条件：首先，它仅仅在HR和LR图像块之间进行裁剪和粘贴，因此能够最小化边界效应；其次，它可以利用整个图像信息，同时由于样本具有随机的HR比率和位置，CutBlur具有正则化效果。</p><blockquote><p>CutBlur satisfies these conditions because it performs cut-and-paste between the LR and HR image patches of the same content. By putting the LR (resp. HR) image region onto the corresponding HR (resp. LR) image region, it can minimize the boundary effect, which majorly comes from a mismatch between the image contents (e.g., Cutout and CutMix). Unlike Cutout, CutBlur can utilize the entire image information while it enjoys the regularization effect due to the varied samples of random HR ratios and locations.</p></blockquote><h2 id="模型从CutBlur中学到了什么？"><a href="#模型从CutBlur中学到了什么？" class="headerlink" title="模型从CutBlur中学到了什么？"></a>模型从CutBlur中学到了什么？</h2><p>与其他DA方法防止分类模型过分自信地做出决策的相似，<strong>CurBlur可以很好地避免SR模型过度锐化图像。</strong></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DA-NewStrategy/3.png" title="Optional title"></p><p>现在，模型必须同时学习“如何”和“何处”来超分辨率图像，这将导致模型学习“多少”它应该应用超分辨率，这为训练提供了有益的正则化效果。</p><blockquote><p>When the SR model takes HR images at the test phase, it commonly outputs over-sharpened predictions, especially where the edges are (Figure 2). CutBlur can resolve this issue by directly providing such examples to the model during the training phase. Not only does CutBlur mitigate the over-sharpening problem, but it enhances the SR performance on the other LR regions, thanks to the regularization effect (Figure 3). Note that the residual intensity has significantly decreased in the CutBlur model. We hypothesize that this enhancement comes from constraining the SR model to dis- criminatively apply super-resolution to the image. Now the model has to simultaneously learn both “how” and “where” to super-resolve an image, and this leads the model to learn “how much” it should apply super-resolution, which pro- vides a beneficial regularization effect to the training.</p></blockquote><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="SR模型和数据集的规模影响"><a href="#SR模型和数据集的规模影响" class="headerlink" title="SR模型和数据集的规模影响"></a>SR模型和数据集的规模影响</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DA-NewStrategy/4.png" title="Optional title"></p><h3 id="不同规模的SR模型"><a href="#不同规模的SR模型" class="headerlink" title="不同规模的SR模型"></a>不同规模的SR模型</h3><p><strong>众所周知，一个大模型比一个小模型受益更多。</strong>为了验证这在SR中是否正确，根据模型大小设置不同的扩展应用概率：对于小模型（SRCNN和CARN）p=0.2，对于大模型（RCAN和EDSR），p=1.0。</p><p>对于小模型，这个方法没有提供任何好处，或者只是略微提高了性能。这表明了小型模型的严重不拟合，导致DA的影响很小。EDSR规模尚可，因此有所提升。</p><h3 id="不同规模的数据集"><a href="#不同规模的数据集" class="headerlink" title="不同规模的数据集"></a>不同规模的数据集</h3><p>减少了用于训练的数据量。使用数据集的50%，10%，和15%。</p><p>SRCNN和CARN提升很小或者干脆没有提升。培训时的验证曲线也可以看出这一点（图4a和4b）。RCAN和EDSR提升很大带来了巨大的好处。</p><h2 id="不同数据集的比较"><a href="#不同数据集的比较" class="headerlink" title="不同数据集的比较"></a>不同数据集的比较</h2><p>当使用了 CutBlur 对模型进行训练，模型在测试集上的性能得到了明显的提升，尤其是在RealSR数据集上，所有模型至少得到了0.22dB 的提升。而 CARN则能够在 RealSR测试集上达到SOTA性能（RCAN basline），性能与LP-KPN近似，使用参数量仅为LP-KPN的22%。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DA-NewStrategy/5.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DA-NewStrategy/6.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DA-NewStrategy/7.png" title="Optional title"></p><h2 id="野外环境下的CutBlur"><a href="#野外环境下的CutBlur" class="headerlink" title="野外环境下的CutBlur"></a>野外环境下的CutBlur</h2><h2 id="其他的一些low-level的视觉任务"><a href="#其他的一些low-level的视觉任务" class="headerlink" title="其他的一些low-level的视觉任务"></a>其他的一些low-level的视觉任务</h2><h1 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h1><p>来自原文叙述的主要贡献：</p><ul><li><p>To the best of our knowledge, we are the <code>first to provide</code> comprehensive analysis of recent data augmentation methods when directly applied to the SISR task.</p></li><li><p>We propose a new DA method, CutBlur, which can reduce unrealistic distortions by regularizing a model to learn not only “how” but also “where” to apply the super-resolution to a given image.</p></li><li><p>Our mixed strategy shows consistent and significant improvements in the SR task, achieving <code>state-of-the-art (SOTA) performance in RealSR</code> [4].</p></li></ul><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>文章：<br><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yoo_Rethinking_Data_Augmentation_for_Image_Super-resolution_A_Comprehensive_Analysis_and_CVPR_2020_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2020/papers/Yoo_Rethinking_Data_Augmentation_for_Image_Super-resolution_A_Comprehensive_Analysis_and_CVPR_2020_paper.pdf</a></p><p>code：<a href="https://github.com/clovaai/cutblur">https://github.com/clovaai/cutblur</a></p><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><ul><li><p>思路非常的新颖，也许会对目前所做的工作有所启发。</p></li><li><p>比较简单。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Rethinking Data Augmentation for Image Super-resolution: A Comprehen</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Clang+OpenMP：初步</title>
    <link href="http://example.com/2020/11/25/Clang-OpenMP%EF%BC%9A%E5%88%9D%E6%AD%A5/"/>
    <id>http://example.com/2020/11/25/Clang-OpenMP%EF%BC%9A%E5%88%9D%E6%AD%A5/</id>
    <published>2020-11-25T02:45:29.000Z</published>
    <updated>2020-11-25T03:34:30.876Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h1><p>完成一个简单的OpenMP并行程序设计，体会多核并行程序的思想。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><h2 id="OpenMP"><a href="#OpenMP" class="headerlink" title="OpenMP"></a>OpenMP</h2><p>OpenMP是由OpenMP Architecture Review Board牵头提出的，并已被广泛接受，用于共享内存并行系统的多处理器程序设计的一套指导性编译处理方案（Compiler Directive）。</p><p>OpenMP支持的编程语言包括C、C++和Fortran；而支持OpenMp的编译器包括Sun Compiler，GNU Compiler和Intel Compiler等。</p><p>OpenMp提供了对并行算法的高层的抽象描述，程序员通过在源代码中加入专用的<strong>pragma</strong>来指明自己的意图，由此编译器可以自动将程序进行并行化，并在必要之处加入同步互斥以及通信。当选择忽略这些pragma，或者编译器不支持OpenMp时，程序又可退化为通常的程序（一般为串行），代码仍然可以正常运作，只是不能利用多线程来加速程序执行。</p><h2 id="Gcc"><a href="#Gcc" class="headerlink" title="Gcc"></a>Gcc</h2><p>GCC是以GPL许可证所发行的自由软件，也是GNU计划的关键部分。GCC的初衷是为GNU操作系统专门编写一款编译器，现已被大多数类Unix操作系统（如Linux、BSD、MacOS X等）采纳为标准的编译器，甚至在微软的Windows上也可以使用GCC。GCC支持多种计算机体系结构芯片，如x86、ARM、MIPS等，并已被移植到其他多种硬件平台。</p><h2 id="Clang"><a href="#Clang" class="headerlink" title="Clang"></a>Clang</h2><p>Clang是一个C++编写、基于LLVM、发布于LLVM BSD许可证下的C/C++/Objective-C/Objective-C++编译器。它与GNU C语言规范几乎完全兼容（当然，也有部分不兼容的内容，包括编译命令选项也会有点差异），并在此基础上增加了额外的语法特性，比如C函数重载（通过<strong>attribute</strong>((overloadable))来修饰函数），其目标（之一）就是超越GCC。</p><h2 id="Mac-OS下的Gcc-Clang"><a href="#Mac-OS下的Gcc-Clang" class="headerlink" title="Mac OS下的Gcc/Clang"></a>Mac OS下的Gcc/Clang</h2><p>有了GCC在前为何还有Clang的出现？</p><p>这是因为Apple使用LLVM在不支持全部OpenGL特性的GPU（Intel低端显卡）上生成代码JIT，令程序仍然能正常运行。之后LLVM与GCC的集成过程发生了一些不快，GCC系统庞大儿笨重，<strong>而Apple大量使用的Object-C在GCC中优先级很低</strong>。此外GCC作为一个纯粹的编译系统，与IDE配合很差。加上许可证方面的要求，<strong>Apple无法使用修改版GCC而闭源</strong>。于是Apple决定从0开始写C family的前端，也就是基于LLVM的Clang了。</p><p><strong>实际上，在Mac OS下，对于OpenMP，早期的Gcc并不支持（大概在4.2版本之前）。后期的Gcc也能做到支持，具体做法是直接下载<code>&lt;omp.h&gt;</code>。</strong></p><h1 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h1><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="Clang环境"><a href="#Clang环境" class="headerlink" title="Clang环境"></a>Clang环境</h3><p>选择使用Clang来完成OpenMP的简单程序。</p><p><code>clang -v</code>：检查本机的Clang环境。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/openmp/1.png" title="Optional title"></p><p>Mac OS通常来说会自带这一框架。</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>代码如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;omp.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[])</span> </span>&#123; </span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel for </span></span><br><span class="line">     <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++ ) &#123; </span><br><span class="line">         <span class="built_in">printf</span>(<span class="string">&quot;i = %d\n&quot;</span>, i); </span><br><span class="line">     &#125; </span><br><span class="line">     <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>显然，当上述代码正常运行时，会按序输出0到9，共10个数字。文件命名为<code>omp.c</code>。</p><p>这里需要注意的是：<code>#pragma omp parallel for</code>。这是因为，<strong>OpenMP编程需要通过编译制导指令制导并行化。</strong>编译制导指令以#pragma omp 开始，后边跟具体的功能指令，格式为：**#pragma omp 指令[子句[,子句] …]**。在上述指令中：</p><ul><li><p>parallel：用在一个结构块之前，<strong>表示这段代码将被多个线程并行执行</strong>。</p></li><li><p>for：用于for循环语句之前，表示将循环计算任务分配到多个线程中并行执行，以实现任务分担，必须由编程人员自己保证每次循环之间无数据相关性。</p></li><li><p>parallel for：parallel和for指令的结合，也是用在for循环语句之前，表示for循环体的代码将被多个线程并行执行，它同时具有并行域的产生和任务分担两个功能。</p></li></ul><blockquote><p>如果没有编译制导指令，在下面的步骤中使用同样的</p></blockquote><h2 id="代码编译与运行"><a href="#代码编译与运行" class="headerlink" title="代码编译与运行"></a>代码编译与运行</h2><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/openmp/2.png" title="Optional title"></p><p>去掉编译制导指令，作为对比，文件名为<code>no_omp.c</code>。会正常输出0到9。</p><p><code>clang -o omp.out omp.c -Xpreprocessor -fopenmp -lomp</code>：用来编译代码。其中，<code>-fopenmp</code>默认使用计算机最大核数进行运行，这个参数是可调的。</p><p>可以看到，在使用了OpenMP之后，变成了乱序输出。</p><p>再次运行代码，发现乱序输出也是随机的，表明是多线程并行处理。</p><hr><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><blockquote><p>百度百科：openmp： <a href="https://baike.baidu.com/item/openmp/3735430?fr=aladdin">https://baike.baidu.com/item/openmp/3735430?fr=aladdin</a></p></blockquote><blockquote><p>百度百科：gcc： <a href="https://baike.baidu.com/item/gcc/17570?fr=aladdin">https://baike.baidu.com/item/gcc/17570?fr=aladdin</a></p></blockquote><blockquote><p>百度百科：clang： <a href="https://baike.baidu.com/item/clang/3698345?fr=aladdin">https://baike.baidu.com/item/clang/3698345?fr=aladdin</a></p></blockquote><blockquote><p>Mac下使用OpenMP编写第一个多线程程序： <a href="https://blog.csdn.net/suki570/article/details/104272040">https://blog.csdn.net/suki570/article/details/104272040</a></p></blockquote><blockquote><p><a href="https://clang-omp.github.io/">https://clang-omp.github.io</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;目标&quot;&gt;&lt;a href=&quot;#目标&quot; class=&quot;headerlink&quot; title=&quot;目标&quot;&gt;&lt;/a&gt;目标&lt;/h1&gt;&lt;p&gt;完成一个简单的OpenMP并</summary>
      
    
    
    
    
    <category term="云计算与分布式系统" scheme="http://example.com/tags/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="OpenMP" scheme="http://example.com/tags/OpenMP/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04虚拟机+Apache：配置与初步使用</title>
    <link href="http://example.com/2020/11/24/Ubuntu16-04%E8%99%9A%E6%8B%9F%E6%9C%BA-Apache%EF%BC%9A%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%88%9D%E6%AD%A5%E4%BD%BF%E7%94%A8/"/>
    <id>http://example.com/2020/11/24/Ubuntu16-04%E8%99%9A%E6%8B%9F%E6%9C%BA-Apache%EF%BC%9A%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%88%9D%E6%AD%A5%E4%BD%BF%E7%94%A8/</id>
    <published>2020-11-24T14:45:50.000Z</published>
    <updated>2020-11-25T01:59:10.784Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h1><p>在VMWare Workstation之上安装CentOS或Ubuntu操作系统（或其他操作系统）。为虚拟机环境中运行的CentOS或Ubuntu配置网络使之可以访问互联网。在虚拟机中的CentOS或Ubuntu环境中安装Apache服务，并在宿主机中访问Apache提供的Web服务。为VMWare Workstation、CentOS或Ubuntu系统编写安装和配置手册，其中包括故障提示及使用到的一些Linux命令。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><h2 id="Apache"><a href="#Apache" class="headerlink" title="Apache"></a>Apache</h2><p>Apache HTTP Server（简称Apache）是Apache软件基金会的一个开放源码的网页服务器，可以在大多数计算机操作系统中运行，由于其多平台和安全性被广泛使用，是最流行的Web服务器端软件之一。它快速、可靠并且可通过简单的API扩展，将Perl/Python等解释器编译到服务器中。</p><p>它可以运行在几乎所有广泛使用的计算机平台上。</p><h2 id="Vmware-Workstation-Fusion"><a href="#Vmware-Workstation-Fusion" class="headerlink" title="Vmware Workstation/Fusion"></a>Vmware Workstation/Fusion</h2><p>VMware Workstation（中文名“威睿工作站”）是一款功能强大的桌面虚拟计算机软件，提供用户可在单一的桌面上同时运行不同的操作系统，和进行开发、测试 、部署新的应用程序的最佳解决方案。VMware Workstation可在一部实体机器上模拟完整的网络环境，以及可便于携带的虚拟机器，其更好的灵活性与先进的技术胜过了市面上其他的虚拟计算机软件。对于企业的 IT开发人员和系统管理员而言， VMware在虚拟网路，实时快照，拖曳共享文件夹，支持 PXE 等方面的特点使它成为必不可少的工具。</p><p><strong>VMware Fusion允许基于Intel的Mac在虚拟机上运行Microsoft Windows，Linux，NetWare或Solaris等操作系统。</strong></p><h1 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h1><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><h3 id="本地环境"><a href="#本地环境" class="headerlink" title="本地环境"></a>本地环境</h3><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/1.png" title="Optional title"></p><h3 id="VMware-Fusion"><a href="#VMware-Fusion" class="headerlink" title="VMware Fusion"></a>VMware Fusion</h3><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/2.png" title="Optional title"></p><h3 id="虚拟机环境"><a href="#虚拟机环境" class="headerlink" title="虚拟机环境"></a>虚拟机环境</h3><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/3.png" title="Optional title"></p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/4.png" title="Optional title"></p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/5.png" title="Optional title"></p><ul><li><p><code>uname -a</code>：查看内核/操作系统/CPU信息。 </p></li><li><p><code>head -n 1 /etc/issue</code>：查看操作系统版本。</p></li><li><p><code>cat /proc/cpuinfo</code>：查看CPU信息：四核，编号为0-3。</p></li></ul><h2 id="Apache框架"><a href="#Apache框架" class="headerlink" title="Apache框架"></a>Apache框架</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>Apache在Ubuntu的默认软件库中可用，因此将使用传统的软件包管理工具进行安装。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/6.png" title="Optional title"></p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/7.png" title="Optional title"></p><ul><li><p><code>sudo apt-get update</code>：更新本地包索引。</p></li><li><p><code>sudo apt-get install apache2</code>：安装apache2。</p></li></ul><p><strong>如果不进行本地包的更新，在安装Apache时可能会报各种各样的错误，一般都与依赖项有关。</strong></p><h3 id="调整防火墙"><a href="#调整防火墙" class="headerlink" title="调整防火墙"></a>调整防火墙</h3><p>在测试Apache之前，需要修改防火墙，允许外部访问默认的Web端口。 </p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/8.png" title="Optional title"></p><ul><li><p><code>sudo ufw app list</code>：获得防火墙允许的应用程序配置文件列表。可以看到，有三种可用于Apache的配置文件：（1）Apache ：此配置文件仅打开端口80（正常，未加密的Web流量）；（2）Apache Full：此配置文件打开端口80（正常，未加密的Web流量）和端口443（TLS/SSL加密流量）；（3）Apache Secure ：此配置文件仅打开端口443（TLS/SSL加密流量）。</p></li><li><p><code>sudo ufw allow &#39;Apache Full&#39;</code>：允许Apache Full配置文件的传入流量。</p></li><li><p><code>sudo ufw status</code>：验证防火墙的更改。</p></li></ul><blockquote><p>如果显示status:inactive，需要<code>sudo ufw enable</code>。</p></blockquote><h3 id="检查安装"><a href="#检查安装" class="headerlink" title="检查安装"></a>检查安装</h3><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/9.png" title="Optional title"></p><ul><li><code>sudo systemctl status apache2</code>：验证安装结果。在上述安装过程结束时，Apache会自动启动默认的Web服务器，所以直接运行该命令就能看到结果，无需使用<code>sudo systemctl start apache2</code>。</li></ul><p>看到绿色的<code>active(running)</code>证明Apache框架已经正常的安装并启动了。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/10.png" title="Optional title"></p><p>在虚拟机的浏览器（火狐）键入<code>127.0.0.1</code>（默认），会看到apache的初始网页。</p><h2 id="在Apache框架下配置新的站点"><a href="#在Apache框架下配置新的站点" class="headerlink" title="在Apache框架下配置新的站点"></a>在Apache框架下配置新的站点</h2><h3 id="新站点配置"><a href="#新站点配置" class="headerlink" title="新站点配置"></a>新站点配置</h3><ul><li><code>sudo /etc/init.d/apache2 restart</code>：停止当前的服务器之后再次启动。</li></ul><p>重启Apache服务器后，按照以下步骤配置一个新的网点：<strong>jfy_test</strong>。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/11.png" title="Optional title"></p><ul><li><p><code>sudo mkdir -p /var/www/jfy_test.com/public_html</code>：第一步是创建目录结构，其中包含将为访客提供的网站数据。document root（Apache寻找内容服务将以此作为根目录）将被设置为<code>/var/www</code>，命名为<code>jfy_test.com</code>。在这个目录下，再创建一个<code>public_html</code>，存放要提供的文件。</p></li><li><p><code>sudo chown -R $USER:$USER /var/www/jfy_test.com/public_html</code>：上述目录结构为当前虚拟机下root用户所拥有。如果希望普通用户也能够修改web目录中的文件，可以通过<code>chown</code>更改权限。<code>$USER</code>给出当前登录用户的id。</p></li><li><p><code>sudo chmod -R 755 /var/www</code>：修改当前用户在<code>/var/www</code>下的读取权限。</p></li></ul><blockquote><p>如果当前用户的权限足够，那么修改权限的这两条指令可以跳过。</p></blockquote><ul><li><code>nano /var/www/jfy_test.com/public_html/index.html</code>：使用nano在第一步给出的目录中创建一个将要展示的网页文件，如下所示：</li></ul><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/12.png" title="Optional title"></p><ul><li><p><code>sudo cp /etc/apache2/sites-available/000-default.conf /etc/apache2/sites-available/jfy_test.com.conf</code>：虚拟主机文件是指定虚拟主机的实际配置的文件，并指示Apache Web服务器如何响应各种域请求。Apache附带一个默认的虚拟主机文件：<code>000-default.conf</code>。这里，将这个默认的虚拟主机文件复制一份，以要新配置的站点为命名。这样做的目的是方便修改。</p></li><li><p><code>sudo nano /etc/apache2/sites-available/jfy_test.com.conf</code>：使用nano打开刚才新建的.conf虚拟主机文件，并做如下修改：</p></li></ul><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/13.png" title="Optional title"></p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/14.png" title="Optional title"></p><ul><li><p><code>sudo a2ensite jfy_test.com.conf</code>：使用<code>a2ensite</code>启用新的站点。</p></li><li><p><code>sudo a2dissite 000-default.conf</code>：使用<code>a2dissite</code>禁用定义的默认站点<code>000-default.conf</code>。</p></li></ul><blockquote><p>上面的两条关于虚拟主机文件指定的指令没有先后。</p></blockquote><p>完成上述操作后，再次重启服务器使上述命令生效。</p><h3 id="检查新配置"><a href="#检查新配置" class="headerlink" title="检查新配置"></a>检查新配置</h3><p>在虚拟机的浏览器中键入<code>127.0.0.1</code>，就可以看到新配置的站点。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/15.png" title="Optional title"></p><p>在宿主机中使用虚拟机的ip就可以访问到该站点。</p><ul><li><code>ifconfig</code>：查看ip地址。</li></ul><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/16.png" title="Optional title"></p><p>在宿主机的浏览器（Safari）中键入虚拟机ip<code>172.16.43.132</code>，可以看到新配置的站点。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/17.png" title="Optional title"></p><ul><li><code>sudo nano /etc/hosts</code>：设置本地主机文件，将请求jfy_test.com发送到<code>172.16.43.132</code>。 </li></ul><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/18.png" title="Optional title"></p><p>然后就可以使用<code>www.jfy_test.com</code>来访问站点。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/19.png" title="Optional title"></p><blockquote><p>设置本地主机文件不是必选项。</p></blockquote><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><ul><li><p>Apache这个框架在安装方面还是比较友好的，只要安装过程不报错，一条指令就解决。</p></li><li><p>Linux默认的系统权限并不足以完成上述工作，这一点比较烦。对于这类Unix系统的权限还需要进一步的学习。</p></li><li><p>可以在虚拟机环境下以预设好的网址来访问站点，但是并没有做到在宿主机上以这个网址做访问。这一步如果能做到的话就更完美了。</p></li></ul><hr><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><blockquote><p>百度百科：apache： <a href="https://baike.baidu.com/item/Apache/6265">https://baike.baidu.com/item/Apache/6265</a></p></blockquote><blockquote><p>百度百科：VMware Workstation： <a href="https://baike.baidu.com/item/VMware%20Workstation/9884359?fr=aladdin">https://baike.baidu.com/item/VMware%20Workstation/9884359?fr=aladdin</a></p></blockquote><blockquote><p>百度百科：VMware Fusion： <a href="https://baike.baidu.com/item/VMware%20Fusion/5661746?fr=aladdin">https://baike.baidu.com/item/VMware%20Fusion/5661746?fr=aladdin</a></p></blockquote><blockquote><p>如何在Ubuntu 16.04上安装Apache Web服务器： <a href="https://www.cnblogs.com/lfri/p/10522392.html">https://www.cnblogs.com/lfri/p/10522392.html</a></p></blockquote><blockquote><p>如何设置Ubuntu的16.04截至Apache的虚拟主机： <a href="https://www.howtoing.com/how-to-set-up-apache-virtual-hosts-on-ubuntu-16-04/">https://www.howtoing.com/how-to-set-up-apache-virtual-hosts-on-ubuntu-16-04/</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;目标&quot;&gt;&lt;a href=&quot;#目标&quot; class=&quot;headerlink&quot; title=&quot;目标&quot;&gt;&lt;/a&gt;目标&lt;/h1&gt;&lt;p&gt;在VMWare Workst</summary>
      
    
    
    
    
    <category term="Apache" scheme="http://example.com/tags/Apache/"/>
    
    <category term="云计算与分布式系统" scheme="http://example.com/tags/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：PolyFit</title>
    <link href="http://example.com/2020/11/10/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9APolyFit/"/>
    <id>http://example.com/2020/11/10/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9APolyFit/</id>
    <published>2020-11-10T07:00:05.000Z</published>
    <updated>2020-11-12T06:30:36.562Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><blockquote><p>感谢WCX、DHF等在本文写作中给出的建议。</p></blockquote><hr><blockquote><p> PolyFit: Perception-Aligned Vectorization of Raster Clip-Art via Intermediate Polygonal Fitting<br>通过中间多边形拟合实现光栅剪贴画的感知对齐矢量化</p></blockquote><p>来自SIGGRAPH 2020的一篇文章，做的是<strong>光栅图像矢量化</strong>。</p><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><blockquote><p>clip-art images是什么？为什么可以做矢量化？为什么要做矢量化？本篇的核心是什么？</p></blockquote><p>Clip-art images，中文译为剪贴画，通常用于数字媒体中。</p><p>光栅剪贴画图像由明显的彩色区域组成，这些区域由清晰的边界隔开，通常允许清晰的心理向量解释。</p><blockquote><p>Raster clip-art images, which consist of distinctly colored regions separated by sharp boundaries typically allow for a clear mental vector interpretation. </p></blockquote><p><strong>目前，由于各种原因，大量的剪贴画图像仍然以光栅格式创建和存储。但实际上，剪贴画图像可以以矢量形式紧凑无损地表示。</strong>图形矢量化后，在对图形进行调整大小等方面的操作时就会极大的提高效率和提升用户体验。这也是光栅图像矢量化算法开发的最直接驱动，即现代媒体的商业用途。</p><blockquote><p>Clip-art images can be compactly and losslessly represented in vector form; yet, for a variety of reasons, large numbers of clip-art images are still created and stored in raster format…… Vectorizing these images would enable resolution-free reuse of artwork created for legacy displays and facilitate a range of operations such as resizing or editing, which are easier to perform on vector rather than raster data. Vectorizing this data in a man- ner consistent with viewer expectations poses unique challenges, motivating the development of algorithms specifically designed for clip-art vectorization</p></blockquote><p>虽然先前的方法成功地将光栅剪贴画分割成符合观众期望的区域，但在满足观察者期望的同时，<strong>将这些区域之间的光栅边界矢量化仍然是一项挑战：现有的算法仍然经常无法产生与人类偏好一致的向量边界（图1b-c，2b-d）</strong>。我们提出了一种在光栅剪贴画图像中实现区域边界矢量化的新方法，该方法明显优于这些早期方法，产生的结果更符合观众的期望（图1e、2e）。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/1.png" title="Optional title"></p><blockquote><p>Notably, while prior methods [Kopf and Lischinski 2011] successfully segment raster clip-art into regions consistent with viewer expectations, vectorizing the raster boundaries between these regions while meeting observer expectations remains challenging; existing algorithms still often fail to produce vector boundaries consistent with human preferences (Figures 1b-c, 2b-d). We present a new approach for vectorization of region boundaries in raster clip-art images that significantly outperforms these earlier approaches, producing results much better aligned with viewer expectations (Figures 1e, 2e).</p></blockquote><p>基于上述情况，这篇文章提出了PolyFit，可以产生与人类偏好相一致的矢量化图像。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="带有人类主观偏向的矢量化过程的评价标准"><a href="#带有人类主观偏向的矢量化过程的评价标准" class="headerlink" title="带有人类主观偏向的矢量化过程的评价标准"></a>带有人类主观偏向的矢量化过程的评价标准</h2><blockquote><p>Sec. 3</p></blockquote><p>本文的思路更偏向人的主观意愿去做矢量化（本来就是做给人看的，主观评价很重要）,所以文章先叙述了带有人类主观偏向的矢量化过程的评价标准：2018H；Koffka 1955；Wagemans 等人，2012，确定的可能影响人类心理矢量化过程的主要标准有：准确性、简单性、规则性和连续性。</p><blockquote><p>accuracy, simplicity, regularity, and continuity</p></blockquote><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/2.png" title="Optional title"></p><ul><li><p>准确性：预示着人类所设想的矢量化在几何上接近输入栅格边界。</p></li><li><p>简单性：主张输出由<strong>最少数量的几何基元</strong>组成，主张优先考虑直线而不是曲线，主张优先选择曲率变化较小的曲线。</p></li><li><p>规则性：观察者希望更观察到的精确或近似的规律性，如栅格输入中存在的平行性、<strong>对称性</strong>。</p></li><li><p>连续性：观察者倾向于将栅格区域边界“臆想”为<strong>片状连续曲线</strong>。</p></li></ul><h2 id="方法概述"><a href="#方法概述" class="headerlink" title="方法概述"></a>方法概述</h2><blockquote><p>Sec. 3</p></blockquote><p>该方法的输入是光栅剪贴画图像，这些图像都具有一个特征：<strong>由许多具有清晰可辨的区域间边界的独特颜色区域组成。</strong></p><p>整个方法可分为两部分：首先使用[Kopf 和 Lischinski 2011]的框架将光栅图像分割成单色区域。然后将所得区域之间的栅格或片状轴对齐的边界转换为符合观众感知的片状平滑曲线。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/3.png" title="Optional title"></p><p>给定输入图像(a)，我们的第一个多边形拟合阶段(b-c)生成一个在给定精度阈值(b)内连接输入顶点的所有可能边缘的图，并计算该图上相对于感知激励成本函数的最短周期，以获得与观察者期望完全一致的多边形拟合(c)。我们的样条拟合（spline fitting）步骤(d-e)使用学习的分类器将最适合的基元组合拟合到每个多边形角(简单曲线、曲线-直线、直线-直线)(d)，并使用获得的基元集计算一个最适合的样条(e)。我们通过对多边形及其对应的样条(f)进行正则化，以获得最终的向量输出(g)，从而获得更规则的结果。</p><p><strong>上图的d-e、f是本文的核心内容，即边界的转化工作。</strong></p><h2 id="中间多边形近似"><a href="#中间多边形近似" class="headerlink" title="中间多边形近似"></a>中间多边形近似</h2><h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><p>文章将中间多边形近似的问题表述为：寻找一个环（cycle），该环是最小化定义在有向图G=(V,E)中的成对和三对边上（pairs and triplets of edges）的能量函数（energy function）。</p><blockquote><p>We formulate the extraction of an intermediate polygonal approximation as the problem of finding a cycle that minimizes an energy function defined on pairs and triplets of edges in a directed graph G = (V,E),wheretheverticesV representcandidatecornersandthe edges E represent candidate polygon segments (see Fig. 5b).</p></blockquote><p>为了方便高效的计算，原文通过只包括“有合理可能成为最终多边形一部分的顶点和边”来保持被操作图的小尺寸。然后，通过图的构造，以平衡准确性、简单性和连续性为标准计算在这个图上的最优周期。</p><p>最终，寻找在成对和三倍边上的能量函数的最优周期问题简化为一个经典的最短周期问题。</p><blockquote><p>and continuity (Sec. 4.2, Fig. 5c) via a graph construction that reduces the problem of finding the optimal cycle of an energy function defined on pairs and triplets of edges to a classical shortest-cycle problem.</p></blockquote><h3 id="图形结构"><a href="#图形结构" class="headerlink" title="图形结构"></a>图形结构</h3><p>使用想塑角到边缘的曼哈顿距离来定义大致的形状：丢弃所有违反曼哈顿距离标准的边缘，并要求所有错误分类的像素都位于直线的一侧，丢弃不满足这一属性的边缘。</p><p>简言之，这部分工作的目的是：<strong>给定大致范围</strong></p><h3 id="多边形近似"><a href="#多边形近似" class="headerlink" title="多边形近似"></a>多边形近似</h3><p>在评估多边形的最优性时，根据第3节中确定的三个感知标准（准确性、简单性和连续性）来评估。</p><h4 id="准确性"><a href="#准确性" class="headerlink" title="准确性"></a>准确性</h4><p>使用之前计算出的错分像素集Pij来衡量每个边缘相对于栅格边界的准确性。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/4.png" title="Optional title"></p><h4 id="连续性"><a href="#连续性" class="headerlink" title="连续性"></a>连续性</h4><p>用多边形角上的角Aijk=eijejk作为连续性的离散代表。Aijk是角处的内外角中较小的角。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/5.png" title="Optional title"></p><h3 id="简单性"><a href="#简单性" class="headerlink" title="简单性"></a>简单性</h3><p>将简单性表达为对最小曲率变化和较小多边形边缘数的偏好。</p><p>以连续角度之间的相似性来衡量曲率变化。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/6.png" title="Optional title"></p><p>为了最大限度地减少拐点的出现（与幅度无关），为每个拐点边缘增加一个绝对拐点惩罚，即binfl=0.1，并使用两个角度中较小的一个（较尖锐的一个）来评估拐点的幅度。这个幅度的饱和极限设置为linfl=90°。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/7.png" title="Optional title"></p><p>简单性也主张减少多边形边的数量。为此，我们给所有图边缘分配一个小的惩罚ε=1e-3，并进一步惩罚冗余的像素长边缘。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/8.png" title="Optional title"></p><h3 id="综合多边形成本"><a href="#综合多边形成本" class="headerlink" title="综合多边形成本"></a>综合多边形成本</h3><p>综合多边形成本C(P)是上述项在所有边和连续边的对和三倍体上的总和。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/9.png" title="Optional title"></p><h2 id="样条拟合"><a href="#样条拟合" class="headerlink" title="样条拟合"></a>样条拟合</h2><blockquote><p>Sec. 5</p></blockquote><p>算法的第二步是将封闭地、片状光滑地样条拟合到多边形上。</p><p>计算这种拟合需要求解几组变量,分别是：</p><ul><li><p>定义样条地基元序列（由其类型定义）；</p></li><li><p>基元与其拟合地栅格边界段之间地映射（特别是基元断点和栅格边界位置之间地对应关系）；</p></li><li><p>基元地形状参数（控制点的位置）。</p></li></ul><h3 id="样条基元"><a href="#样条基元" class="headerlink" title="样条基元"></a>样条基元</h3><p>根据以下设定：</p><ul><li><p>多边形和样条都要准确地近似于输入的栅格，中间多边形边缘的切线要与最终样条的<strong>切线</strong>密切相关；</p></li><li><p>希望样条将通过接近多边形边缘中点，并且在这些中点附近的样条切线与边缘切线相似</p></li><li><p>希望角样条部分至少和它们匹配的多边形角一样连续（即最多有一个C0不连续），并期望它们尊重多边形角所施加的基元数的上界（即两个）。</p></li></ul><p>确定了三种角跨基元配置，它们反映了简单性和连续性之间所有不同的平衡选择。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/10.png" title="Optional title"></p><p>总体目标是产生一个最佳平衡平滑性、简单性和输入栅格的准确性的养条。为了找到这样的养条，在分类过程中，使用优先考虑连续性来考虑配置类型。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/11.png" title="Optional title"></p><h3 id="训练和验证"><a href="#训练和验证" class="headerlink" title="训练和验证"></a>训练和验证</h3><p>对拥有100棵树的森林进行了训练，在23张不同分辨率的输入图像上手工标注多边形角，显示出各种各样的几何构型。50%用于训练，50%用于测试，在测试数据集上达到了99.3%的准确率。</p><p>二元决策使用的是置信度，阈值是0.75。</p><h2 id="规范化"><a href="#规范化" class="headerlink" title="规范化"></a>规范化</h2><blockquote><p>Sec. 6</p></blockquote><p>因为前面提到过，人类观察者能够识别输入数据中的规律性，并期望在拟合或矢量化过程中保留这些规律性，所以，<strong>这部分工作的目的是在最终输出中保留输入的正则性，关注的是对称性、平行性、连续性和轴对齐。</strong></p><ul><li><p>正交和轴对齐的边：通过不允许对两个入射多边形角使用（单一）曲线基元配置（从而迫使方法沿这些边缘使用曲线线或线型配置），使长度至少为各自边界框边范围50%的轴对齐边缘更加突出。</p></li><li><p>平行边缘：检测并强制执行存在于栅格输入中的突出的轴对齐平行边缘：如果边缘之间的距离不超过它们的重叠长度，才认为这些边缘是突出的。</p></li><li><p>对称性：如果两个栅格对称性发生冲突，会优先考虑非规则化多边形中先验存在的对称性，并优先考虑较长的对称边界路径。</p></li><li><p>（曲线）延续：在光栅层面(轴对齐)和多边形层面(任意方向)都检测并强制执行连续。</p></li></ul><h2 id="多色输入"><a href="#多色输入" class="headerlink" title="多色输入"></a>多色输入</h2><blockquote><p>Sec. 7</p></blockquote><p>上面叙述的都是一个由两个颜色均匀的区域组成的图像的情况。但现实生活中，图像通常包含多个彩色区域。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/12.png" title="Optional title"></p><p>对于这种输入，处理方法是：将两个相邻的区域都将其交界处分类为平滑配置，只有当分别对区域进行矢量化会产生一个以交界处为顶点的平滑配置类型的多边形，如果不是这种情况，最多只允许两个相邻区域中的一个区域被分类为光滑配置。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><blockquote><p>Sec. 8</p></blockquote><h2 id="与以往结果的对比"><a href="#与以往结果的对比" class="headerlink" title="与以往结果的对比"></a>与以往结果的对比</h2><p>文章将其结果与2018H等最近两个排名最高的方法在81个输入(Potrace[Selinger 2003]以及2018H)上产生的结果进行了比较。测试数据集包括37个单一区域边界的输入，28个低分辨率多色输入，以及16个中高分辨率的多色输入。对所有方法都使用默认参数。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/13.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/14.png" title="Optional title"></p><p>该部分实验主要着眼在区域分辨率为100以下的输入。对于较高的分辨率，拟合精度对于最终结果起决定性作用，所有方法产生的结果都相当相似。<strong>但在较低的分辨率下（16×及以下），原文方法有着明显的优势，并且在94%的情况下优于2018H的结果。</strong></p><h2 id="与手动矢量化的对比"><a href="#与手动矢量化的对比" class="headerlink" title="与手动矢量化的对比"></a>与手动矢量化的对比</h2><p>文章将其结果与艺术家进行的手动矢量化结果进行了对比。因为手动矢量化多色输入非常耗时（一个区域需要 30多分钟才能准确拟合)，所以这部分只针对2018H提供的15张矢量化二元图像进行了试验。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/15.png" title="Optional title"></p><p>在55%的结果中，参与者认为本文方法得出的结果比艺术家给出的结果更好。</p><p>当艺术家的结果被认为是更好的输入时（如图17c），猜测是由于识别–因为艺术家的输出反映了作者对所􏰑绘形状的知识。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文给出了PolyFit这种新的剪贴画矢量化方法，并给出了与现有的替代方法相比的结果。</p><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul><li><p>能产生更符合观察者意愿的结果（文章的核心立意的努力方向）。</p></li><li><p>在低分辨率数据上表现得特别好（结果提到了16×、32×和64×）。</p></li></ul><h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h2><blockquote><p>Sec. 8</p></blockquote><p>对于分辨率为32×和64×的单个区域，该方法分别需要0.5秒和1.2秒（中位数）来完成矢量化。大部分的运行时间是分布在初始花键拟合和后续的角反馈循环之间的时间大致相等。多边形􏰐取阶段平均占总运行时间的 10%到20%，并且随着输入中存在更多的正则性而增加，这就需要对多边形进行更多的正则化工作。</p><p>上述时间是在八代英特尔酷睿 i7、3.7GHz主频CPU下进行的实验结果。远快于2018H的时间（后者平均需要30到50倍的时间）。</p><h2 id="局限"><a href="#局限" class="headerlink" title="局限"></a>局限</h2><blockquote><p>Sec. 8</p></blockquote><p>该方法目前的实现只对单个边界进行规范化。在许多情况下，它默认实现了边界间的平行性。对这种平行性的明确检测和执行将是我们方法的一个重要的实际扩展。</p><blockquote><p>Our current implementation only regularizes individual boundaries. It achieves inter-boundary parallelism by default for many cases. Explicit detection and enforcement of such parallelism would be an important practical extension of our method. In the future, one can similarly extend our regularization step to address regularities between regions, e.g., using continuation detection, complementing our core method.</p></blockquote><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>本文的研究建立在样本点的逼近、笔触美化、自然图像矢量化、艺术家生成的图像矢量化这几个领域和解决剪贴画矢量化的一些商业软件包的基础上。其中本文主要引用了Hoshyari等在2018年发表的Perception-Driven Semi-Structured Boundary Vectorization，本文工作是在该文的基础上做的改进。此外，本文使用的中间多边形拟合方法受到Potrace[Selinger 2003]启发，在其基础上约束多边形以准确地保存输入细节，有更准确的最终拟合。</p><h2 id="Perception-Driven-Semi-Structured-Boundary-Vectorization"><a href="#Perception-Driven-Semi-Structured-Boundary-Vectorization" class="headerlink" title="Perception-Driven Semi-Structured Boundary Vectorization"></a>Perception-Driven Semi-Structured Boundary Vectorization</h2><blockquote><p>ACM Transaction on Graphics 37, 4 (2018)<br>https: //doi.org/10.1145/3197517.3201312</p></blockquote><p>这篇文章针对艺术家生成的光栅输入的可操作的矢量化算法。给出了一种算法，能够通过将学习到的分类器预测与关于角认知的见解相结合，从有限的训练数据中获得感知上一致的角分类。方法步骤如下：</p><ul><li><p>模型学习：使用随机森林从带有人工标注的角的训练栅格图像集合中学习推理局部的几何环境，并使用一个训练好的分类器计算局部角概率。</p></li><li><p>模型整合：将数据驱动的模型预测与后续的基于感知的角处理步骤相结合，逐渐修剪模型输出的角集，直到使用得到的角计算的边界矢量化符合标准。</p></li><li><p>模型优化：框架定位角的同时，并在它们之间拟合简单的G1连续样条曲线。角集最终确定后，继续进一步简化输出。</p></li></ul><p>框架包括三个主要步骤：(a)潜在角检测、(b)迭代角去除、(c)全局正则化。颜色区分不同的曲线类型。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/16.png" title="Optional title"></p><h2 id="Potrace-a-polygon-based-tracing-algorithm"><a href="#Potrace-a-polygon-based-tracing-algorithm" class="headerlink" title="Potrace: a polygon-based tracing algorithm"></a>Potrace: a polygon-based tracing algorithm</h2><blockquote><p><a href="http://potrace.sourceforge.net/">http://potrace.sourceforge.net</a></p></blockquote><p>将位图转换为矢量轮廓图，这种逆向过程称为跟踪。</p><p>这篇文章描述了一个简单，有效的跟踪算法：Potrace。该算法输出是由Bezier曲线构成的光滑轮廓，它使用多边形作为图像的中间表示。</p><ul><li><p>路径分解：位图被分解成许多路径，这些路径形成了黑白区域之间的边界。</p></li><li><p>最优多边形生成：用最优多边形逼近每条路径。</p></li><li><p>光滑轮廓：将每个多边形转化为光滑的轮廓。</p></li><li><p>曲线优化（可选）：通过在可能的地方连接连续的Bezier曲线片段来优化生成的曲线。</p></li><li><p>输出：以所需的格式生成输出。</p></li></ul><p>一个完整的示例：(a)原始位图；(b)路径分解和最优多边形；(c)顶点调整、角点分析和平滑；(d)曲线优化；(e)最终输出。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/17.png" title="Optional title"></p><hr><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="光栅图像与矢量图像"><a href="#光栅图像与矢量图像" class="headerlink" title="光栅图像与矢量图像"></a>光栅图像与矢量图像</h2><p>所有的电子艺术图像可被分为两种核心类型：光栅图像和矢量图像。</p><p><strong>简言之，光栅图像由像素点组成，矢量是由连接的线组成的图像。</strong></p><h3 id="光栅图像"><a href="#光栅图像" class="headerlink" title="光栅图像"></a>光栅图像</h3><h4 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h4><p>光栅图也称为<strong>位图</strong>、点阵图、像素图。</p><p>简言之，最小单位由像素构成的图，就被称为光栅图像。在光栅图像中，只包含像素点的信息，每个像素点有自己的颜色。</p><p>光栅图形最为人知晓的特征，就是<strong>它在放大时会失真</strong>。</p><p>这种格式的图适合存储图形不规则，而且颜色丰富没有规律的图，比如照相，扫描等。</p><h4 id="分辨率"><a href="#分辨率" class="headerlink" title="分辨率"></a>分辨率</h4><p>光栅图像或扫描图像的分辨率以DPI（Dots Per Inch，每英寸点数）表示。</p><p>DPI原来是印刷上的记量单位，意思是每英寸上，所能印刷的网点数（Dot Per Inch）。但随着数字输入，输出设备快速发展，大多数的人也将数字影像的解析度用DPI表示。</p><p>但较为严谨的情形下，印刷时计算的网点（Dot）和电脑显示器的显示像素（Pixel）并非相同，所以较专业的人士，会用PPI(Pixel Per Inch)表示数字影像的解析度，以区分二者。</p><h4 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h4><p>为了准确地记录光栅图像文件，图形软件必须跟踪大量信息，包括像素点集中每个像素的确切位置和颜色。这就导致光栅图像的文件很大。高DPI和更大的颜色深度也会产生更大的文件大小。典型的“2X3” 150 dpi黑白光栅图像徽标的文件大约为70K。保存为300 dpi 24位（百万种颜色）光栅图像文件时，大小可能会超过100倍。</p><p>常见的光栅图像格式包括BMP（Windows位图），PCX（画笔），TIFF（标签交错格式），JPEG（联合图像专家组），GIF（图形交换格式），PNG（便携式网络图形），PSD（Adobe PhotoShop）和CPT（Corel PhotoPAINT）。</p><h3 id="矢量图像"><a href="#矢量图像" class="headerlink" title="矢量图像"></a>矢量图像</h3><h4 id="特征-1"><a href="#特征-1" class="headerlink" title="特征"></a>特征</h4><p>矢量图像是生成对象的连接线和曲线的集合。在矢量插图程序中创建矢量图像时，会插入节点或绘图点，并且线条和曲线将注释连接在一起。这与“连接点”的原理相同。</p><h4 id="分辨率-1"><a href="#分辨率-1" class="headerlink" title="分辨率"></a>分辨率</h4><p><strong>矢量图像的分辨率由数学定义（不是像素），所以它们可以按比例放大或缩小而不会失真。</strong>当插图（绘图）程序向上或向下调整矢量图像的大小时，它只是将对象的数学描述乘以缩放因子。<strong>这也是矢量图形在剪贴画中特别受欢迎的一个重要原因。</strong></p><h4 id="文件-1"><a href="#文件-1" class="headerlink" title="文件"></a>文件</h4><p>矢量图像不需要跟踪图像中的每个像素，只需要数学描述。因此，矢量文件的文件大小非常小。矢量文件的主要内容就是数学的描述。<strong>所以，矢量文件非常适合通过网络传输。</strong></p><p>常见的矢量格式包括EPS（Encapsulated PostScript），WMF（Windows图元文件），AI（Adobe Illustrator），CDR（CorelDraw），DXF（AutoCAD），SVG（可缩放矢量图形）和PLT（Hewlett Packard图形语言图文件）。</p><h2 id="曼哈顿距离"><a href="#曼哈顿距离" class="headerlink" title="曼哈顿距离"></a>曼哈顿距离</h2><p>曼哈顿距离为：在平面直角坐标系中，两点在坐标轴方向上的距离之和，即d(i,j)=|xi-xj|+|yi-yj|。</p><p>对于一个具有正南正北、正东正西方向规则布局的城镇街道，从一点到达另一点的距离正是在南北方向上旅行的距离加上在东西方向上旅行的距离，因此，曼哈顿距离又称为出租车距离。</p><p>曼哈顿距离不是距离不变量，当坐标轴变动时，点间的距离就会不同。</p><p>曼哈顿距离示意图在早期的计算机图形学中，屏幕是由像素构成，是整数，点的坐标也一般是整数，原因是浮点运算很昂贵，很慢而且有误差，如果直接使用欧氏距离(欧几里德距离：在二维和三维空间中的欧氏距离的就是两点之间的距离），则必须要进行浮点运算，如果使用曼哈顿距离，则只要计算加减法即可，这就大大提高了运算速度，而且不管累计运算多少次，都不会有误差。</p><h2 id="多边形拟合"><a href="#多边形拟合" class="headerlink" title="多边形拟合"></a>多边形拟合</h2><p>这是一个专门的研究方向。不管是已有的进行多边形拟合的轮子还是使用多边形拟合做的图形学研究，都有这个概念。</p><blockquote><p>opencv里有多边形拟合的函数approxPolyDP，natlab里有名为polyfit的函数等。</p></blockquote><p>所介绍的这篇论文使用了这个概念去做矢量化。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;感谢WCX、DHF等在本文写作中给出的建议。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt; PolyFit: P</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="基础知识" scheme="http://example.com/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    <category term="计算机图形学" scheme="http://example.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：Image Super-Resolution as a Defense Against Adversarial Attacks</title>
    <link href="http://example.com/2020/10/12/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AImage-Super-Resolution-as-a-Defense-Against-Adversarial-Attacks/"/>
    <id>http://example.com/2020/10/12/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AImage-Super-Resolution-as-a-Defense-Against-Adversarial-Attacks/</id>
    <published>2020-10-12T06:40:56.000Z</published>
    <updated>2020-10-26T08:02:46.442Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><p>20年发在TIP上的一篇文章。</p><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>CNN在很多计算机视觉的任务都有好的表现，比如图像分类、目标检测、语义分割和视觉问答等。以及在现实生活中的场景也有好的应用，比如汽车的自动驾驶和疾病诊断模型。</p><p>这么多的应用，就要求这些基于CNN的模型具有良好的鲁棒性。但是有一些研究表明，CNN很容易被扭曲的自然图像和一些精心制作的、人类不可察觉的附加扰动欺骗。这些被称为对抗性例子的扭曲图像，已经被进一步证明可以在不同的体系结构之间传输。比如，从Inception v3模型生成的对抗性示例能够欺骗CNN的其他架构。</p><p>业界很早就注意到了这个问题，并且对这些对抗性的攻击做了防御方向的研究。这些研究可分为两类：</p><ul><li><p>模型特定：其目的是通过对抗训练或参数平滑调整特定模型的参数。这种方法通常需要计算量大的可微变换。此外，这些转换很容易受到进一步的攻击，因为对手可以利用可微模块来规避这些攻击。</p></li><li><p>模型不可知论：它们通过应用各种变换来减轻输入图像域中的对抗性扰动的影响。这类技术的例子包括JPEG压缩、基于中心凹的方法，这些方法裁剪图像背景、随机像素偏移和随机图像填充和重新调整大小。与可微模型特定方法相比，大多数模型不可知方法计算速度快，并且在输入域进行变换，使其更为有利。然而，这些方法大多在去除敌方噪声时丢失了关键的图像内容，导致对非攻击图像的分类性能较差。</p></li></ul><blockquote><p>We can broadly categorize these defenses along two directions: the first being model-specific mechanisms, which aim to regularize a specific model’s parameters through adversarial training or parameter smoothing [18], [20], [21], [28], [30]. Such methods often require differentiable transformations that are computationally demanding. Moreover these transformations are vulnerable to further attacks, as the adversaries can circumvent them by exploiting the differentiable modules. The second category of defenses are model-agnostic. They mitigate the effect of adversarial perturbations in the input image domain by applying various transformations. Examples of such techniques include JPEG compression [31], [32], foveation-based methods, which crop the image background [33], random pixel deflection [27] and random image padding &amp; re-sizing [19]. Compared with differentiable model-specific methods, most of the model-agnostic approaches are computationally faster and carry out transformations in input domain, making them more favorable. However, most of these approaches lose critical image content when removing adversarial noise, which results in poor classification performance on non-attacked images.</p></blockquote><p>基于上，这篇文章提出了一种基于模型不可知的防御机制，它可以抵抗最近提出的各种各样的对手攻击，并且不受信息损失的影响。</p><p>提出的防御是基于SR的，它选择性地向图像中添加高频分量，并消除对手添加的噪声干扰。我们把流形样本重新映射到图1上。通过小波域滤波进一步抑制了附加噪声的影响，并通过对高分辨率图像的全局合并操作将其固有地最小化。提出的图像超分辨率和基于小波滤波的防御方法形成了一个联合不可微模块，该模块可以有效地恢复敌方扰动图像的原始类别标签。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DAAA/1.png" title="Optional title"></p><h1 id="对抗性攻击"><a href="#对抗性攻击" class="headerlink" title="对抗性攻击"></a>对抗性攻击</h1><p>对输入样本故意添加一些人无法察觉的细微的干扰，导致模型以高置信度给出一个错误的输出，称为对抗样本。</p><p>以深度学习为主流的人工智能应用越来越广泛之后，陆续又出现了对于人工智能应用的攻击，主要分为两种：一是白盒测试，即深度学习的模型架构和参数都已经的情况下，这种场景的攻击一般可以进行参数的修改来达到攻击的效果；二是黑盒测试，即上述情况未知的情况下进行攻击，这时候采用的攻击手段主要是对抗样本。</p><p>对抗样本现在已经广泛应用于人脸识别、声纹识别等相关应用场景。</p><blockquote><p>2013：Intriguing properties of neural networks</p></blockquote><blockquote><p>2014：Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images</p></blockquote><p>原文中提到了6种对抗性攻击方法。</p><h2 id="FGSM"><a href="#FGSM" class="headerlink" title="FGSM"></a>FGSM</h2><blockquote><p>Explaining and Harnessing Adversarial Examples<br><a href="https://arxiv.org/abs/1412.6572">https://arxiv.org/abs/1412.6572</a></p></blockquote><p>在图像攻击算法中，FGSM（fast gradient sign method）是非常经典的一个算法。</p><p>在训练分类模型时，网络基于输入图像学习特征，然后经过softmax层得到分类概率，接着损失函数基于分类概率和真实标签计算损失值，回传损失值并计算梯度（也就是梯度反向传播），最后网络参数基于计算得到的梯度进行更新，网络参数的更新目的是使损失值越来越小，这样模型分类正确的概率也就越来越高。</p><p><strong>图像攻击的目的是不修改分类网络的参数，而是通过修改输入图像的像素值使得修改后的图像能够扰乱分类网络的分类。</strong></p><p><strong>常规的分类模型训练在更新参数时都是将参数减去计算得到的梯度，这样就能使得损失值越来越小，从而模型预测对的概率越来越大。</strong>既然无目标攻击是希望模型将输入图像错分类成正确类别以外的其他任何一个类别都算攻击成功，那么只需要损失值越来越大就可以达到这个目标，也就是模型预测的概率中对应于真实标签的概率越小越好，这和原来的参数更新目的正好相反。<strong>因此只需要在输入图像中加上计算得到的梯度方向，这样修改后的图像经过分类网络时的损失值就比修改前的图像经过分类网络时的损失值要大，换句话说，模型预测对的概率变小了。</strong></p><p>简言之，FGSM算法的主要内容，一方面是基于输入图像计算梯度，另一方面更新输入图像时是加上梯度，而不是减去梯度，这和常见的分类模型更新参数正好背道而驰。</p><p>根据一般的训练过程，可以将损失值回传到输入图像并计算梯度。接下来可以通过<code>sign</code>函数计算梯度的方向，sign函数是用来求数值符号的函数，比如对于大于0的输入，输出为1，对于小于0的输入，输出为-1，对于等于0的输入，输出为0。<strong>之所以采用梯度方向而不是采用梯度值是为了控制扰动的L∞距离</strong>，这是FGSM算法的评价指标。</p><h2 id="I-FGSM"><a href="#I-FGSM" class="headerlink" title="I-FGSM"></a>I-FGSM</h2><blockquote><p>Adversarial examples in the physical world<br><a href="https://arxiv.org/abs/1607.02533">https://arxiv.org/abs/1607.02533</a></p></blockquote><p>FGSM算法从梯度的角度做攻击，速度比较快，这是该算法比较创新的地方。但是FGSM算法只涉及单次梯度更新，有时候单次更新并不足以攻击成功，因此，在此基础上推出迭代式的FGSM，这就是I-FGSM（iterative FGSM）。</p><h2 id="MI-FGSM"><a href="#MI-FGSM" class="headerlink" title="MI-FGSM"></a>MI-FGSM</h2><blockquote><p>Boosting Adversarial Attacks with Momentum<br><a href="http://arxiv.org/abs/1710.06081">http://arxiv.org/abs/1710.06081</a></p></blockquote><p>基于动量迭代梯度的方法。</p><p>除了基于迭代梯度的方法用梯度来迭代扰动输入以使损失函数最大化以外，基于动量的方法还在迭代过程中沿损失函数的梯度方向累积速度矢量，为了稳定更新方向并避免出现不良的局部最大值。我们表明，由动量迭代方法生成的对抗样本在白盒和黑盒攻击中均具有较高的成功率。所提出的方法减轻了白盒攻击和可传递性之间的折衷，并且比单步方法和原始迭代方法具有更强的攻击算法。</p><h2 id="DeepFool"><a href="#DeepFool" class="headerlink" title="DeepFool"></a>DeepFool</h2><blockquote><p>DeepFool: a simple and accurate method to fool deep neural networks<br><a href="https://arxiv.org/abs/1511.04599">https://arxiv.org/abs/1511.04599</a></p></blockquote><p>基于迭代且线性近似的方案提出了一种计算对抗样本的方法DeepFool。大量的实验结果表明，这是一种非常精确且有效的对抗扰动计算方法，它可以提供一个有效的方式去评估分类器的鲁棒性并且可以通过恰当的微调改善分类器的表现。</p><h2 id="C-amp-W"><a href="#C-amp-W" class="headerlink" title="C&amp;W"></a>C&amp;W</h2><blockquote><p>Towards Evaluating the Robustness of Neural Networks<br><a href="https://arxiv.org/abs/1608.04644">https://arxiv.org/abs/1608.04644</a></p></blockquote><h2 id="DIFGSM-and-MDIFGSM"><a href="#DIFGSM-and-MDIFGSM" class="headerlink" title="DIFGSM and MDIFGSM"></a>DIFGSM and MDIFGSM</h2><blockquote><p>Improving transferability of adversarial examples with input diversity<br><a href="https://arxiv.org/abs/1803.06978">https://arxiv.org/abs/1803.06978</a></p></blockquote><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>文章给出的防御方法属于第二类防御机制。</p><p>整个方法可分为两个部分，它们共同构成一个不可微管道，很难绕过。首先应用小波去噪来抑制任何噪声模式。然后，核心部分是超分辨率操作，它在提高像素分辨率的同时同时消除对抗模式。</p><p>实验表明，仅图像超分辨率就足以恢复分类器对正确类别的信念；然而，第二步提供了额外的鲁棒性，因为它是一个不可微的去噪操作。</p><h2 id="Super-Resolution-as-a-Defense-Mechanism"><a href="#Super-Resolution-as-a-Defense-Mechanism" class="headerlink" title="Super Resolution as a Defense Mechanism"></a>Super Resolution as a Defense Mechanism</h2><h3 id="背景解释"><a href="#背景解释" class="headerlink" title="背景解释"></a>背景解释</h3><p>方法源于<code>manifold assumption</code>（流形假设），假设自然图像位于低维的manifolds上（这解释了为什么低维深层特征表示能够准确地捕捉真实数据集的结构），扰动图像位于自然图像的低维流形之外。</p><p><strong>尽管在视觉上看起来完全相同，但敌对和干净的数据并不是孪生的。</strong></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DAAA/2.png" title="Optional title"></p><p>上图显示了自然图像的低维流形。来自真实世界数据集（比如ImageNet）的数据点是从自然图像的分布中取样的，可以认为它们位于流形上。这类图像被称为<code>in-domain</code>。通过添加对抗性噪声来破坏这些域内图像，可以使图像脱离流形。一个学习从流形外的图像生成流形上的图像的模型可以在检测和防御敌方攻击方面发挥很大的作用。</p><p>那么解决的思路就是使用图像超分辨率作为一个映射函数，将多种对抗性样本重新映射到自然图像流形上。以这种方式，通过增强图像的视觉质量来实现对抗干扰的鲁棒性。这种方法比其他截断关键信息以获得健壮性的防御机制具有显著的优势。</p><h3 id="SR-Network"><a href="#SR-Network" class="headerlink" title="SR Network"></a>SR Network</h3><p>防御机制的一个必要特征是能够抑制对手添加的欺诈干扰。<strong>由于这些扰动通常是高频细节</strong>，使用一个超分辨率网络，它明确地使用剩余学习来关注这些细节。这些细节被添加到每个残差块的低分辨率输入中，最终生成高质量、超分辨率的图像。这项工作中考虑用到的是EDSR。</p><h3 id="对光谱分布的影响"><a href="#对光谱分布的影响" class="headerlink" title="对光谱分布的影响"></a>对光谱分布的影响</h3><p>对抗性图像包含高频模式，超分辨率操作进一步向恢复图像注入高频模式。</p><p>这有两个主要的好处：一是新增加的高频模式平滑了图像的频率响应（第5列）；二是超分辨率摧毁了试图愚弄模型的对抗模式。</p><blockquote><p>这里没看懂</p></blockquote><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DAAA/3.png" title="Optional title"></p><p>左下角示出了干净图像和恢复图像之间的确切区别，这说明了与原始图像相比，恢复图像相对更干净，但是具有更多的高频细节。对比原始噪声信号（图3中的左上角）和恢复图像中的剩余噪声（图3中的左下角），我们可以观察到SR网络丢弃了大部分的噪声扰动；但是仍然存在稀疏的噪声痕迹。此外，SR网络强化了图像中沿显著边界的高频变化（注意沿鸟类边界的响应）。</p><h2 id="小波降噪"><a href="#小波降噪" class="headerlink" title="小波降噪"></a>小波降噪</h2><p><strong>在空间或频域中进行图像去噪会导致纹理细节的丢失，这不利于这篇文章想要做到在去噪图像上实现像图像一样的干净性能的目标。</strong></p><p><code>wavelet shrinkage</code>（小波收缩）的主要原理是：真实世界信号的离散小波变换（DWT）本质上是稀疏的。<strong>与图像平滑去除图像中高频分量不同，真实世界图像的小波变换具有与重要图像特征对应的大系数，通过对较小系数应用阈值可以去除噪声。</strong></p><blockquote><p>这里也没看懂</p></blockquote><h3 id="阈值化"><a href="#阈值化" class="headerlink" title="阈值化"></a>阈值化</h3><p>阈值参数决定了如何有效地收缩小波系数和去除图像中的敌对噪声。</p><p>在实际应用中，有两种阈值方法：硬阈值和软阈值。硬阈值：<strong>完全保留大于t的系数</strong>，将小噪声系数降为零，然后进行小波逆变换，得到保留关键信息并抑制噪声的图像。</p><p>文章给出的模型选择了软阈值，因为它减少了在硬阈值中发生的突变。此外，硬阈值过度平滑图像，这会降低干净的非对抗性图像的分类精度。</p><p>在实验中，做了<code>visureshrink</code>和<code>bayeshrink</code>软阈值，发现后者的性能更好，提供了视觉上更好的去噪效果。</p><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DAAA/4.png" title="Optional title"></p><p>算法1给出了端到端防御方案的算法描述。</p><p>首先用软小波去噪来平滑对抗性噪声的影响。其次，采用超分辨率作为映射函数来提高图像的视觉质量。超分辨率图像将敌方示例映射到高分辨率空间中的自然图像流形，而在低分辨率空间中，自然图像流形位于流形之外。然后，恢复的图像通过与生成对抗性示例的相同的预先训练的模型。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>做了很多的对比实验，这里给出在ILSVRC验证集的5000幅图像上与最新防御机制的性能的比较。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DAAA/5.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DAAA/6.png" title="Optional title"></p><p>上标给出了5000个ILSVRC验证集图像上各种防御机制的<code>destruction rates</code>。破坏率定义为成功防御图像的比率，100%的破坏率意味着在应用防御机制后，所有图像都被正确分类。</p><p>“无防御”显示了模型对生成的对抗性图像的性能。“无防御”下的值较低表示攻击很强。结果表明，与单步攻击相比，迭代攻击具有更好的欺骗模型的能力。然而，迭代攻击是不可转移的，更容易防御。类似地，与非目标攻击相比，有针对性的攻击更容易防御，因为它们往往过度符合攻击模型。</p><p>对于迭代攻击（C&amp;W和DeepFool），Random resize+Padding和PD都获得了相似的性能，成功地恢复了大约90%的图像。在比较中，提出的基于超分辨率的防御可以恢复大约96%的图像。对于单步攻击类别，随机调整大小+填充无法防御。为了克服这一局限性，采用了对抗性增强的集成模型进行防御。</p><ul><li><p>与基于JPEG压缩的防御方法相比，该方法对FGSM（ε=10）的性能提高了31.1%。</p></li><li><p>在单步攻击类别（FGSM-10），该方法比随机调整+填充和PD分别高出26.7%和21.0%。</p></li><li><p>最近提出的强攻击（MDI2FGSM），所有的防御技术（JPEG压缩、随机大小调整+填充、绗缝+TVM和PD）都严重失败，仅恢复图像的1.3%、5.8%、1.7%和21.9%。相比之下，所提出的基于图像超分辨率的防御方法可以成功地恢复31.3%的图像。</p></li></ul><p>以及给出了为什么要用EDSR的理由（就是效果好）。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DAAA/7.png" title="Optional title"></p><h1 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h1><p>来自原文的叙述：</p><ul><li><p>Through extensive empirical evaluations, we show image super-resolution to be an effective defense strategy against a wide range of recently proposed state-of-the-art attacks in the literature [15], [16], [34]–[37]. Using Class Activation Map visualizations, we demonstrate that super-resolution can successfully divert the attention of the classifier from random noisy patches to more distinctive regions of the attacked images (see Fig. 8 and 9).</p></li><li><p>Super-resolving an adversarial image projects it back to the natural image manifold learned by deep image classification networks.</p></li><li><p>Unlike existing image transformation based techniques, which introduce artifacts in the process of overcoming adversarial noise, the proposed scheme retains critical image content, and thus minimally impacts the classifier’s performance on clean, non-attacked images.</p></li><li><p>The proposed defense mechanism tackles adversarial attacks with no knowledge of the target model’s architecture or parameters. This can easily complement other existing model-specific defense methods.</p></li></ul><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>code：<a href="https://github.com/aamir-mustafa/super-resolution-adversarial-defense">https://github.com/aamir-mustafa/super-resolution-adversarial-defense</a></p><p>文章：<a href="http://arxiv.org/abs/1901.01677v1">http://arxiv.org/abs/1901.01677v1</a></p><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><ul><li><p>这篇文章做的是对抗样本的防御算法，整体思路是用小波+SR。并不是SISR的发展研究。</p></li><li><p>文章提到了算法的优点，其中有一点：“our purification approach is equally applicable to other computer vision tasks beyond classification”，这么看来泛用性还是很强。</p></li><li><p>说实话写的比较难，看不太懂很多细节的部分，尤其是小波那部分为什么要用，提到了“真实采样”的特点，确实没看懂。包括实验做的也很烦。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;20年发在TIP上的一篇文章。&lt;/p&gt;
&lt;h1 id=&quot;立意&quot;&gt;&lt;a href=&quot;#立意&quot; class=&quot;headerlink&quot; title=&quot;立意&quot;&gt;&lt;/a&gt;立</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：Wavelet-SRNet</title>
    <link href="http://example.com/2020/09/18/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AWavelet-SRNet/"/>
    <id>http://example.com/2020/09/18/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AWavelet-SRNet/</id>
    <published>2020-09-18T14:09:13.000Z</published>
    <updated>2020-10-12T06:26:27.387Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Wavelet-SRNet: A Wavelet-based CNN for Multi-scale Face Super Resolution</p></blockquote><p>2017年的一篇文章，核心论点是小波变换+CNN，针对的问题是人脸的SR。</p><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>文章提出，在对低分辨率图像做上采样的时候，传统CNN方法的问题：</p><ul><li><p>性能的降低；</p></li><li><p>输出过于平滑，导致了细节遗失（翻译成白话就是上采样的主观效果差）；</p></li></ul><blockquote><blockquote><p>When dealing with very low resolution (LR) images, the performance of these CNN based methods greatly degrades. Meanwhile, these methods tend to produce over-smoothed outputs and miss some textural details.</p></blockquote><blockquote><p>深度学习类的方法，导致过度平滑的原因：使用MSE作为loss。<br>Most of these convolutional methods use MSE loss to learn the map function of LR/HR image pairs, which leads to over-smooth outputs when the input resolution is very low and the magnification is large.</p></blockquote></blockquote><ul><li>低倍数下才有良好的效果。</li></ul><blockquote><p>Besides, they seem to only work well on limited up-scaling factors (2× or 4×) and degrades greatly when ultra-resolving a very small input (like 16 × 16 or smaller).</p></blockquote><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/1.png" title="Optional title"></p><p>HR图像在输入Wavelet-SRNet之前首先会做小波包分解：通过小波包分解将图像解析为一组具有相同大小的小波系数：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/2.png" title="Optional title"></p><p>Wavelet-SRNet选用最简单的小波：haar小波，此小波足以描述不同频率的人脸信息。另外，使用快速小波变换（2-D fast wavelet transform ，FWT）来计算haar小波：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/3.png" title="Optional title"></p><p>Wavelet-SRNet分为三个子网：</p><h2 id="embedding"><a href="#embedding" class="headerlink" title="embedding"></a>embedding</h2><p><strong>embedding网络的作用是将低分辨率输入表示为一组特征地图。</strong></p><p>嵌入网络以<code>3×h×w</code>的低分辨率图像为输入，将其表示为一组特征映射。所有卷积的大小都是相同的3×3，步长为1，pad为1，使得嵌入网络中的每个特征图与输入图像的大小相同。</p><p>通过embedding网络，将输入的LR图像映射成<code>Ne×h×w</code>的特征映射，不需要上采样或下采样，其中Ne是最后一层的信道尺寸。</p><h2 id="wavelet-prediction"><a href="#wavelet-prediction" class="headerlink" title="wavelet prediction"></a>wavelet prediction</h2><p><strong>经过embedding网络之后，利用小波预测网对相应的小波系数图像进行估计。</strong></p><p>将小波包分解到小波网络的每一个子网中，并将小波包分解成相应的小波网络。这里将所有3×3的卷积填充物设置为步长为1、pad为1的卷积（与embedding网络相似），使得每个推导出的小波系数与LR输入的大小相同，即<code>3×h×w</code>。</p><p>此外，由于Haar小波变换系数之间的高度独立性，<strong>每两个子网之间不允许任何信息交互，这使得我们的网络具有可扩展性。</strong></p><blockquote><p>可扩展性：wavelet prediction net中的子网数量Nw可根据需求进行调整。</p></blockquote><p>在预测网中，用不同的子网数目很容易实现不同的放大倍数。例如，Nw=16和Nw=64分别代表4倍和8倍的放大倍数。</p><h2 id="reconstruction-networks。"><a href="#reconstruction-networks。" class="headerlink" title="reconstruction networks。"></a>reconstruction networks。</h2><p><strong>最后，重建网络从这些系数图像重建出高分辨率图像。</strong></p><p>利用重构网络将总尺寸为<code>Nw×3×h×w</code>的小波图像变换成<code>3×（r×h）×（r×w）</code>的原始图像空间。由一个填充尺寸为r×r，步长为r的解卷积层（deconvolution）组成。</p><p>虽然解卷积层的大小取决于放大倍数r，但它可以由一个常数小波重构矩阵初始化并在训练中固定。因此，它对整个网络的可扩展性没有影响。</p><p>最后，三个子网络之间的输入输出映射关系：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/4.png" title="Optional title"></p><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>整个loss函数分为三部分：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/5.png" title="Optional title"></p><h2 id="full-image-loss（MSE）"><a href="#full-image-loss（MSE）" class="headerlink" title="full-image loss（MSE）"></a>full-image loss（MSE）</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/6.png" title="Optional title"></p><p>MSE是超分辨率方法中最常使用MSE损失函数。</p><p>MSE是在图像空间上的一个限制，<strong>因为MSE几乎不能获取到高频纹理细节信息。</strong>Wavelet-SRNet的full-image loss一方面是在图像空间上的限制，另一方面也能在平滑度和纹理细节上达到一个平衡。</p><blockquote><p>A traditional MSE loss in image space, which is called full-image loss, is also used to get a balance between s- moothness and textures. </p></blockquote><h2 id="wavelet-based-loss"><a href="#wavelet-based-loss" class="headerlink" title="wavelet-based loss"></a>wavelet-based loss</h2><p>这里同样分为两部分：</p><h3 id="wavelet-prediction-loss"><a href="#wavelet-prediction-loss" class="headerlink" title="wavelet prediction loss"></a>wavelet prediction loss</h3><p>wavelet prediction loss相当于在小波域上的加权MSE，定义如下：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/7.png" title="Optional title"></p><p>W=(λ1,λ2,⋅⋅⋅，λNw)，一个平衡不同组小波系数重要性的权重矩阵；C是真实值，C尖是小波系数；绝对值的项用来提取全局拓扑信息</p><p>这个loss函数应该更关注局部纹理信息，故高频系数的权重应该更大一点。</p><blockquote><p>The former one is a weight- ed version of MSE in wavelet domain…… More attention can be paid on local textures with bigger weights appointed to high-frequency coefficients.</p></blockquote><h3 id="texture-loss"><a href="#texture-loss" class="headerlink" title="texture loss"></a>texture loss</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/8.png" title="Optional title"></p><p>texture loss是为了避免高频小波系数收敛为0。α和ϵ是slack values，可以使高频小波系数不为0，因此避免了纹理细节的下降。</p><blockquote><p>The texture loss is designed to prevent high-frequency wavelet coefficients from converging to zero….. It keeps high-frequency wavelet coefficients non-zero and hence prevents the degradation of texture details.</p></blockquote><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h2><p>输入的LR图片做了一组对比：一组是双三次插值，一组是小波包变换的近似系数。</p><blockquote><p>Two types of low-resolution images are taken as input, one of which is down-sampled by bicubic interpolation and the other is the approximation coefficient of wavelet packet decomposition.</p></blockquote><p>模型可适应任意大小的输入，因为没有全连接层。</p><blockquote><p>Since our network is a fully convolutional architecture without fully-connected layers, it can be applied to the input of arbitrary size.</p></blockquote><p>给出了网络的设置。</p><blockquote><p>Our model is implemented with the Caffe frame- work [11]. The loss in (6) is minimized using SGD with a batch size of 256. For the hyper-parameters, we set em- piricallyλ1 = 0.01,λ2 = λ3 = ··· = λNw = 1,μ = 1,k=2,γk =γk+1 =···=γNw =1,ν=0.1. The learning rate is set to 0.01 initially and reduced by a factor of 10 each 3000 iterations. It takes about 14, 000 ∼ 16, 000 iterations for our network to converge.</p></blockquote><p>两个数据集做训练。</p><blockquote><p>Our experiments are implemented on two datasets: CelebA and Helen. There are 202,599 faces in CelebA and 2,230 faces in Helen. In the training phase, we use the large train set of CelebA(162,700 images) for training and the validation set(19,867 images) of CelebA for validation. In the testing phase, we evaluate with the 19,962-image test set of CelebA and the 330-image test set of Helen, assuring no over-lapped images appearing in both the training and testing phase. The images are cropped around the face with eyes aligned horizontally.</p></blockquote><p>为了保证对照训练的公平，所有的数据集都使用了眼睛对齐的人脸照片，且在下采样之前没有做其他额外的处理。</p><blockquote><p>For a fair comparison, we use the same set of eyes-aligned faces for all the methods with no extra preprocessing before down-sampling.</p></blockquote><p>使用的评价指标：SSIM和PSNR。</p><blockquote><p>We adopt PSNR(dB) and SSIM for quantitative metric, and calculate PSNR on the luminance channel, following by [35], and SSIM on the three channels of RGB.</p></blockquote><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="不同放大倍数的定性比较"><a href="#不同放大倍数的定性比较" class="headerlink" title="不同放大倍数的定性比较"></a>不同放大倍数的定性比较</h3><p>与双三次插值对比了不同放大倍数下的效果。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/9.png" title="Optional title"></p><h3 id="不同方法的定性比较"><a href="#不同方法的定性比较" class="headerlink" title="不同方法的定性比较"></a>不同方法的定性比较</h3><p>8倍放大下不同方法的对比。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/10.png" title="Optional title"></p><h3 id="鲁棒性讨论"><a href="#鲁棒性讨论" class="headerlink" title="鲁棒性讨论"></a>鲁棒性讨论</h3><p>讨论了模型在面对未知高斯模糊，姿态和遮挡的鲁棒性。</p><p>在图6中，低分辨率人脸由高斯模糊核生成，其步长为8，对应于8×下采样。高斯模糊核的σ从0增加到6，其中σ=0表示最近邻插值下采样。如图6所示，当σ&lt;4时，模型方法显示出一定的鲁棒性，当σ&gt;=4时生成更平滑的面。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/11.png" title="Optional title"></p><p>作为比较，CBN的结果与平均面更接近。对于姿态变化，如图7所示，CBN无法重建大姿态的可信人脸，可能是由于空间预测不准确。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/12.png" title="Optional title"></p><h3 id="不同放大倍数的定量比较"><a href="#不同放大倍数的定量比较" class="headerlink" title="不同放大倍数的定量比较"></a>不同放大倍数的定量比较</h3><p>四个放大倍数：(32×32,4×)，(16×16,8×)，(8×8,8×)以及(8×8,16×)，其中（m×m，n×）为m×m输入分辨率，放大倍数为n。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/13.png" title="Optional title"></p><h1 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h1><p>来自原文的叙述：</p><ul><li><ol><li>A novel wavelet-based approach is proposed for <code>CNN-based face SR problem</code>. To the best of our knowledge, this is <code>the first</code> attempt to transform single image SR to wavelet coefficients prediction task in deep learning framework - albeit many wavelet-based researches exist for SR.</li></ol></li><li><ol start="2"><li>A flexible and extensible fully convolutional neural network is presented to make the best use of wavelet transform. It can apply to different input-resolution faces with multiple upscaling factors.</li></ol></li><li><ol start="3"><li>We qualitatively and quantitatively explore multiscale face super-resolution, especially on <code>very low input resolutions</code>. Experimental results show that our proposed approach outperforms state-of-the-art face SR methods.</li></ol></li></ul><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>code：<a href="https://github.com/hhb072/WaveletSRNet">https://github.com/hhb072/WaveletSRNet</a></p></li><li><p>文章：<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper.pdf">http://openaccess.thecvf.com/content_ICCV_2017/papers/Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper.pdf</a></p></li></ul><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><ul><li><p>第一次用了小波包变换和深度的结合在这个问题上，有开创意义。</p></li><li><p>放大倍数方面，文章给出了32，对模型的描述是：可以接受任何大小的数据以及可拓展性。意思就是说放大的倍数也是比较有前景的（如果效果真的像叙述的一样有力的话）。</p></li><li><p>做的问题是有针对性的，就是正面、无任何遮挡或者偏移的人脸。</p></li><li><p>后面在做鲁棒性的讨论时，也凸显出了模型的局限性（实际上就是给出了大家都无法解决的问题）。</p></li></ul><hr><h1 id="小波变换"><a href="#小波变换" class="headerlink" title="小波变换"></a>小波变换</h1><p>一种进行信号时频分析和处理的方法。</p><blockquote><p>理论依据：傅里叶变换 -&gt; 短时傅里叶变换 -&gt; 小波变换<br>这三种变换的本质是将信号从时域转换为频域。</p><p>上面的论文里用到的：小波包变换，由小波变换而来</p></blockquote><h2 id="傅立叶变换（FT）"><a href="#傅立叶变换（FT）" class="headerlink" title="傅立叶变换（FT）"></a>傅立叶变换（FT）</h2><p>傅里叶变换的核心是<strong>从时域到频域的变换，而这种变换是通过一组特殊的正交基来实现的</strong>。</p><blockquote><p>如何理解傅里叶变换公式？ - 苗华栋的回答 - 知乎<br><a href="https://www.zhihu.com/question/19714540/answer/1119070975">https://www.zhihu.com/question/19714540/answer/1119070975</a></p></blockquote><p>傅立叶变换的三个核心：</p><ul><li><p>傅立叶的重要论断：<strong>任何连续周期信号都可以由一组适当的正弦曲线组合而成。</strong></p></li><li><p>为什么要<strong>变换</strong>？因为最容易理解的表现形式不一定是最方便做计算的。变换（计算空间）的意义是将原来空间中难以处理的问题变换到方便计算的空间去。</p></li><li><p>变换的<strong>空间</strong>是什么？时域和频域。</p></li></ul><h2 id="短时傅立叶变换（STFT）"><a href="#短时傅立叶变换（STFT）" class="headerlink" title="短时傅立叶变换（STFT）"></a>短时傅立叶变换（STFT）</h2><h3 id="傅立叶变换的缺陷"><a href="#傅立叶变换的缺陷" class="headerlink" title="傅立叶变换的缺陷"></a>傅立叶变换的缺陷</h3><p>在傅立叶变换的基础上提出新的讨论，是因为傅立叶变换是有缺陷的：<strong>对非平稳过程，傅里叶变换有局限性。</strong></p><p>再进一步的说明是，傅里叶变换处理<strong>非平稳信号</strong>有天生缺陷。它只能获取一段信号总体上包含哪些频率的成分，但是对各成分出现的时刻并无所知。因此时域相差很大的两个信号，可能频谱图一样。</p><p>然而平稳信号大多是人为制造出来的，自然界的大量信号几乎都是非平稳的。<strong>对于这样的非平稳信号，只知道包含哪些频率成分是不够的，我们还想知道各个成分出现的时间。</strong>理解信号频率随时间变化的情况，各个时刻的瞬时频率及其幅值，也就是时频分析。</p><h3 id="加窗"><a href="#加窗" class="headerlink" title="加窗"></a>加窗</h3><p>想到知道频率随时间变化的情况，一个简单的思想就是“加窗”：<strong>把整个时域过程分解成无数个等长的小过程，每个小过程近似平稳，再傅里叶变换，就知道在哪个时间点上出现了什么频率了。</strong></p><p>短时傅立叶变换的基本思想由此而来：将信号加滑动时间窗，并对窗内信号做傅立叶变换，得到信号的时变频谱。</p><h2 id="小波变换（WT）"><a href="#小波变换（WT）" class="headerlink" title="小波变换（WT）"></a>小波变换（WT）</h2><h3 id="短时傅立叶变换的缺陷"><a href="#短时傅立叶变换的缺陷" class="headerlink" title="短时傅立叶变换的缺陷"></a>短时傅立叶变换的缺陷</h3><p>同上，新的讨论一定是因为前者有缺陷：提到了加窗，那么问题就是，这个“窗函数”如何定义？窗太宽太窄都会出现问题：<strong>窗太窄，窗内的信号太短，会导致频率分析不够精准，频率分辨率差。窗太宽，时域上又不够精细，时间分辨率低。</strong></p><p>所以窄窗口时间分辨率高、频率分辨率低，宽窗口时间分辨率低、频率分辨率高。对于时变的非稳态信号，高频适合小窗口，低频适合大窗口。<strong>然而STFT的窗口是固定的，在一次STFT中宽度不会变化，所以STFT还是无法满足非稳态信号变化的频率的需求。</strong></p><h3 id="可变窗的STFT"><a href="#可变窗的STFT" class="headerlink" title="可变窗的STFT"></a>可变窗的STFT</h3><p>一种解决上述问题的方法是：让窗口大小变起来，多做几次STFT不就可以了吗？</p><p>小波变换就有着这样的思路，但事实上小波并不是这么做的。至于为什么不采用可变窗的STFT呢，有很多解释，公认的缺陷是这样做的话STFT做不到正交化。</p><blockquote><p>大部分的解释里有这样的说法：小波变换能够提供一个随频率改变的“时间-频率”窗口。这也是一种理解性的说法，通过改变基底做到了可变窗，重点应该是基底的改变。</p></blockquote><h3 id="改变基底"><a href="#改变基底" class="headerlink" title="改变基底"></a>改变基底</h3><p>小波变换的出发点和STFT还是不同的：直接把傅里叶变换的基底做了改变，<strong>将无限长的三角函数基换成了有限长的会衰减的小波基。</strong></p><p><strong>小波变换就是为了解决对非平稳信号的分解问题而产生的数学方法。</strong>相比于傅里叶变换使用一组无限长的三角函数基进行信号拟合，小波变换使用的是一组正交的、迅速衰减的小波函数基进行信号拟合。这种小波函数基可通过其尺度变量和平移变量，获得不同的频率和时间位置。因此在利用这种小波函数基对信号进行分解时，可以用较少的小波函数基就拟合出突变信号（稀疏编码特性），同时也能获得不同频率的信号分量在时域上的出现位置。</p><h3 id="WT的总体理解"><a href="#WT的总体理解" class="headerlink" title="WT的总体理解"></a>WT的总体理解</h3><p>小波变换是这样一个过程：首先将原始信号作为输入信号，通过一组正交的小波基分解成高频部分和低频部分，然后将得到的低频部分作为输入信号，又进行小波分解，得到下一级的高频部分和低频部分，以此类推。随着小波分解的级数增加，其在频域上的分辨率就越高。这就是多分辨率分析（MRA，MultiResolution Analysis）。</p><p>从数学的角度理解，<strong>在小波变换中，一个位于希尔伯特空间中的函数，可以分解成一个尺度函数和一个小波函数，其中尺度函数对应原始函数中的低频部分，小波函数对应原始函数中的高频部分。</strong>通过尺度函数可以构建对原始信号的低通滤波器，通过小波函数可以构建对原始信号的高通滤波器。</p><blockquote><p>希尔伯特空间：完备的内积空间。</p><p>Linear Space → Normal Linear Space → Banach Space</p><p>Normal Linear Space → Inner Product Space<br>→ Banach Space</p><p>Inner Product Space → Euclid Space<br>→ Hilbert Space</p></blockquote><p>从信号处理的角度理解，在小波变换中，信号可通过信号滤波器分解为高频分量（高频子带）和低频分量（低频子带），高频子带又称为细节（detailed）子带，低频子带又称为近似（approximate）子带。细节子带是由输入信号通过高通滤波器后再进行下采样得到的，近似子带是由输入信号通过低通滤波器后再进行下采样得到的。</p><blockquote><p>小波变换的内容很多，一下两下真的说不清楚（想说清楚我也看不懂），但基本原理就是这些。随着应用的深入在做学习比较靠谱。</p></blockquote><h3 id="二进小波变换（DWT）"><a href="#二进小波变换（DWT）" class="headerlink" title="二进小波变换（DWT）"></a>二进小波变换（DWT）</h3><p>在小波变换中，若对尺度按幂级数作离散化，同时对平移保持连续变化，则此类小波变换称为二进小波变换（Dyadic Wavelet Transform）。</p><h3 id="紧支撑小波基"><a href="#紧支撑小波基" class="headerlink" title="紧支撑小波基"></a>紧支撑小波基</h3><p>在小波变换中，紧支撑小波基是性质较好的一类小波基，紧支撑（Compact Support）函数是指这样的一类函数：其自变量仅在0附近的取值范围内能得到非零函数值，而在其他区间取值，则得到的函数值全为零。能得到非零函数值的自变量取值区间被称为该函数的支撑区间。</p><p>一个函数的支撑区间长度主要由其尺度参数决定。支撑区间越大，计算复杂度越高，边界拖尾效应越明显。不仅如此，支撑区间越大，会产生更多的高幅值小波系数，关于这个结论的解释，可参考傅里叶变换使用无限长（支撑区间大）的三角函数基进行信号拟合的情况，相比于使用信号迅速衰减（支撑区间小）的小波基，三角函数基拟合信号时需要更多的数量。因此在选择小波基时，以支撑长度较短的小波基为宜。</p><p>此外，小波基的正交性也是一类重要的性质，它确保了信号的分解没有冗余（最优分解）。</p><h2 id="小波包变换（WPT）"><a href="#小波包变换（WPT）" class="headerlink" title="小波包变换（WPT）"></a>小波包变换（WPT）</h2><h3 id="小波变换的缺陷"><a href="#小波变换的缺陷" class="headerlink" title="小波变换的缺陷"></a>小波变换的缺陷</h3><p>同上，小波变换仍存在着不足之处，<strong>由于正交小波变换只对信号的低频部分做进一步分解，而对高频部分也即信号的细节部分不再继续分解</strong>，所以小波变换能够很好地表征一大类以低频信息为主要成分的信号，但它不能很好地分解和表示包含大量细节信息（细小边缘或纹理）的信号，如非平稳机械振动信号、遥感图象、地震信号和生物医学信号等。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>小波包分解（Wavelet Packet Decomposition），又称为最优子带树结构（Optimal Subband Tree Structuring）是对小波变换的进一步优化。</p><p><strong>主要思想：在小波变换的基础上，在每一级信号分解时，除了对低频子带进行进一步分解，也对高频子带进行进一步分解。最后通过最小化一个代价函数，计算出最优的信号分解路径，并以此分解路径对原始信号进行分解。</strong></p><p>小波包变换可以对高频部分提供更精细的分解，而且这种分解既无冗余，也无疏漏，所以对包含大量中、高频信息的信号能够进行更好的时频局部化分析。</p><p>从函数理论的角度来看，小波包分解是将信号投影到小波包基函数张成的空间中。从信号处理的角度来看，它是让信号通过一系列中心频率不同但带宽相同的滤波器。</p><h2 id="小波函数基"><a href="#小波函数基" class="headerlink" title="小波函数基"></a>小波函数基</h2><p>无论是小波变换还是小波包变化，一个重要的环节是选择合适的小波函数基进行信号分解。</p><p>在选择合适的小波函数基时，需要考虑的因素</p><ul><li><p>小波的对称性：主要体现在保证信号重构时不会产生相位畸变，即是不会产生重构信号的相位失真。</p></li><li><p>小波的正则性：保证了信号的光滑和可微性，对于大部分小波而言（非全部），其与消失矩存在关系：小波的消失矩越大，正则性也就越大。</p></li><li><p>选择与输入信号的波形相似性高的小波：意义在于使数据压缩和降噪变得更容易（信号的拟合和分解都更容易）。</p></li></ul><p>几种常用的小波函数基：</p><h3 id="Haar小波"><a href="#Haar小波" class="headerlink" title="Haar小波"></a>Haar小波</h3><p>最简单的一个小波函数，其具有紧支撑性和正交性，函数图像为在支撑区间[0,1)上的单个矩形波。Haar小波在时域上不连续，作为基本小波时性能不是很好。</p><h3 id="Daubechies小波"><a href="#Daubechies小波" class="headerlink" title="Daubechies小波"></a>Daubechies小波</h3><h3 id="Biorthogonal小波"><a href="#Biorthogonal小波" class="headerlink" title="Biorthogonal小波"></a>Biorthogonal小波</h3><h3 id="Symlets小波"><a href="#Symlets小波" class="headerlink" title="Symlets小波"></a>Symlets小波</h3><h3 id="Mexican-Hat小波"><a href="#Mexican-Hat小波" class="headerlink" title="Mexican Hat小波"></a>Mexican Hat小波</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Wavelet-SRNet: A Wavelet-based CNN for Multi-scale Face Super Resolu</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>博客网站的建设问题集合</title>
    <link href="http://example.com/2020/09/11/%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99%E7%9A%84%E5%BB%BA%E8%AE%BE%E9%97%AE%E9%A2%98%E9%9B%86%E5%90%88/"/>
    <id>http://example.com/2020/09/11/%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99%E7%9A%84%E5%BB%BA%E8%AE%BE%E9%97%AE%E9%A2%98%E9%9B%86%E5%90%88/</id>
    <published>2020-09-11T12:47:24.000Z</published>
    <updated>2020-11-16T05:46:31.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>19年3月份开始了解并使用hexo环境下搭建的博客，当时的搭建过程是极其顺利的。所以在建设初期并没有想到要做这样的记录。</p><p>直到4天前，也就是9月7日，我在对上一篇，关于视频摘要的博客进行更新时，试图上传一个比较大的文件到git上时出现了问题，导致了我的博客无法再进行更新。</p><p>在我进行了一系列垂死挣扎的操作之后，选择了重装整个hexo环境以及按照原来的模样重新搭建博客。整个过程下来，我真的是心力憔悴，也遇到了许多新的问题。我突然觉得，有必要将这些问题做个记录。一是为了以后查自己找方便，二是能为别人做个参考，三是希望能从这些问题中，学习或者领悟到一些当程序员的必要的品质。毕竟对于程序员来说，最重要的部分就是查bug和调bug，其余的时间也说不定都是在写bug。</p><p><strong>这里将记录本博客建设过程中遇到的一切有意义的问题。</strong>遇到问题就会记录或对相应的部分做更新，没有什么上下文关联。</p><p><strong>时刻牢记：遇到bug，baidu、google以及Official documents才是正道，和大佬交流会有奇效。</strong></p><hr><h1 id="hexo博客的搭建和重新部署问题"><a href="#hexo博客的搭建和重新部署问题" class="headerlink" title="hexo博客的搭建和重新部署问题"></a>hexo博客的搭建和重新部署问题</h1><h2 id="从零开始搭建hexo博客"><a href="#从零开始搭建hexo博客" class="headerlink" title="从零开始搭建hexo博客"></a>从零开始搭建hexo博客</h2><p>这个话题在网上做一下搜索，写的人很多，不做赘述。</p><p>唯一需要注意的就是，不同操作系统环境下可能会有一些小的差别，但是总的步骤基本都是一致的。无非就是本地环境安装以及github的仓库设置，最后再学一下在terminal上敲hexo命令就齐活了。</p><p>我的本地环境是macOS，类UNIX的系统中下布置hexo环境还是十分友好的。<del>Win：？</del></p><blockquote><p>Mac 系统下搭建hexo个人博客：<a href="https://www.jianshu.com/p/77db3862595c">https://www.jianshu.com/p/77db3862595c</a></p></blockquote><h2 id="重新部署hexo博客"><a href="#重新部署hexo博客" class="headerlink" title="重新部署hexo博客"></a>重新部署hexo博客</h2><p>这里的“重新部署”，包括以下的情况：</p><ul><li><p>博客本身是没有问题的，只是想换一个新的环境去部署。比如说更换了个人PC等情况。</p></li><li><p>博客的建设方面出现了问题，包括但不限于本地环境的更改、提交新版本时的出现的不可逆的问题（基本上都是可逆的，不可逆都是因为个人能力问题<del>问就是菜</del>）等。</p></li></ul><p>对于第一种情况，略微做一下网络搜索，也有很多人写，而且基本和从零开始搭建一样，只是中间个别重复的步骤不用再执行一遍而已，比如git仓库的设置。</p><p>第二种情况就比较有说法了（我就是那个发现了bug之后又发现自己特别菜然后重新布置了整个博客的人）：</p><h3 id="Node-js和npm的版本"><a href="#Node-js和npm的版本" class="headerlink" title="Node.js和npm的版本"></a>Node.js和npm的版本</h3><blockquote><p>Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Coding上，是搭建博客的首选框架。</p></blockquote><p>hexo需要Node.js支持。布置hexo环境时需要首先安装Node.js。</p><p>如果是初次安装的话，直接Node.js安装最新版本即可。通常来说，nmp会跟随着Node.js一起安装。</p><p>但如果本地环境在hexo部署前已经有Node.js的环境，这时就要注意了：<strong>hexo会随着其依赖项的更新而更新。如果本地的Node.js版本太久远，一定要记得更新。</strong>截止到写作时，目前官方文档给出的建议是：Node.js 版本需不低于 10.13，建议使用 Node.js 12.0 及以上版本。</p><p>还需要注意的是，<strong>Node.js做更新时，需要手动对nmp单独做更新</strong>。</p><blockquote><p>node版本更新：<a href="https://www.jianshu.com/p/0a247218b486">https://www.jianshu.com/p/0a247218b486</a></p></blockquote><h3 id="启动页面显示extends-includes-layout-pug等"><a href="#启动页面显示extends-includes-layout-pug等" class="headerlink" title="启动页面显示extends includes/layout.pug等"></a>启动页面显示extends includes/layout.pug等</h3><p>重新布置完成后，启动界面显示如下信息：</p><pre><code>extends includes/layout.pug block content include includes/recent-posts.pug include includes/partial</code></pre><p>这也明显是依赖项问题，执行如下命令即可。</p><pre><code>npm install --save hexo-renderer-jade hexo-generator-feed hexo-generator-sitemap hexo-browsersync hexo-generator-archive</code></pre><p>上述命令会有各种各样的Warning，比较显眼的一个是：<strong>jade做了rename</strong>，请安装rename后的包。我是按照它的提示做了新的安装，但是并不清楚不安装这个新的包会有什么问题。</p><p>上述命令运行时，如果提示权限不足，命令中带上<code>sudo</code>；如果npm在安装包时提示<code>rollbackFailedOptional</code>，可以简单的理解为网络不好，改用淘宝的npm镜像即可：</p><pre><code>npm config set disturl https://npm.taobao.org/dist</code></pre><h3 id="用于提交git的ssh"><a href="#用于提交git的ssh" class="headerlink" title="用于提交git的ssh"></a>用于提交git的ssh</h3><p>如果之前使用过ssh的话，直接在本地文件中找到ssh的文件，复制内容到GitHub上即可。<strong>ssh可以重复使用，即使更换了新的仓库也无所谓。</strong> <del>不用ssh，不嫌麻烦的话每次更新输入账号密码也行</del></p><hr><h1 id="向博客中提交大文件"><a href="#向博客中提交大文件" class="headerlink" title="向博客中提交大文件"></a>向博客中提交大文件</h1><p>这次博客的奔溃就是因为这个原因。我在github上hexo项目的issue中有提问，叙述了这次经历。</p><blockquote><p>Error uploading large file #4523：<a href="https://github.com/hexojs/hexo/issues/4523">https://github.com/hexojs/hexo/issues/4523</a></p></blockquote><p>有一位大佬给出了一个解答，并且得到了开发者的赞同，然后关闭了问题：</p><p><img src="/images/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/1.png" title="Optional title"></p><p><img src="/images/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/2.png" title="Optional title"></p><p>hexo的命令中，<code>hexo d</code>的作用是<strong>将blog本地路径中生成的public提交到git的仓库中</strong>。一看这个解答，我也觉得很有道理：无非就是解决提交版本的问题嘛！</p><p><strong>但是我犯了一个致命的错误：提交git的路径。</strong>实际上我在第一天还在迷糊的时候就注意到了这一点，并隐隐觉得这是问题的根本：我要是在blog的子目录下进行这个工作，不就把blog的里整个内容提交进去了吗？但实际上git仓库里只有public中的文件啊！</p><p>于是，我找到了回答我问题大佬的主页，发现了他的blog地址，并在他的blog中寻找到了他的邮箱，然后发邮件进行询问。出现了以下的讨论：</p><p><img src="/images/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/3.png" title="Optional title"></p><p><img src="/images/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/4.png" title="Optional title"></p><p><img src="/images/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/5.png" title="Optional title"></p><p>是的，当我重新布置了整个环境之后大佬才给出了回复。属实无奈，我并没有办法实践这位大佬给出的方法<del>再作一遍死就行</del>，不过这应该就是终极的解决方法。非常感谢这位大佬。</p><p>实际上macOS会自动的隐藏一些配置类文件（我的为数不多的实践和记忆告诉我，大概Win10也会），如果不手动去显示隐藏文件的话，不会看到上述邮件中的提到的<code>.depoy.git</code>目录。实际上在此之前我也查看过隐藏文件，只是并没有察觉到是这里的问题。</p><p><code>cmd+shift+.</code>，以及<code>git</code>的一整套命令，这次牢记。</p><p><img src="/images/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/6.png" title="Optional title"></p><hr><h1 id="theme中的问题"><a href="#theme中的问题" class="headerlink" title="theme中的问题"></a>theme中的问题</h1><p>theme中的问题主要体现在网页的显示上。对网页效果展示有强迫症或者力求美观，这些问题会凸显出来。</p><h2 id="Cxo"><a href="#Cxo" class="headerlink" title="Cxo"></a>Cxo</h2><p>截止到本篇发布的时间，在用的theme是Cxo。在hexo.io的主题展示页面搜索Cxo就可找到。</p><blockquote><p><a href="https://github.com/Longlongyu/hexo-theme-Cxo">https://github.com/Longlongyu/hexo-theme-Cxo</a></p></blockquote><p>选择这个主题的原因是因为比较简单，符合审美。</p><p><del>因为头像是猫猫头，我永远喜欢猫猫头</del></p><p><strong>这个主题的作者2018年还有稀疏的commit，2019和2020完全没进行过更新，也就是说跑路了！由于hexo的各种依赖项是一直在更新的，所以现在还想用这个主题的话，会出现很多的问题。还想要用的同好们，慎选！</strong></p><p><del>独占猫猫头</del></p><h3 id="不蒜子访问计数显示"><a href="#不蒜子访问计数显示" class="headerlink" title="不蒜子访问计数显示"></a>不蒜子访问计数显示</h3><p>这个属于老生常谈，大部分19年以后不更新的主题都会遇到这个问题。具体原因是因为18年10月份以后不蒜子的域名更改了，导致script引用不了，从而无法进行统计。</p><p>不蒜子官网做了解释（注意下方小红字），以及新的写法：</p><p><img src="/images/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/7.png" title="Optional title"></p><p>具体做法：在下面的路径里按照官网提示修改就行。</p><pre><code>layout/includes/partial/head.pug</code></pre><h3 id="“翻页”箭头显示错误"><a href="#“翻页”箭头显示错误" class="headerlink" title="“翻页”箭头显示错误"></a>“翻页”箭头显示错误</h3><p>主页中换页处的下一页本该显示为“&gt;”，但是显示为“&amp;#62”。</p><p>更新本地环境后出现的问题，猜测也是由依赖项的更新所导致的。</p><p>已经有人提过issue且得到了可以解决问题的回复。实际上就是编码的问题。以及向前翻页的箭头的编码可以改成<code>&amp;lt</code>。</p><blockquote><p><a href="https://github.com/Longlongyu/hexo-theme-Cxo/issues/42">https://github.com/Longlongyu/hexo-theme-Cxo/issues/42</a></p></blockquote><h3 id="当前查看进度条不适用"><a href="#当前查看进度条不适用" class="headerlink" title="当前查看进度条不适用"></a>当前查看进度条不适用</h3><p>单篇博客写的太长的话，还没有拉到底部，进度条显示就已经满了。</p><p>19年就有这样的问题。</p><p>未解决。</p><h3 id="目录显示错误"><a href="#目录显示错误" class="headerlink" title="目录显示错误"></a>目录显示错误</h3><p>部分时候出现目录不随着显示的内容而变动，同时也有一些博客的目录是正常的。</p><p>更新本地环境后出现的问题，无法猜到到底是什么原因导致的。</p><p>未解决。</p><hr><blockquote><p>2020年11月12日更新</p></blockquote><h1 id="更新时出现SSH失效"><a href="#更新时出现SSH失效" class="headerlink" title="更新时出现SSH失效"></a>更新时出现SSH失效</h1><p>使用<code>hexo g -d</code>进行更新时，出现如下提示。</p><p><img src="/images/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/8.png" title="Optional title"></p><p>根据报错，是SSH密钥的访问权限问题。</p><p>以<code>This private key will be ignored</code>为关键词进行搜索，进行如下操作后（任意目录均可）恢复正常。</p><p><img src="/images/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/9.png" title="Optional title"></p><p>之前因为在根目录敲了一些不该敲的命令，重装了电脑的系统。猜测是因为<code>重装系统</code>导致的这个问题。</p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;19年3月份开始了解并使用h</summary>
      
    
    
    
    
    <category term="闲谈" scheme="http://example.com/tags/%E9%97%B2%E8%B0%88/"/>
    
    <category term="hexo博客建设" scheme="http://example.com/tags/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/"/>
    
  </entry>
  
  <entry>
    <title>基础：视频摘要</title>
    <link href="http://example.com/2020/09/03/%E5%9F%BA%E7%A1%80%EF%BC%9A%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/"/>
    <id>http://example.com/2020/09/03/%E5%9F%BA%E7%A1%80%EF%BC%9A%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/</id>
    <published>2020-09-03T10:53:58.000Z</published>
    <updated>2020-09-09T11:55:58.472Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><h2 id="视频摘要概念简介"><a href="#视频摘要概念简介" class="headerlink" title="视频摘要概念简介"></a>视频摘要概念简介</h2><blockquote><p>什么是视频摘要？为什么要研究这项技术？</p></blockquote><p>近年来，随着人们对社会公共安全的需求不断增长，各大公共场所都安装了大量的监控摄像头，用于提高安防系统的侦查和报警水平。大量的监控摄像头实时录制了海量的视频数据，<strong>但这些视频数据的利用率极低</strong>。因为监控探头在每一天的24小时不间断记录，而往往人们所需要的信息只是很短的一部分，通常都是在成百上千个小时的监控视频中寻找个位数分钟的有价值部分。所以，如何在海量的数字视频中准确地找到感兴趣的视频片段已成为行业的迫切需求与巨大挑战。</p><p><strong>视频摘要是通过分析视频的结构与内容，从原视频中提取出有意义的信息，并重新组合有意义信息，浓缩成可以充分表现视频语义内容的概要。</strong></p><p>视频摘要是用一段静态图像或动态视频序列的长视频内容的缩略总结，供用户进行浏览和分析。视频摘要技术可以按照用户需求，将几十小时视频浓缩为十几分钟甚至更少，大幅度缩短视频观看浏览的时间，再利用视频目标特征检索技术，对视频底层特征进行语义分析，辅助用户迅速定位感兴趣目标。视频摘要与检索技术可以辅助用户充分挖掘海量视频监控录像中的有意义信息，提高海量监控视频录像分析与响应效率。</p><h2 id="视频摘要技术分类"><a href="#视频摘要技术分类" class="headerlink" title="视频摘要技术分类"></a>视频摘要技术分类</h2><p>视频摘要技术在近20年内已被国内外的学者广泛关注，并取得了一定的研究成果。</p><p>按照摘要结果表现形式的不同，视频摘要可分为静态视频摘要与动态视频摘要。</p><p><strong>目前，监控领域的视频摘要技术主要有两种形式：静态视频摘要中的基于关键帧的摘要和动态视频摘要中的基于对象的视频摘要。二者都可以大大缩短视频长度，方便对视频的观看、分析和检索。</strong></p><blockquote><p>基于关键帧的视频摘要的最小单位是“帧”，存储空间较小且方便传输，但不能完整表示每个目标的运动轨迹，不利于视频目标检索。基于对象的视频摘要的最小单位是“对象”，可以最大限度的减少时间冗余信息与空间冗余信息，且为视频检索等上层开发提供对象结构，能在监控安防中快速响应紧急事件，定位到事件相关“对象”，但存在处理复杂，摘要生成困难的问题。</p></blockquote><h3 id="静态视频摘要"><a href="#静态视频摘要" class="headerlink" title="静态视频摘要"></a>静态视频摘要</h3><p>静态的视频摘要是以一系列从原始视频流中抽取出的<strong>静态语义单元</strong>来表现视频的内容。静态语义单元是如关键帧、标题、 幻灯片等可以概括表示视频镜头内容的静态特征信息。</p><blockquote><p>目前静态视频摘要研究主要是基于关键帧选取方法开展的，关键帧又称故事板，是从原始视频中提取的反映镜头中主要信息内容的一帧或多帧图像。通过多个关键帧组合成视频摘要，允许用户通过少量的关键帧快速浏览原始视频的内容，并提供快速检索。经典的关键帧选取的算法主要利用颜色、运动矢量等视觉特征去区分帧间的差异性。但差异性的区分计算依赖阈值的选择，选择过程计算量较大，实时性困难。</p></blockquote><p>基于关键帧的视频摘要具有结果简单，观看方便的优点。但由于其以静态图像为结果的表达形式，很难准确地表达视频的内在语义，且对“对象”动态特性的描述不全面，所以仅仅适用于视频的精彩瞬间生成，无法适应需要进行“对象”特性分析的场合。</p><h3 id="动态视频摘要"><a href="#动态视频摘要" class="headerlink" title="动态视频摘要"></a>动态视频摘要</h3><p>基于对象的视频摘要，是一种近些年来提出的主要用于监控视频领域的动态视频摘要技术，通过提取用户感兴趣对象，然后重组感兴趣对象得到视频摘要。</p><blockquote><p>对象的提取主要使用背景建模技术、运动目标检测与跟踪技术及视觉分析技术，对象的重组主要考虑用户关注度、压缩比、多视频融合等因素。</p></blockquote><p>这种方法能有效地保持视频内容随时间动态变化的特征，同时最大限度地减少时间-空间冗余，但存在对象提取困难及很难解决复杂场景下的视频摘要生成的问题。</p><p>实际上，可将静态视频技术中的提取关键帧的技术融入到动态视频摘要中：即直接提取视频中的关键帧合成新的视频。此类方法虽然也可以缩短视频的时长，但是合成后视频给人一种快进看电影的感觉，而且实际使用较少。</p><h2 id="视频摘要技术评估方法"><a href="#视频摘要技术评估方法" class="headerlink" title="视频摘要技术评估方法"></a>视频摘要技术评估方法</h2><p>视频摘要技术中最常用的评估方法是基于用户对视频内容的主观理解，让多个观众对视频中重要内容进行标注打分，综合所有人员的标注结果作为参考标准。然后，将视频摘要方法得到的视频摘要结果与参考标准进行比较，得到准确度来衡量视频摘要结果好坏。</p><p>在已有视频摘要研究中，最常用的评估方法是计算Precision、Recall和F-measure值，在有些方法中也会用到mAP（mean Average Precision）。</p><h1 id="部分视频摘要方法复现及评估"><a href="#部分视频摘要方法复现及评估" class="headerlink" title="部分视频摘要方法复现及评估"></a>部分视频摘要方法复现及评估</h1><h2 id="SeDMi"><a href="#SeDMi" class="headerlink" title="SeDMi"></a>SeDMi</h2><ul><li>论文：Video Synopsis based on a Sequential Distortion Minimization Method</li></ul><blockquote><p>4个引用。</p></blockquote><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/1.png" title="Optional title"></p><blockquote><p>Fig. 1 illustrates a scheme of the proposed system architecture. The proposed<br>method can be divided into several steps. Initially, we estimate the CLD for each frame of the original video. Next, we performed shot detection (see Section 3.1). Based on the shot detection results and to the given parameter α we estimate the number of frames per shot that the video synopsis (see Section 3.1). Finally, the video distortion is sequentially minimized according to the proposed methods resulting to the video synopsis (see Section 3.2).</p></blockquote><ul><li>代码：官方提供的代码为matlab版本。</li></ul><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/2.png" title="Optional title"></p><ul><li>示例视频：foreman.avi。</li></ul><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/3.png" title="Optional title"></p><ul><li><p>实验：<strong>最主要的参数为“关键帧抽取百分比”</strong>，论文中给出的结果里，这项参数设置为百分之十。修改为0.2、0.05以及0.02分别运行。</p></li><li><p>结果：在上述4组关键帧抽取百分比下运行，代码运行的时常都不超过10s。可在此处下载结果：<a href="/download/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/SeDMi.zip">SeDMi在foreman下的四组结果视频</a> 。</p></li><li><p>观感：<strong>非常像快进</strong>，而且foreman.avi本身就很短，看不太出效果来。</p></li><li><p>注意：后期版本的Matlab（2018a）或者是Mac环境下，使用VideoReader函数读取视频文件时，不支持.avi格式，需要进行格式转换（大概率是Mac环境不兼容，上面的实验都将avi转为mp4后才不会报错）。</p></li></ul><h2 id="Web-based-video-synopsis-system-using-Apache-Spark"><a href="#Web-based-video-synopsis-system-using-Apache-Spark" class="headerlink" title="Web-based video synopsis system using Apache Spark"></a>Web-based video synopsis system using Apache Spark</h2><ul><li>简介：基于Apache Spark的视频概要系统，分为前端页面和后台算法处理。</li></ul><blockquote><p>The project proposes a system that can be divided into two parts: the frontend web pages and the backstage processing algorithm based on Apache Spark. In the web pages, users can upload as well as download video clip. This project designs and implements a synopsis algorithm in the backstage which uses background subtraction to grab all the frames containing moving targets in a video and then concatenate all the frames into a new shorter clip. The algorithm connects to Spark cluster. After users upload a video to be condensed on web page, the algorithm will process it and return success information to users after generating the new video. Then users can download the new video from the download page.</p></blockquote><ul><li>代码：<a href="https://github.com/question0914/VideoSynopsis">https://github.com/question0914/VideoSynopsis</a></li></ul><blockquote><p>5个星标，用的人不是很多。</p></blockquote><ul><li>运行：clone下来之后本地运行网页，给提交视频的按钮过不去：会提示无法找到h5代码中该处的form标签下的/video/upload，所以就无法验证运行结果。</li></ul><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/4.png" title="Optional title"></p><ul><li>其他：有前端的话感觉比较容易实现和看结果，这一项比较希望能复现。后端代码用的是java。</li></ul><h2 id="DimensionFour"><a href="#DimensionFour" class="headerlink" title="DimensionFour"></a>DimensionFour</h2><blockquote><p>这个模型没有打底的论文，不清楚为什么叫这个名字。</p></blockquote><ul><li>简介：一个git上的开源工具，用到了Keras YOLOv3做目标识别，用到了ImageAI（猜测是用来做遮挡物后的人的运动预测），然后做动态物体的标注，可能会有合成的操作（后面会根据运行的结果提到）。</li></ul><blockquote><p>YOLOv3：</p><blockquote><p><a href="https://github.com/qqwweee/keras-yolo3">https://github.com/qqwweee/keras-yolo3</a><br><a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf">https://pjreddie.com/media/files/papers/YOLOv3.pdf</a></p></blockquote></blockquote><ul><li>代码：<a href="https://github.com/kevinvincent/DimensionFour">https://github.com/kevinvincent/DimensionFour</a></li></ul><blockquote><p>8个星标，用的人不是很多。</p></blockquote><ul><li>示例视频：项目提供了dataset，共有三个视频，根据长度的不同命名为long、preferred以及short，都是街头监控探头拍下的画面，主要的对象为移动的人：</li></ul><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/5.png" title="Optional title"></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/6.png" title="Optional title"></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/7.png" title="Optional title"></p><ul><li><p>实验：先要对视频进行预处理，然后对提取出来的视频帧做集合和标注。</p></li><li><p>结果：在本机的cpu版本的tf下运行，最长的视频在1h内处理完成，最短的视频处理时间不到10min。但是经过处理后的视频的大小会有非常明显的变大。视频格式由.mp4转为.avi。以下给出的是long的展示：</p></li></ul><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/8.png" title="Optional title"></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/9.png" title="Optional title"></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/10.png" title="Optional title"></p><ul><li><p>观感：整体效果而言，对每个移动的人的标注做的很不错。视频有拼接的痕迹，不是很明显：移动到画面边缘的人会直接消失；在遮挡物背后的人的有些画面的处理会变得模糊。但是缩放比例不是特别出色，5min的long.mp4处理后也有4min，但各个视频的缩放比例确实不一样，推测是根据具体的场景来做的，不是按比例抽帧。</p></li><li><p>注意：tensorflow==1.7.0、keras==2.1.6、Opencv、h5py、ImageAI，以及下载需要yolo.h5（代码中会检测，没有这个模型文件的话会自动下载，需要科学上网，238m，早期版本的yolo文件不适用）。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;h2 id=&quot;视频摘要概念简介&quot;</summary>
      
    
    
    
    
    <category term="视频摘要" scheme="http://example.com/tags/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/"/>
    
    <category term="基础知识" scheme="http://example.com/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：SRGAN</title>
    <link href="http://example.com/2020/07/18/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASRGAN/"/>
    <id>http://example.com/2020/07/18/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASRGAN/</id>
    <published>2020-07-18T02:41:02.000Z</published>
    <updated>2020-09-09T11:58:05.509Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</p></blockquote><p><strong>这篇文章第一次将将生成对抗网络用在了解决超分辨率问题上。</strong></p><blockquote><p>生成对抗网络，Generative Adversarial Network，GAN；</p><blockquote><p>GAN = G + D + 其他；</p></blockquote><blockquote><p>G：以次充好；D：明辨是非；其他：具体问题具体分析；</p></blockquote></blockquote><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/1.png" title="Optional title"></p><h2 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h2><p>G：残差块 + 卷积层 + BN层 + PReLU</p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>常规的卷积层。</p><h3 id="BN层"><a href="#BN层" class="headerlink" title="BN层"></a>BN层</h3><blockquote><p>Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift</p></blockquote><p>在多数网络中，BN层的作用主要有以下四个：</p><ul><li>可以选择比较大的初始学习率，显著提高网络的训练速度</li></ul><blockquote><p>在此之前的深度网络的训练，需要慢慢的调整学习率，甚至在网络训练到中间的时候，还需要考虑对学习率进行渐进的调整，有了BN之后，可以采用初始很大的学习率，然后学习率的衰减速度也很大，因为这个算法收敛很快。当然这个算法即使在选择了较小的学习率，也比以前的收敛速度快，因为它具有快速训练收敛的特性。</p></blockquote><ul><li><p>无需再理会过拟合中drop out、L2正则项参数的选择问题。采用BN算法后，网络可以移除这两项了参数，或者可以选择更小的L2正则约束参数了，因为BN具有提高网络泛化能力的特性；</p></li><li><p>再也无需使用使用局部响应归一化层（局部响应归一化是Alexnet网络用到的方法），因为BN本身就是一个归一化网络层；</p></li><li><p>可以把训练数据彻底打乱。</p></li></ul><blockquote><p>防止每批训练的时候，某一个样本都经常被挑选到，原文说可以提高1%的精度。</p></blockquote><p>BN在SRGAN里并不是重点。<strong>ESRGAN有改进到这个点。</strong></p><h3 id="Residual-blocks"><a href="#Residual-blocks" class="headerlink" title="Residual blocks"></a>Residual blocks</h3><blockquote><p>为什么要使用残差块？意义如何？</p></blockquote><p>在生成器的前6层网络中，运用了残差块。</p><p>使用残差块的意义是：当损失函数从D开始反向传播回G的时候，实际上进过来很多层。<strong>众所周知，越深的网络隐藏参数越多，在反向传播的过程中也越容易梯度弥散。</strong>而且残差连接的方法，就有效的保证了梯度信息能够有效的传递而增强生成对抗网络的鲁棒性。</p><p><strong>很多的GAN类模型有这样的操作。</strong></p><p><strong>ESRGAN也针对残差做了改进。</strong></p><h2 id="为什么使用GAN来解决Image-SR的问题？"><a href="#为什么使用GAN来解决Image-SR的问题？" class="headerlink" title="为什么使用GAN来解决Image SR的问题？"></a>为什么使用GAN来解决Image SR的问题？</h2><p>传统插值方法可以看做把像素复制放大倍数后，用某种固定的卷积核去卷积。</p><p>类似于SRCNN这样的基于卷积神经网络的方案也容易理解：学习上述卷积核，根据构建出的超分图像与真实的高分辨率图像(Ground Truth)的差距去更新网络参数。这种方法的损失函数一般都采用均方误差MSE。</p><p>当前监督SR效果的算法（损失函数）的优化目标是使恢复后的HR图像与ground truth间的均方差（MSE）最小化。这样做的目的是：取得很高的PSNR。但是由于PSNR是基于像素级图像（pixel-wise image）的差异来定义的，因此PSNR捕捉到和人的感官非常密切的差异（纹理细节）的能力十分有限，<strong>因此最高的PSNR不一定能反映人感官上最好的结果。</strong>放大倍数越大，越不平滑的情况下PSNR越低。不过从视觉上说最为真实，因为过于平滑会使得图像内部物体的边缘看起来模糊。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/2.png" title="Optional title"></p><p>上图中的图三是SRGAN给出的结果。PSNR虽然比图二低了近0.1，但是图二的脸和手明显是模糊的，看起来很不真实，图三的各个细节都很清晰，真实度比图二更高。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/3.png" title="Optional title"></p><blockquote><p>while GAN drives the reconstruction towards the natural image manifold producing perceptually more convincing solutions.</p></blockquote><p>这张图也说明的是上述问题。一旦放大倍数超过4，那么基于MSE优化的网络产生出来的HR图像在纹理细节方面就会过于平滑，看起来就像是糊成一团，但就是这样PSNR还有很高的得分。</p><p><strong>换言之，SRGAN没有以PSNR/MSE为“最高指示”，它更注重人的主观的评价（SRGAN在PSNR的表现确实很一般）。</strong></p><p>为了证明“高PSNR并不能带来良好的感官效果”这个观点，原文给出了一个指标，称为Mean opinion score（MOS），是26位评判者的打分。在这26位评委眼中，SRGAN产生的图像更真实：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/4.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/5.png" title="Optional title"></p><h2 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h2><p>D：VGG19 + LeakyReLU + max-pooling</p><h3 id="VGG19"><a href="#VGG19" class="headerlink" title="VGG19"></a>VGG19</h3><blockquote><p>Very deep convolutional networks for large-scale image recognition</p></blockquote><p>上面提到了SRGAN以人的主观感受为主旨来进行网络的训练。</p><p>于是会出现一个无法避免的问题：即使在同一个领域内以及对评判人做过筛选，人的感官也是一定是极其主观的评判。<strong>换言之，人的主观感受是无法用数学的语言去确切表达的。</strong>那么该如何实现以感官的标准来指定监督算法（损失函数）呢？</p><p><strong>作者所选择的是基于VGG的内容损失。</strong>先基于预训练的19层VGG网络的ReLU激活层来定义损失函数。内容损失的实质就是从VGG19网络里提取的特征图之间欧式距离的损失函数，无论是超分辨率还是艺术风格的转移，效果都非常好。</p><h2 id="如何使用GAN解决Image-SR的问题？"><a href="#如何使用GAN解决Image-SR的问题？" class="headerlink" title="如何使用GAN解决Image SR的问题？"></a>如何使用GAN解决Image SR的问题？</h2><p>GAN的工作过程：给定一个低分辨率图片作为噪声z的输入，通过生成器的变换把噪声的概率分布空间尽可能的去拟合真实数据的分布空间。</p><p>而在SRGAN中生成器的输入不再是噪声，而是低分辨率图像；而判别器结构跟普通的GAN没有什么区别。</p><p>综上，SRGAN的功能叙述为：<strong>把LR看成是一个噪声z的输入，那么G的作用就是生成的是一个fake的HR，D的作用是要去分辨G生成的fakeHR与原始的HR之间的区别，给出判断。</strong></p><h1 id="min-max方程"><a href="#min-max方程" class="headerlink" title="min-max方程"></a>min-max方程</h1><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/6.png" title="Optional title"></p><p>对于判别器D，希望D最大,所以加号之前的log部分应该最大,意味着判别器D可以很好的识别出，真实的高分辨率图像是”true”。而要让log尽可能的大，加号后的这部分中的ΘD(ΘG(z))要尽可能的小，意味着生成模型复原的图片应该尽可能的被D视为”FALSE”。</p><p>对于生成器G，应该让G尽可能的小，加号前面的式子并没有G，加号后面的式子中要让ΘG尽可能地小,就要ΘD(ΘG(Z))尽可能的大，也就是说本来就一张低分辨率生成的图片，判别器却被迷惑了，以为是一张原始的高分辨率图片。</p><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="Perceptual-loss-function"><a href="#Perceptual-loss-function" class="headerlink" title="Perceptual loss function"></a>Perceptual loss function</h2><blockquote><p>Perceptual Losses for Real-Time Style Transfer and Super-Resolution and Super-Resolution</p></blockquote><p>最重要的就是本文特别定制的生成器D的损失函数，可以说就是为了这个损失函数才采用GAN的。这个特制的损失函数被称为感知损失，Perceptual loss function。结构如下：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/7.png" title="Optional title"></p><p>其中，加号左边是Content loss（内容损失），加号右边是Adversarial loss（对抗损失，包括系数）。</p><h3 id="Content-loss"><a href="#Content-loss" class="headerlink" title="Content loss"></a>Content loss</h3><p>与先前的基于深度学习的超分方法相比，SRGAN的D在Loss上只有一个明显的变化：Loss不再单是对构建出来图片与真实高分辨率图片求MSE，而是<strong>加上对构建出图片的特征图与真实高分辨率图片在VGG19下的特征图求MSE</strong>。</p><p>简言之，内容损失=原MSE+特征图MSE：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/8.png" title="Optional title"></p><blockquote><p>原文中没有上述公式。</p><p>原文中也并未明确地指出要加和原MSE，只是给出了VGG19特征图的MSE。</p></blockquote><p>加号左边是原MSE：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/9.png" title="Optional title"></p><p>其中，θσ是网络参数，ILR是低分辨率图像，减号后面的部分是重建出来的高分辨率图像，  减号之前的是真实的高分辨率图像，r、W、H分别是图片数量、图片宽和高，都可以看成常数。</p><p>加号右边是VGG19特征图的MSE，称为VGG loss：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/10.png" title="Optional title"></p><p>VGG loss与原MSE相比，多的部分是φij，指的是第i个maxpooling层前的第j个卷积的特征图。</p><blockquote><p>VGG Loss的权重，1e-6是个超参，是TensorLayer的设置，设置到这么小应该是因为用了所有的特征图。</p></blockquote><h3 id="Adversarial-loss"><a href="#Adversarial-loss" class="headerlink" title="Adversarial loss"></a>Adversarial loss</h3><p>除了内容损失以外，还要加上一个GAN原有的对抗损失：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/11.png" title="Optional title"></p><p>其中，log内的部分是判别器对于生成超分图片的输出，-log(x)在（0,1）上是个单调递减的函数。前面提到，生成器D希望log内部分的值越大越好，也就是-log(x)越小越好，因此梯度更新的时候需要最小化对抗损失。</p><h3 id="生成器D的Loss"><a href="#生成器D的Loss" class="headerlink" title="生成器D的Loss"></a>生成器D的Loss</h3><p>综上，生成器D的Loss</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/12.png" title="Optional title"></p><p>这一部分在Tensorflow平台上的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">g_gan_loss = <span class="number">1e-3</span> * tl.cost.sigmoid_cross_entropy(logits_fake, tf.ones_like(logits_fake))</span><br><span class="line">mse_loss = tl.cost.mean_squared_error(fake_patchs, hr_patchs, is_mean=<span class="literal">True</span>)</span><br><span class="line">vgg_loss = <span class="number">2e-6</span> * tl.cost.mean_squared_error(feature_fake, feature_real, is_mean=<span class="literal">True</span>)</span><br><span class="line">g_loss = mse_loss + vgg_loss + g_gan_loss</span><br></pre></td></tr></table></figure><p>代码中的logits_real和logits_fake分别判别器对是真实高分图片、GAN生成的高分图片的输出。fake_patchs, hr_patchs分别是生成器的输出、真实的高分图片。feature_fake、feature_real是构建的图片、真实图片在VGG网络中的特征图。</p><h2 id="判别器D的Loss"><a href="#判别器D的Loss" class="headerlink" title="判别器D的Loss"></a>判别器D的Loss</h2><p>D的Loss用到了sigmoid交叉熵。</p><p>这一部分在Tensorflow平台上的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d_loss1 = tl.cost.sigmoid_cross_entropy(logits_real, tf.ones_like(logits_real))</span><br><span class="line">d_loss2 = tl.cost.sigmoid_cross_entropy(logits_fake, tf.zeros_like(logits_fake))</span><br><span class="line">d_loss = d_loss1 + d_loss2</span><br></pre></td></tr></table></figure><blockquote><p>损失函数softmax_cross_entropy、binary_cross_entropy、sigmoid_cross_entropy之间的区别与联系：<a href="https://www.jianshu.com/p/47172eb86b39">https://www.jianshu.com/p/47172eb86b39</a></p></blockquote><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>实验部分给出的结论是</p><ul><li><p>未针对实时视频SR进行优化；</p></li><li><p>较浅的网络有可能提供非常有效的替代方案，但质量性能会有小幅度的降低。而更深层次的网络架构是也可以给出更高的结果。原文针对这一点给出的解释是：推测ResNet的设计对深层网络的性能有很大的影响：即使是更深层次的网络（B&gt;16）也可以进一步提高SRResNet的性能，但代价是需要更长的培训和测试时间。且由于高频伪影的出现，更深层次网络的SRGAN变体越来越难以训练。</p></li></ul><blockquote><p> In contrast to Dong et al. [10], we found deeper network architectures to be beneficial. We speculate that the ResNet design has a substantial impact on the performance of deeper networks. We found that even deeper networks (B &gt; 16) can further increase the performance of SRResNet, however, come at the cost of longer training and testing times (c.f. supple- mentary material). We further found SRGAN variants of deeper networks are increasingly difficult to train due to the appearance of high-frequency artifacts.</p></blockquote><p>对比实验做的是SRGAN在不同的卷积网络的深度下的效果，显然也是网络越深效果越好。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/13.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/14.png" title="Optional title"></p><p>VGG54取得了最好的视觉效果，原文给出的解释是：更深层的网络层在远离像素空间的情况下代表更高抽象的特征。</p><blockquote><p>In this work, we found lSR VGG/5.4 to yield the perceptually most convincing results, which we attribute to the potential of deeper network layers to represent features of higher abstraction [68, 65, 40] away from pixel space.</p></blockquote><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>文章：<a href="https://arxiv.org/pdf/1609.04802.pdf">https://arxiv.org/pdf/1609.04802.pdf</a></p></li><li><p>code：<a href="https://github.com/tensorlayer/srgan">https://github.com/tensorlayer/srgan</a> 、<a href="https://github.com/aitorzip/PyTorch-SRGAN">https://github.com/aitorzip/PyTorch-SRGAN</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Photo-Realistic Single Image Super-Resolution Using a Generative Adv</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：SRCNN</title>
    <link href="http://example.com/2020/07/18/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASRCNN/"/>
    <id>http://example.com/2020/07/18/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASRCNN/</id>
    <published>2020-07-18T02:38:15.000Z</published>
    <updated>2020-09-09T11:58:37.788Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Learning a Deep Convolutional Network for Image Super-Resolution</p></blockquote><p><strong>SRCNN是深度学习用在超分辨率重建上的开山之作。</strong></p><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p>SRCNN的网络结构非常简单，仅仅用了三个卷积层，网络结构如下所示：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/17.png" title="Optional title"></p><p>SRCNN网络包含三个模块：Patch extraction and representation（块析出与表示）、Non-linear mapping（非线性映射）、Reconstruction（重构）。<strong>这三个模块对应三个卷积操作。</strong></p><ul><li><p>第一层CNN：对输入图片的特征提取。（9x9x64卷积核）；</p></li><li><p>第二层CNN：对第一层提取的特征的非线性映射（1x1x32卷积核）；</p></li><li><p>第三层CNN：对映射后的特征进行重建，生成高分辨率图像（5x5x1卷积核）。</p></li></ul><p>在进行卷积操作之前，SRCNN对图像进行了一个预处理：将输入的低分辨率图像进行bicubic插值，将低分辨率图像放大成目标尺寸。接下来才是上面提到的三层卷积网络拟合非线性映射，最后输出高分辨率图像结果。</p><blockquote><p>插值后的图像依旧称为“低分辨率图像”，并用Y表示；将ground-truth（真实的高分辨率图像）用X表示；将网络记为映射函数F（·）；</p></blockquote><p>网络的总思路来源于稀疏编码，作者将上述三个过程表述为：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/18.png" title="Optional title"></p><h2 id="Patch-extraction-and-representation"><a href="#Patch-extraction-and-representation" class="headerlink" title="Patch extraction and representation"></a>Patch extraction and representation</h2><p>块析出和表示的目的是通过输入图像Y获得一系列特征图：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/19.png" title="Optional title"></p><p>其中W1和B1表示滤波器（卷积核）的权重和偏置，max操作对应ReLU激活函数。</p><blockquote><p>Here W1 is of a size c × f1 × f1 × n1, where c is the number of channels in the input image, f1 is the spatial size of a filter, and n1 is the number of filters.<br>B1 is an n1-dimensional vector, whose each element is associated with a filter.</p></blockquote><blockquote><p>卷积+激活操作。</p></blockquote><h2 id="Non-linear-mapping"><a href="#Non-linear-mapping" class="headerlink" title="Non-linear mapping"></a>Non-linear mapping</h2><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/20.png" title="Optional title"></p><p>其中W2和B2表示滤波器的权重和偏置，max操作依旧对应ReLU激活函数。</p><blockquote><p>第二层和第一层的思路完全一致，实际上这里想表述的是可以添加更多的层，但是这样的操作会显著增加网络的计算开销。</p><blockquote><p>It is possible to add more convolutional layers (whose spatial supports are 1× 1) to increase the non-linearity. But this can significantly increase the complexity of the model, and thus demands more training data and time. In this paper, we choose to use a single convolutional layer in this operation, because it has already provided compelling quality.</p></blockquote></blockquote><h2 id="Reconstruction"><a href="#Reconstruction" class="headerlink" title="Reconstruction"></a>Reconstruction</h2><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/21.png" title="Optional title"></p><blockquote><p>Here W3 is of a size n2 ×f3 ×f3 ×c, and B3 is a c-dimensional vector.<br>W3：线型滤波器</p><blockquote><p>we expect that W3 behaves like first projecting the coefficients onto the<br>image domain and then averaging. In either way, W3 is a set of linear filters.</p></blockquote></blockquote><p>重构依旧是卷积操作，但是这里没有激活函数了。</p><p><strong>根据上面的三个公式，SRCNN仅有6个需要学习的参数：W1、B1、W2、B2、W3、B3。</strong></p><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>损失的计算也仅仅需要网络的输出F(Y)与真实高分图像X，<strong>损失函数选择MSE损失：</strong></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/22.png" title="Optional title"></p><p>选择MSE作为Loss的原因：有利于提高PSNR。</p><blockquote><p>Using MSE as the loss function favors a high PSNR.</p></blockquote><p>实际上，卷积神经网络对损失函数的限制只有一个，即损失函数为可导函数。<strong>如果在训练过程中给出一个更好的感知激励指标，则网络可以灵活地适应该指标。</strong>但原文提到“会做研究但很难实现”。</p><blockquote><p>The PSNR is a widely-used metric for quantitatively evaluating image restoration quality, and is at least partially related to the perceptual quality. It is worth noticing that the convolu- tional neural networks do not preclude the usage of other kinds of loss functions, if only the loss functions are derivable. If a better perceptually motivated metric is given during training, it is flexible for the network to adapt to that metric. We will study this issue in the future. On the contrary, such a flexibility is in general difficult to achieve for traditional “hand-crafted” methods.</p></blockquote><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><h2 id="实验结论"><a href="#实验结论" class="headerlink" title="实验结论"></a>实验结论</h2><p>分别以Running Time和PSNR为指标并与多种方法进行了对比实验。</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/23.png" title="Optional title"></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/24.png" title="Optional title"></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/25.png" title="Optional title"></p><blockquote><p>SC：sparse coding，稀疏编码</p><blockquote><p>Image super-resolution via sparse representation, 2010</p></blockquote><p>K-SVD</p><blockquote><p>On single image scale-up using sparse representations, 2012</p></blockquote><p>NE+LLE：neighbour embedding + locally linear embedding，邻域嵌入+局部线性嵌入</p><blockquote><p>Super-resolution through neighbor embedding, 2004</p></blockquote><p>NE+NNLS：neighbour embedding + non-negative least squares，邻域嵌入+非负最小二乘法</p><blockquote><p>Low-complexity single image super-resolution based on nonnegative neighbor embedding, 2012</p></blockquote><p>ANR：Anchored Neighbourhood Regression，锚定邻域回归</p><blockquote><p>Anchored neighborhood regression for fast example-based super-resolution, 2013</p></blockquote></blockquote><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/26.png" title="Optional title"></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/27.png" title="Optional title"></p><p>给出了4个种类的图片的SR结果及对比。原文中提到，在“baby”的类别上PSNR并未达到最好的效果。以下给出两组。</p><blockquote><p>经典的“Butterfly”和原文中提到的PSNR不达标的“Baby”。</p><blockquote><p>In spite of the best average PSNR values, the proposed SRCNN does not achieve the highest PSNR on images “baby” and “head” from Set5. Nevertheless, our results are still visually appealing (see Figure 10).</p></blockquote></blockquote><h2 id="模型对数据量的敏感性"><a href="#模型对数据量的敏感性" class="headerlink" title="模型对数据量的敏感性"></a>模型对数据量的敏感性</h2><p>文章使用SRCNN分别在ImageNet和Timofte数据集这两个数据集上分别进行了实验。在ImageNet上获得的PSNR值明显高于在Timofte数据集上的结果。这里的结论是：<strong>在迭代次数相同的情况下，数据量的增加可能会提高网络的性能。</strong></p><h2 id="模型对卷积核数量的敏感性"><a href="#模型对卷积核数量的敏感性" class="headerlink" title="模型对卷积核数量的敏感性"></a>模型对卷积核数量的敏感性</h2><p>SRCNN中第一层包含n1=64个卷积核，第二层包含n2=32个卷积核。<strong>根据理论，增加网络的卷积核数量必然会提升模型的性能。</strong></p><p>作者尝试做了额外的增加以及减少卷积核数量两种情况：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/28.png" title="Optional title"></p><p>从上表可以看出，增加卷积核数量确实会获得更高的PSNR值，但是同时会增加计算时间。<strong>所以需要在时间和质量之间做一个权衡。</strong></p><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>训练数据集：Timofte数据集（包含91幅图像）和ImageNet数据集（数据量超大）；</p></li><li><p>文章：<a href="http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2014_deepresolution.pdf">http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2014_deepresolution.pdf</a> ；</p></li><li><p>code：<a href="http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html">http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html</a> ,有matlab版本和Caffe版本；</p></li><li><p>文章比较简单，内容都在字面上，比较好理解。方法超前但是借鉴的痕迹很重，因此没有那种“专事专办”的味道。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Learning a Deep Convolutional Network for Image Super-Resolution&lt;/p&gt;</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>基础：图像超分辨率</title>
    <link href="http://example.com/2020/07/18/%E5%9F%BA%E7%A1%80%EF%BC%9A%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    <id>http://example.com/2020/07/18/%E5%9F%BA%E7%A1%80%EF%BC%9A%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/</id>
    <published>2020-07-18T02:37:07.000Z</published>
    <updated>2020-09-09T11:58:54.104Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><ul><li><p>图像超分辨率，Image Super Resolution（SR）；</p></li><li><p>概念：SR是指通过<strong>软件或硬件</strong>的方法，从观测到的低分辨率（low resolution）图像重建出相应的高分辨率（high resolution）图像；</p></li></ul><blockquote><p>更直白的表述为：提高图像的分辨率，使被观察图像给出更清晰的图像表述。</p><blockquote><p>图像超分辨率重建关注的是恢复图像中丢失的细节，即高频信息。在大量的电子图像应用领域，人们经常期望得到高分辨率（简称HR）图像。但由于设备、传感器等原因，我们得到的图像往往是低分辨率图像（LR）。增加空间分辨率最直接的解决方法就是通过传感器制造技术减少像素尺寸（例如增加每单元面积的像素数量）；另外一个增加空间分辨率的方法是增加芯片的尺寸，从而增加图像的容量。因为很难提高大容量的偶合转换率，所以这种方法一般不认为是有效的，因此，引出了图像超分辨率技术。</p></blockquote><blockquote><p>图像分辨率：指图像中存储的信息量，是每英寸图像内有多少个像素点，分辨率的单位为PPI（Pixels Per Inch），通常叫做像素每英寸。一般情况下，图像分辨率越高，图像中包含的细节就越多，信息量也越大。图像分辨率分为空间分辨率和时间分辨率。通常，分辨率被表示成每一个方向上的像素数量，例如64*64的二维图像。但分辨率的高低其实并不等同于像素数量的多少，例如一个通过插值放大了5倍的图像并不表示它包含的细节增加了多少。</p></blockquote></blockquote><ul><li><p>应用：在监控设备、卫星图像遥感、数字高清、显微成像、视频编码通信、视频复原和医学影像等领域都有重要的应用价值。</p></li><li><p>分类：SR的应用方向大致可分为两种。一是Image SR。只参考当前低分辨率图像，不依赖其他相关图像，称之为单幅图像的超分辨率（single image super resolution，SISR）。Image SR的方法多种多样，从最基础的插值到现如今的深度学习，是一个非常热门的研究方向。另一是Video SR。参考多幅图像或多个视频帧的超分辨率技术，称之为多帧视频/多图的超分辨率（multi-frame super resolution）。</p></li></ul><hr><h1 id="传统的Image-SR技术"><a href="#传统的Image-SR技术" class="headerlink" title="传统的Image SR技术"></a>传统的Image SR技术</h1><h2 id="基于插值的Image-SR"><a href="#基于插值的Image-SR" class="headerlink" title="基于插值的Image SR"></a>基于插值的Image SR</h2><h3 id="何为“插值”？"><a href="#何为“插值”？" class="headerlink" title="何为“插值”？"></a>何为“插值”？</h3><p>通过某个点周围若干个已知点的值，以及周围点和此点的位置关系，根据一定的公式，算出此点的值，就是插值法。</p><p>例如，有如下2*2的图像，不同颜色代表不同的像素点值：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/1.png" title="Optional title"></p><p>使用插值算法给上面的这张图像做细节扩充，形成一张4*4大小的图像：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/2.png" title="Optional title"></p><p>由于已经得知原2*2图像中的4个像素点的值，那么在接下来的操作中，仅需要求剩余12个点（黑色）的值即可。通过某个点周围若干个已知点的值，以及周围点和此点的位置关系，根据一定的公式，算出此点的值，就是插值法。如何把原图像的点摆放在新图中（确定具体坐标）；未知的点计算时，需要周围多少个点参与，公式如何。对于上面的问题，<strong>不同的方案选择，就是不同的插值算法。</strong></p><p><strong>常用的插值算法有：最邻近元法,双线性内插法,双三次内插法等</strong>。</p><p>实际上，这类插值算法，提升的图像细节有限，所以使用较少。通常，通过多幅图像之间的插值算法来重建是一个手段。</p><blockquote><p>以2*2为例。放大2倍后，得到一个4*4的图片。其中(0,0)的灰度值与之前的(0,0)相同。(2,0) 与之前的 (1,0)的灰度值相同。那么，这个放大图像的 (1,0)的灰度值等于什么呢？等于原图像的（1/2,0）的灰度值。但是原图像并没有这个点，通过以下插值方法，可计算该点的像素值灰度。</p></blockquote><h3 id="最邻近插值"><a href="#最邻近插值" class="headerlink" title="最邻近插值"></a>最邻近插值</h3><p>这是最简单的插值算法。思路是：当图片放大时，缺少的像素通过直接使用与之最近原有颜色生成。</p><blockquote><p>可理解为“按比例放大”：放大之后缺少的内容，直接照搬最近的已知的内容。</p></blockquote><p>首先要确定：原图像的像素摆放在新图中的具体坐标。新坐标对应源图中的坐标可以由如下公式得出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">srcX &#x3D; dstX * ( srcWidth &#x2F; dstWidth )</span><br><span class="line">srcY &#x3D; dstY * ( srcHeight &#x2F; dstHeight )</span><br></pre></td></tr></table></figure><blockquote><p>这里依旧可以用“按比例放大”来理解：将srcWidth / dstWidth称为缩放系数K。</p></blockquote><blockquote><p>坐标必然是整数，但是上述公式不一定会得到整数（K不一定为整数）。当2*2放大到4*4，结果和2.2.1中相同。但如果将2*2放大到3*3，就会出现新的像素点坐标计算出来有小数点的情况。这里需要采用的策略是四舍五入的方法（也可直接舍掉小数位），把非整数坐标转换成整数。</p></blockquote><p>接下来，以图片的左上角建立坐标，并将待求象素的四已知邻象素中左上角的看作[i,j]，则待求像素的值依赖于以下四个已知坐标：[i,j]、[i,j+1]、[i+1,j]、[i+1,j+1]，对于每一个待求象素，将距离待求象素最近的邻灰度赋给待求象素。</p><blockquote><p>当待求像素距离四个已知邻象素的距离中的两个及以上都是最小时，给定一个统一的规则即可，例如：要么都取原坐标中的小值，要么都取大值等。</p></blockquote><blockquote><p>图像右侧和下侧的待求像素并不像图像中部的像素，拥有四个已知邻象素。它们最多拥有1个（图像右下角的区域）或2个邻象素，规则也是同样的。</p></blockquote><p>对于上面的2*2图像，扩大到3*3和4*4时，将得到：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/3.png" title="Optional title"></p><blockquote><p>由于2*2实在是太小，下面的两种插值法的到的3*3和4*4实际上和上面这个结果是一致的。</p></blockquote><p><strong>最邻近插值有着明显的缺陷，它会使结果图像产生明显可见的锯齿。</strong></p><h3 id="双线性插值"><a href="#双线性插值" class="headerlink" title="双线性插值"></a>双线性插值</h3><p>在数学上，双线性插值是有两个变量的插值函数的线形插值扩展，其核心思想是在两个方向分别进行一次线性插值。</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/4.png" title="Optional title"></p><p>假设已知上图中已知红色数据点的值，要求是通过双线性插值得到绿色数据点的值。将需求描述为数学方式：预得到未知函数f在点P(x,y)的值，并假设已知函数f有Q11、Q12、Q21、Q22四个坐标。</p><p>首先在x方向进行线性插值，得到：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/5_1.png" title="Optional title"><br><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/5_2.png" title="Optional title"></p><p>然后在y方向进行线性插值，得到：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/6.png" title="Optional title"></p><p>那么未知函数f就可以描述如下：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/7_1.png" title="Optional title"><br><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/7_2.png" title="Optional title"></p><p>如果选择一个坐标系统使得f的四个已知点坐标分别为 (0, 0)、(0, 1)、(1, 0) 和 (1, 1)，那么插值公式就可以化简为：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/8.png" title="Optional title"></p><p>用矩阵运算表示为：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/9.png" title="Optional title"></p><p>双线性内插法的结果通常不是线性的，线性插值的结果与插值的顺序无关。双线性内插法的计算比最邻近点法复杂，计算量较大但，没有灰度不连续的缺点。具有低通滤波性质，使高频分量受损，图像轮廓可能会有一点模糊。</p><blockquote><p>先进行y方向的插值，然后进行x方向的插值，所得到的结果同先x后y是一致的。</p></blockquote><p><strong>相对于最近邻插值简单的四舍五入，双线性插值的处理更为科学，优化了边缘保护。</strong></p><h3 id="双三次内插法"><a href="#双三次内插法" class="headerlink" title="双三次内插法"></a>双三次内插法</h3><blockquote><p>Cubic Convolution Interpolation for Digtial Image Processing</p></blockquote><p>双三次插值（bicubic）又称双立方插值。</p><p>在数值分析这个数学分支中，双三次插值是二维空间中最常用的插值方法。在这种方法中，函数f在点(x,y)的值可以通过矩形网格中最近的十六个采样点的加权平均得到，在这里需要使用两个多项式插值三次函数，每个方向使用一个。</p><p>双三次插值的本质就是用使用了两次Cubic Interpolation。</p><h4 id="Cubic-Interpolation"><a href="#Cubic-Interpolation" class="headerlink" title="Cubic Interpolation"></a>Cubic Interpolation</h4><h5 id="f-0-、f-1-已知"><a href="#f-0-、f-1-已知" class="headerlink" title="f(0)、f(1)已知"></a>f(0)、f(1)已知</h5><blockquote><p>假设已知f(0)，f(1)以及其导数一共四个值。用Cubic Interpolation计算f(0.5)的值。</p></blockquote><p>假设：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/10.png" title="Optional title"></p><p>因为已知f(0)、f(1)以及其导数四个值：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/11.png" title="Optional title"></p><p>可得：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/12.png" title="Optional title"></p><p>通过解出上述四个代数，可以算出f(0.5)。 </p><h5 id="f-0-、f-1-未知"><a href="#f-0-、f-1-未知" class="headerlink" title="f(0)、f(1)未知"></a>f(0)、f(1)未知</h5><blockquote><p>大多数情况下，并不知道f(0)、f(1)的导数。</p></blockquote><p>设f(-1)=P0、f(0)=P1、f(1)=P2、f(2)=P3。有：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/13.png" title="Optional title"></p><p>后两值一般使用相邻两个点的斜率代替。但在更多的情况下，并不知道边缘值的斜率（因为边缘点的相邻两个点知道不完全）。所以有如下两个方法去代替：</p><ul><li><p>Left: p0 = p1、Right: p3 = p2</p></li><li><p>Left: p0 = 2p1 - p2、Right: p3 = 2p2 - p1</p></li></ul><h4 id="两次Cubic-Interpolation"><a href="#两次Cubic-Interpolation" class="headerlink" title="两次Cubic Interpolation"></a>两次Cubic Interpolation</h4><blockquote><p>原来的一次导数，变成了偏导。</p></blockquote><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/14.png" title="Optional title"></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/15.png" title="Optional title"></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/16.png" title="Optional title"></p><p><strong>这四个点的灰度值必然满足第一个方程。</strong>通过上述三组方程，求出所有未知数。从而计算f(0.5,0)。</p><h3 id="插值法的总结"><a href="#插值法的总结" class="headerlink" title="插值法的总结"></a>插值法的总结</h3><p>对于上述三种不同的插值技术：</p><p><strong>特性</strong>：最邻近插值，名称非常直白，核心是四舍五入选取最接近的整数。这样的做法就会导致像素的变化不连续，在图像中的体现就是会有锯齿。双线性插值则是利用与坐标轴平行的两条直线去把小数坐标分解到相邻的四个整数坐标点的和。双三次插值与双线性插值类似，只不过用了相邻的16个点。</p><p><strong>坐标权重</strong>：前两种方法能回保证两个方向的坐标权重和为1，但是双三次插值不能保证这点，所以又可能去出现像素值越界的情况，需要截断。</p><p><strong>效果与泛用性</strong>：双三次插值通常能产生效果最好、最精确的插补图形，但它速度也是最慢的。双线性插值的速度则要快一些，但没有前者精确。在商业性图像编辑软件中，经常采用的是速度最快，但也是最不准确的最近相邻插值。</p><h2 id="基于重建的Image-SR"><a href="#基于重建的Image-SR" class="headerlink" title="基于重建的Image SR"></a>基于重建的Image SR</h2><h3 id="何为“重建”？"><a href="#何为“重建”？" class="headerlink" title="何为“重建”？"></a>何为“重建”？</h3><p>基于重建的SR，其基础是均衡及非均衡采样定理。这类算法的思路大多都是通过多帧低分辨率的图像恢复出一幅高分辨率图像，利用低分辨率图像中的丰富信息来进行超分辨处理，从而获得比观测图像分辨率更高的图像，即它假设低分辨率的输入采样信号（图像）能很好地预估出原始的高分辨率信号（图像）。绝大多数超分辨率算法都属于这一类，进一步的分类则有频域法和空域法。</p><blockquote><p>Nyquist-Shannon采样定理：如果希望得到的采样型号有旋转和相位变化，那么采样周期要小于整数周期的1/2，采样频率应该大于原始频率的2倍。同理，对于模拟信号，如果希望得到信号的各种特性，采样频率应该大于原始模拟信号的最大频率的两倍，否则将发生混叠（相位/频率模糊）。</p></blockquote><blockquote><p>在统计、信号处理和相关领域中，混叠是指取样信号被还原成连续信号时产生彼此交叠而失真的现象。当混叠发生时，原始信号无法从取样信号还原。而混叠可能发生在时域上，称做时间混叠，或是发生在频域上，被称作空间混叠。</p></blockquote><h4 id="频率域方法"><a href="#频率域方法" class="headerlink" title="频率域方法"></a>频率域方法</h4><p>频率域方法是图像超分辨率重建中一类重要方法，其中最主要的是消混叠重建方法。</p><blockquote><p>消混叠重建方法是通过解混叠而改善图像的空间分辨率实现超分辨率复原。在原始场景信号带宽有限的假设下，利用离散傅立叶变换和连续傅立叶变换之间的平移、混叠性质，给出了一个由一系列欠采样观察图像数据复原高分辨率图像的公式。多幅观察图像经混频而得到的离散傅立叶变换系数与未知场景的连续傅立叶变换系数以方程组的形式联系起来，方程组的解就是原始图像的频率域系数，再对频率域系数进行傅立叶逆变换就可以实现原始图像的准确复原。</p></blockquote><h4 id="空间域方法"><a href="#空间域方法" class="headerlink" title="空间域方法"></a>空间域方法</h4><p>在空间域方法中，其线性空域观测模型涉及全局和局部运动、光学模糊、帧内运动模糊、空间可变点扩散函数、非理想采样等内容。空域方法具有很强的包含空域先验约束的能力，主要包括非均匀空间样本内插、迭代反投影方法（IBP）、凸集投影法（POCS）、最大后验概率、最优和自适应滤波方法、确定性重建方法等。</p><h3 id="非均匀空间样本内插"><a href="#非均匀空间样本内插" class="headerlink" title="非均匀空间样本内插"></a>非均匀空间样本内插</h3><p>非均匀空域样本内插法是最直观的超分辨率图像重建算法，一般包含三个基本步骤：（1）运动估计；（2）非线性内插得到高分辨率图像；（3）解除混叠。非均匀空间样本内插法首先对低分辨率视频序列进行运动补偿，继而采用内插的方法产生分辨率较高的合成图像，以这个合成图像中作为初始值，再用Landweber迭代法重建超分辨率图像，最后对超分辨率图像进行去模糊操作。</p><p>非均匀空间样本内插方法运算量较低，能适用于实时任务；但过于简单化，在重建时不能得到比低分辨率图像中更多地频率成分，退化模型受限制，只适用于模糊和嗓声特性对全部低分辨率图像都一样的情况，也没有使用先验约束。</p><h3 id="凸集投影法（POCS）"><a href="#凸集投影法（POCS）" class="headerlink" title="凸集投影法（POCS）"></a>凸集投影法（POCS）</h3><p>凸集投影算法是把未知图像假设为一个适宜的希尔伯特空间中的元素，关于未知图像的每一个先验知识或约束限制了希尔伯特空间中的一个封闭凸集的解，引入幅度边界的限制，导出求解未知图像的迭代公式,由初始估计迭代计算超分辨率图像。利用集合论方法来恢复超分辨率图像，它有效地利用了空间范围观察模型，同时允许包含先验信息。</p><p>综上，POCS算法是一个迭代过程，给定超分辨率图像空间上的任意一个点，来定位一个能满足所有凸集的点。</p><h3 id="迭代反投影法（IBP）"><a href="#迭代反投影法（IBP）" class="headerlink" title="迭代反投影法（IBP）"></a>迭代反投影法（IBP）</h3><p>顾名思义，该算法也是一个迭代过程。在迭代反投影法中，首先估计一个高分辨率图像作为初始解（通常采用的是单幅低分辨率图像的额差值结果），然后根据系统模型，计算其模拟低分辨率图像（1984年提出时，示例为线型模型）。如果初始解与与原始的高分辨率图像精确相等，并且模拟成像过程符合实际情况，则模拟低分辨率序列应与观察到的实际的低分辨率图像相等。当两者不同时，将它们之间的误差反向投影到初始解上，使其得到修正。当误差满足要求时，结束迭代过程，给出最终结果。</p><p><strong>IBP算法直观，计算简单，但是由于没有考虑嗓声的影响，对高频嗓声非常敏感，并且由于逆问题的病态性，该方法没有唯一解，选择系数也有一些难度。与POCS等一些空间域方法相比，IBP算法难以利用空间先验信息。</strong></p><h3 id="统计复原类方法"><a href="#统计复原类方法" class="headerlink" title="统计复原类方法"></a>统计复原类方法</h3><p>统计复原方法将SR看成是一个统计估计问题。它为求解病态的超分辨率问题加入必要的先验约束，为能得到满意解提供了可能。<strong>常用的统计复原方法包括最大后验概率（MAP）估算法和最大似然（ML）估算方法。</strong>最大后验概率就是在己知低分辨率序列的前提下，使出现高分辨率图像的后验概率达到最大，最大似然估算方法可认为是最大后验概率先脸模型下的特例。</p><blockquote><p>最大后验概率估计是后验概率分布的众数。利用最大后验概率估计可以获得对实验数据中无法直接观察到的量的点估计。</p></blockquote><blockquote><p>最大似然估计一种重要而普遍的求估计量的方法。最大似然法明确地使用概率模型，其目标是寻找能够以较高概率产生观察数据的系统发生树。</p></blockquote><h3 id="混合MAP-POCS方法"><a href="#混合MAP-POCS方法" class="headerlink" title="混合MAP/POCS方法"></a>混合MAP/POCS方法</h3><p>POCS和MAP是重建类SR算法中效果出众的两种，两者都很容易引入先验知识。其中POCS算法保持图像边缘和细节的能力很强，但收敛稳定性不高，没有唯一解，降嗓能力不强；MAP有唯一解且收效稳定性高，降嗓能力强，但边缘和细节保持能力不如POCS。混合MAP/POCS方法则结合两者的优势特征，得到HR图像的最佳估计。</p><p>混合MAP/POCS方法有效结合了全部先验知识，并且能确保唯一的最优解，是一种重建效果较好的算法。</p><h3 id="滤波类方法"><a href="#滤波类方法" class="headerlink" title="滤波类方法"></a>滤波类方法</h3><p><strong>IBP、POCS、MAP、MAP/POCS等几种超分辨率算法的运算量大，只能应用于实时性要求不高的图像SR情况，如遥感图像和医学图像的超分辨率处理。</strong>在实时性要求比较高的情况下，例如实时视频的超分辨率复原，要求算法既能提高图像的分辨率，运算复杂度又比较低。滤波的方法，如自适应滤波、Wiener滤波和Kalman滤波等方法运算复杂度低，适用于对运算速度要求比较高的场合。 </p><h2 id="基于学习的Image-SR"><a href="#基于学习的Image-SR" class="headerlink" title="基于学习的Image SR"></a>基于学习的Image SR</h2><p>在超分辨率重建的过程中，随着要求分辨率倍数的增加，低分辨率图像序列的冗余信息已经不能提供满足要求的高频细节。通过增加低分辨率图像序列帧数的方法得到的高频信息也是不能满足实际要求的。在这种情况下，利用神经网络的方法，通过学习训练来获得图像的先验信息，可以得到包含更多细节的高分辨率图像。</p><p><strong>从理论上讲，如果训练集合是通用的，就可以利用这个训练集合对各种类型的图像进行放大。</strong></p><p>机器学习领域（非深度学习邻域）的一些主流的Image SR方法如下：</p><ul><li><p>Example-based方法</p></li><li><p>邻域嵌入方法</p></li><li><p>支持向量回归方法</p></li><li><p>虚幻脸</p></li></ul><blockquote><p>很多人脸图像是被现场照相机获取的,由于环境限制或设备原因,这些图像经常存在分辨率较低的问题。在人脸分析识别领域,怎样恢复人脸图像已经成为一个重要的课题。这个问题在Baker和Kanade的先驱工作中第一次被定义为虚幻脸。</p></blockquote><ul><li>稀疏表示法</li></ul><hr><h1 id="基于深度学习的Image-SR技术"><a href="#基于深度学习的Image-SR技术" class="headerlink" title="基于深度学习的Image SR技术"></a>基于深度学习的Image SR技术</h1><p>基于深度学习的图像超分辨率重建的大值研究流程如下：</p><ul><li><p>给出初始图像输入Image_1；</p></li><li><p>然后将初始图像输入进行分辨率的降低，称为Image_2；</p></li><li><p>通过各种深度学习算法，将Image2重建为Image3，且Image3和Image1的分辨率一致（SR的工作部分），再由PSNR等方法比较Image1与Image3，验证SR重建的效果，并根据效果调节神经网络中的节点模型和参数；</p></li><li><p>迭代第三步直到得到满意的结果。</p></li></ul><p>接下来给出近年来较为有代表性的基于深度学习的Image SR方法：</p><blockquote><p>发展历程：SRCNN -&gt; FSRCNN -&gt; ESPCN -&gt; VDSR -&gt; SRGAN -&gt; ESRGAN -&gt; EDSR</p><p>额外的一篇：PULSE</p></blockquote><h2 id="SRCNN"><a href="#SRCNN" class="headerlink" title="SRCNN"></a>SRCNN</h2><p>见《论文研读：SRCNN》。</p><h2 id="FSRCNN"><a href="#FSRCNN" class="headerlink" title="FSRCNN"></a>FSRCNN</h2><blockquote><p>Accelerating the Super-Resolution Convolutional Neural Networks</p></blockquote><p>这篇文章指出，SRCNN在速度方面有着显著的限制。体现在：</p><ul><li><p>低分辨率图像需要上采样（双三次插值）；</p></li><li><p>非线性映射步骤，参数量依旧影响速度。</p></li></ul><p><strong>这篇文章主要的目的是对SRCNN进行加速。</strong>文章重新设计SRCNN结构，体现在以下三个方面：</p><ul><li><p><strong>在网络的最后新增添了一个解卷积层</strong>，作用是从没有插值的低分辨率图像直接映射到高分辨率图像；</p></li><li><p>重新改变输入特征维数；</p></li><li><p>使用了更小的卷积核但是使用了更多的映射层。</p></li></ul><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/29.png" title="Optional title"></p><p>对于上述第一个问题，采用解卷积层代替三次插值，针对第二个问题，添加萎缩层和扩张层，并将中间那一个大层用一些小层（卷积核大小是3*3）来代替。整个网络结构类似于漏斗的形状，中间细两端粗。这个网络不仅仅速度快，而且不需要更改参数（除新增添的在最后的解卷积层）。</p><p>损失函数：同SRCNN一致的MSE。</p><blockquote><p>Cost function: Following SRCNN, we adopt the mean square error (MSE) as the cost function.</p></blockquote><p>激活函数：改成了PReLU。</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/prelu.png" title="Optional title"></p><blockquote><p>PReLU（Parametric Rectified Linear Unit），带参数的ReLU。i表示不同的通道，如果ai=0，那么PReLU退化为ReLU；如果ai是一个很小的固定值（如ai=0.01），则PReLU退化为Leaky ReLU（LReLU）。有实验证明，与ReLU相比，LReLU对最终的结果几乎没什么影响。</p></blockquote><p>实验结果：</p><p>不同倍数、不同数据集、不同方法的对比：在SET5上的结果最好，放大2、3倍时的效果最好。</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/30.png" title="Optional title"></p><p>经典的lenna的结果，SET14在3倍下的模型。</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/31.png" title="Optional title"></p><p>三个贡献：</p><ul><li><p>设计漏斗结构的卷积网络，不需要预处理操作；</p></li><li><p><strong>速度提升；</strong></p></li><li><p>训练速度快。</p></li></ul><p>链接：<a href="https://arxiv.org/pdf/1608.00367v1.pdf">https://arxiv.org/pdf/1608.00367v1.pdf</a></p><p>代码：<a href="https://github.com/yifanw90/FSRCNN-TensorFlow">https://github.com/yifanw90/FSRCNN-TensorFlow</a></p><h2 id="ESPCN"><a href="#ESPCN" class="headerlink" title="ESPCN"></a>ESPCN</h2><blockquote><p>Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</p></blockquote><p>这篇文章指出，像SRCNN这一类的方法，需要将低分辨率图像通过上采样插值得到与高分辨率图像相同大小的尺寸，再输入到网络中，这意味着要在较高的分辨率上进行卷积操作，从而增加了计算复杂度。</p><p>然后这篇文章给出了一种直接在低分辨率图像尺寸上提取特征，计算得到高分辨率图像的高效方法，称为ESPCN。网络结构如下所示：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/32.png" title="Optional title"></p><p>最大贡献：亚像素卷积层/子像素卷积层/pixel shuffle</p><p>SR的流程里需要将一张低分辨率图像转换成一张高分辨率图像。如果直接用deconvolution作为upscale手段的话，通常会带入过多人工因素进来（有不少论文提到这个）。<strong>在反卷积里会存在大量补0的区域，这可能对结果有害。</strong>因此pixel shuffle通过亚像素卷积，实现从低分辨图到高分辨图的重构，通过将多通道feature上的单个像素组合成一个feature上的单位即可，每个feature上的像素就相当于新的feature上的亚像素。</p><p>所以在图像超分辨的任务多使用pixel shuffle的方式获得高分辨图像（ESPCN等很多论文都有）。</p><blockquote><p>亚像素：在相机成像的过程中，获得的图像数据是将图像进行了离散化的处理，由于感光元件本身的能力限制，到成像面上每个像素只代表附近的颜色。例如两个感官原件上的像素之间有4.5um的间距，宏观上它们是连在一起的，微观上它们之间还有无数微小的东西存在，这些存在于两个实际物理像素之间的像素，就被称为“亚像素”。亚像素实际上应该是存在的，只是缺少更小的传感器将其检测出来而已，因此只能在软件上将其近似计算出来。</p><blockquote><p>若输出是原来的 r * r 倍（如，r=2，200x200 变成 400x400），则输出的 channel 数是输入 channel 数除以 r * r （如200x200x40 变成 400x400x10）。</p></blockquote><blockquote><p>一文搞懂 deconvolution、transposed convolution、sub-­pixel or fractional convolution：<a href="https://www.cnblogs.com/shine-lee/p/11559825.html#convolution%E8%BF%87%E7%A8%8B">https://www.cnblogs.com/shine-lee/p/11559825.html#convolution过程</a></p></blockquote></blockquote><p>链接：<a href="https://arxiv.org/pdf/1609.05158.pdf">https://arxiv.org/pdf/1609.05158.pdf</a></p><p>code：<a href="https://github.com/JuheonYi/VESPCN-tensorflow">https://github.com/JuheonYi/VESPCN-tensorflow</a></p><h2 id="VDSR"><a href="#VDSR" class="headerlink" title="VDSR"></a>VDSR</h2><blockquote><p>Accurate Image Super-Resolution Using Very Deep Convolutional Networks</p></blockquote><p>本文提出，输入的低分辨率图像和输出的高分辨率图像在很大程度上是相似的，也就是指低分辨率图像携带的低频信息与高分辨率图像的低频信息相近，训练时带上这部分会多花费大量的时间，实际上只需要学习高分辨率图像和低分辨率图像之间的高频部分残差即可。残差网络结构的思想特别适合以这个思路来解决SR问题，可以说影响了之后的深度学习超分辨率方法。</p><blockquote><p>对SRCNN收敛速度的分析有点牵强，主要是提出了论文的基于残差建模。</p><blockquote><p>何恺明在2015年的时候提出了残差网络ResNet。ResNet的提出，解决了之前网络结构比较深时无法训练的问题，性能也得到了提升，ResNet也获得了CVPR2016的best paper。残差网络结构(residual network)被应用在了大量的工作中。</p></blockquote></blockquote><p>这篇文章依旧以SRCNN开启叙述，先提了SRCNN的优势，然后指出了SRCNN的三个缺点：</p><ul><li><p>依赖小图像区域的上下文；</p></li><li><p>收敛太慢；</p></li><li><p>只能做单个倍数的采样。</p></li></ul><blockquote><p>first, it relies on the context of small image regions; second, training converges too slowly; third, the network only works for a single scale</p></blockquote><p>然后针对上面三个问题来概括本篇创新点：</p><ul><li><p>Context（增大感受野）；</p></li><li><p>Convergence（残差学习和高学习率）；</p></li><li><p>Scale Factor（使用mutil-scale）。</p></li></ul><p>提出如下的VDSR：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/33.png" title="Optional title"></p><ul><li><p>20层的卷积核大小都为3*3*64，</p></li><li><p>使用插值将LR图片放大到期望的尺寸，再作为网络的输入</p></li><li><p>每经过一层，feature map将会变小，论文只用补0的方法来保持其尺寸不变。</p></li></ul><p>Loss依旧使用的是MSE：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/34.png" title="Optional title"></p><p>链接：<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Kim_Accurate_Image_Super-Resolution_CVPR_2016_paper.pdf">https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Kim_Accurate_Image_Super-Resolution_CVPR_2016_paper.pdf</a></p><p>code：<a href="https://github.com/huangzehao/caffe-vdsr">https://github.com/huangzehao/caffe-vdsr</a> 、<a href="https://github.com/Jongchan/tensorflow-vdsr">https://github.com/Jongchan/tensorflow-vdsr</a> 、<a href="https://github.com/twtygqyy/pytorch-vdsr">https://github.com/twtygqyy/pytorch-vdsr</a></p><h2 id="SRGAN"><a href="#SRGAN" class="headerlink" title="SRGAN"></a>SRGAN</h2><p>见《论文研读：SRGAN》。</p><h2 id="ESRGAN"><a href="#ESRGAN" class="headerlink" title="ESRGAN"></a>ESRGAN</h2><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/35.png" title="Optional title"></p><p>ESRGAN的整体框架和SRGAN保持一致。相比SRGAN，ESRGAN有5处改进：</p><h3 id="去除BN层"><a href="#去除BN层" class="headerlink" title="去除BN层"></a>去除BN层</h3><blockquote><p>为什么要去除BN层？</p><p>How does batch normalization help optimization</p></blockquote><p>对于有些像素级图片生成任务来说，BN效果不佳。对于图片分类等任务，只要能够找出关键特征，就能正确分类，这算是一种粗粒度的任务，在这种情形下通常BN是有积极效果的。但是对于有些输入输出都是图片的像素级别图片生成任务，比如图片风格转换等应用场景，使用BN会带来负面效果，这很可能是因为在Mini-Batch内多张无关的图片之间计算统计量，弱化了单张图片本身特有的一些细节信息。</p><p>以图像超分辨率来说，网络输出的图像在色彩、对比度、亮度上要求和输入一致，改变的仅仅是分辨率和一些细节。而BN，对图像来说类似于一种对比度的拉伸，任何图像经过BN后，其色彩的分布都会被归一化。也就是说，它破坏了图像原本的对比度信息，所以BN的加入反而影响了网络输出的质量。ResNet可以用BN，但也仅仅是在残差块当中使用。</p><h3 id="用Dense-Block替换Residual-Block"><a href="#用Dense-Block替换Residual-Block" class="headerlink" title="用Dense Block替换Residual Block"></a>用Dense Block替换Residual Block</h3><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/36.png" title="Optional title"></p><p>BN的作用是网络更容易优化，不容易陷入局部极小值。ESRGAN去掉了BN，可以猜想，如果保持原有的Residual Block结构，网络会变得非常难易训练，而且很容易陷入局部极小值导致结果不好。</p><p>而DenseNet的解空间非常平滑，换言之，DenseNet相比其他网络要容易训练的多，Dense Block和BN提升网络性能的原因是相同的。</p><p><strong>总的来说，去掉BN层是因为BN层有副作用，但是BN也有众多的优点且与Dense Block的作用相似，那么用Dense Block替换Residual Block是要弥补去掉BN带来的负面效果。</strong></p><h3 id="使用Relativistic-GAN改进对抗损失函数"><a href="#使用Relativistic-GAN改进对抗损失函数" class="headerlink" title="使用Relativistic GAN改进对抗损失函数"></a>使用Relativistic GAN改进对抗损失函数</h3><h3 id="使用relu激活前的特征图计算损失"><a href="#使用relu激活前的特征图计算损失" class="headerlink" title="使用relu激活前的特征图计算损失"></a>使用relu激活前的特征图计算损失</h3><p>原文给出的解释如下：</p><ul><li><p>激活后的特征图变的非常稀疏，丢失了很多信息；</p></li><li><p>使用激活后的特征图会造成重建图片在亮度上的不连续。</p></li></ul><p>生成器的损失函数为：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/37.png" title="Optional title"></p><blockquote><p>ESRGAN在生成器上的Loss几乎沿用了SRGAN，但是命名是有出入的。</p></blockquote><h3 id="使用network-interpolation平衡主客观指标"><a href="#使用network-interpolation平衡主客观指标" class="headerlink" title="使用network interpolation平衡主客观指标"></a>使用network interpolation平衡主客观指标</h3><p>基于GAN的方法有一个缺点，经常会生成奇怪的纹理，而非GAN的方法总是缺失细节，能不能把两种方法生成的图片加权相加呢？将这样的思路称为Network Interpolation，网络插值。</p><p>具体做法是，训练一个非GAN的网络，在这个网络的基础上fine-tuning出GAN的生成器，然后把两个网络的参数加权相加：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/38.png" title="Optional title"></p><p>链接：<a href="https://arxiv.org/pdf/1809.00219.pdf">https://arxiv.org/pdf/1809.00219.pdf</a></p><p>code：<a href="https://github.com/xinntao/ESRGAN%E2%80%8Bgithub.com">https://github.com/xinntao/ESRGAN​github.com</a></p><h2 id="EDSR和MDSR"><a href="#EDSR和MDSR" class="headerlink" title="EDSR和MDSR"></a>EDSR和MDSR</h2><blockquote><p>Enhanced Deep Residual Networks for Single Image Super-Resolution</p></blockquote><p>EDSR是NTIRE2017超分辨率挑战赛上获得冠军的方案。</p><p>EDSR的最大贡献是去除了SRResNet上的BN。</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/39.png" title="Optional title"></p><p>原文中提到，原始的ResNet最一开始是被提出来解决高层的计算机视觉问题，比如分类和检测，直接把ResNet的结构应用到像超分辨率这样的低层计算机视觉问题，显然不是最优的。由于批规范化层消耗了与它前面的卷积层相同大小的内存，在去掉这一步操作后，相同的计算资源下，EDSR就可以堆叠更多的网络层或者使每层提取更多的特征，从而得到更好的性能表现。EDSR用L1范数样式的损失函数来优化网络模型。在训练时先训练低倍数的上采样模型，接着用训练低倍数上采样模型得到的参数来初始化高倍数的上采样模型，这样能减少高倍数上采样模型的训练时间，同时训练结果也更好。</p><blockquote><p>训练结果给的是2、3、4倍，效果依次递减。也就是2倍的PSNR和SSIM效果是最好的。</p></blockquote><p>文中同时还给出了一个能同时进行不同上采样倍数的网络结构MDSR：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/40.png" title="Optional title"></p><p>链接：见4.2.2节。</p><p>code：<a href="https://github.com/jmiller656/EDSR-Tensorflow">https://github.com/jmiller656/EDSR-Tensorflow</a> 、<a href="https://github.com/thstkdgus35/EDSR-PyTorch">https://github.com/thstkdgus35/EDSR-PyTorch</a> 、<a href="https://github.com/LimBee/NTIRE2017">https://github.com/LimBee/NTIRE2017</a></p><h2 id="PULSE"><a href="#PULSE" class="headerlink" title="PULSE"></a>PULSE</h2><blockquote><p>PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models</p></blockquote><p>来自2020年CVPR，一篇非常amazing的文章：PULSE最多能将16*16放大至1024*1024，即64倍放大。</p><blockquote><p>Starting with a pre-trained GAN, our method operates only at test time, generating each image in about 5 seconds on a single GPU.</p></blockquote><p>传统方法处理此类问题时，一般拿到LR图像后，会“猜测”需要多少额外的像素，然后试着将此前处理过的HR图像中相应的像素，匹配给LR图像。而这种单纯匹配像素的结果是，像头发和皮肤的纹理这种区域，会出现像素匹配错位的现象。而且该方法还会忽略了HR图像中，感光性等感知细节。所以最终在平滑度、感光度上出现问题，结果依然会显得模糊或者不真实。</p><p>PULSE则给出了一个新的思路：在拿到一张LR图像后，PULSE系统不会慢慢添加新的细节，而是遍历GAN生成的HR图像，将这些HR图像对应的LR图像与原图对比，找到最接近的那张。实际上就是用LR图片做反推导：找到最相似的LR版本，那么再反推回去，这张LR图像所对应的HR图像，就是最终要输出的结果。</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/41.png" title="Optional title"></p><p>使用的基本模型是GAN。在复原的过程中，该网络会“想象”出一些原本不存在的特征，即使是原本LR照片中无法看到的细节，比如毛孔、细纹、睫毛、头发和胡茬等，经过其算法处理后，都能看得一清二楚。</p><p>实验结果：在著名的高分辨率人脸数据集CelebA HQ用64×，32×和8×的比例因子进行了这些实验。并要求40个人对通过PULSE和其他五种缩放方法生成的1440张图像进行MOS评分，PULSE的效果最佳，得分几乎与真实的高质量照片一样高。</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/42.png" title="Optional title"></p><p>对不同类型的人脸的复原Success rates也做了统计：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/43.png" title="Optional title"></p><p>比较在意的人像位置：头发、眼睛、嘴唇：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/44.png" title="Optional title"></p><p>该模型有如下局限性：</p><ul><li>只针对人脸（虚幻脸）；</li></ul><blockquote><p>We show extensive experimental results demonstrating the efficacy of our approach in the domain of face super-resolution (also known as face hallucination).</p></blockquote><blockquote><p>However, we also note significant limitations when evaluated on natural images past the standard benchmark.</p></blockquote><ul><li>不能用于识别身份：无法将安全摄像头拍摄的失焦、不能识别的照片，变成真人的清晰图像。</li></ul><blockquote><p>意思就是说仅会生成不存在但看上去很真实的新面孔。</p></blockquote><p>链接：<a href="https://arxiv.org/pdf/2003.03808.pdf">https://arxiv.org/pdf/2003.03808.pdf</a></p><p>code：<a href="https://github.com/adamian98/pulse">https://github.com/adamian98/pulse</a></p><hr><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="常用的Image-SR评价指标"><a href="#常用的Image-SR评价指标" class="headerlink" title="常用的Image SR评价指标"></a>常用的Image SR评价指标</h2><h3 id="PSNR"><a href="#PSNR" class="headerlink" title="PSNR"></a>PSNR</h3><p>􏱏􏱐􏰝􏱑􏱒􏰆峰值信噪比（Peak Signal to Noise Ratio，PSNR）是一种极其普遍且在各个领域进行图像处理工作时都广泛使用的评估图像质量的客观量测法。评价的指向为经过压缩、降噪等操作后的图像，相对于原图像的噪声强度。失真程度越大，该指标给出的值就越小。</p><p>给定不含噪声的图像I（大小为M*N）和带有噪声的图像K，将均方误差MSE定为：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/45.png" title="Optional title"></p><p>PSNR则定义为：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/46.png" title="Optional title"></p><p>作为一种客观的评价指标，众多的实验都指出，PSNR同人的眼睛所看到的图像的视觉品质的主观评价是由很大出入的，也就是说，PSNR评分高的图像，在人眼的主观评价中，反而可能不如PSNR低分图像。实际上该指标并未考虑视觉评价的感知特性。</p><h3 id="SSIM"><a href="#SSIM" class="headerlink" title="SSIM"></a>SSIM</h3><p>结构相似性（Structural Similarity index，SSIM）是对比两图相似程度的指标。SSIM分别使用协方差、均值和标准差作为图片的结构相似程度、亮度和对比度的评价指标，这三个因素共同组合成图片失真程度的评价。三个评价的范围都是从0到1，无单位。若进行评价的两张图像完全相同，SSIM就会等于1。</p><p>给定进行对比的两张图像x和y，SSIM定义为：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/47.png" title="Optional title"></p><p>其中，α，β，γ&gt;0，l(x,y)是亮度的评价，c(x,y)是对比度的评价，s(x,y)是结构的评价。</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/48.png" title="Optional title"></p><p>c1，c2，c3均为非零常数，μ为均值，σ为方差和协方差。在实际工程计算中，一般设定α，β，γ=1，c3=c2/2。</p><p>SSIM具有对称性，即SSIM(x,y)=SSIM(y,x)。</p><p>SSIM是视频及图像质量评估的一种常见且效果良好的算法，不过它依久具有同PSNR相似的缺陷。</p><h2 id="NTIRE"><a href="#NTIRE" class="headerlink" title="NTIRE"></a>NTIRE</h2><p>NTIRE英文全称是New Trends in Image Restoration and Enhancement，也就是<strong>“图像恢复与增强的新趋势”</strong>，是近年来计算机图像修复领域最具影响力的一场赛事，每年都会吸引大量的关注者和参赛者。</p><p>该比赛自2017年开始第一届，到今年已经举办了四届。2017年该比赛共拥有三个赛道，而今年的比赛增加到了5个赛道（2020.6比赛结果已出），分别是：</p><h3 id="2020年赛道"><a href="#2020年赛道" class="headerlink" title="2020年赛道"></a>2020年赛道</h3><h4 id="Real-World-Image-Super-Resolution"><a href="#Real-World-Image-Super-Resolution" class="headerlink" title="Real-World Image Super-Resolution"></a>Real-World Image Super-Resolution</h4><p>经典赛道，自2017年开始就有的图像超分辨率。</p><h4 id="Image-Dehazing"><a href="#Image-Dehazing" class="headerlink" title="Image Dehazing"></a>Image Dehazing</h4><p>经典赛道，自2017年开始就有的图像去雾。</p><h4 id="Image-Demoireing"><a href="#Image-Demoireing" class="headerlink" title="Image Demoireing"></a>Image Demoireing</h4><p>图像去摩尔纹，是今年的新赛道。</p><blockquote><p>摩尔纹是一种在数码照相机或者扫描仪等设备上，感光元件出现的高频干扰的条纹，是一种会使图片出现彩色的高频率不规则的条纹。</p></blockquote><h4 id="Spectral-Reconstruction-from-an-RGB-Image"><a href="#Spectral-Reconstruction-from-an-RGB-Image" class="headerlink" title="Spectral Reconstruction from an RGB Image"></a>Spectral Reconstruction from an RGB Image</h4><p>该任务的目的是从RGB图像中重建全场景高光谱（HS）信息。</p><h4 id="Video-Quality-Mapping"><a href="#Video-Quality-Mapping" class="headerlink" title="Video Quality Mapping"></a>Video Quality Mapping</h4><p>该挑战拟解决从源视频域到目标视频域的质量映射问题。挑战包括两个子任务：监督轨道和弱监督轨道。 其中，轨道1提供了一个新的Internet视频基准数据集，要求算法以监督训练的方式学习从压缩程度更高的视频到压缩程度更低的视频间的映射关系。 在轨道2中，需要算法来学习从一个设备到另一个设备的质量映射关系。</p><h3 id="Image-SR赛道冠军论文"><a href="#Image-SR赛道冠军论文" class="headerlink" title="Image SR赛道冠军论文"></a>Image SR赛道冠军论文</h3><h4 id="2017"><a href="#2017" class="headerlink" title="2017"></a>2017</h4><blockquote><p>EDSR</p></blockquote><p><a href="https://arxiv.org/abs/1707.02921v1">Enhanced Deep Residual Networks for Single Image Super-Resolution</a></p><h4 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h4><blockquote><p>WDSR</p></blockquote><p><a href="https://arxiv.org/abs/1808.08718">Wide Activation for Efficient and Accurate Image Super-Resolution</a></p><h4 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h4><blockquote><p>UDSR</p></blockquote><h4 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h4><p><a href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w31/Ji_Real-World_Super-Resolution_via_Kernel_Estimation_and_Noise_Injection_CVPRW_2020_paper.html">Real-World Super-Resolution via Kernel Estimation and Noise Injection</a></p><h3 id="NTIRE中Image-SR赛道结果综述"><a href="#NTIRE中Image-SR赛道结果综述" class="headerlink" title="NTIRE中Image SR赛道结果综述"></a>NTIRE中Image SR赛道结果综述</h3><blockquote><p>Single Image SR -&gt; Real Image SR -&gt; Real-World Image SR<br>从最开始的模拟下采样（Bicubic）图像到现在的Real-World Image，可见现在的挑战任务越来越走向通用化和实用化，当然这也意味着难度的升级。</p></blockquote><p><a href="http://ieeexplore.ieee.org/document/8014883/">NTIRE 2017 Challenge on Single Image Super-Resolution: Methods and Results</a></p><p><a href="http://ieeexplore.ieee.org/document/8575282/">NTIRE 2018 Challenge on Single Image Super-Resolution: Methods and Results</a></p><p><a href="https://ieeexplore.ieee.org/xpl/conhome/8972688/proceeding">NTIRE 2019 Challenge on Real Image Super-Resolution: Methods and Results</a></p><p><a href="http://ieeexplore.ieee.org/document/9022354/">NTIRE 2020 Challenge on Real-World Image Super-Resolution: Methods and Results</a></p><h2 id="Image-SR相关资源（持续更新）"><a href="#Image-SR相关资源（持续更新）" class="headerlink" title="Image SR相关资源（持续更新）"></a>Image SR相关资源（持续更新）</h2><ul><li><p>2000-2020的Image SR文献总结：<a href="https://github.com/YapengTian/Single-Image-Super-Resolution">https://github.com/YapengTian/Single-Image-Super-Resolution</a></p></li><li><p>CVPR2020｜图像重建(超分辨率，图像恢复，去雨，去雾，去模糊，去噪等)相关论文汇总：<a href="https://blog.csdn.net/Kobaayyy/article/details/106815083">https://blog.csdn.net/Kobaayyy/article/details/106815083</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;图像超分辨</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="基础知识" scheme="http://example.com/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：Single Image Deraining: From Model-Based to Data-Driven and Beyond</title>
    <link href="http://example.com/2020/03/30/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASingle-Image-Deraining-From-Model-Based-to-Data-Driven-and-Beyond/"/>
    <id>http://example.com/2020/03/30/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASingle-Image-Deraining-From-Model-Based-to-Data-Driven-and-Beyond/</id>
    <published>2020-03-30T07:06:06.000Z</published>
    <updated>2020-09-09T12:26:58.700Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>这是一篇关于单图像去雨和视频去雨方法的综述性论文。作者在文中总结了过去20年内的一些去雨方法，并对它们进行分类，对它们的结果做了定性和定量分析以及结果之间的对比。</p><blockquote><blockquote><p>The goal of single-image deraining is to restore the rain-free background scenes of an image degraded by rain streaks and rain accumulation. The early single-image deraining methods employ a cost function, where various priors are developed to represent the properties of rain and background layers. Since 2017, single-image deraining methods step into a deep-learning era, and exploit various types of networks, i.e. convolutional neural networks, recurrent neural networks, generative adversarial networks, etc., demonstrating impressive performance. </p></blockquote><blockquote><p>Given the current rapid development, in this paper, we provide a comprehensive survey of deraining methods over the last decade. We summarize the rain appearance models, and discuss two categories of deraining approaches: model-based and data-driven approaches. For the former, we organize the literature based on their basic models and priors. For the latter, we discuss developed ideas related to architectures, constraints, loss functions, and training datasets. </p></blockquote><blockquote><p>We present milestones of single-image deraining methods, review a broad selection of previous works in different categories, and provide insights on the historical development route from the model-based to data-driven methods. We also summarize performance comparisons quantitatively and qualitatively. Beyond discussing the technicality of deraining methods, we also discuss the future directions.</p></blockquote></blockquote><blockquote><blockquote><p>单幅图像排空的目标是恢复因雨水条纹和雨水堆积而退化的图像的无雨背景场景。早期的单图像排空方法采用成本函数，其中开发了各种先验以表示雨层和背景层的属性。自2017年以来，单图像排空方法进入了深度学习时代，并利用了各种类型的网络，即卷积神经网络，递归神经网络，生成对抗网络等，展示了令人印象深刻的性能。</p></blockquote><blockquote><p>鉴于当前的快速发展，本文对过去十年中的去雨方法进行了全面的调查。我们总结了降雨外观模型，并讨论了两种减雨方法：基于模型的方法和基于数据驱动的方法。对于前者，我们根据其基本模型和先验知识来整理文献。对于后者，我们讨论与体系结构，约束，损失函数和训练数据集有关的已开发思想。</p></blockquote><blockquote><p>我们介绍了单图像排空方法的里程碑，回顾了不同类别的大量先前作品，并提供了从基于模型的方法到数据驱动方法的历史发展路线的见解。我们还定量和定性总结了性能比较。除了讨论去雨方法的技术性之外，我们还讨论了未来的发展方向。</p></blockquote></blockquote><h1 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h1><blockquote><p>An early study of video deraining was started in 2004 by Garg and Nayar [3]. They analyze rain dynamic appearances, and develop an approach to remove rain streaks from videos. Kang et al. [4] was a pioneer in the single image deraining by publishing a method in 2012. The method extracts the high-frequency layer of a rain image, and decomposes the layer further into rain and non-rain components using dictionary learning and sparse coding. Starting from 2017, by the publications of [1, 18], data-driven deep-learning methods that learn features automatically become dominant in the literature.</p></blockquote><blockquote><blockquote><p>Garg和Nayar [3]于2004年开始了对视频去雨的早期研究。他们分析了雨水的动态外观，并开发了一种方法来消除视频中的雨水条纹。 </p></blockquote><blockquote><p>Kang等。 [4]是通过在2012年发布一种方法来进行单幅图像去雨的先驱。该方法提取了降雨图像的高频层，并使用字典学习和稀疏编码将其进一步分解为降雨和非降雨成分。 </p></blockquote><blockquote><p>从[1，18]的出版物开始，从2017年开始，学习特征的数据驱动的深度学习方法自动成为文献中的主导。</p></blockquote></blockquote><ul><li>单一图像去雨研究的历史：在2017年之前，典型的方法是基于模型的方法（或非深度学习方法）。基于模型的方法的主要发展受到以下观念的推动：图像分解（2012年），稀疏编码（2015年）和基于先验的高斯混合模型（2016年）。自2017年以来，单图像清除方法进入了数据驱动方法（或深度学习方法）时期。</li></ul><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SID/11.png" title="Optional title"></p><blockquote><p>单一图像排空方法的里程碑：图像分解，稀疏编码，高斯混合模型，深度卷积网络，生成对抗网络以及半/无监督学习。在2017年之前，典型的方法是基于模型的方法（或非深度学习方法）。自2017年以来，单图像去雨方法进入了数据驱动方法（或深度学习方法）时期。</p></blockquote><ul><li>基于模型的方法及相关论文。</li></ul><blockquote><blockquote><p>Model-based methods rely more on the statistical analysis of rain streaks and background scenes. The methods enforce handcrafted priors on both rain and background layers, then build a cost function and optimize it. The priors are extracted from various ways: Luo et al. [5] learn dictionaries for both rain streak and background layers, Li et al. [6] build Gaussian mixture models from clean images to model background scenes, and from rain patches of the input image to model rain streaks, Zhu et al. [7] enforce a certain rain direction based on rain-dominated regions so that the background textures can be differentiated from rain streaks.</p></blockquote><blockquote><p>基于模型的方法更多地依赖于雨条纹和背景场景的统计分析。该方法在雨层和背景层都执行手工制作的先验，然后构建成本函数并对其进行优化。先验是通过各种方式提取的：Luo等。 [5] Li等人学习了有关雨条纹和背景层的词典。［6］建立了高斯混合模型，从干净的图像到背景场景，从输入图像的雨斑到雨条纹，朱等人。 [7]基于降雨为主的地区实施一定的降雨方向，以便可以将背景纹理与降雨条纹区分开。</p></blockquote></blockquote><ul><li>基于数据驱动的相关方法和论文：数据驱动方法的主要发展：深度卷积网络（2017），生成对抗网络（2019）和半/无监督方法（2019）。在2017-2019年间，关于这种深度学习方法的论文超过30篇，大大超过了2017年之前的枯竭论文数量。</li></ul><blockquote><blockquote><p>In recent years, the popularity of data-driven methods has overtaken model-based methods. These methods exploits deep net- works to automatically extract hierarchical features, enabling them to model more complicated mappings from rain images to clean ones. Some rain-related constraints are usually injected into the networks to learn more effective features, such as rain masks [1], background features [8], etc. Architecture wise, some methods utilize recurrent network [1], or recursive network [9] to remove rain progressively. There are also a series of works focusing on the hierarchical information of deep features, e.g. [10, 11].<br>While deep networks lead to a rapid progress in deraining performance, many of these deep-learning deraining methods train the networks in a fully supervised way. This can cause a problem, since to obtain paired images of rain and rain-free images is intractable. The simplest solution is to utilize synthetic images. Yet, there are domain gaps between synthetic rain and real rain images, which can make the deraining performance not optimum. To overcome the problem, unsupervised/semi-supervised methods thatexploitrealrainimages [12]and [13]areintroduced.</p></blockquote><blockquote><p>近年来，数据驱动方法的普及已经取代了基于模型的方法。这些方法利用深层网络自动提取层次结构特征，从而使它们能够建模从降雨图像到干净图像的更复杂的映射。通常会向网络中注入一些与雨水相关的约束条件，以学习更有效的功能，例如防雨罩[1]，背景特征[8]等。在架构上，某些方法利用循环网络[1]或递归网络[9] ]逐步去除雨水。还有一系列针对深度特征的分层信息的作品，例如[10，11]。<br>虽然深层网络在去雨性能方面取得了迅速的进步，但许多深度学习去雨方法都以完全受监督的方式训练网络。这可能会引起问题，因为难以获得雨水和无雨水图像的配对图像。最简单的解决方案是利用合成图像。然而，合成降雨和真实降雨图像之间存在域间隙，这可能会使去雨性能不是最佳的。为了克服这个问题，引入了无监督/半监督的方法来利用真实的雨像[12]和[13]。</p></blockquote></blockquote><h1 id="RAINDROP-APPEARANCE-MODELS"><a href="#RAINDROP-APPEARANCE-MODELS" class="headerlink" title="RAINDROP APPEARANCE MODELS"></a>RAINDROP APPEARANCE MODELS</h1><p>此段结合雨滴的物理特性，给出合成雨模型的合成方法。</p><blockquote><blockquote><p>The shape of a raindrop is usually approximated by a spherical shape [14]. </p></blockquote><blockquote><p>雨滴的形状通常近似为球形[14]。</p></blockquote></blockquote><blockquote><blockquote><p>As a result, in most rain synthetic models, rain streaks are assumed to be superimposed on the background image.</p></blockquote><blockquote><p>最终，在大多数降雨合成模型中，降雨条纹是假定叠加在背景图像上。</p></blockquote></blockquote><h1 id="LITERATURE-SURVEY"><a href="#LITERATURE-SURVEY" class="headerlink" title="LITERATURE SURVEY"></a>LITERATURE SURVEY</h1><h2 id="Synthetic-Rain-Models"><a href="#Synthetic-Rain-Models" class="headerlink" title="Synthetic Rain Models"></a>Synthetic Rain Models</h2><p>六种合成雨模型。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SID/33.png" title="Optional title"></p><blockquote><blockquote><p>However, as we mentioned in the beginning of this section that all these models are heuristic; implying that they might not entirely correct physically. Nevertheless, as shown in the literature, they can be effective, at least to some extent, for image deraining.</p></blockquote><blockquote><p>但是，正如我们在本节开始时提到的那样，所有这些模型都是启发式的。暗示它们可能在物理上不完全正确。 然而，如文献所示，它们可以至少在一定程度上有效地消除图像。</p></blockquote></blockquote><h3 id="Additive-Composite-Model-（ACM）"><a href="#Additive-Composite-Model-（ACM）" class="headerlink" title="Additive Composite Model （ACM）"></a>Additive Composite Model （ACM）</h3><p>现有研究中使用的最简单和流行的降雨模型是加法复合模型[4，6]，它遵循方程式：O = B + S。</p><blockquote><p>其中B表示背景层，S表示雨条纹层。 O是由于雨条纹而劣化的图像。在此，该模型假设降雨条纹的外观仅与背景重叠，并且在降雨退化图像中没有降雨积聚。</p></blockquote><h3 id="Screen-Blend-Model-（SBM）"><a href="#Screen-Blend-Model-（SBM）" class="headerlink" title="Screen Blend Model （SBM）"></a>Screen Blend Model （SBM）</h3><p>罗等[5]提出了一种非线性复合模型，称为背景混合模型：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SID/22.png" title="Optional title"></p><blockquote><p>其中◦表示逐点乘法运算。不同于加性复合模型，背景层和雨层相互影响。 罗等[5]声称，背景混合模型可以对真实降雨图像的某些视觉属性（例如内部反射的效果）进行建模，从而生成视觉上更真实的降雨图像。雨层和背景层的组合取决于信号。这意味着，当背景昏暗时，雨层将主导雨图像的外观；并且，当背景明亮时，背景层将主导图像。</p></blockquote><h3 id="Heavy-Rain-Model-（HRM）"><a href="#Heavy-Rain-Model-（HRM）" class="headerlink" title="Heavy Rain Model （HRM）"></a>Heavy Rain Model （HRM）</h3><blockquote><p>杨等[1]提出了一个包括降雨条纹和降雨积聚的降雨模型。这是去雨文献中包含两个降雨现象的第一个模型。雨水积聚或遮雨效果是大气中的水颗粒和无法单独看到的远距离雨水条纹的结果。雨水积累的视觉效果类似于雾气或雾气，导致对比度低。 考虑到降雨的两个主要方面：Koschmieder模型（用于近似浑浊介质中场景的视觉外观）以及方向和形状不同的重叠降雨条纹，引入了一种新颖的降雨模型.</p></blockquote><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SID/55.png" title="Optional title"></p><blockquote><p>其中，St表示条纹方向相同的雨条纹层。t标记雨条纹层，而s是最大雨条纹层数。A是整体大气光，α是大气透射率。</p></blockquote><h3 id="Rain-Model-with-Occlusion-（ROM）"><a href="#Rain-Model-with-Occlusion-（ROM）" class="headerlink" title="Rain Model with Occlusion （ROM）"></a>Rain Model with Occlusion （ROM）</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SID/44.png" title="Optional title"></p><blockquote><p>刘等[15]将大雨模型扩展为可识别视频中雨的遮挡意识雨模型。该模型将雨条纹分为两种类型：添加到背景层的透明雨条纹和完全遮盖背景层的不透明雨条纹。这些不透明雨条纹的位置由称为信赖图的地图指示。</p></blockquote><h3 id="Comprehensive-Rain-Model-CRM"><a href="#Comprehensive-Rain-Model-CRM" class="headerlink" title="Comprehensive Rain Model (CRM)"></a>Comprehensive Rain Model (CRM)</h3><blockquote><p>杨等[2]结合以上所有内容在综合降雨模型中提到了退化因素用于在视频中模拟雨的外观。它考虑了时间雨景的属性，尤其是快速变化的降雨通常会引起闪烁的杂音。这种可见强度发生变化沿时间维度的时间称为降雨积累流。此外，它还考虑了其他因素，包括降雨条纹，降雨积聚和降雨遮挡。</p></blockquote><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SID/66.png" title="Optional title"></p><h3 id="Depth-Aware-Rain-Model-（DARM）"><a href="#Depth-Aware-Rain-Model-（DARM）" class="headerlink" title="Depth-Aware Rain Model （DARM）"></a>Depth-Aware Rain Model （DARM）</h3><blockquote><p>Hu等[16]进一步将α连接到场景深度d，以创建深度感知雨模型.</p></blockquote><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SID/77.png" title="Optional title"></p><h2 id="Deraining-Challenges"><a href="#Deraining-Challenges" class="headerlink" title="Deraining Challenges"></a>Deraining Challenges</h2><ul><li>建模雨水图像的困难在现实世界中，雨水可能以多种不同的方式在视觉上出现。雨条纹的大小，形状，比例，密度，方向等可能会有所不同。同样，雨的积累取决于各种水颗粒和大气条件。此外，下雨的外观也极大地依赖于背景场景的纹理和深度。所有这些都导致对降雨的外观进行建模的困难，因此导致渲染物理上正确的降雨图像是一项复杂的任务。</li><li>去雨问题的不适性即使使用仅考虑雨条纹的简单降雨模型，从退化的图像估计背景场景也是不适性的问题。原因是我们只有由带有雨和背景场景的融合信息的光产生的像素强度值。更糟的是，在某些情况下，背景信息可能会完全被雨条纹或浓雨积累或两者同时遮挡。</li><li>难以找到合适的先验信息。由于特征空间中的雨水和背景信息可能会重叠，因此将它们分开并不容易。 背景纹理可能被错误地认为是雨，导致不正确的排水。 因此，有必要对背景纹理和雨水有很强的先验性。 但是，要找到这些先验是困难的，因为背景纹理是多种多样的，并且某些与雨条纹或雨水堆积的外观相似。</li><li>真正的配对真相。大多数深度学习方法都依赖配对的雨水和干净的背景图像来训练其网络。但是，要获得真实的降雨图像及其确切的干净背景图像对是很棘手的。 即使对于静态背景，照明条件也总是会变化。这个困难不仅影响深度学习方法，而且影响评估任何方法的有效性。当前，为了进行定性评估，所有方法都依赖于人类主观判断，以确定恢复后的图像是否良好。 对于定量评估，所有当前方法都依赖于合成图像。不幸的是，到目前为止，合成图像和真实图像之间还存在很大差距。</li></ul><h2 id="Single-image-Deraining-Methods"><a href="#Single-image-Deraining-Methods" class="headerlink" title="Single-image Deraining Methods"></a>Single-image Deraining Methods</h2><h3 id="Model-based-Methods"><a href="#Model-based-Methods" class="headerlink" title="Model-based Methods"></a>Model-based Methods</h3><h4 id="Sparse-Coding-Methods"><a href="#Sparse-Coding-Methods" class="headerlink" title="Sparse Coding Methods"></a>Sparse Coding Methods</h4><p>[56]将输入向量表示为基本向量的稀疏线性组合。这些基向量的集合称为字典，其用于重建特定类型的信号，例如信号。排水问题中有雨条纹和背景信号。</p><p>Lin等[4]首次尝试使用形态学成分分析通过图像分解进行单图像排水。通过字典学习和稀疏编码，将最初提取的雨图像的高频分量进一步分解为雨分量和非雨分量。这项先锋工作成功地消除了稀疏的小雨条纹。但是，它极大地依赖于双边过滤器的预处理，因此会产生模糊的背景细节。</p><p>在后续工作中，Luo等人[5]增强了降雨的稀疏性，并将互斥性引入了区分性稀疏编码（DSC）中，以帮助准确地将降雨/背景层与其非线性复合物分离。由于具有互斥性，DSC保留了干净的纹理细节；但是，它在输出中显示出一些残留的降雨条纹，特别是对于大而密集的降雨条纹。</p><p>为了进一步提高建模能力，Zhu等人[7]构建了一个迭代的层分离过程，以使用背景特定的先验技术从背景层中去除雨水条纹，并从雨层中去除背景的纹理细节。从数量上讲，该方法在某些合成数据集上可获得与同期发布的基于深度学习的方法（即JORDER [1]和DDN [18]）相当的性能。但是，从质量上说，在真实图像上，该方法在处理大雨的情况下往往会失败，因为大雨可能会在不同的方向上移动。</p><p>为了模拟雨条纹的方向和稀疏度，邓等人[17]制定了方向群稀疏模型（DGSM），其中包括三个稀疏项，代表着雨纹的内在方向和结构知识。它可以有效消除模糊的雨纹，但不能消除锐利的雨纹。</p><h4 id="Gaussian-Mixture-Model"><a href="#Gaussian-Mixture-Model" class="headerlink" title="Gaussian Mixture Model"></a>Gaussian Mixture Model</h4><p>Li等[6]应用高斯混合模型（GMM）来模拟雨层和背景层。背景层的GMM是从具有不同背景场景的真实图像中离线获取的。建议从输入图像中选择不具有背景纹理的雨斑来训练雨层的GMM。利用总变化量来消除小火花雨。该方法能够有效地去除小规模和中等规模的雨斑，但是不能处理大而尖的雨斑。</p><h3 id="Deep-Learning-Based-Methods"><a href="#Deep-Learning-Based-Methods" class="headerlink" title="Deep Learning Based Methods"></a>Deep Learning Based Methods</h3><h4 id="Deep-CNNs"><a href="#Deep-CNNs" class="headerlink" title="Deep CNNs"></a>Deep CNNs</h4><p>[1]构建一个联合降雨检测和清除网络。它可以处理大雨，重叠的雨条纹和雨水堆积。该网络可以通过预测二元防雨罩来检测雨水的位置，并采用递归框架来去除雨水条纹并逐步清除雨水积聚。该方法在下大雨的情况下取得了良好的效果。但是，它可能会错误地去除垂直纹理并生成曝光不足的照明。</p><p>同年，傅等人[18，19]尝试通过深层细节网络（DetailNet）去除雨水条纹。该网络仅将高频细节作为输入，并预测雨水残留和清晰的图像。该论文表明，删除网络输入中的背景信息是有益的，因为这样做可以使训练更容易，更稳定。但是，该方法仍然不能处理大而尖的雨纹。</p><p>继杨等[1]和傅等[18，19]，提出了许多基于CNN的方法[8，10，20-23]。这些方法采用了更先进的网络架构，并注入了与降雨相关的新先验。他们在数量和质量上都取得了更好的结果。但是，由于它们受完全监督的学习范式的局限性（即使用合成降雨图像），它们在处理训练中从未见过的真实降雨条件时往往会失败。</p><h4 id="Generative-Adversarial-Networks"><a href="#Generative-Adversarial-Networks" class="headerlink" title="Generative Adversarial Networks"></a>Generative Adversarial Networks</h4><p>生成对抗网络为了捕获无法建模和合成的某些视觉降雨属性，引入对抗学习以减少生成的结果与真实清晰图像之间的领域差距。典型的网络体系结构由两部分组成：生成器和鉴别器，其中鉴别器试图评估所生成的结果是真实的还是伪造的，这提供了额外的反馈以使生成器正规化，以产生更令人愉悦的结果。</p><p>张等[24]直接将条件生成对抗网络（CGAN）用于单图像除雨任务。CGAN能够捕获超出信号保真度的视觉属性，并以更好的照明，颜色和对比度分布呈现结果。但是，当测试降雨图像的背景与训练集中的背景不同时，CGAN有时可能会生成视觉伪像。</p><p>Li等[13]提出了一种结合物理驱动网络和对抗学习精炼网络的单幅图像去雨方法。第一阶段从合成数据中学习并估算与物理相关的成分，即降雨条纹，透射率和大气光。在第二细化阶段，提出了深度引导GAN，以补偿丢失的细节并在第一阶段抑制引入的伪影。从真实的降雨数据中学习，通过这些方法得出的结果在视觉上具有显着性改进，即彻底清除雨水堆积，并实现更加平衡的亮度分布。然而，因为基于GAN的方法并不擅长捕获细粒度细节信号，这些方法也无法正确模拟真实的雨水条纹的外观。</p><h1 id="代码和数据"><a href="#代码和数据" class="headerlink" title="代码和数据"></a>代码和数据</h1><p>作者在github发布了一个存储库，包括74篇雨水去除论文的直接链接，9种视频雨水去除方法和20种单图像雨水去除方法的源代码，19个相关项目页面，6个合成数据集和4个真实数据集，以及4个常用的图像质量度量。</p><p>&lt;hongwang01/Video-and-Single-Image-Deraining&gt;</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>按照原文的序号列出。</p><p>之前进行过阅读的论文序号：</p><ul><li>[1]：基于模型的去雨方法。</li><li>[19]：基于深度CNN网络的去雨方法，称为DerainNet。</li><li>[22]：提出一种密度感知多路稠密连接神经网络算法，DID-MDN。</li><li>[24、28]：GAN相关。</li><li>[29]：基于循环旋转CNN的不确定性多尺度残差学习。</li></ul><p>[1] W. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo, and S. Yan, “Deep joint rain detection and removal from a single image,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, July 2017.<br>[2] W. Yang, J. Liu, and J. Feng, “Frame-consistent recurrent video deraining with dual-level flow,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2019.<br>[3] K. Garg and S. K. Nayar, “Detection and removal of rain from videos,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, vol. 1, 2004, pp. I–528.<br>[4] L. W. Kang, C. W. Lin, and Y. H. Fu, “Automatic single- image-based rain streaks removal via image decomposition,” IEEE Trans. on Image Processing, vol. 21, no. 4, pp. 1742– 1755, April 2012.<br>[5] Y. Luo, Y. Xu, and H. Ji, “Removing rain from a single image via discriminative sparse coding,” in Proc. IEEE Int’l Conf. Computer Vision, 2015, pp. 3397–3405.<br>[6] Y. Li, R. T. Tan, X. Guo, J. Lu, and M. S. Brown, “Rain streak removal using layer priors,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, 2016, pp. 2736–2744.<br>[7] L. Zhu, C. Fu, D. Lischinski, and P. Heng, “Joint bi- layer optimization for single-image rain streak removal,” in Proc. IEEE Int’l Conf. Computer Vision, Oct 2017, pp. 2545– 2553.<br>[8] Z. Fan, H. Wu, X. Fu, Y. Huang, and X. Ding, “Residual- guide network for single image deraining,” in ACM Trans. Multimedia, 2018, pp. 1751–1759.<br>[9] D. Ren, W. Zuo, Q. Hu, P. Zhu, and D. Meng, “Progressive image deraining networks: A better and simpler baseline,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2019.<br>[10] G. Li, X. He, W. Zhang, H. Chang, L. Dong, and L. Lin, “Non-locally enhanced encoder-decoder network for single image de-raining,” in ACM Trans. Multimedia. ACM, 2018, pp. 1056–1064.<br>[11] X. Fu, B. Liang, Y. Huang, X. Ding, and J. Paisley, “Lightweight pyramid networks for image deraining,” IEEE Trans. on Neural Networks and Learning Systems, pp. 1–14, 2019.<br>[12] X.Jin,Z.Chen,J.Lin,Z.Chen,andW.Zhou,“Unsupervised single image deraining with self-supervised constraints,” in Proc. IEEE Int’l Conf. Image Processing, Sep. 2019, pp. 2761–2765.<br>[13] R. Li, L.-F. Cheong, and R. T. Tan, “Heavy rain image restoration: Integrating physics model and conditional adver- sarial learning,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2019.<br>[14] K. Garg and S. K. Nayar, “Vision and rain,” Int. J. Comput. Vision, vol. 75, no. 1, pp. 3–27, October 2007.<br>[15] J. Liu, W. Yang, S. Yang, and Z. Guo, “Erase or fill? deep joint recurrent rain removal and reconstruction in videos,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2018, pp. 3233–3242.<br>[16] X. Hu, C.-W. Fu, L. Zhu, and P.-A. Heng, “Depth-attentional features for single-image rain removal,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2019.<br>[17] L.-J. Deng, T.-Z. Huang, X.-L. Zhao, and T.-X. Jiang, “A di- rectional global sparse model for single image rain removal,” Applied Mathematical Modelling, vol. 59, pp. 662 – 679, 2018.<br>[18] X. Fu, J. Huang, X. Ding, Y. Liao, and J. Paisley, “Re- moving rain from single images via a deep detail network,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, Honolulu, Hawaii, USA, July 2017.<br>[19] ——, “Clearing the skies: A deep network architecture for single-image rain removal,” IEEE Trans. on Image Process- ing, vol. 26, no. 6, pp. 2944–2956, June 2017.<br>[20] R. Li, L.-F. Cheong, and R. T. Tan, “Single Image Deraining using Scale-Aware Multi-Stage Recurrent Network,” arXiv e-prints, p. arXiv:1712.06830, Dec 2017.<br>[21] X. Li, J. Wu, Z. Lin, H. Liu, and H. Zha, “Recurrent squeeze-and-excitation context aggregation net for single image deraining,” in Proc. IEEE European Conf. Computer Vision, 2018, pp. 262–277.<br>[22] H. Zhang and V. M. Patel, “Density-aware single image de- raining using a multi-stream dense network,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2018.<br>[23] J. Pan, S. Liu, D. Sun, J. Zhang, Y. Liu, J. Ren, Z. Li, J. Tang, H. Lu, Y.-W. Tai, and M.-H. Yang, “Learning dual convolu-tional neural networks for low-level vision,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2018.<br>[24] H. Zhang, V. Sindagi, and V. M. Patel, “Image De-raining Using a Conditional Generative Adversarial Network,” arXiv e-prints, p. arXiv:1701.05957, Jan 2017.<br>[25] W. Wei, D. Meng, Q. Zhao, Z. Xu, and Y. Wu, “Semi- supervised transfer learning for image rain removal,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recog- nition, June 2019.<br>[26] S. Li, I. B. Araujo, W. Ren, Z. Wang, E. K. Tokuda, R. H. Junior, R. Cesar-Junior, J. Zhang, X. Guo, and X. Cao, “Single image deraining: A comprehensive benchmark anal- ysis,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2019.<br>[27] D. Eigen, D. Krishnan, and R. Fergus, “Restoring an image taken through a window covered with dirt or rain,” in Proc. IEEE Int’l Conf. Computer Vision, December 2013.<br>[28] R. Qian, R. T. Tan, W. Yang, J. Su, and J. Liu, “Attentive generative adversarial network for raindrop removal from a single image,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2018.<br>[29] R. Yasarla and V. M. Patel, “Uncertainty guided multi-scale residual learning-using a cycle spinning cnn for single image de-raining,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2019.<br>[30] W. Yang, J. Liu, S. Yang, and Z. Guo, “Scale-free single image deraining via visibility-enhanced recurrent wavelet learning,” IEEE Trans. on Image Processing, vol. 28, no. 6, pp. 2948–2961, June 2019.<br>[31] X. Li, J. Wu, Z. Lin, H. Liu, and H. Zha, “Rescan: Re- current squeeze-and-excitation context aggregation net,” in Proc. IEEE European Conf. Computer Vision, Oct. 2018.<br>[32] Y. Wang, S. Liu, C. Chen, and B. Zeng, “A hierarchical approach for rain or snow removing in a single color image,” IEEE Trans. on Image Processing, vol. 26, no. 8, pp. 3936– 3950, Aug 2017.<br>[33] W. Yang, R. T. Tan, J. Feng, J. Liu, S. Yan, and Z. Guo, “Joint rain detection and removal from a single image with contex- tualized deep networks,” IEEE Trans. on Pattern Analysis and Machine Intelligence, pp. 1–1, 2019.<br>[34] T. Wang, X. Yang, K. Xu, S. Chen, Q. Zhang, and R. W. Lau, “Spatial attentive single-image deraining with a high quality real rain dataset,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2019.<br>[35] R. Li, L.-F. Cheong, and R. T. Tan, “Single Image Deraining using Scale-Aware Multi-Stage Recurrent Network,” ArXiv e-prints, December 2017.<br>[36] M. S. Gerald Schaefer, “Ucid: an uncompressed color image database,” 2003.<br>[37] P. K. Nathan Silberman, Derek Hoiem and R. Fergus, “In- door segmentation and support inference from rgbd images,” in Proc. IEEE European Conf. Computer Vision, 2012.<br>[38] C. Godard, O. Mac Aodha, and G. J. Brostow, “Unsupervised monocular depth estimation with left-right consistency,” 2017.<br>[39] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban scene understanding,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, 2016.<br>[40] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik, “Contour detection and hierarchical image segmentation,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 5, pp. 898–916, May 2011.<br>[41] Zhou Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality assessment: from error visibility to structural similarity,” IEEE Trans. on Image Processing, vol. 13, no. 4, pp. 600–612, April 2004.<br>[42] S. Gu, D. Meng, W. Zuo, and L. Zhang, “Joint convolutional analysis and synthesis sparse representation for single image layer separation,” in Proc. IEEE Int’l Conf. Computer Vision, Oct 2017, pp. 1717–1725.<br>[43] A. C. Brooks, X. Zhao, and S. Member, “Structural similarity quality metrics in a coding context: Exploring the space of realistic distortions,” IEEE Trans. on Image Processing, pp. 1261–1273, 2008.<br>[44] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a completely blind image quality analyzer,” IEEE Signal Processing Letters, vol. 20, no. 3, pp. 209–212, March 2013.<br>[45] N. Venkatanath, D. Praneeth, B. M. Chandrasekhar, S. S. Channappayya, and S. S. Medasani, “Blind image quality evaluation using perception based features,” in Proc. IEEE National Conf. Communications, 2008.<br>[46] A. Mittal, A. K. Moorthy, and A. C. Bovik, “Blind/referenceless image spatial quality evaluator,” in Conf. Record of Asilomar Conf. on Signals, Systems and Computers, Nov 2011, pp. 723–727.<br>[47] L. Zhang, L. Zhang, and A. C. Bovik, “A feature-enriched completely blind image quality evaluator,” IEEE Trans. on Image Processing, vol. 24, no. 8, pp. 2579–2591, Aug 2015.<br>[48] L. Liu, B. Liu, H. Huang, and A. C. Bovik, “No-reference image quality assessment based on spatial and spectral en- tropies,” Signal Processing: Image Communication, vol. 29, no. 8, pp. 856 – 863, 2014.<br>[49] C. Ma, C.-Y. Yang, X. Yang, and M.-H. Yang, “Learn- ing a no-reference quality metric for single-image super- resolution,” Comput. Vis. Image Underst., vol. 158, pp. 1–16, May 2017.<br>[50] X. Chen, Q. Zhang, M. Lin, G. Yang, and C. He, “No- reference color image quality assessment: from entropy to perceptual quality,” EURASIP Journal on Image and Video Processing, vol. 2019, no. 1, p. 77, Sep 2019. [Online]. Available: <a href="https://doi.org/10.1186/s13640-019-0479-7">https://doi.org/10.1186/s13640-019-0479-7</a><br>[51] S. Gabarda and G. Cristo ́bal, “Blind image quality assess- ment through anisotropy,” J. Opt. Soc. Am. A, vol. 24, no. 12, pp. B42–B51, Dec 2007.<br>[52] L. Zhang, L. Zhang, and A. C. Bovik, “A feature-enriched completely blind image quality evaluator,” IEEE Trans. on Image Processing, vol. 24, no. 8, pp. 2579–2591, Aug 2015.<br>[53] M. A. Saad, A. C. Bovik, and C. Charrier, “A dct statistics- based blind image quality index,” IEEE Signal Processing Letters, vol. 17, no. 6, pp. 583–586, June 2010.<br>[54] H. Wang, Y. Wu, M. Li, Q. Zhao, and D. Meng, “A Survey on Rain Removal from Video and Single Image,” arXiv e- prints, p. arXiv:1909.08326, Sep 2019.<br>[55] R. A. Bradley and M. E. Terry, “Rank analysis of incom- plete block designs: The method of paired comparisons,” Biometrika, vol. 39, no. 3-4, pp. 324–345, 12 1952.<br>[56] M. Elad and M. Aharon, “Image denoising via learned dictionaries and sparse representation,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2006,pp. 895–900.<br>[57] A.Yamashita,Y.Tanaka,andT.Kaneko,“Removalofadher-<br>ent waterdrops from images acquired with stereo camera,” in IEEE/RSJ Int’l Conf. on Intelligent Robots and Systems, Aug 2005, pp. 400–405.<br>[58] A. Yamashita, I. Fukuchi, and T. Kaneko, “Noises removal from image sequences acquired with moving camera by esti- mating camera motion from spatio-temporal information,” in IEEE/RSJ Int’l Conf. on Intelligent Robots and Systems, Oct 2009, pp. 3794–3801.<br>[59] S. You, R. T. Tan, R. Kawakami, Y. Mukaigawa, and K. Ikeuchi, “Adherent raindrop modeling, detectionand re- moval in video,” IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 38, no. 9, pp. 1721–1733, Sep. 2016.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h1&gt;&lt;p&gt;这是一篇关于单图像去雨和视频</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像去雨" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>“去雨”相关论文研读（二）</title>
    <link href="http://example.com/2020/01/01/%E2%80%9C%E5%8E%BB%E9%9B%A8%E2%80%9D%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://example.com/2020/01/01/%E2%80%9C%E5%8E%BB%E9%9B%A8%E2%80%9D%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%88%E4%BA%8C%EF%BC%89/</id>
    <published>2020-01-01T03:34:29.000Z</published>
    <updated>2020-09-09T12:26:46.820Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="Clearing-the-Skies-A-deep-network-architecture-for-single-image-rain-removal"><a href="#Clearing-the-Skies-A-deep-network-architecture-for-single-image-rain-removal" class="headerlink" title="Clearing the Skies: A deep network architecture for single-image rain removal"></a>Clearing the Skies: A deep network architecture for single-image rain removal</h1><blockquote><p>晴空万里：一种用于单图像雨水去除的深层网络结构</p></blockquote><p>该论文第一次将深度卷积神经网络CNN用于单幅图像去雨，同时在细节层而不是在图像域训练网络。</p><p>之前的方法仅通过低层特性将雨纹从对象细节中分离出来。当物体的结构和方向与雨纹相似时，这些方法很难同时去除雨纹和保存结构信息。另一方面，人类可以利用上下文信息等高级功能，轻松地在一张图像中分辨出雨纹。因此，提出了一种基于深度CNN网络的去雨方法。称为DerainNet。</p><h2 id="DerainNet结构"><a href="#DerainNet结构" class="headerlink" title="DerainNet结构"></a>DerainNet结构</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A82/1.png" title="Optional title"></p><p>首先将输入的含雨图像分离成高频细节层及低频的base 层，将高频细节层作为输入，使用CNN进行特征的提取，得到去雨后的细节层，然后分别对base层及去雨后的细节层进行图像增强，然后相加得到输出图像。</p><h2 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A82/2.png" title="Optional title"></p><h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A82/3.png" title="Optional title"></p><h2 id="图像增强"><a href="#图像增强" class="headerlink" title="图像增强"></a>图像增强</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A82/4.png" title="Optional title"></p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><p>DerainNet直接自动从数据中学习了干净层和雨的细节层(即，高分辨率层)之间的非线性映射函数，同时进行了去雨和图像增强来改善视觉效果。</p><p>使用图像处理领域的知识来修正目标函数，提高去雨质量，而不是使用增加神经元或叠加隐藏层等常用策略来有效地逼近期望的映射函数。</p><h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><p>350张室外清晰图片，（UCID、BSD、Google），使用Photoshop生成训练数据集。添加14种不同方向和强度的雨纹。</p><h3 id="数据及代码是否开源"><a href="#数据及代码是否开源" class="headerlink" title="数据及代码是否开源"></a>数据及代码是否开源</h3><p>未提及。</p><hr><h1 id="Uncertainty-Guided-Multi-Scale-Residual-Learning-using-a-Cycle-Spinning-CNN-for-Single-Image-De-Raining"><a href="#Uncertainty-Guided-Multi-Scale-Residual-Learning-using-a-Cycle-Spinning-CNN-for-Single-Image-De-Raining" class="headerlink" title="Uncertainty Guided Multi-Scale Residual Learning-using a Cycle Spinning CNN for Single Image De-Raining"></a>Uncertainty Guided Multi-Scale Residual Learning-using a Cycle Spinning CNN for Single Image De-Raining</h1><blockquote><p>基于循环旋转CNN的不确定性多尺度残差学习</p></blockquote><hr><blockquote><p>CNN<br><a href="https://blog.csdn.net/liangchunjiang/article/details/79030681">https://blog.csdn.net/liangchunjiang/article/details/79030681</a></p></blockquote><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;Clearing-the-Skies-A-deep-network-architecture-for-single-image-rain-removal</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像去雨" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>“去雨”相关论文研读</title>
    <link href="http://example.com/2019/12/30/%E2%80%9C%E5%8E%BB%E9%9B%A8%E2%80%9D%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    <id>http://example.com/2019/12/30/%E2%80%9C%E5%8E%BB%E9%9B%A8%E2%80%9D%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/</id>
    <published>2019-12-30T07:48:45.000Z</published>
    <updated>2020-09-09T12:26:19.162Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="Attentive-Generative-Adversarial-Network-for-Raindrop-Removal-from-A-Single-Image"><a href="#Attentive-Generative-Adversarial-Network-for-Raindrop-Removal-from-A-Single-Image" class="headerlink" title="Attentive Generative Adversarial Network for Raindrop Removal from A Single Image"></a>Attentive Generative Adversarial Network for Raindrop Removal from A Single Image</h1><blockquote><p>一幅单一图像中雨滴去除的专注生成对抗性网络</p></blockquote><p>这篇论文主要介绍了一个新的利用单张图片去除图片中雨滴的算法。</p><p>作者使用了两种不同的神经网络来实现这三步，作者使用了Generative Network来识别图像中的雨滴，并对其进行复原。通过使用Discriminative Network来对复原后的图片进行识别，以提高复原质量。</p><p>作者使用GAN作为自己方法的算法架构。GAN通过中至少两个模块（生成模块与识别模块）的相互博弈学习产生相当好的输出。作者的生成模块和识别模块都使用神经网络组成。</p><h2 id="Generative-Network"><a href="#Generative-Network" class="headerlink" title="Generative Network"></a>Generative Network</h2><p>作者的生成模块由两个副网络组成，分别是Attentive-recurrent network和Contextual autoencoder。</p><blockquote><p>Given an input image degraded by raindrops, our generative network attempts to produce an image as real as possible and free from raindrops. The discriminative network will validate whether the image produced by the generative network looks real.</p></blockquote><blockquote><p>如果输入的图像因雨滴而退化，我们的生成网络试图生成尽可能真实、不受雨滴影响的图像。判别网络将验证生成网络产生的图像是否真实。</p></blockquote><h3 id="Attentive-recurrent-Network"><a href="#Attentive-recurrent-Network" class="headerlink" title="Attentive-recurrent Network"></a>Attentive-recurrent Network</h3><p>attentive-recurrent network的作用在于寻找原始输入图片中可能存在雨滴的部分，并将这些部分构造成attention map，从而为contextual autoencoder指定雨滴部分及雨滴附近的需要注意的区域。这个网络可以说是作者整个方法的核心部分，正是attention map的引入使得后续的操作变得精准而简单，为图像的复原提供了较好的前提条件。</p><p>attention map的实质是一个大小与原图相等的二维数组，数组中的每一个元素的值是0-1内的一个值。元素的值越大，在后续的操作中，网络会给其及其附近的区域进行优先度更高的操作。</p><p>关于attention map的数值在图片中被雨滴遮盖部分的分布，总的概括就是雨滴的边缘数值最大，雨滴的正中心数值相对来说最小。</p><blockquote><p>Our attention map, which is learned at each time step, is a matrix ranging from 0 to 1, where the greater the value, the greater attention it suggests, as shown in the visualization in Fig. 3. Unlike the binary mask, M, the attention map is a non-binary map, and represents the increasing attention from non-raindrop regions to raindrop regions, and the values vary even inside raindrop regions. This increasing attention makes sense to have, since the surrounding regions of raindrops also needs the attention, and the transparency of a raindrop area in fact varies (some parts do not totally occlude the background, and thus convey some background information).</p></blockquote><blockquote><p>我们的注意力图，是在每个时间步骤中学习的，是一个从0到1的矩阵，其中值越大，它所表示的注意力就越多，如图3中的可视化所示。与二值掩码M不同，注意映射是一种非二值映射，它代表着从非雨滴区域到雨滴区域的注意力的增加，雨滴区域内部的关注度也是不同的。这种注意力的增加是有意义的，因为雨滴周围的区域也需要注意，而雨滴区域的透明度实际上是不同的(有些部分并不完全遮住背景，从而传达了一些背景信息)。</p></blockquote><p>attractive-recurrent network的组成可以由先前的图片中得出，每一层由若干个Residual Block，一个LSTM和一个Convs组成。需要说明的是，图中的每一个“Block”由5层ResNet组成，其作用是得到输入图片的精确特征与前一个Block的模。</p><blockquote><p>Our convolution LSTM unit consists of an input gate i t , a forget gate  ft , an output gate  ot as well as a cell state ct . The interaction between states and gates along time dimension is defined as follows:</p></blockquote><blockquote><p>我们的卷积LSTM单元包括一个输入门 i t 、一个忘记门ft、一个输出门ot以及一个单元状态ct。状态与门随时间维度的相互作用定义如下：</p></blockquote><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A8/1.png" title="Optional title"></p><blockquote><p>where X t is the features generated by ResNet. C t encodes the cell state that will be fed to the next LSTM. H t represents the output features of the LSTM unit. Operator ∗ represents the convolution operation. The LSTM’s output feature is then fed into the convolutional layers, which generate a 2D attention map. In the training process, we initialize the values of the attention map to 0.5. In each time step, we concatenate the current attention map with the input image and then feed them into the next block of our recurrent network.</p></blockquote><blockquote><p>其中，X t 是由ResNet生成的特征； C t 对将要转递到下一个LSTM的状态进行编码； H t代表LSTM单元的输出特性；运算符 * 表示卷积运算。LSTM的输出特征随后被输入到卷积层，这将产生一个2D的注意图。在训练过程中，我们将注意力图的值初始化为0.5。在每个时间步骤中，我们将当前的注意力映射与输入连接起来，然后将它们输入到我们的递归网络的下一个块中。</p></blockquote><p>attention map并不是一次生成的，其生成过程是一个不断叠加，不断优化的过程。作者让神经网络对分别有雨点和没有雨点的图片进行学习，并找出两幅图中信息不相等的部分，并对其进行一定的运算，以找到图片中的所有雨滴.</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A8/2.png" title="Optional title"></p><blockquote><p>In training the generative network, we use pairs of images with and without raindrops that contain exactly the same background scene. The loss function in each recurrent block is defined as the mean squared error (MSE) between the output attention map at time step t, or A t , and the binary mask, M. We apply this process N time steps. The earlier attention maps have smaller values and get larger when approaching the N th time step indicating the increase in confidence.</p></blockquote><blockquote><p>在训练生成网络时，我们使用包含和不包含雨滴的具有完全相同背景场景的图像对。每个循环块中的损失函数定义为在时间步长t的输出注意映射，或者说At与二值掩码M之间的均方误差(MSE)。我们在N个时间步骤中应用这个过程。较早的注意映射值较小，且随着时间步长的增加而变大，这说明信任度的增加。</p></blockquote><h3 id="Contextual-Autoencoder"><a href="#Contextual-Autoencoder" class="headerlink" title="Contextual Autoencoder"></a>Contextual Autoencoder</h3><p>该部分的组成由图2可以看出，该部分的作用在于根据Attractive-recurrentNetwork得出的attention map对于雨点图像进行还原。很讨巧的是，由于attention map的给出，这一部分的工作类似于将attention map中attention值较高的部分通过该部分周围的图片信息形成的新的色块进行替换，从而实现图片信息的还原。</p><p>直接得到的复原图像G(I)并不是十分清晰，其中有一些复原点还很模糊。这是因为在复原过程中除了复原点周围过少造成的无法复原外，还有一部分是由于在卷积过程中的大小变换造成的，作者分别对两类误差进行了量化描述。</p><h2 id="Discriminative-Network"><a href="#Discriminative-Network" class="headerlink" title="Discriminative Network"></a>Discriminative Network</h2><p>GAN中Generative network的作用类似于学生根据题目进行解答，Discriminative network的作用则类似于老师。</p><p>由先前对于Generative network的介绍可以看出，最终输出的复原图像有的误差。为了鉴别这部分的误差是否能被看出，从而引入了Discriminative network。其工作原理是将Generative Network生成的复原图像和原图像作为输入，让Discriminative network对其进行鉴别，判断其是否是同一个图像，若是同一图像，则该图像的复原是成功的，若不是，则不成功，需要继续复原。</p><p>在这里作者又一次引入了attention map来简化操作。由于Generative Network的操作区域只有“attention map”所标注的区域。所以只对这些区域的误差进行判别，既减少了计算量，也在一定程度上提高了识别精度。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><blockquote><p>Our novelty lies on the use of the attention map in both generative and discriminative network. We also consider that our method is the first method that can handle relatively severe presence of raindrops, which the state of the art methods in raindrop removal fail to handle.</p></blockquote><blockquote><p>我们的方法新奇之处在于注意力图在生成网络和判别网络中的使用。我们还认为，我们的方法是第一种能够处理相对严重的雨滴存在的方法，这是目前最先进的雨滴清除方法所不能处理的。</p></blockquote><h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><p>论文里所用到的两类图片，即干净图片和雨滴污染之后的图片均是由作者及其研究团队自行拍摄完成。所用设备原文中有提到。雨滴图片是使用玻璃板驾在镜头之前，人为泼水模拟雨滴。</p><h3 id="代码和数据是否开源"><a href="#代码和数据是否开源" class="headerlink" title="代码和数据是否开源"></a>代码和数据是否开源</h3><p>代码和数据开源。</p><hr><h1 id="Density-aware-Single-Image-De-reaining-using-a-Multi-stream-Dense-Network"><a href="#Density-aware-Single-Image-De-reaining-using-a-Multi-stream-Dense-Network" class="headerlink" title="Density-aware Single Image De-reaining using a Multi-stream Dense Network"></a>Density-aware Single Image De-reaining using a Multi-stream Dense Network</h1><blockquote><p>使用多流密集网络的密度感知单图像去雨</p></blockquote><p>这篇论文提出一种密度感知多路稠密连接神经网络算法，DID-MDN，来雨量密集估计和去雨。这种方法可以使网络自动地判断雨密度，然后有效地去除雨线。</p><p>这个方法主要针对的是，目前的去雨方法只能处理一类雨水的图片，而没有考虑不同密度和尺度的雨水，因此通用性不是很好。如果是将数据集给扩展成不同的雨水密度的数据集，单一的网络无法处理这种多密度的雨水分布。如果是为不同密度的雨水设计网络，又缺少灵活性，所以这篇文章的目的是设计一个网络可以同时处理不同密度雨水的去雨工作。基本思路是先设计一个密度分类网络，得到雨水的密度估计，然后将其fuse到雨水去除网络，实现雨水的自动去除。</p><h2 id="DID-MDN"><a href="#DID-MDN" class="headerlink" title="DID-MDN"></a>DID-MDN</h2><blockquote><p>Density-aware Image De-raining Multi-stream Dense Network</p></blockquote><p>这种网络可以自动的判断雨量密度信息（大、中、小）。这种方法包括两步：雨密度分类和雨线去除。</p><p>（1）为了准确估计雨密度级别，一种新的残差网络利用残差模块在有雨的做分类。</p><p>（2）雨线去除算法基于多流密集连接网络，它考虑了雨线的尺寸和形状信息。</p><p>一旦雨密度级别估计出来，融合已估计的雨密度信息到多流密集连接网络，得到最终的除雨的输出结果。</p><p>DID-MDN主要包括两个模块：Residual-aware Rain-density Classifier和Multi-stream Dense Network。</p><h3 id="Residual-aware-Rain-density-Classifier"><a href="#Residual-aware-Rain-density-Classifier" class="headerlink" title="Residual-aware Rain-density Classifier"></a>Residual-aware Rain-density Classifier</h3><blockquote><p>残差感知雨密度分类器</p></blockquote><p>过去的一些方法总是存在着不足：</p><p>其一，不同雨密度下效果不好。</p><blockquote><p>As discussed above, even though some of the previous methods achieve significant improvements on the de-raining performance, they often tend to over de-rain or under de-rain the image. This is mainly due to the fact that a single network may not be sufficient enough to learn different rain-densities occurring in practice.</p></blockquote><blockquote><p>如上所述，尽管先前的一些方法在去雨性能上取得了显著的改进，但它们往往倾向于过去雨或欠去雨图像。这主要是因为一个单一的网络可能不足以了解实际中发生的不同雨密度。</p></blockquote><p>其二，通用分类模型不好使。</p><blockquote><p>However, we observed that directly fine-tuning such a ‘deep’ model on our task is not an efficient solution. This is mainly due to the fact that high-level features (deeper part) of a CNN tend to pay more attention to localize the discriminative objects in the input image 46. Hence, relatively small rain-streaks may not be localized well in these high-level features. In other words, the rain-streak information may be lost in the high-level features and hence may degrade the overall classification performance. </p></blockquote><blockquote><p>然而，我们观察到，直接微调这样一个“深度”模型对我们的任务不是一个有效的解决方案。这主要是因为CNN的高层次特征（更深的部分）倾向于更关注输入图像中的可分辨对象的局部化。因此，相对较小的雨带可能不会很好地局部化在这些高层特征中。换言之，雨痕信息可能在高层特征中丢失，因此可能降低总体分类性能。</p></blockquote><p>所以这里需要估计出带雨图片和干净图片的残差也就是雨水信息，这里使用了一个多流的densenet网络，估计得到雨水的残差信息，然后输入到分类网络。残差估计模块可以看做是特征提取模块。这里在训练时，首先对残差特征提取模块进行训练，然后训练分类网络。最后两步合在一起训练，损失函数表示为:</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A8/3.png" title="Optional title"></p><p>其中第一项是残差信息像素的欧式距离，第二项是分类的交叉熵损失函数。</p><h3 id="Multi-stream-Dense-Network"><a href="#Multi-stream-Dense-Network" class="headerlink" title="Multi-stream Dense Network"></a>Multi-stream Dense Network</h3><blockquote><p>多流密集感知去雨网络</p></blockquote><p>由于不同的雨水密度不一样，所以这里使用多尺度的特征更有利于捕捉不同的雨水密度信息，通过使用不同卷积核的densenet实现。最后通过一个concat将这些信息融合在一起，从而融合多尺度的雨水信息。为了将雨水密度信息来引导去雨过程，上采样之后的雨水label被concat到这些雨水feature上，然后这些concat之后的特征被用来估计雨水的残差信息，用有雨水的图片减去残差信息获得coarse的去雨图片，最后通过一个refine的网络去获得输出的去雨之后的图片。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A8/4.png" title="Optional title"></p><p>最后损失函数被定义为:</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A8/5.png" title="Optional title"></p><p>其中第二项是去雨图片的欧式距离，最后一项是基于relu1_2的VGG的feature loss。</p><h2 id="其他-1"><a href="#其他-1" class="headerlink" title="其他"></a>其他</h2><h3 id="贡献-1"><a href="#贡献-1" class="headerlink" title="贡献"></a>贡献</h3><blockquote><p>This paper makes the following contributions:</p><ol><li>AnovelDID-MDNmethodwhichautomaticallydeter- mines the rain-density information and then efficiently removes the corresponding rain-streaks guided by the estimated rain-density label is proposed.</li><li>Based on the observation that residual can be used as a better feature representation in characterizing the rain- density information, a novel residual-aware classifier to efficiently determine the density-level of a given rainy image is proposed in this paper.</li><li>A new synthetic dataset consisting of 12,000 training images with rain-density labels and 1,200 test images is synthesized. To the best of our knowledge, this is the first dataset that contains the rain-density label in- formation. Although the network is trained on our syn- thetic dataset, it generalizes well to real-world rainy images.</li><li>Extensive experiments are conducted on three highly challenging datasets (two synthetic and one real- world) and comparisons are performed against several recent state-of-the-art approaches. Furthermore, an ab- lation study is conducted to demonstrate the effects of different modules in the proposed network.</li></ol></blockquote><blockquote><p>DID-MDN自动的判断雨量密度信息，然后通过估计出来的雨量密度标签去除雨线；基于残差可以作为更好的描述雨密度信息的特征，论文提出残差感知分类器来判断雨量密度；合成数据集包括 12000 带有雨量密度的训练图片和1200张测试图片；据作者所知这是第一个包含雨量标签的数据集。尽管是在合成数据上训练的，它可以泛化到真实世界的图片；在三个高挑战性的数据集上（两个合成、一个真实）和比赛上进行了大量的实验。</p></blockquote><h3 id="数据-1"><a href="#数据-1" class="headerlink" title="数据"></a>数据</h3><p>来自论文【4.1. Synthetic Dataset】。</p><h3 id="代码和数据是否开源-1"><a href="#代码和数据是否开源-1" class="headerlink" title="代码和数据是否开源"></a>代码和数据是否开源</h3><p>未提及。</p><hr><h1 id="Densely-Connected-Pyramid-Dehazing-Network"><a href="#Densely-Connected-Pyramid-Dehazing-Network" class="headerlink" title="Densely Connected Pyramid Dehazing Network"></a>Densely Connected Pyramid Dehazing Network</h1><blockquote><p>密集连接金字塔除雾网络</p></blockquote><blockquote><p>这是一片研究图像去雾的论文。选择这样一篇与“去雨”不相合但是有一定的相关性的论文，是想要借鉴一些这类问题的共性，即图像进化或者图像去污，而不只是着眼于“去雨”。</p></blockquote><blockquote><p>此篇略读。</p></blockquote><p>基于大气散射模型的图像去雾研究很多，主要的步骤是估计transmission map和自然光。但是绝大部分研究更多的放在精确的估计transmission map， 而利用经验公式去估计自然光，似乎都默认好的transmission map会带来比较好的去雾效果，对自然光的估计并没有很重视。现有方法主要分为基于先验信息（prior-based)的和基于深度学习(learning-based)的方法来估计transmission map。</p><p>作者认为现有的方法存在以下不足：首先是transmission map估计还不够准确。另外是估计transmission map/ 自然光等步骤是分开估计（优化）的，并非end to end，因此作者认为上述几个部分同时估计的情况下，效果会有提升。</p><p>文章中所叙述的网络包含以下几个部分：</p><h2 id="Pyramid-densely-connected-network"><a href="#Pyramid-densely-connected-network" class="headerlink" title="Pyramid densely connected network"></a>Pyramid densely connected network</h2><h3 id="dense-block"><a href="#dense-block" class="headerlink" title="dense block"></a>dense block</h3><p>用dense block做CNN的基本block来提取feature。dense block被认为是有利于融合多尺度特征。</p><h3 id="multi-level-pyramid-pooling-method"><a href="#multi-level-pyramid-pooling-method" class="headerlink" title="multi-level pyramid pooling method"></a>multi-level pyramid pooling method</h3><p>该方法是将得到的特征用不同尺寸的pooling，四个池化层，4，8，16，32分别进行，然后再扩张回到原尺寸，就算多尺度的feature。</p><p>同时优化多个task的网络还是比较难训练的，作者使用leverage stage-wise 训练的方案，先分别优化网络的每一个部分，然后再联合优化整个网络。</p><h3 id="自然光估计"><a href="#自然光估计" class="headerlink" title="自然光估计"></a>自然光估计</h3><p>作者同样假设大气光是均匀的e.g. A(z)=c constant。采用U-net来估计图像的大气光。</p><h3 id="去雾图像"><a href="#去雾图像" class="headerlink" title="去雾图像"></a>去雾图像</h3><p>根据得到的tranmission map和大气光，按照大气散射模型，估计得到去雾图像。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A8/6.png" title="Optional title"></p><h3 id="discriminator"><a href="#discriminator" class="headerlink" title="discriminator"></a>discriminator</h3><blockquote><p>与第一篇的相似之处。</p></blockquote><p>增加了一个discriminator来refine生成的transmission map， A(z)。 discriminator的作用就是让产生的transmission map 和 A(z) 像ground-truth 靠拢，而更加难以分辨。</p><h3 id="边沿的loss和总loss"><a href="#边沿的loss和总loss" class="headerlink" title="边沿的loss和总loss"></a>边沿的loss和总loss</h3><p>边沿的loss按照如下公式进行。总loss的训练方面，由于多task的同时训练不好收敛，作者提出initialization stage，先分块优化每一个小block，然后再统一训练。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A8/7.png" title="Optional title"></p><h2 id="其他-2"><a href="#其他-2" class="headerlink" title="其他"></a>其他</h2><h3 id="代码和数据是否开源-2"><a href="#代码和数据是否开源-2" class="headerlink" title="代码和数据是否开源"></a>代码和数据是否开源</h3><p>开源。</p><p>github：<a href="https://github.com/hezhangsprinter/DCPDN">https://github.com/hezhangsprinter/DCPDN</a></p><hr><h1 id="Deep-Joint-Rain-Detection-and-Removal-from-a-Single-Image"><a href="#Deep-Joint-Rain-Detection-and-Removal-from-a-Single-Image" class="headerlink" title="Deep Joint Rain Detection and Removal from a Single Image"></a>Deep Joint Rain Detection and Removal from a Single Image</h1><blockquote><p>单图像深度联合雨水检测与去除</p></blockquote><p>本文在现有的模型上，开发了一种多任务深度学习框架，学习了三个方面，包括二元雨条纹映射(binary rain streak map)，雨条纹外观和干净的背景。特别是新添加的二元雨条纹映射，其损失函数可以为神经网络提供额外的强特征。对于雨带积累现象（暴雨形成的如烟如雾的现象），采取循环雨检测和清除，以迭代和渐进方式清除。</p><p>针对去雨问题已经提出了各种算法，当前算法主要存在的问题如下：因为雨水和背景纹理的内在重叠性，当前大部分算法会平滑没有雨区域的纹理细节；雨水在图像中引起的变化是复杂的，但是当前对雨水常用的模型没有很好的覆盖真是雨水图像中的一些重要因素，例如水气，雨水的不同形状，或方向；一个重要的信息： spatial contextual information 没有被考虑。</p><p>为此该文章提出了一个新的模型用于去雨,思路大致如下： </p><p>（1）首先提出了一个基于区域的雨水模型，在模型中使用了一个二值雨水图，如果该像素位置有可见雨，那么二值图中的值为1，否则为0。 </p><p>（2）基于上面建立的模型，构建一个深度网络用于检测雨水和去除雨水。可以自动检测出雨水区域，对这些区域进行雨水去除。 </p><p>（3）提出了一个回归雨水检测去除网络，a recurrent rain detection and removal network。</p><h2 id="雨水模型及变化"><a href="#雨水模型及变化" class="headerlink" title="雨水模型及变化"></a>雨水模型及变化</h2><h3 id="常用雨水模型"><a href="#常用雨水模型" class="headerlink" title="常用雨水模型"></a>常用雨水模型</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A8/8.png" title="Optional title"></p><p>这里B是没有雨水的图像，S是rain streak layer雨水层，O是有雨水的图像。基于该模型，去雨水被看做是两个信号的分离问题，基于观察到的O恢复B和S。</p><p>这个模型有两个问题：</p><p>（1）没有区分对待 heavy rain 和 light rain；</p><p>（2）没有区分对待 rain and non-rain regions。</p><h3 id="广义雨水模型"><a href="#广义雨水模型" class="headerlink" title="广义雨水模型"></a>广义雨水模型</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A8/9.png" title="Optional title"></p><p>针对上述问题，提出了一个广义的雨水模型。</p><p>引入的 R 是一个二值图，1表示该位置有雨，0表示没雨。</p><h3 id="Rain-Accumulation-and-Heavy-Rain"><a href="#Rain-Accumulation-and-Heavy-Rain" class="headerlink" title="Rain Accumulation and Heavy Rain"></a>Rain Accumulation and Heavy Rain</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A8/10.png" title="Optional title"></p><p>针对雨雾和问题，提出了上述模型来解决。</p><p>上式中每个St表示雨纹相同即方向和形状类似。</p><blockquote><p>t is the overlapping streak numbers</p></blockquote><p>t表示图像中含有的雨纹个数，即方向和形状类似种类，其中B是原图，O是带雨的图片，其他的量代表雨滴带来的影响。S指叠加的雨滴的强度，R指含雨滴范围的一个 binary mask，A对雨雾进行建模。</p><p>之所以将S、R分别描述并分别用网络预测，是为了避免只回归S影响了图中不含雨滴的部分，R实际上描述了雨滴存在的区域，这也是标题中rain detection的含义。t指的是图片中多个方向的雨叠加的效果，训练所用的合成雨的图片就是多次叠加的结果。最后A描述了一个图像整体的偏移，这是由大雨中远处大量雨滴叠加造成的类似雾的效果，实际算法中也用了去雾算法做处理。</p><h2 id="其他-3"><a href="#其他-3" class="headerlink" title="其他"></a>其他</h2><h3 id="数据-2"><a href="#数据-2" class="headerlink" title="数据"></a>数据</h3><p>训练数据和测试方法上都是沿用之前工作的方法。训练数据都是使用不带雨的图片人工合成带雨的图片，并从图中抽取patch进行训练。在测试流程上，对于合成图片，主要比较衡量图片结构相似度的SSIM指标。对于真实环境的带雨图片，主要是视觉上的qualitative比较。</p><h3 id="数据和代码是否开源"><a href="#数据和代码是否开源" class="headerlink" title="数据和代码是否开源"></a>数据和代码是否开源</h3><p>查询网页有提及会开源，但未找到。</p><hr><h1 id="Image-De-raining-Using-a-Conditional-Generative-Adversarial-Network"><a href="#Image-De-raining-Using-a-Conditional-Generative-Adversarial-Network" class="headerlink" title="Image De-raining Using a Conditional Generative Adversarial Network"></a>Image De-raining Using a Conditional Generative Adversarial Network</h1><blockquote><p>基于条件生成对抗网络的图像去雨</p></blockquote><p>这篇论文提出一种基于cGAN网络的一种新的去雨网络。</p><p>主要是试图使用具有强大的生成建模能力的条件GAN网络加上一个强制约束，即去雨图像必须与相应的背景图像不可区分。同时GAN网络的对抗损失提供了额外的正则化。同时，还提出了一种新的细化损失函数，旨在减少GAN网络引入的伪影。</p><p>ID-CGAN主要由两个子网络组成：密集连接的生成器（generator）和多尺度的判别器(discriminator)。</p><p>生成器子网络使用了密集连接网络。判别器主要通过多尺度池来捕获上下文信息。同时在GAN网络训练时会引入伪影，这里引入一个改进的感知损失作为额外的损失函数来去除伪影。</p><h2 id="GAN目标函数"><a href="#GAN目标函数" class="headerlink" title="GAN目标函数"></a>GAN目标函数</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A8/11.png" title="Optional title"></p><p>G为生成器，D是判别器。</p><h2 id="对称结构Generator"><a href="#对称结构Generator" class="headerlink" title="对称结构Generator"></a>对称结构Generator</h2><p>在分离之后，新的域中的背景图像必须要转换回原来的域，这便要求了对称结构的使用。密集块体使强梯度流动成为可能，并提高了参数效率。此外，我们还引入了跨越密集块的跳跃连接，以有效地利用来自不同级别的特性，并保证更好的收敛性。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A8/12.png" title="Optional title"></p><p>其中，CBLP是一组卷积层，后面依次是BN 、leaky ReLU激活函数和池化模块，括号内的数字表示每个块的输出特征映射的通道数。</p><h2 id="多尺度Discriminator"><a href="#多尺度Discriminator" class="headerlink" title="多尺度Discriminator"></a>多尺度Discriminator</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A8/13.png" title="Optional title"></p><p>前人工作发现，基于Patch的判别是有效的，但是仍然不能捕捉到足够的上下文全局信息。因此，需要一个更强大的判别器捕捉局部和全局信息来判断图像是真是假。</p><h2 id="感知损失函数"><a href="#感知损失函数" class="headerlink" title="感知损失函数"></a>感知损失函数</h2><p>GANs训练中会引入伪影，于是引入感知损失来去除伪影。</p><p>新的损失函数为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A8/14.png" title="Optional title"></p><p>其中，每个像素的欧几里得损失为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A8/15.png" title="Optional title"></p><p>感知损失为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A8/16.png" title="Optional title"></p><p>对抗损失为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/%E5%8E%BB%E9%9B%A8/17.png" title="Optional title"></p><h2 id="其他-4"><a href="#其他-4" class="headerlink" title="其他"></a>其他</h2><h3 id="贡献-2"><a href="#贡献-2" class="headerlink" title="贡献"></a>贡献</h3><p>提出了一种基于cGAN网络的框架用于处理单幅图像去雨问题，同时不需要任何后续处理。</p><p>提出了一个密集连接的生成器子网络。</p><p>提出了一种多尺度鉴别器，利用局部信息和全局信息来判断去雨图像的真伪。</p><h3 id="数据-3"><a href="#数据-3" class="headerlink" title="数据"></a>数据</h3><p>使用PS自建合成数据集。700张训练图，100张测试，256x256。50张真实世界图片。</p><p>评价指标：PSNR、SSIM、UQI、VIF。</p><h3 id="数据和代码是否开源-1"><a href="#数据和代码是否开源-1" class="headerlink" title="数据和代码是否开源"></a>数据和代码是否开源</h3><p>开源。</p><p><a href="https://github.com/hezhangsprinter/ID-CGAN">https://github.com/hezhangsprinter/ID-CGAN</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;Attentive-Generative-Adversarial-Network-for-Raindrop-Removal-from-A-Single-</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像去雨" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>“一场战争”</title>
    <link href="http://example.com/2019/12/25/%E2%80%9C%E4%B8%80%E5%9C%BA%E6%88%98%E4%BA%89%E2%80%9D/"/>
    <id>http://example.com/2019/12/25/%E2%80%9C%E4%B8%80%E5%9C%BA%E6%88%98%E4%BA%89%E2%80%9D/</id>
    <published>2019-12-25T01:39:48.000Z</published>
    <updated>2020-09-09T12:01:28.901Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，不可转载。</p></blockquote><hr><blockquote><p>这是一场战争，我从战场归来。</p></blockquote><p>半年的时光就这么匆匆过去，我也完成了这场不知结果的考试。</p><p>现在我在图书馆，坐在这半年里我一直坐着的位置。按理说现在应该是休息的时间，只是我睡不着。说来也奇怪，考研养成的早起习惯，到今天都没有消退掉。想想之前，早起过一段时间，只要放假的日子侵蚀一两天，也就变得懒散至极，10点钟起床是常有的事情。</p><p>其实，我并不想赋予这场考试和我这半年的努力以太多的意义，因为这本身就可能是无意义的事情。和我一样战斗的朋友们也有很多，沉浸在这样的氛围里，不自觉地就把自己的所作所为当成了一种必要，当成了一种无畏的勇气和自然的选择。倒不是说这样子有什么不好，只是对我自己，我更习惯保持一种平和。</p><p>我幻想过考完之后的狂欢：睡觉睡到天昏地暗，打游戏打到日月无光。考前的那个周五晚上，我甚至下了决心，我要把我这半年欠着的所有游戏，在一周之内打得干干净净。想法是真的美好，现实也是真的无力。考完的那晚上就已经非常没劲头了。放假的第一天，上午处理了点学期末的正事，然后开始过肥宅生活直到今早。打游戏？一个小时就变得索然无味。看视频？电视剧综艺我本来就不爱看，考研前攒着的一些喜欢的up主的视频，完事之后就不想看了。睡觉？我熬夜到一两点，早晨照样7点钟醒。</p><p>仿佛一起事情都在这场战争宣告结束之后变得没有意义了起来。所以我就思考，是这场考试赋予我意义，还是我赋予这场考试意义？我觉着都不是。</p><p>在我看来，“宠辱偕忘”是人生的最高境界，也是我追求和向往的人格。不管是考前还是考后，其实我一直在给自己洗脑：你考不上的，这个考试你就是分母，就是炮灰，你就是人菜瘾大。其实这还是有点反向效果的，越是这样我就越想考上，越是这样我就越想翻身，越是这样我就觉得我越不该是这样。我本来就不该是这样。只不过这么想呢，宠倒是忘了大半，辱的感觉越来越重了。毕竟以毒攻毒和双管齐下是两种疗法，这我懂。</p><p>说起来，这半年也体味了非比寻常的生活。我不止一次的想过如果我坚持不下去了会怎样。失眠的晚上总是止不住的心跳加速，我真的好怕自己就交代在这儿了。坐在图书馆里摸鱼的时候，前半分钟想的是我怎么这么堕落，后半分钟想的是我前半分钟看的那个视频真有意思。晚上从图书馆出来，不想回宿舍，就绕着学校一圈一圈的走，走到路上都没有几个人了，才慢慢走回去。</p><p>我原本以为我是很强大的，不管是从内心还是在表面。我已经过了两年没有什么人陪伴的生活，再过半年“强化版”的也无大所谓。可事实上，每一天我都比前一天更加渴望有人能够一起交流。我这样的性格，要去主动找一个研友，在我自己这里显得没多大必要。另外一个考研的舍友是一位极度佛系的人，总之叫人家来图书馆学习是一件很难的事情，人家更享受宿舍的生活。宿舍里另有一位同学，倒是经常一起聊天，只不过并不是同一战场的朋友。图书馆里经常碰到的一位同班的同学成了这段时间的好朋友，大家关系一般，他也是一个独立惯了的人，只是枯燥的生活逼得大家团结在一起。我信任这种力量，这样的力量是比所学到的知识更加有用的收获。</p><p>中间还有过一段玻璃心的日子。其实后半段的两三个月都非常玻璃心。我关掉了朋友圈，关掉了qq，微博取关了周围的大部分人，只是不想看到他们的生活过得比我好。每天也不怎么和人交流，因为我也不知道说些什么样的话。沉默是另一种力量，是一种磨砺，哪怕在沉默中灭亡，都要比叽叽喳喳的人更有尊严。我更明白了沉默的意义，比以前。即使这半年的努力全部都是白给，但这种生活的体味，一定是不可多得的财富。</p><p>考完政治英语的那一天晚上，临睡前，莫名的想到了家人，都是一些悲观的设想。我突然觉得我对自己和家人是不是都没有做到位？如果这半年都是白费的，那么我给予周围人、家人和我自己的这么多负面情绪，连一个好的解释都没有。考前我的母亲就说要去医院体检，这件事我一直挂在心上。可现在考完了也是一堆事情压在身上，想要早回家都不现实。所以我一定要把这种残忍给我的家人们吗？多陪陪他们和实现自己的梦想，到底该选什么？我做不出选择，因为我都想要。我觉得我不称职，我只能感谢能够理解我的人。</p><p>我想要更加热爱生活，我想要做好生活中的每一件事情。追逐是一个过程，结果不是一场宣判，没有什么东西可以充当宣判，如果有的话，那只能是我自己。因为这半年的时间让我明白，不管是怎样难过的日子，都会过来的。这放在以前只是一句无力的安慰，但是我现在真切的感受到了，除了死亡，没有什么过不去的事情。只是我更希望我以后不要再体会到这些了，因为这些都是在每一个睡不着的晚上，给自己的遗言。</p><p>痛苦的意义是让以后的日子不再痛苦，成长的过程是让以后的日子少些幼稚。即使幼稚，即使痛苦，即使这样的标签会跟随一生，那又怎样？我有前进的欲望，足矣。珍惜现在的生活，才是最有意义的，除此之外，都是虚妄。现在的我，很淡然，足矣。</p><p>感谢在这半年里鼓励过着我的人，有从小玩到大的发小，有一同战斗的战友，有给予我深刻思考的信友，有老师，有家人。今后的我会不吝自己的热忱和真诚，只因为曾经有人在我战斗的时候给予我力量，也为了以后的我在战斗的时候能有更多的力量源泉。</p><p>这是一场战争，我从战场归来。不知道这是败退还是凯旋，我无谓输赢，我必然是赢家。</p><hr><p align="right">2019年12月25日 星期三上午11时主区图书馆</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，不可转载。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;这是一场战争，我从战场归来。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;半年的时光就这么匆匆过去，我也完成了这场不知结果的考试。&lt;/p&gt;
&lt;p&gt;现在</summary>
      
    
    
    
    
    <category term="闲谈" scheme="http://example.com/tags/%E9%97%B2%E8%B0%88/"/>
    
    <category term="考研" scheme="http://example.com/tags/%E8%80%83%E7%A0%94/"/>
    
  </entry>
  
  <entry>
    <title>操作系统五：文件管理系统</title>
    <link href="http://example.com/2019/06/16/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%BA%94%EF%BC%9A%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/"/>
    <id>http://example.com/2019/06/16/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%BA%94%EF%BC%9A%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/</id>
    <published>2019-06-16T14:40:05.000Z</published>
    <updated>2019-06-16T15:38:43.920Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！<br>代码、文章及图片挂载地址：<a href="https://github.com/MoyangSensei/OS">https://github.com/MoyangSensei/OS</a></p></blockquote><h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本实验要求在模拟的I/O系统之上开发一个简单的文件系统。用户通过create,open,read等命令与文件系统交互。文件系统把磁盘视为顺序编号的逻辑块序列，逻辑块的编号为0至L−1。I/O系统利用内存中的数组模拟磁盘。</p><h2 id="I-O系统"><a href="#I-O系统" class="headerlink" title="I/O系统"></a>I/O系统</h2><p>实际物理磁盘的结构是多维的：有柱面、磁头、扇区等概念。I/O系统的任务是隐藏磁盘的结构细节，把磁盘以逻辑块的面目呈现给文件系统。逻辑块顺序编号，编号取值范围为0至L−1，其中L表示磁盘的存储块总数。实验中，我们可以利用数组ldisk[C][H][B]构建磁盘模型，其中CHB分别表示柱面号，磁头号和扇区号。每个扇区大小为512字节。I/O系统从文件系统接收命令，根据命令指定的逻辑块号把磁盘块的内容读入命令指定的内存区域，或者把命令指定的内存区域内容写入磁盘块。文件系统和I/O系统之间的接口由如下两个函数定义：<br>• read_block(int i, char *p);<br>该函数把逻辑块i的内容读入到指针p指向的内存位置，拷贝的字符个数为存储块的长度B。<br>• write block(int i, char *p);<br>该函数把指针p指向的内容写入逻辑块i，拷贝的字符个数为存储块的长度B。此外，为了方便测试，我们还需要实现另外两个函数：一个用来把数组ldisk 存储到文件；另一个用来把文件内容恢复到数组。</p><h2 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h2><p>文件系统位于I/O系统之上。</p><h3 id="用户与文件系统之间的接口"><a href="#用户与文件系统之间的接口" class="headerlink" title="用户与文件系统之间的接口"></a>用户与文件系统之间的接口</h3><p>文件系统需提供如下函数；create, destroy, open, read, write。<br>• create(filename): 根据指定的文件名创建新文件。<br>• destroy(filename): 删除指定文件。<br>• open(filename): 打开文件。该函数返回的索引号可用于后续的read, write, lseek,或close操作。<br>• close(index): 关闭制定文件。<br>• read(index, mem_area, count): 从指定文件顺序读入count个字节memarea指定的内存位置。读操作从文件的读写指针指示的位置开始。<br>• write(index, mem_area, count): 把memarea指定的内存位置开始的count个字节顺序写入指定文件。写操作从文件的读写指针指示的位置开始。<br>• lseek(index, pos): 把文件的读写指针移动到pos指定的位置。pos是一个整数，表示从文件开始位置的偏移量。文件打开时，读写指针自动设置为0。每次读写操作之后，它指向最后被访问的字节的下一个位置。lseek能够在不进行读写操作的情况下改变读写指针能位置。<br>• directory: 列表显示所有文件及其长度。</p><h3 id="文件系统的组织"><a href="#文件系统的组织" class="headerlink" title="文件系统的组织"></a>文件系统的组织</h3><p>磁盘的前k个块是保留区，其中包含如下信息：位图和文件描述符。位图用来描述磁盘块的分配情况。位图中的每一位对应一个逻辑块。创建或者删除文件，以及文件的长度发生变化时，文件系统都需要进行位图操作。前k个块的剩余部分包含一组文件描述符。每个文件描述符包含如下信息：<br>• 文件长度，单位字节<br>• 文件分配到的磁盘块号数组。该数组的长度是一个系统参数。在实验中我们可以把它设置为一个比较小的数，例如3。</p><h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p>我们的文件系统中仅设置一个目录，该目录包含文件系统中的所有文件。除了不需要显示地创建和删除之外，目录在很多方面和普通文件相像。目录对应0号文件描述符。初始状态下，目录中没有文件，所有，目录对应的描述符中记录的长度应为0，而且也没有分配磁盘块。每创建一个文件，目录文件的长度便增加一分。目录文件的内容由一系列的目录项组成，其中每个目录项由如下内容组成：<br>• 文件名<br>• 文件描述符序号</p><h3 id="文件的创建与删除"><a href="#文件的创建与删除" class="headerlink" title="文件的创建与删除"></a>文件的创建与删除</h3><p>创建文件时需要进行如下操作；<br>• 找一个空闲文件描述符(扫描ldisk [0]～ldisk [k - 1])<br>• 在文件目录里为新创建的文件分配一个目录项（可能需要为目录文件分配新的磁盘块）<br>• 在分配到的目录项里记录文件名及描述符编号．<br>• 返回状态信息（如有无错误发生等）</p><p>删除文件时需要进行如下操作（假设文件没有被打开）：<br>• 在目录里搜索该文件的描述符编号<br>• 删除该文件对应的目录项并更新位图<br>• 释放文件描述符<br>• 返回状态信息</p><h3 id="文件的打开与关闭"><a href="#文件的打开与关闭" class="headerlink" title="文件的打开与关闭"></a>文件的打开与关闭</h3><p>文件系统维护一张打开文件表．打开文件表的长度固定，其表目包含如下信息：<br>• 读写缓冲区<br>• 读写指针<br>• 文件描述符号</p><p>文件被打开时，便在打开文件表中为其分配一个表目；文件被关闭时，其对应的表目被释放。读写缓冲区的大小等于一个磁盘存储块。打开文件时需要进行的操作如下：<br>• 搜索目录找到文件对应的描述符编号<br>• 在打开文件表中分配一个表目<br>• 在分配到的表目中把读写指针置为０，并记录描述符编号<br>• 读入文件的第一块到读写缓冲区中<br>• 返回分配到的表目在打开文件表中的索引号</p><p>关闭文件时需要进行的操作如下：<br>• 把缓冲区的内容写入磁盘<br>• 释放该文件在打开文件表中对应的表目<br>• 返回状态信息</p><h3 id="读写"><a href="#读写" class="headerlink" title="读写"></a>读写</h3><p>文件打开之后才能进行读写操作．读操作需要完成的任务如下：</p><ol><li>计算读写指针对应的位置在读写缓冲区中的偏移</li><li>把缓冲区中的内容拷贝到指定的内存位置，直到发生下列事件之一：<br>• 到达文件尾或者已经拷贝了指定的字节数。这时，更新读写指针并返回相应信息<br>• 到达缓冲区末尾。这时，把缓冲区内容写入磁盘，然后把文件下一块的内容读入磁盘。最后返回第2步。</li></ol><p>其他操作请同学们自己考虑。</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>为了能够对我们的模拟系统进行测试，请编写一个操纵文件系统的外壳程序或者一个菜单驱动系统。</p><hr><h1 id="解答"><a href="#解答" class="headerlink" title="解答"></a>解答</h1><h2 id="Liunx视角下的文件"><a href="#Liunx视角下的文件" class="headerlink" title="Liunx视角下的文件"></a>Liunx视角下的文件</h2><p><strong>在LINUX系统中有一个重要的概念：一切都是文件。</strong>其实这是UNIX哲学的一个体现，而Linux是重写UNIX而来，所以这个概念也就传承了下来。在UNIX系统中，把一切资源都看作是文件，包括硬件设备。UNIX系统把每个硬件都看成是一个文件，通常称为设备文件，这样用户就可以用读写文件的方式实现对硬件的访问。</p><p>Linux启动时，第一个必须挂载的是根文件系统；若系统不能从指定设备上挂载根文件系统，则系统会出错而退出启动。之后可以自动或手动挂载其他的文件系统。因此，一个系统中可以同时存在不同的文件系统。</p><p>对于Linux系统来说，从物理磁盘到文件系统有如下概念：</p><p>我们知道文件最终是保存在硬盘上的。硬盘最基本的组成部分是由坚硬金属材料制成的涂以磁性介质的盘片，不同容量硬盘的盘片数不等。每个盘片有两面，都可记录信息。盘片被分成许多扇形的区域，每个区域叫一个扇区，每个扇区可存储128×2的N次方（N＝0.1.2.3）字节信息。在DOS中每扇区是128×2的2次方＝512字节，盘片表面上以盘片中心为圆心，不同半径的同心圆称为磁道。硬盘中，不同盘片相同半径的磁道所组成的圆柱称为柱面。磁道与柱面都是表示不同半径的圆，在许多场合，磁道和柱面可以互换使用，我们知道，每个磁盘有两个面，每个面都有一个磁头，习惯用磁头号来区分。扇区，磁道（或柱面）和磁头数构成了硬盘结构的基本参数，帮这些参数可以得到硬盘的容量，基计算公式为：</p><pre><code>存储容量＝磁头数×磁道（柱面）数×每道扇区数×每扇区字节数</code></pre><p>基本要点有：</p><pre><code>（1）硬盘有数个盘片，每盘片两个面，每个面一个磁头（2）盘片被划分为多个扇形区域即扇区（3）同一盘片不同半径的同心圆为磁道（4）不同盘片相同半径构成的圆柱面即柱面（5）存储容量＝磁头数×磁道（柱面）数×每道扇区数×每扇区字节数（6）信息记录可表示为：××磁道（柱面），××磁头，××扇区</code></pre><p>那么这些空间又是怎么管理起来的呢？unix/linux使用了一个简单的方法。它将磁盘块分为以下三个部分：</p><p>（1）超级块，文件系统中第一个块被称为超级块。这个块存放文件系统本身的结构信息。比如，超级块记录了每个区域的大小，超级块也存放未被使用的磁盘块的信息。<br>（2）I-切点表。超级块的下一个部分就是i-节点表。每个i-节点就是一个对应一个文件/目录的结构，这个结构它包含了一个文件的长度、创建及修改时间、权限、所属关系、磁盘中的位置等信息。一个文件系统维护了一个索引节点的数组，每个文件或目录都与索引节点数组中的唯一一个元素对应。系统给每个索引节点分配了一个号码，也就是该节点在数组中的索引号，称为索引节点号<br>（3）数据区。文件系统的第3个部分是数据区。文件的内容保存在这个区域。磁盘上所有块的大小都一样。如果文件包含了超过一个块的内容，则文件内容会存放在多个磁盘块中。一个较大的文件很容易分布上千个独产的磁盘块中。</p><p>而Linux正统的文件系统(如ext2、ext3)一个文件由目录项、inode和数据块组成：</p><pre><code>目录项:包括文件名和inode节点号。Inode：又称文件索引节点，是文件基本信息的存放地和数据块指针存放地。数据块：文件的具体内容存放地。</code></pre><h2 id="总体设计与代码"><a href="#总体设计与代码" class="headerlink" title="总体设计与代码"></a>总体设计与代码</h2><h3 id="磁盘分布"><a href="#磁盘分布" class="headerlink" title="磁盘分布"></a>磁盘分布</h3><p>磁盘第0块对应于文件描述符位图，共328bit；第1块到第4块，共324*8 = 1024bit对应于1024个数据块位图；接下来100块，每块一个文件描述符；接下来有1024块为数据块。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> FILEBLOCK 1</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> DATABLOCK 4</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> FILEDESSIZE 100</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> L 1129</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> B 32</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> KEEPSIZE  105</span></span><br><span class="line"><span class="keyword">char</span> ldisk[L][B];</span><br><span class="line"><span class="keyword">char</span> <span class="built_in">map</span>[<span class="number">8</span>] = &#123;<span class="number">127</span>,<span class="number">64</span>,<span class="number">32</span>,<span class="number">16</span>,<span class="number">8</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>&#125;;</span><br><span class="line"><span class="keyword">char</span> temp_block[B];</span><br><span class="line"><span class="keyword">int</span> noofblock;</span><br></pre></td></tr></table></figure><h3 id="文件系统组织"><a href="#文件系统组织" class="headerlink" title="文件系统组织"></a>文件系统组织</h3><p>文件系统的组织磁盘的前K个块是保留区，其中包含如下信息：位图和文件描述符。位图用来描述磁盘块的分配情况。位图中的每一位对应一个逻辑块。创建或者删除文件，以及文件的长度发生变化时，文件系统都需要进行位图操作。前K个块的剩余部分包含一组文件描述符。每个文件描述符包含如下信息：</p><ul><li><p>文件长度，单位字节</p></li><li><p>文件分配到的磁盘块号数组，该数组的长度是一个指定的系统参数。</p></li></ul><h3 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h3><p>主函数中包括系统的初始化和输入指令选择功能。功能选择模块使用输出string字符串并与关键字进行匹配来实现。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    InitFMS();</span><br><span class="line">    load();</span><br><span class="line">    printTips();</span><br><span class="line">    <span class="keyword">int</span> opCounter=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(;;)&#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;[&quot;</span>&lt;&lt;++opCounter&lt;&lt;<span class="string">&quot;]：&quot;</span>;</span><br><span class="line">        <span class="built_in">string</span> op=<span class="string">&quot;&quot;</span>;</span><br><span class="line">        <span class="built_in">cin</span>&gt;&gt;op;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(!op.compare(<span class="string">&quot;exit&quot;</span>))&#123;</span><br><span class="line">            savedata();</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(!op.compare(<span class="string">&quot;create&quot;</span>))&#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;File Name: &quot;</span>);</span><br><span class="line">            <span class="keyword">char</span> filename1[<span class="number">10</span>];</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot;%s&quot;</span>,filename1);</span><br><span class="line">            <span class="keyword">if</span>(!create(filename1))&#123;</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            getchar();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(!op.compare(<span class="string">&quot;delete&quot;</span>))&#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;File Name: &quot;</span>);</span><br><span class="line">            <span class="keyword">char</span> filename[<span class="number">10</span>];</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot;%s&quot;</span>,filename);</span><br><span class="line">            getchar();</span><br><span class="line">            destroy(filename);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(!op.compare(<span class="string">&quot;open&quot;</span>))&#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;File Name: &quot;</span>);</span><br><span class="line">            <span class="keyword">char</span> filename[<span class="number">10</span>];</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot;%s&quot;</span>,filename);</span><br><span class="line">            getchar();</span><br><span class="line">            <span class="keyword">if</span>(!open(filename))&#123;</span><br><span class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;ERROR: 请输入正确的文件名！&quot;</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;接下来的指令需为读写指令，请输入:&quot;</span>);</span><br><span class="line">            <span class="built_in">cin</span>&gt;&gt;op;</span><br><span class="line">            <span class="keyword">if</span>(!op.compare(<span class="string">&quot;read&quot;</span>))&#123;</span><br><span class="line">                getchar();</span><br><span class="line">                <span class="keyword">char</span> temp[B];</span><br><span class="line">                read(<span class="literal">NULL</span>,temp,B);</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;%s\n&quot;</span>,temp);</span><br><span class="line">                close(filename);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(!op.compare(<span class="string">&quot;write&quot;</span>))&#123;</span><br><span class="line">                getchar();</span><br><span class="line">                <span class="keyword">char</span> str[<span class="number">100</span>];</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;请输入写入内容：\n&quot;</span>);</span><br><span class="line">                <span class="built_in">scanf</span>(<span class="string">&quot;%s&quot;</span>,str);</span><br><span class="line">                write(<span class="literal">NULL</span>,str,<span class="built_in">strlen</span>(str)+<span class="number">1</span>);</span><br><span class="line">                close(filename);</span><br><span class="line">                getchar();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(!op.compare(<span class="string">&quot;dir&quot;</span>))&#123;</span><br><span class="line">            directory();</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(!op.compare(<span class="string">&quot;clean&quot;</span>))&#123;</span><br><span class="line">            InitFMS();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(!op.compare(<span class="string">&quot;help&quot;</span>))&#123;</span><br><span class="line">            printTips();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;ERROR: 请正确输入指令！&quot;</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="指令功能"><a href="#指令功能" class="headerlink" title="指令功能"></a>指令功能</h3><p>本文件系统提供以下指令功能：</p><p>指令名称|对应的函数名|函数位置|功能描述<br>:-:|:-:|:-:<br>exit|-|-|退出文件管理系统（自动保存文件）<br>help|void printTips()|AuxiliaryFunction.h|显示指令功能的操作提示信息<br>dir|bool destroy(char *filename)|SystemFunction.h|显示文件目录<br>create|bool create(char *filename)|SystemFunction.h|创建未存在的文件<br>delete|bool destroy(char *filename)|SystemFunction.h|删除已存在的文件<br>open|bool open(char *filename)|SystemFunction.h|打开已存在的文件<br>close|bool close(char *index)|SystemFunction.h|关闭已打开的文件<br>write|bool write(char *index, char *mem_area, int count)|SystemFunction.h|读取已存在的文件<br>clean|void InitFMS()|SystemFunction.h|清空所有存储内容</p><p>部分重点实现的函数如下：</p><h4 id="create"><a href="#create" class="headerlink" title="create"></a>create</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">create</span><span class="params">(<span class="keyword">char</span> *filename)</span></span>&#123;</span><br><span class="line">    <span class="keyword">char</span> *p = filename;</span><br><span class="line">    <span class="keyword">int</span> length=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>((length = <span class="built_in">strlen</span>(filename))&gt;<span class="number">10</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;文件名太长！\n&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> index  = getfileheadblock();</span><br><span class="line">    setfileheadblock();</span><br><span class="line">    p=filename;</span><br><span class="line">    filedes fd;</span><br><span class="line">    <span class="built_in">memcpy</span>(&amp;fd.name,filename,length+<span class="number">1</span>);</span><br><span class="line">    fd.length = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;<span class="number">10</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        fd.allo[i]=<span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    fd.allo[<span class="number">0</span>] = getfreedatablock();</span><br><span class="line">    setfileheadblock();</span><br><span class="line">    <span class="built_in">memset</span>(temp_block,<span class="number">-1</span>,B);</span><br><span class="line">    write_block(fd.allo[<span class="number">0</span>],temp_block);</span><br><span class="line">    write_block(FILEBLOCK+DATABLOCK+index,(<span class="keyword">char</span>*)&amp;fd);</span><br><span class="line">    addfile(index);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="open"><a href="#open" class="headerlink" title="open"></a>open</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">open</span><span class="params">(<span class="keyword">char</span> *filename)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    filedes fd;</span><br><span class="line">    read_block(DATABLOCK+FILEBLOCK,(<span class="keyword">char</span>*)&amp;fd);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;<span class="number">10</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(fd.allo[i]==<span class="number">-1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;文件不存在!\n&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        read_block(fd.allo[i],temp_block);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;B; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(temp_block[j]!=<span class="number">-1</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                filedes temp_fd;</span><br><span class="line">                read_block(DATABLOCK+FILEBLOCK+temp_block[j],(<span class="keyword">char</span>*)&amp;temp_fd);</span><br><span class="line">                <span class="keyword">if</span>(<span class="built_in">memcmp</span>(filename,temp_fd.name,<span class="built_in">strlen</span>(filename))==<span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    openFileTable.id = temp_block[j];</span><br><span class="line">                    openFileTable.index = <span class="number">0</span>;</span><br><span class="line">                    <span class="keyword">if</span>(temp_fd.allo[<span class="number">0</span>]==<span class="number">-1</span>)</span><br><span class="line">                    &#123;</span><br><span class="line">                        openFileTable.p==<span class="literal">NULL</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    read_block(temp_fd.allo[<span class="number">0</span>],openFileTable.Buffer);</span><br><span class="line">                    openFileTable.p = openFileTable.Buffer;</span><br><span class="line">                    openFileTable.dsc = temp_fd;</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                </span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="write"><a href="#write" class="headerlink" title="write"></a>write</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">write</span><span class="params">(<span class="keyword">char</span> *index, <span class="keyword">char</span> *mem_area, <span class="keyword">int</span> count)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> left=B-(openFileTable.p-openFileTable.Buffer);</span><br><span class="line">    <span class="keyword">while</span> (count)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(count-left&gt;<span class="number">0</span>) <span class="comment">//当前缓冲区不能满足请求，再取</span></span><br><span class="line">        &#123;</span><br><span class="line">            count-=left;</span><br><span class="line">            <span class="keyword">if</span>(openFileTable.p==<span class="literal">NULL</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;访问超过范围\n&quot;</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">memcpy</span>(openFileTable.Buffer,mem_area,left);</span><br><span class="line">            write_block(openFileTable.dsc.allo[openFileTable.index],openFileTable.Buffer);</span><br><span class="line">            openFileTable.index++;</span><br><span class="line">            <span class="keyword">if</span>(openFileTable.dsc.allo[openFileTable.index]==<span class="number">-1</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                openFileTable.p=<span class="literal">NULL</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            openFileTable.p = openFileTable.Buffer;</span><br><span class="line">            </span><br><span class="line">            left = B;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(openFileTable.p==<span class="literal">NULL</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;访问超过范围\n&quot;</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">memcpy</span>(openFileTable.p,mem_area,count);</span><br><span class="line">            write_block(openFileTable.dsc.allo[openFileTable.index],openFileTable.Buffer);</span><br><span class="line">            openFileTable.p+=count;</span><br><span class="line">            openFileTable.dsc.length+=count;</span><br><span class="line">            write_block(DATABLOCK+FILEBLOCK+openFileTable.id,(<span class="keyword">char</span>*)&amp;openFileTable.dsc);</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="dir"><a href="#dir" class="headerlink" title="dir"></a>dir</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">printTips</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;----------------------------------------&quot;</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">10</span>;i++)&#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;TIPSNAME[i];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;<span class="number">10</span>-TIPSNAME[i].length();j++)&#123;</span><br><span class="line">            <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot; &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;TIPSINFO[i];</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;----------------------------------------&quot;</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="测试运行结果"><a href="#测试运行结果" class="headerlink" title="测试运行结果"></a>测试运行结果</h2><p><img src="/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/lab5/1.png" alt="创建新文件" title="Optional title"><br><img src="/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/lab5/2.png" alt="打开新文件并写入内容" title="Optional title"><br><img src="/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/lab5/3.png" alt="删除文件" title="Optional title"><br><img src="/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/lab5/4.png" alt="help指令和clean指令" title="Optional title"></p><hr><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><blockquote><p><a href="https://www.cnblogs.com/alantu2018/p/8461749.html">Linux文件系统详解：https://www.cnblogs.com/alantu2018/p/8461749.html</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;br&gt;代码、文章及图片挂载地址：&lt;a href=&quot;https://github.com/MoyangSensei/OS&quot;&gt;https://github.com/MoyangSensei/OS&lt;/a&gt;&lt;/p&gt;
&lt;/b</summary>
      
    
    
    
    
    <category term="操作系统" scheme="http://example.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>深度学习笔记（2）：使用神经网络识别手写数字</title>
    <link href="http://example.com/2019/06/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97/"/>
    <id>http://example.com/2019/06/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97/</id>
    <published>2019-06-04T14:27:45.000Z</published>
    <updated>2020-09-09T12:16:51.114Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>时间长没有深度学习的新博客了，这也快到假期了，这门课程的学习还是得捡起来。</p><p>在此之前一直在考虑一个问题：这门纯属自学的课程，我要按照什么样的方式或者顺序去学呢？想来想去最好的方法还是找一本教科书，按照教科书的内容去学，会比我个人从百度上以查找的方式和联想今天自己该学什么内容这样子会好一些。在此先挂上我所选用的教科书的信息。教科书以后说不定是会换的，但是现阶段还是打算照着这本书的目录进行学习。</p><p><img src="/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2/1.png" alt="封面" title="Optional title"></p><p><img src="/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2/2.png" alt="版权信息" title="Optional title"></p><h1 id="本章前言"><a href="#本章前言" class="headerlink" title="本章前言"></a>本章前言</h1><p>大多数进行深度学习或者机器学习的人有极大概率接触过<strong>MINST手写数据集</strong>。</p><p><img src="/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2/3.png" alt="MINST手写数据集大概长这样" title="Optional title"></p><p>这个数据集在这类课程中的地位大概就和在学习编程语言的时候进行的第一次控制台输出的”hello world!“一样。</p><p><strong>本章的目的，就是通过实现手写数字识别的神经网络，来好好了解一下什么是深度学习。</strong></p><hr><h1 id="感知器"><a href="#感知器" class="headerlink" title="感知器"></a>感知器</h1><blockquote><p>什么是神经网络？神经网络由什么组成？它如何进行工作？</p></blockquote><p>首先解释一种被称为“感知器”的人工神经元：一个感知器接受一个或多个二进制输入，给出一个二进制输出。对于每一个输入，给予<strong>权重</strong>表示该输入相对于输出的重要性。神经元的输出为二值输出，由其阈值决定，该阈值也是神经元的一个参数。</p><p><img src="/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2/4.png" alt="感知器" title="Optional title"></p><p><img src="/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2/5.png" alt="神经元的输出函数" title="Optional title"></p><p><strong>可以将上述感知器看作依据权重来做出决定的设备。</strong>随着权重和阈值的变化，将得到不同的“设备”（决策模型）。但单个感知器并不可能用来做出全部的决策，因为决策往往是复杂的，是多元的。<strong>既然单个感知器做不到这样的事情，那么我们就用多个感知器如何？</strong>看看下面的感知器网络：</p><p><img src="/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2/6.png" alt="一个简单的感知器网络" title="Optional title"></p><p><strong>在这个网络中，每一层网络都能比前一层网络考虑的内容更复杂。换言之，每一层网络都比前一层网络考虑了更多的细节。</strong>实际上我们完全可以用感知器来实现任何逻辑功能，即与或非的运算。这说明感知器的运算是具有通用性的，但从另外一个方向考虑，<strong>难道我们不是在学习一种新的逻辑计算设备？</strong>不过实际情况就是这样，只不过是感知器区别于一般的逻辑计算设备的思路是我们可以设计我们自己的<strong>学习算法，来自动调整人工神经元的权重和偏置（阈值），这一点是非常非常非常重要的。</strong></p><p>最后，<strong>简化感知器的数学描述</strong>，两个变动：将求和写为向量的点乘、将感知器的阈值（-threshold）用偏置b代替。</p><p><img src="/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2/7.png" alt="简化感知器的数学描述" title="Optional title"></p><h1 id="S型神经元"><a href="#S型神经元" class="headerlink" title="S型神经元"></a>S型神经元</h1><blockquote><p>学习算法看上去也太理想太好用了，那么如何实现神经元的学习算法？</p></blockquote><p>假设我们有一个感知器网络来帮助我们进行手写数字图像的分类。我们希望该网络能自动的学习权重和偏置，来提高正确分类的成功率。<strong>为了看清楚这个网络是怎样的学习的，我们常规的做法是把网络中的参数作微小的改动。</strong>如果这个微小的改动对于网络的结果输出的影响是巨大的，那么我们就能让学习算法变得可能。</p><p><img src="/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2/8.png" alt="感知器网络对于微小变化的感知" title="Optional title"></p><p>一个例子：假如你的网络总会把8错误的认为成9，那么我们可以通过<strong>反复的进行细微改动网络参数，使8越来越容易被识别成8，这时，我们就说这个网络在进行学习。</strong></p><p>但实际上这个想法里有很大的问题：网络中单个感知器上一个权重或偏置的微小改动有时候会引起那个感知器的输出完全翻转，如0变到1。<strong>这样的翻转可能接下来引起其余网络的行为以极其复杂的方式完全改变。</strong></p><blockquote><p>抓工作不能狗熊掰棒子，去过的每个地方都要抓反馈。——摘自习近平参加十二届全国人大四次会议湖南代表团审议的讲话，2016年3月8日。</p></blockquote><p>因此，虽然8可能被正确分类，但网络在其它图像上的行为很可能以一些很难控制的方式被完全改变。这么做，8是分对了，那么012345679呢？这使得逐步修改权重和偏置来让网络接近期望行为变得困难。</p><p>对于上述问题，引入S神经元来解决问题：S型神经元和感知器类似，不同是S型神经元受整个网络的微小偏置和改动的影响是较小的。</p><p>使用和感知器相同的方式可描述S型神经元，它们都是多输入单输出的单元。它们之间的区别是：<strong>感知器的输入输出都是二值的，而S型神经元的输入不是二值的，它的每一个输入是介于[0，1]的一个值。</strong>同样的，S型神经元也有用来描述重要性的权重$w$和描述阈值的偏置$b$。</p><p>其输出也不是0和1，变为：σ(w·x+b)，其中σ被称为sigmoid函数（sigmoid函数具体是什么不重要，重要的是这个函数绘制出来的形状）：</p><p><img src="/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2/9.png" alt="sigmoid函数绘制出来的形状" title="Optional title"></p><p>对于上面的sigmoid函数表达式，设置一个具有输入x、权重w和偏置b的S型神经元的输出为：</p><p><img src="/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2/10.png" alt="一个sigmoid函数下的S型神经元示例" title="Optional title"></p><p>对于上述输出中的e^-z来说：</p><ul><li><p>当z=w·x+b是一个很大的正数时，该输出趋近于1；</p></li><li><p>当z=w·x+b是一个很大的负数时，该输出趋近于0；</p></li><li><p>只有在z取中间的一些值的时候，这个S型神经元和感知器才有较大的出入。</p></li></ul><p><strong>如果σ是个阶跃函数，既然输出会依赖于w 4+ 6是正数还是负数2,那么S型神经元会成为一个感知器。所以σ函数的平滑特性，是重点中的重点：σ的平滑意呋着杈重和偏置的微小变化，即∆w和∆σ，会从神经元产生一个微小的输出变化∆output。</strong></p><p><img src="/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2/11.png" alt="output的函数表达" title="Optional title"></p><h1 id="神经网络的架构"><a href="#神经网络的架构" class="headerlink" title="神经网络的架构"></a>神经网络的架构</h1><blockquote><p>有了学习算法之后，我们如何搭建一个神经网络？</p></blockquote><p>看看下面的网络架构：</p><p><img src="/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2/12.png" alt="一个典型的神经网络架构" title="Optional title"></p><p>做如下定义：</p><ul><li>最右边的一列称作输出层（output layer）</li><li>最左边的一列称作输入层（input layer）</li><li>所有不是最左边也不是最右边的神经列称作隐藏层（hidden layer）</li></ul><p>设计网络的输入输出层通常是比较直接的。例如，假设我们尝试确定一张手写数字的图像上 是否写的是“9”。那么我们可以将图片像素的强度进行编码作为输入神经元来设计网络。 如果图像是一个64 x 64的灰度图像，那么我们会需要4096 = 64 x 64个输入神经元，每个强度 取0和1之间合适的值。输出层只需要包含一个神经元，当输出值小于0.5时表示“输入图像不 是一个9”，大于0.5的值表不“输入图像是一个9”。</p><p><strong>而对于隐藏层来说，其设计流程是非常复杂的，也是神经网络架构设计中最重要的部分。</strong>目前已有众多的最优法则来帮助我们完成这件事情，让我们所设计的网络的行为更加接近我们内心所想。</p><h1 id="一个简单的分类手写数字的网络"><a href="#一个简单的分类手写数字的网络" class="headerlink" title="一个简单的分类手写数字的网络"></a>一个简单的分类手写数字的网络</h1><blockquote><p>根据上面的理论，构造一个简单的分类手写数字的网络！</p></blockquote><p>首先，一个需要说明的问题是，这里的分类网路针对的问题仅仅是识别问题，而图像分割问题则不在考虑范围之内。<strong>实际上，只要有足够精准的识别算法，分割问题便不难解决。</strong></p><p>我们设想使用一个三层的神经网络来识别单个数字：</p><p><img src="/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2/13.png" alt="识别单个数字的三层神经网络" title="Optional title"></p><p>对上面的神经网络做出解释：</p><p>网络的输入层包含绐输入像素的值进行编码的神经元：因为绐网络的训练数据会有很多扫描得到的28x28的手写数字的图像组成，所有输入层包含有784=28x28个神经元。为了简化，上图中已经忽略了784中大部分的输入神经元。输入像素是灰度级的，值为0.0表示白色，值为1.0表示黑色，中间数值表示逐渐暗淡的灰色。</p><p>网络的第二层是一个隐藏层。我们用n来表示神经元的数量，我们将绐n实验不同的数值。示例中用一个小的隐藏层来说明，仅仅包含n=15个神经元。</p><p>网络的输出层包含有10个神经元。如果第一个神经元激活，即输出1，那么表明网络认为 数字是一个0。如果第二个神经元激活，就表明网络认为数字是一个1。依此类推。即此处把输出神经元的输出赋予编号0到9,并计算出那个神经元有最高的激活值。</p><p>接下来的这个问题非常重要，它将帮助我们理解<strong>隐藏层在干什么</strong>：有人可能会好奇为什么用10个输出神经元。毕竟我们的任务是能让神经网络告诉我们哪个数字（0，1，2，…，9)能和输入图片匹配。一个看起来更自然的方式就是使用4个输出神经元，把每一个当做一个二进制值，结果取决于它的输出更靠近0还是1。四个神经元足够编码这个问题了，因为2^3=8 &lt; 10 &lt;2^4=16 。<strong>所以为什么我们反而要用10个神经元呢？这样做难道效率不低吗？</strong></p><p><strong>对于这个问题，大家的第一反应就是：10个输出神经元的神经网络比4个的识别效果更好。</strong>那么接下来的问题是：<strong>为什么使用10个输出神经元的神经网络更有效呢。有没有什么启发性的方法能提前告诉我 们用10个输出编码比使用4个输出编码更有好呢？</strong></p><p>假设我们用到的这10个输出编码，<strong>假设它们之中的每一个的作用是识别图片的一部分（这个假设在这个情景中极其重要！但是在这个情景之外就不重要了）</strong>。如果我们识别0的过程如下：</p><p><img src="/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2/14.png" alt="要被识别的0" title="Optional title"></p><p><img src="/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2/15.png" alt="将0分成4个识别部分" title="Optional title"></p><p><strong>在上面的识别过程中，如果所有隐藏层的这四个神经元被激活那么我们就可以推断出这个数字是0。</strong>当然，这不是推断出0的唯一方式，还能通过很多其他合理的方式得到0(通过上述图像的转换，或者稍微变形)。但至少在这个例子中我们可以推断出输入的数字是0。</p><p>假设神经网络以上述方式运行，则可以绐出一个合理的理由去解释为什么用10个输出而不是4个：<strong>如果我们有4个输出，那么第一个输出神经元将会尽力去判断数字的最高有效位是什么。把数字的最高有效位和数字的形状联系起来并不是一个简单的问题。</strong></p><p>上面只是一个启发性的方法。<strong>没有什么理由表明这个三层的神经网络必须按照上面描述的方式运行，即隐藏层是用来探测数字的组成形状。</strong>可能一个聪明的学习算法将会找到一些合适的权重能让我们仅仅用4个输出神经元就行。但这个启发性的方法是很有效的，我们可以通过这种方法去构建我们想要的神经网路并使它尽可能达到高效。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h1&gt;&lt;p&gt;时间长没有深度学习的新</summary>
      
    
    
    
    
    <category term="基础知识" scheme="http://example.com/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>编译原理：基于SLR(1)分析法的语法制导翻译及中间代码生成</title>
    <link href="http://example.com/2019/05/31/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%EF%BC%9A%E5%9F%BA%E4%BA%8ESLR-1-%E5%88%86%E6%9E%90%E6%B3%95%E7%9A%84%E8%AF%AD%E6%B3%95%E5%88%B6%E5%AF%BC%E7%BF%BB%E8%AF%91%E5%8F%8A%E4%B8%AD%E9%97%B4%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/"/>
    <id>http://example.com/2019/05/31/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%EF%BC%9A%E5%9F%BA%E4%BA%8ESLR-1-%E5%88%86%E6%9E%90%E6%B3%95%E7%9A%84%E8%AF%AD%E6%B3%95%E5%88%B6%E5%AF%BC%E7%BF%BB%E8%AF%91%E5%8F%8A%E4%B8%AD%E9%97%B4%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/</id>
    <published>2019-05-31T05:07:30.000Z</published>
    <updated>2020-09-10T08:33:16.032Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>此为编译原理课程的最后一个与算法有关的实验博客。</p><p>总的来说还是很不容易的。5个算法，难度依次递增，coding时间、代码量也是依次递增，不过写出它们的快乐也是依次递增的。说起来也是自己给自己增加coding难度，<strong>为了在所有代码中保持贯穿一致的数据结构</strong>，多做了很多的工作。不过写到现在也觉得这些工作是非常值得的，不管是看起来还是用起来都是十分赏心悦目的。</p><p>尤其是这一次，大概是几个实验中bug最少的一个了。当我把用代码生成的DFA和我手写的DFA进行对比，发现<strong>完全一模一样</strong>的时候，那真的是非常开心了！写到现在，对于C++ STL的运用也更加的娴熟，这也是我学习这门课程的意外收获之一。</p><p><strong>本博客会对之前4个博客的内容做综合总结。</strong></p><hr><h1 id="目标任务"><a href="#目标任务" class="headerlink" title="目标任务"></a>目标任务</h1><h2 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h2><p>完成以下描述赋值语句 SLR(1)文法语法制导生成中间代码四元式的过程。</p><p>G[E]:<br>A -&gt; V=E<br>E -&gt; E+T∣E-T∣T<br>T -&gt; T*F∣T/F∣F<br>F -&gt; (E)∣i<br>V -&gt; i</p><h2 id="设计说明"><a href="#设计说明" class="headerlink" title="设计说明"></a>设计说明</h2><p>终结符号i为用户定义的简单变量，即标识符的定义。</p><h2 id="设计要求"><a href="#设计要求" class="headerlink" title="设计要求"></a>设计要求</h2><ul><li><p>(1) 构造文法的 SLR(1)分析表，设计语法制导翻译过程，给出每一产生式对应的语义动作;</p></li><li><p>(2) 设计中间代码四元式的结构;</p></li><li><p>(3) 输入串应是词法分析的输出二元式序列，即某赋值语句“专题1”的输出结果，输出为赋值语句的四元式序列中间文件;</p></li><li><p>(4) 设计两个测试用例(尽可能完备，正确和出错)，并给出测试结果;</p></li></ul><hr><h1 id="解答"><a href="#解答" class="headerlink" title="解答"></a>解答</h1><h2 id="SLR-1-分析法"><a href="#SLR-1-分析法" class="headerlink" title="SLR(1)分析法"></a>SLR(1)分析法</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p><strong>由于大多数适用的程序设计语言的文法不能满足LR(O)文法的条件，即使是描述一个实数变量说明的这样一个简单文法也不一定是LR(O)文法。</strong>因此对于LR(O)规范族中有冲突的项目集(状态)用向前查看一个符号的办法进行处理，以解决冲突。这种办法将能满足一些文法的需要，因为只对有冲突的状态才向前查看一个符号，以确定做那种动作，因而称这种分析方法为简单的LR(1)分析法，用SLR(1)表示。</p><p><strong>SLR(1)分析法属于LR分析法中的一种</strong>，是通过使用输入串中下一个记号来指导它的动作，<strong>它大大地提高了LR(0)分析的能力</strong>。</p><p>它通过两种方法做到这一点：</p><ul><li><p>首先，它在一个移进之前先考虑输入记号以确保存在着一个恰当的DFA 。</p></li><li><p>其次，使用构造的非终结符的Follow集合来决定是否应执行一个归约。</p></li></ul><h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><p>对于给定文法G和一语句，若要进行SLR(1)分析，一般需要进行以下步骤：</p><ul><li><p>(1) 写出已知文法G的扩展文法G’</p></li><li><p>(2) 写出扩展文法G’的初始项目集</p></li><li><p>(3) 根据状态转移构建识别G’活前缀的DFA（项目集编号为I0-IN）</p></li><li><p>(4) 求得文法G的FOLLOW集合</p></li><li><p>(5) 根据步骤(3)和(4)构建SLR(1)分析表</p></li></ul><h2 id="文法扩展"><a href="#文法扩展" class="headerlink" title="文法扩展"></a>文法扩展</h2><p>所谓G的拓展文法G’，即映入一个新的开始符号，并添加一个新的产生式：将新的开始符号作为新产生的左部，将原开始符号作为产生式的右部。</p><p>设新增的开始符号为Z，对于本实验所给文法，拓展文法G’中新增如下产生式：</p><pre><code>Z -&gt; A</code></pre><h2 id="扩展文法G’的LR-0-项目集"><a href="#扩展文法G’的LR-0-项目集" class="headerlink" title="扩展文法G’的LR(0)项目集"></a>扩展文法G’的LR(0)项目集</h2><p>构成识别一个文法活前缀的DFA项目集(状态)的全体，称之为这个文法的LR(0)项目集规范族。</p><p>对于活前缀，定义为：对于任一文法G[S]，若新的开始符号经过任意次推导得到αAω，继续经过一次推导得到αβω，若γ是αβ的前缀，则称γ是G的一个活前缀。</p><p>设分割符为“.”，对上述扩展文法G’，其全部的LR(0)项目为：</p><pre><code>Z=.A  Z=A.A -&gt; .V=E  A -&gt; V.=E  A -&gt; V=.E  A -&gt; V=E.E -&gt; .E+T  E -&gt; E.+T  E -&gt; E+.T  E -&gt; E+T.E -&gt; .E-T  E -&gt; E.-T  E -&gt; E-.T  E -&gt; E-T.E -&gt; .T  E -&gt; T.T -&gt; .T*F  T -&gt; T.*F  T -&gt; T*.F  T -&gt; T*F.T -&gt; .T/F  T -&gt; T./F  T -&gt; T/.F  T -&gt; T/F.T -&gt; .F  T -&gt; F.F -&gt; .(E)  F -&gt; (.E)  F -&gt; (E.)  F -&gt; (E).F -&gt; .i  F -&gt; i.V -&gt; .i  V -&gt; i.</code></pre><h2 id="识别G’活前缀的DFA"><a href="#识别G’活前缀的DFA" class="headerlink" title="识别G’活前缀的DFA"></a>识别G’活前缀的DFA</h2><p>首先引入如下概念：LR(0)项目集的闭包函数CLOSURE和状态转换函数GO：</p><h3 id="LR-0-项目集的闭包函数CLOSURE"><a href="#LR-0-项目集的闭包函数CLOSURE" class="headerlink" title="LR(0)项目集的闭包函数CLOSURE"></a>LR(0)项目集的闭包函数CLOSURE</h3><p>闭包函数CLOSURE(I)的定义如下：　　</p><ul><li>(1) I的项目均在CLOSURE(I)中。　　</li><li>(2) 若A -&gt; α·Bβ属于CLOSURE(I)，则每一形如B -&gt; ·γ的项目也属于CLOSURE(I)。　　</li><li>(3) 重复b)直到不出现新的项目为止。即CLOSURE(I)不再扩大。</li></ul><h3 id="LR-0-项目集的状态转换函数GO"><a href="#LR-0-项目集的状态转换函数GO" class="headerlink" title="LR(0)项目集的状态转换函数GO"></a>LR(0)项目集的状态转换函数GO</h3><p>转换函数GO(I，X)的定义：　　</p><ul><li>GO(I，X)＝CLOSURE(J)</li></ul><p>其中：I为包含某一项目的状态，J＝{任何形如A -&gt; αX·β的项目| A -&gt; α·Xβ属于I}。</p><p>这样就可以使用闭包函数和转换函数构造文法G′的LR(0)项目集规范族。</p><h3 id="LR-0-项目集规范族"><a href="#LR-0-项目集规范族" class="headerlink" title="LR(0)项目集规范族"></a>LR(0)项目集规范族</h3><p>使用闭包函数和转换函数构造文法G′的LR(0)项目集规范族，步骤如下：　　</p><ul><li><p>(1) 置项目【新开始符号 -&gt; ·原开始符号】为初态集的核，然后对核求闭包，得到初态的项目集。</p></li><li><p>(2) 对初态集或其它所构造的项目集应用转换函数GO(I，X)=CLOSURE(J)，求出新状态J的项目集。</p></li><li><p>(3) 重复(2)直到不出现新的项目为止。</p></li></ul><h3 id="DFA"><a href="#DFA" class="headerlink" title="DFA"></a>DFA</h3><p>至此，我们所要构造的识别文法全部或前缀的DFA为：</p><pre><code>M=&#123;C,V,GO,I0,C&#125;</code></pre><p><strong>其中，C为M的状态集也就是文法的LR(0)项目集规范族；V是M的字母表也就是文法的字汇表；GO为M的映像也就是如上定义的状态转换函数GO。</strong></p><p>对上述扩展文法G’，识别文法全部或前缀的DFA为：</p><p><img src="/images/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/5/1.png" alt="图1" title="Optional title"></p><p>其中，蓝色编号为项目集的编号，绿色编号为有向边。</p><p><strong>在本文法的DFA中，共有20个项目集和38条有向边。</strong></p><h2 id="文法G的FOLLOW集合"><a href="#文法G的FOLLOW集合" class="headerlink" title="文法G的FOLLOW集合"></a>文法G的FOLLOW集合</h2><p>在构建SLR分析表的时候，文法G的FOLLOW集合用来构建归约过程。对于文法的FOLLOW集合求取，步骤见《编译原理：LL(1)语法分析》。</p><p>对于上述文法G，其FOLLOW集合为：</p><p><img src="/images/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/5/2.png" alt="图2" title="Optional title"></p><h2 id="构建SLR-1-分析表"><a href="#构建SLR-1-分析表" class="headerlink" title="构建SLR(1)分析表"></a>构建SLR(1)分析表</h2><h3 id="LR-0-分析表的构造"><a href="#LR-0-分析表的构造" class="headerlink" title="LR(0)分析表的构造"></a>LR(0)分析表的构造</h3><p>LR(0)分析表的ACTION表项和GOTO表项可按如下方法构造：</p><ul><li><p>(1)若项目A -&gt;α • aβ属于 Ik 且 GO (Ik, a)= Ij, 期望字符a 为终结符，则置ACTION[k, a] =sj (j表示新状态Ij);<br>【如果圆点不在项目k最后且圆点后的期待字符a为终结符，则ACTION[k, a] =sj (j表示新状态Ij)】</p></li><li><p>(2)若项目A -&gt;α • Aβ属于 Ik，且GO (Ik, A)= Ij,期望字符 A为非终结符，则置GOTO(k, A)=j (j表示文法中第j个产生式);<br>【如果圆点不在项目k最后且圆点后的期待字符A为非终结符，则GOTO(k, A)=j (j表示文法中第j个产生式)】</p></li><li><p>(3)若项目A -&gt;α •属于Ik, 那么对任何终结符a, 置ACTION[k, a]=rj；其中，假定A-&gt;α为文法G 的第j个产生式；<br>【如果圆点在项目k最后且k不是S’ -&gt;S，那么对所有终结符a，ACTION[k, a]=rj (j表示文法中第j个产生式)】</p></li><li><p>(4)若项目S’ -&gt;S • 属于Ik, 则置ACTION[k, #]为“acc”;<br>【如果圆点在项目k最后且k是S’ -&gt;S，则ACTION[k, #]为“acc”】</p></li><li><p>(5)分析表中凡不能用上述规则填入信息的空白格均置上“出错标志”.</p></li></ul><h3 id="SLR-1-分析表的构造"><a href="#SLR-1-分析表的构造" class="headerlink" title="SLR(1)分析表的构造"></a>SLR(1)分析表的构造</h3><p>当LR(0)出现多重定义分析动作的时候，对于含有冲突的项目集：</p><pre><code>I(i)=&#123;B-&gt;α • Aβ, A-&gt;α •, C-&gt;α •&#125;</code></pre><p>对于任何输入符号a，可运用如下规则消去冲突：</p><ul><li><p>(1) 当a=b时，置[i,b]=”移进”；</p></li><li><p>(2) 当a属于FOLLOW(A)时，置[i,a]={按产生式A-&gt;α •规约}；</p></li><li><p>(3) 当a属于FOLLOW(C)时，置[i,a]={按产生式C-&gt;α •规约}；</p></li><li><p>(4) 当a不属于上述三种情况之一时，置[i,a]=”ERROR”。</p></li></ul><p><strong>需要注意的是，对于是LR(0)文法的文法来说，其LR(0)分析表和SLR(1)分析表是一样的（因为没有可消去的冲突，其余的构造方法又是一致的）。</strong></p><p>对于上述文法G’，根据DFA和FOLLOW集合构造SLR(1)分析表，如下：</p><p><img src="/images/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/5/3.png" alt="图3" title="Optional title"></p><p>上述结果为本次实验代码生成的SLR分析表和对产生式的编号。<br><del>实际上是我懒得再把表格敲一编到这儿了就直接拿截图代替了233333</del></p><h2 id="根据构造好的SLR-1-分析表进行分析"><a href="#根据构造好的SLR-1-分析表进行分析" class="headerlink" title="根据构造好的SLR(1)分析表进行分析"></a>根据构造好的SLR(1)分析表进行分析</h2><p>SLR(1)分析分析流程同LR分析，流程图如下：</p><p><img src="/images/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/5/4.png" alt="图4" title="Optional title"></p><h2 id="生成中间代码四元式"><a href="#生成中间代码四元式" class="headerlink" title="生成中间代码四元式"></a>生成中间代码四元式</h2><p>在做SLR（1）分析时，在归约动作进行的同时，需要执行语义动作，并生成生成中间代码四元式子。</p><h3 id="四元式结构"><a href="#四元式结构" class="headerlink" title="四元式结构"></a>四元式结构</h3><p>定义四元式结构如下：</p><ul><li>(op, agr1, arg2, result)</li></ul><p>其中，op为二元运算符；result为两运算对象arg1、arg2通过运算符op进行运算所得到的结果变量；arg1和arg2分别为op左边和右边的运算对象，<strong>该对象可以是产生式中的非终结符号，也可以是先前产生的result临时变量</strong>。</p><h3 id="如何产生四元式？"><a href="#如何产生四元式？" class="headerlink" title="如何产生四元式？"></a>如何产生四元式？</h3><p>一般的，语句中有多少个运算符，对应就要产生多少个中间代码四元式。<strong>对于含有运算符的产生式来说，如果发生的归约动作使用了这个产生式，那么就把该产生式的左部和右部按照上述结构进行记录：</strong></p><pre><code>op=产生式右部的运算符arg1=产生式右部的运算符的左边的运算对象arg2=产生式右部的运算符的右边的运算对象result=产生式左部的非终结符号</code></pre><p>如果按照产生式中的进行记录，那么对于所有的四元式来说，无法看出它们之间的关系。对应的做法是按照顺序记录这些四元式，在所有记录都完成之后，<strong>按照从上到下的顺序将任意四元式的运算对象arg1、arg2替换为上面一个四元式的result</strong>。这里需要注意的是，<strong>在记录的时候一定要按照语法分析的顺序进行记录</strong>，否则就无法进行顺序替换：<strong>按语法顺序进行的记录就是该四元式在原语句中的映射顺序记录</strong>。</p><h2 id="文件结构及函数简介"><a href="#文件结构及函数简介" class="headerlink" title="文件结构及函数简介"></a>文件结构及函数简介</h2><p>本专题大部分文件和功能函数与专题1同名，具体的文件结构和功能函数简介见《编译原理：词法分析程序》。</p><p>新增文件SLR.h，内含的函数用SLR分析表的构造及文法的扩充等。</p><p>文件firstAndFollow.h同《编译原理：LL(1)语法分析》所实现的功能一致。</p><h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><p>详细的主函数流程如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">const</span> <span class="keyword">char</span> * argv[])</span> </span>&#123;</span><br><span class="line">    <span class="comment">//初始化参数</span></span><br><span class="line">    initAll();</span><br><span class="line">    <span class="comment">//读取lab1中输出的二元组结果文件</span></span><br><span class="line">    readFile_Result();</span><br><span class="line">    <span class="comment">//读取lab1中输出的标识符结果文件</span></span><br><span class="line">    readFile_Identifier();</span><br><span class="line">    <span class="comment">//过滤读取结果，得到纯粹的标识符内容</span></span><br><span class="line">    getPureMorphemeAndSentence();</span><br><span class="line">    <span class="comment">//输出过滤后的语句</span></span><br><span class="line">    printSentenceInfo();</span><br><span class="line">    <span class="comment">//读取文法，求得包含所有的非终结符号的不重复集合</span></span><br><span class="line">    readFile_Formula();</span><br><span class="line">    <span class="comment">//求得包含所有的终结符号的不重复集合</span></span><br><span class="line">    getAllVT();</span><br><span class="line">    <span class="comment">//求得文法的first集合和follow集合以及由此构造分析表，并写入txt文件中</span></span><br><span class="line">    getFirstAndFollow();</span><br><span class="line">    <span class="comment">//文法扩充</span></span><br><span class="line">    grammarExpand();</span><br><span class="line">    <span class="comment">//求识别文法全部活前缀的DFA</span></span><br><span class="line">    DFA();</span><br><span class="line">    <span class="comment">//将所求的DFA写入txt文件中</span></span><br><span class="line">    writeFile_DFA();</span><br><span class="line">    <span class="comment">//构造SLR分析表</span></span><br><span class="line">    getSLRAnalyseTable();</span><br><span class="line">    <span class="comment">//将所求的SLR分析表写入txt文件中</span></span><br><span class="line">    writeFile_SLR1AnalyseTable();</span><br><span class="line">    <span class="comment">//进行分析</span></span><br><span class="line">    <span class="keyword">if</span>(getAnalyseResult())&#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;匹配成功!&quot;</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        <span class="comment">//匹配成功，输出分析栈结果</span></span><br><span class="line">        printAnalyseResultInfo();</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="comment">//匹配失败，输出错误位置信息</span></span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;匹配失败!&quot;</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        printError();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>构造识别文法所有活前缀的DFA，首先需要构造好初态I(0)：即把【新开始符号 -&gt; ·原开始符号】加入I(0)中。然后执行循环。循环的内容为：从第0个状态开始先求取状态的闭包，然后读取所有可读取的符号，并把读取过后构造的不重复新状态加入到项目集中，等待接下来的读取。跳出循环的条件为：当项目集的个数不再增大且所有的项目集都经过求取闭包和符号移进。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">DFA</span><span class="params">()</span></span>&#123;</span><br><span class="line">    dfaCounter+=<span class="number">1</span>;</span><br><span class="line">    <span class="built_in">multimap</span>&lt;<span class="keyword">char</span>,<span class="built_in">string</span>&gt;::iterator itBegin;</span><br><span class="line">    itBegin=Sentence.find(newBegin);</span><br><span class="line">    <span class="comment">//0状态的初始化： [新开始符号-&gt;.原开始符号]</span></span><br><span class="line">    dfaForG_prim[<span class="number">0</span>].dfa.insert(<span class="built_in">make_pair</span>(newBegin, fenGe+itBegin-&gt;second));</span><br><span class="line">    <span class="comment">//对0状态求闭包</span></span><br><span class="line">    dfaForG_prim[<span class="number">0</span>].dfa=closure(dfaForG_prim[<span class="number">0</span>].dfa);</span><br><span class="line">    <span class="comment">//从0状态开始循环</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;dfaCounter;i++)&#123;</span><br><span class="line">        <span class="comment">//读取所有可读取的字符，作为新状态，并对新状态求闭包</span></span><br><span class="line">        <span class="comment">//不重复的新状态则加入dfa中，等待之后的读取</span></span><br><span class="line">        getNextSta(i);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数**void getNextSta(int dfaCount)**用来求当前状态的闭包和对该闭包进行读取移进的工作：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">getNextSta</span><span class="params">(<span class="keyword">int</span> dfaCount)</span></span>&#123;</span><br><span class="line">    <span class="comment">//将该状态中全部可以读取的字符做成集合</span></span><br><span class="line">    <span class="comment">//可以读取的字符：所有产生式中的紧挨着分割符的下一个字符</span></span><br><span class="line">    <span class="built_in">set</span>&lt;<span class="keyword">char</span>&gt; nextChar;</span><br><span class="line">    <span class="keyword">for</span>(<span class="built_in">multimap</span>&lt;<span class="keyword">char</span>,<span class="built_in">string</span>&gt;::iterator it_1=dfaForG_prim[dfaCount].dfa.begin();it_1!=dfaForG_prim[dfaCount].dfa.end();it_1++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;it_1-&gt;second.length()<span class="number">-1</span>;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(it_1-&gt;second[i]==fenGe[<span class="number">0</span>])&#123;</span><br><span class="line">                nextChar.insert(it_1-&gt;second[i+<span class="number">1</span>]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//没有可以读取的字符，该状态是终结状态</span></span><br><span class="line">    <span class="keyword">if</span>(nextChar.size()==<span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//有可以读取的字符，按上述所求得的集合中的内容读取</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="built_in">set</span>&lt;<span class="keyword">char</span>&gt;::iterator it_1=nextChar.begin();it_1!=nextChar.end();it_1++)&#123;</span><br><span class="line">        <span class="built_in">multimap</span>&lt;<span class="keyword">char</span>,<span class="built_in">string</span>&gt; temp;</span><br><span class="line">        <span class="keyword">for</span>(<span class="built_in">multimap</span>&lt;<span class="keyword">char</span>,<span class="built_in">string</span>&gt;::iterator it_2=dfaForG_prim[dfaCount].dfa.begin();it_2!=dfaForG_prim[dfaCount].dfa.end();it_2++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;it_2-&gt;second.length()<span class="number">-1</span>;i++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(it_2-&gt;second[i]==fenGe[<span class="number">0</span>]&amp;&amp;it_2-&gt;second[i+<span class="number">1</span>]==*it_1)&#123;</span><br><span class="line">                    <span class="built_in">string</span> tempString=it_2-&gt;second;</span><br><span class="line">                    <span class="comment">//分隔符移进（读取）</span></span><br><span class="line">                    <span class="comment">//使用位操作交换第i个位置内容(分隔符)和第i+1个位置的内容</span></span><br><span class="line">                    tempString[i]=tempString[i]^tempString[i+<span class="number">1</span>];</span><br><span class="line">                    tempString[i+<span class="number">1</span>]=tempString[i]^tempString[i+<span class="number">1</span>];</span><br><span class="line">                    tempString[i]=tempString[i]^tempString[i+<span class="number">1</span>];</span><br><span class="line">                    temp.insert(<span class="built_in">make_pair</span>(it_2-&gt;first,tempString));</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                </span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//对移进之后的产生式求取闭包temp</span></span><br><span class="line">        temp=closure(temp);</span><br><span class="line">        <span class="keyword">bool</span> ifRepeat=<span class="literal">false</span>;</span><br><span class="line">        <span class="comment">//对temp查重</span></span><br><span class="line">        <span class="comment">//如果temp和现在dfa中的任意一个状态重合，则该temp不加入dfa状态中</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;dfaCounter;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(temp.size()==dfaForG_prim[i].dfa.size())&#123;</span><br><span class="line">                <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; rComPare1;</span><br><span class="line">                <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; rComPare2;</span><br><span class="line">                <span class="keyword">for</span>(<span class="built_in">multimap</span>&lt;<span class="keyword">char</span>,<span class="built_in">string</span>&gt;::iterator it_3=dfaForG_prim[i].dfa.begin(), it_4=temp.begin();it_3!=dfaForG_prim[i].dfa.end();it_3++,it_4++)&#123;</span><br><span class="line">                    <span class="comment">//将所对比的两个状态的左右部全部加入向量中，便于比较</span></span><br><span class="line">                    rComPare1.push_back(it_3-&gt;second);</span><br><span class="line">                    rComPare1.push_back(<span class="string">&quot;&quot;</span>+it_3-&gt;first);</span><br><span class="line">                    rComPare2.push_back(it_4-&gt;second);</span><br><span class="line">                    rComPare2.push_back(<span class="string">&quot;&quot;</span>+it_4-&gt;first);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span>(rComPare1==rComPare2)&#123;</span><br><span class="line">                    <span class="comment">//当前temp有重复</span></span><br><span class="line">                    ifRepeat=<span class="literal">true</span>;</span><br><span class="line">                    <span class="comment">//重复的内容有边连到当前状态</span></span><br><span class="line">                    dfaLine[dfaLineCounter].from=dfaCount;</span><br><span class="line">                    dfaLine[dfaLineCounter].read=*it_1;</span><br><span class="line">                    dfaLine[dfaLineCounter].to=i;</span><br><span class="line">                    dfaLineCounter+=<span class="number">1</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(!ifRepeat)&#123;</span><br><span class="line">            <span class="comment">//当前temp不重复</span></span><br><span class="line">            <span class="comment">//当前temp有边指向下一个temp</span></span><br><span class="line">            dfaLine[dfaLineCounter].from=dfaCount;</span><br><span class="line">            dfaLine[dfaLineCounter].read=*it_1;</span><br><span class="line">            dfaLine[dfaLineCounter].to=dfaCounter;</span><br><span class="line">            dfaLineCounter+=<span class="number">1</span>;</span><br><span class="line">            <span class="comment">//当前temp加入dfa中成为一个新状态</span></span><br><span class="line">            dfaForG_prim[dfaCounter].dfa=temp;</span><br><span class="line">            dfaCounter+=<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述函数过程中调用了closure函数，用来求取当前状态的闭包：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">multimap&lt;char,string&gt; closure(multimap&lt;char,string&gt; a)&#123;</span><br><span class="line">    <span class="comment">//int oldSize=a.size();</span></span><br><span class="line">    <span class="keyword">for</span>(;;)&#123;</span><br><span class="line">        <span class="comment">//记录集合的大小</span></span><br><span class="line">        <span class="keyword">int</span> oldSize=a.size();</span><br><span class="line">        <span class="comment">//求取闭包temp</span></span><br><span class="line">        <span class="comment">//紧挨着分割符的下一个字符如果是非终结字符，则把以该非终结字符作为左部的所有产生式加入该状态，并使其右部的第一个位置加入分割符</span></span><br><span class="line">        <span class="built_in">multimap</span>&lt;<span class="keyword">char</span>,<span class="built_in">string</span>&gt; temp=a;</span><br><span class="line">        <span class="keyword">for</span>(<span class="built_in">multimap</span>&lt;<span class="keyword">char</span>,<span class="built_in">string</span>&gt;::iterator it_a=a.begin();it_a!=a.end();it_a++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;it_a-&gt;second.length()<span class="number">-1</span>;i++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(it_a-&gt;second[i]==fenGe[<span class="number">0</span>])&#123;</span><br><span class="line">                    <span class="keyword">if</span>(isCaptain(it_a-&gt;second[i+<span class="number">1</span>]))&#123;</span><br><span class="line">                        <span class="keyword">for</span>(<span class="built_in">multimap</span>&lt;<span class="keyword">char</span>,<span class="built_in">string</span>&gt;::iterator it_1=Sentence.begin();it_1!=Sentence.end();it_1++)&#123;</span><br><span class="line">                            <span class="keyword">if</span>(it_1-&gt;first==it_a-&gt;second[i+<span class="number">1</span>])&#123;</span><br><span class="line">                                <span class="comment">// 左 -&gt; 分割符 + 右</span></span><br><span class="line">                                temp.insert(<span class="built_in">make_pair</span>(it_1-&gt;first, fenGe+it_1-&gt;second));</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//temp内部去重</span></span><br><span class="line">        <span class="comment">//去除重复的产生式</span></span><br><span class="line">        <span class="keyword">int</span> tempSize=temp.size();</span><br><span class="line">        <span class="keyword">for</span>(;;)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="built_in">multimap</span>&lt;<span class="keyword">char</span>,<span class="built_in">string</span>&gt;::iterator it_1=temp.begin();it_1!=temp.end();it_1++)&#123;</span><br><span class="line">                <span class="keyword">for</span>(<span class="built_in">multimap</span>&lt;<span class="keyword">char</span>,<span class="built_in">string</span>&gt;::iterator it_2=it_1;it_2!=temp.end();it_2++)&#123;</span><br><span class="line">                    <span class="keyword">if</span>(it_1!=it_2&amp;&amp;it_1-&gt;first==it_2-&gt;first&amp;&amp;!it_1-&gt;second.compare(it_2-&gt;second))&#123;</span><br><span class="line">                        <span class="comment">//erase擦去重复的产生式</span></span><br><span class="line">                        temp.erase(it_2);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(tempSize==temp.size())&#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                tempSize=temp.size();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        a=temp;</span><br><span class="line">        <span class="comment">//状态不再扩大，证明是闭包，跳出</span></span><br><span class="line">        <span class="keyword">if</span>(oldSize==a.size())&#123;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>构造SLR分析表和进行分析则完全按照上述算法进行，不再赘述。</p><h2 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h2><p>依据所给文法写出给出两正确语句，并验证结果：</p><h3 id="var-d-var-a-var-b-var-c"><a href="#var-d-var-a-var-b-var-c" class="headerlink" title="var_d=var_a+var_b*var_c"></a>var_d=var_a+var_b*var_c</h3><p>将此表达式写入专题1中的SourceProgram.txt文件，运行专题1程序：</p><p><img src="/images/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/5/5.png" alt="图5" title="Optional title"></p><p>查看标识符文件Identifier.txt：</p><p><img src="/images/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/5/6.png" alt="图6" title="Optional title"></p><p>查看词素二元组文件ResultofLexicalAnalysis.txt：</p><p><img src="/images/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/5/7.png" alt="图7" title="Optional title"></p><p>可以看到对于专题1来说，该语句是符合要求的。</p><p>接下来运行本专题的SLR(1)分析及中间代码生成程序：</p><p><img src="/images/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/5/8.png" alt="图8" title="Optional title"></p><p>查看生成的DFA文件DFA.txt：</p><p><img src="/images/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/5/9.png" alt="图9" title="Optional title"></p><p>生成的SLR分析表文件SLR1AnalyseTable.txt见【3.6.1 SLR(1)分析表的构造】。</p><h3 id="var-a-var-b-var-c-var-d-var-e-var-f"><a href="#var-a-var-b-var-c-var-d-var-e-var-f" class="headerlink" title="var_a=var_b-(var_c+var_d/var_e)*var_f"></a>var_a=var_b-(var_c+var_d/var_e)*var_f</h3><p>将此表达式写入专题1中的SourceProgram.txt文件，运行专题1程序：</p><p><img src="/images/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/5/10.png" alt="图10" title="Optional title"></p><p>查看标识符文件Identifier.txt：</p><p><img src="/images/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/5/11.png" alt="图11" title="Optional title"></p><p>查看词素二元组文件ResultofLexicalAnalysis.txt：</p><p><img src="/images/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/5/12.png" alt="图12" title="Optional title"></p><p>可以看到对于专题1来说，该语句是符合要求的。</p><p>接下来运行本专题的SLR(1)分析及中间代码生成程序：</p><p><img src="/images/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/5/13.png" alt="图13" title="Optional title"><br><img src="/images/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/5/14.png" alt="图14" title="Optional title"></p><p>上述测试结果证明了两个程序可按设计本意顺利完成功能。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>至此，编译原理与算法有关的代码编写及博客写作就告一段落。纵观所有内容，将优缺点及后续需要改进的地方进行总结：</p><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul><li><p>使用编程语言统一（c++）、<strong>数据结构统一</strong>、输入内容统一（实验要求）、输出方式统一（包括控制台输出与文件输出）；</p></li><li><p>语法分析的4个程序的代码整体架构统一，具体为：<strong>总控程序+分析依据构造+分析算法</strong>；</p></li><li><p>细节功能封装体现的较为到位；</p></li><li><p>功能操作方便、代码泛用性高：在循环中经常使用的变量或常量均设置了全局变量，对不同规模、不同内容的文法或语句进行测试或者在别的工程中引用当前工程功能的时候，无需再次修改代码，仅需要修改txt文件即可。</p></li></ul><h2 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h2><ul><li><p>代码不够简洁：主要体现在对数据结构的遍历中（循环嵌套严重、为了方便循环不设置跳出等）；</p></li><li><p>在进行词法分析程序（1）编写的时候没有考虑到后面的内容，导致了很多无用或者费力的代码。</p></li></ul><h2 id="以后要做的事情"><a href="#以后要做的事情" class="headerlink" title="以后要做的事情"></a>以后要做的事情</h2><ul><li><p>程序整合：整合所有功能，精简无用、重复代码；</p></li><li><p>进一步进行功能函数的封装；</p></li><li><p>可视化前端编写。<br><del>有时间有心情我就写</del></p></li></ul><hr><p>最后，庆祝历时两个多月的编译原理的算法相关coding的完成！</p><p>记录时间：2019年4月——2019年5月31日17时37分</p><blockquote><p>最后的最后，我还想说一句，编译原理这些个东西也太难理解太难写了，我终于写完了2333～～<br>接下来的实验6是1和5的整合相当于白给，博客也懒得再写太多了都是些重复的东西，实验7是选做也相当于白给以后再说吧233333～～</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h1&gt;&lt;p&gt;此为编译原理</summary>
      
    
    
    
    
    <category term="编译原理" scheme="http://example.com/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>数字图像处理：包装生产线装瓶质量检测</title>
    <link href="http://example.com/2019/05/21/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%EF%BC%9A%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E7%BA%BF%E8%A3%85%E7%93%B6%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B/"/>
    <id>http://example.com/2019/05/21/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%EF%BC%9A%E5%8C%85%E8%A3%85%E7%94%9F%E4%BA%A7%E7%BA%BF%E8%A3%85%E7%93%B6%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B/</id>
    <published>2019-05-21T11:51:08.000Z</published>
    <updated>2019-05-22T08:27:06.977Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>包装生产线的质量检测：一家用瓶子装各种工业化学品的装瓶公司听说你成功解决了成像问题，并雇佣你设计一种检测瓶子未装满的方法。当瓶子在传送带上运动，并通过自动装填机和封盖机进行包装时有如下图所示的情景。当液体平面低于瓶颈底部和瓶子肩部的中间点时，认为瓶子未装满。瓶子的横断面上的倾斜部分及侧面定义为瓶子的肩部，瓶子在不断移动。公司有一个图像系统，装备了有效捕捉静止图像的前端闪光照明设备。所以你可以得到非常清晰的图像。基于以上你得到的资料，提出一种检测未完全装满的瓶子的解决方案。清楚地表述你做的所有设想和很可能对你提出的解决方案产生影响的假设。并实现它。</p><hr><h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><ul><li><p>所选用的编程语言为python3.6；</p></li><li><p>编译环境：<br>Jupyter Notebook（此前被称为 IPython notebook）是一个交互式笔记本，支持运行 40 多种编程语言。<br>Jupyter Notebook 的本质是一个 Web 应用程序，便于创建和共享文学化程序文档，支持实时代码，数学方程，可视化和markdown。用途包括：数据清理和转换，数值模拟，统计建模，机器学习等等。</p></li></ul><p><img src="/images/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/11.png" alt="搭载在Anaconda上的Jupyter Notebook" title="Optional title"></p><hr><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>本问题的核心需求是检测图片中未满足要求的部分并标出。</p><p>首先，<strong>什么部分是“未满足要求”的部分？</strong>根据题意我们可以定义：每一个没有装满的容器会被视为未满足要求的部分。</p><p>接下来的问题是，<strong>“没有装满”的容器指的是什么样的容器？</strong>或者换一个问法：什么样的容器是合格的？对于我们来说，我们可以很轻松的判断出来，没有装满的容器是所给图片中间的那一个，而其余四个容器仅有瓶口部分是没有装满的，这样的容器我们把它视作合格。</p><p><img src="/images/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/22.png" alt="没有装满（不合格）的容器" title="Optional title"></p><p>根据上面的标注，我们可以找到一个最明显的标志：未合格容器的空白部分明显比合格部分的空白面积大。分析到此，此题的核心需求就变成了：<strong>求出每一个容器未装满部分在图片中的面积，并将高于正常值的容器指出。</strong></p><h2 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h2><p>根据问题分析和题目标注，需要先针对所给情景作出如下断言和假设：</p><ul><li><p>原题中叙述：“公司有一个图像系统，装备了有效捕捉静止图像的前端闪光照明设备。所以你可以得到非常清晰的图像”，<strong>由此断言：所得图像的质量稳定的，即图像大小和图像内容分布（容器在图片中的相对位置）是不变的；</strong></p></li><li><p>所给的图片中仅有中间的一个容器所装的液体是明显不足的，其他四个容器的液体平面是一样高的，<strong>由此假设：其余四个容器为合格的产品，它们在瓶口处的空余部分是正常的。</strong>这一假设非常重要，<strong>它将帮助我们定义不合格产品的阈值。</strong></p></li></ul><h2 id="图像对象检测"><a href="#图像对象检测" class="headerlink" title="图像对象检测"></a>图像对象检测</h2><p>本问题所属的问题大类为图像对象检测：图像对象检测是利用图像处理与模式识别等领域的理论和方法，从图像中定位感兴趣的目标，需要准确地判断每个目标的具体类别，并给出每个目标的<strong>边界框</strong>。</p><p>对于本情景而言，所给图像为灰度图像，若想从这一角度来探索问题的解决方法，最容易想到的思想就是<strong>二值化</strong>。</p><p>在数字图像处理中，二值图像占有非常重要的地位，<strong>图像的二值化使图像中数据量大为减少，从而能凸显出目标的轮廓</strong>。本体背景和所检测部位已经分别是黑白两色，非常适合这个思路。</p><p>读取图像<br>首先介绍本程序主要用到的py包： scikit-image（aka skimage）是图像处理和计算机视觉算法的集合。<strong>主要的软件包skimage提供了一些用于在图像数据类型之间转换的实用程序</strong>。</p><p><img src="/images/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/33.png" alt="导入文件包，读取图像，输出结果" title="Optional title"></p><p>读取图像，输出图像结果，可以看到，<strong>imread函数会将图像按照行列存储为二维数组，且是归一化的灰度图</strong>。</p><p><img src="/images/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/44.png" alt="输出读取结果及图像大小" title="Optional title"></p><p>输出图像及其大小，验证是否读取正确。</p><p><img src="/images/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/55.png" alt="图像文件信息" title="Optional title"></p><p>输出的大小为（556，1004），再查看图片文件，发现可以匹配，同时说明该数据结构是按照像素的行列进行读取和存储的。</p><h2 id="二值化"><a href="#二值化" class="headerlink" title="二值化"></a>二值化</h2><p>对图像进行二值化：以归一化值域中心0.5为界线，像素值低于0.5的区域变为黑色，否则变为白色。</p><p><img src="/images/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/66.png" alt="图像二值化" title="Optional title"></p><h2 id="图像无用部分处理"><a href="#图像无用部分处理" class="headerlink" title="图像无用部分处理"></a>图像无用部分处理</h2><p><img src="/images/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/77.png" alt="二值化后的图像" title="Optional title"></p><p>二值化后的图片如上所示，可以看出，<strong>即使对图片进行了二值化，我们想要关注的部分的轮廓也并不是非常清晰的：蓝色标记才是我们想要关注的部分，但是还有很多其他的没有被处理为背景的部分（红色标记）也出现在图中</strong>，这些部分很可能是容器侧壁的反光造成的。尤其是对于中间未合格的容器来说，它的空余部分下面还有一部分侧壁的空白与之相连，<strong>这样就造成了所要检测的区域和无关区域一同构成了连通域</strong>。</p><p>很容易明晰的一点是：我们说某一个容器所装的液体量不合格，<strong>意思是只要该容器有很小的该装液体的部分被检测到没有装液体</strong>，至于该不合格产品到底少装了多少，我们并不关心。所以我们只要看容器从瓶口部分及其往下面衍生的一部分即可，至于瓶体的下半身完全可以不用检测。</p><p>基于此，我们对该图像的无用区域进行切割和处理：按照4:6的比例分割图像，并把下60%部分舍去，直接作为背景。</p><p><img src="/images/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/88.png" alt="二值化图像无用部分处理" title="Optional title"></p><h2 id="连通域"><a href="#连通域" class="headerlink" title="连通域"></a>连通域</h2><p>根据上述处理结果，我们就可以很明显的看出合格品和不合格品差别了。接下来，我们需要对图片中的每一块白色部分的面积做统计，这些白色部分在数字图像处理中被统称为“连通域”。</p><p>首先使用skimage.measure的字函数label获取连通域信息。这个函数的作用是：输入二值化后的数字图像矩阵，返回的内容为连通域的个数和带有连通域标记的矩阵。对于每一个连通域，都为他们赋予一个整数值。</p><p>输出的数量为16，则我们可以知道，该图像中共有17个连通域，出去编号为0的黑色背景，共有16个白色的区域，它们的编号为1-16。</p><p><img src="/images/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/99.png" alt="输出连通域信息：无排序" title="Optional title"></p><p>将各连通域面积进行排序输出：</p><p><img src="/images/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/1010.png" alt="输出连通域信息：排序后" title="Optional title"></p><p>数据如下：</p><pre><code>[1.0000e+00 3.0000e+00 1.0000e+01 1.3000e+01 2.0000e+01 2.1000e+01 2.5000e+01 3.2000e+01 6.6000e+01 7.1000e+01 1.2900e+02 6.8410e+031.0780e+04 1.3383e+04 1.3391e+04 2.7571e+04]</code></pre><p>再根据我们对于图片内容的观察，我们可以很容易的从数据中推断出一些内容：<strong>最大的数2.7571e+04代表了不合格区域的面积，即中间的容器</strong>，接下来的4个数6.8410e+03、1.0780e+04、1.3383e+04、1.3391e+04代表了四个合格容器，而在这四个数中，<strong>6.8410e+03与另外三个数不在一个数量级上的原因是：它代表了最右边那个不完整的合格容器</strong>。1.0780e+04比6.8410e+03稍大一些，从图中可以看出它代表了最左边那个不完整的合格容器。</p><p><strong>剩下的两个数1.3383e+04、1.3391e+04非常接近，我们便可由此断言：合格容器的空白部分上限就在这两个值附近。在这里，我们定义了判断是否合格的阈值。</strong></p><h2 id="过滤无关连通域"><a href="#过滤无关连通域" class="headerlink" title="过滤无关连通域"></a>过滤无关连通域</h2><p>上面分析了有用的值，剩下的值很明显就是无用的，因为我们在图中仅有上述5个区域需要进行判断，那么其余的连通域就是无关的连通域。我们需要将这些连通域处理为背景。</p><p>在进行上述处理前，需要先得到排序过后的面积和原面积数组的映射关系：</p><p><img src="/images/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/1111.png" alt="排序映射" title="Optional title"></p><p>接下来，过滤无关连通域：</p><p><img src="/images/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/1212.png" alt="过滤无关连通域" title="Optional title"></p><h2 id="判断并得到结果"><a href="#判断并得到结果" class="headerlink" title="判断并得到结果"></a>判断并得到结果</h2><p>先来查看过滤后的结果，为不同标记的连通域赋予不同的颜色，背景依旧为黑色：</p><p><img src="/images/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/1313.png" alt="过滤无关连通域之后的结果" title="Optional title"></p><p>可以看到，需要被判断的5个连通域都被我们用不同的标记给出了。接下来，只需要用我们在上面给出的阈值对每个连通域进行判断即可。</p><p><img src="/images/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/1414.png" alt="对连通域的面积进行判断" title="Optional title"></p><p>查看判断后的最终结果：</p><p><img src="/images/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/1515.png" title="Optional title"><br><img src="/images/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/1616.png" alt="最终结果" title="Optional title"></p><p>可以看到，<strong>正确的部分被彩色填充，错误的部分依旧是白色的</strong>。</p><p>将这个结果和我们在一开始时对于图片的判断进行对比，发现结果完全正确，我们的程序可以达到预期的结果。</p><hr><h1 id="附录：代码"><a href="#附录：代码" class="headerlink" title="附录：代码"></a>附录：代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入文件包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> skimage <span class="keyword">as</span> img</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">from</span> skimage.measure <span class="keyword">import</span> label</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取图像</span></span><br><span class="line"><span class="comment">#io.imread返回的存储结构为 &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br><span class="line">bottle = io.imread(<span class="string">&#x27;bottle.png&#x27;</span>, as_gray=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#输出读取的结果</span></span><br><span class="line">print(bottle)</span><br><span class="line"></span><br><span class="line"><span class="comment">#显示所读取的图像信息</span></span><br><span class="line">print(<span class="string">&quot;size: &quot;</span>)</span><br><span class="line">print(bottle.shape)</span><br><span class="line">io.imshow(bottle)</span><br><span class="line"></span><br><span class="line"><span class="comment">#图像二值化：以归一化值域中心0.5为界线，像素值低于0.5的区域变为黑色，否则变为白色</span></span><br><span class="line">rows,cols=bottle.shape</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(rows):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(cols):</span><br><span class="line">        <span class="keyword">if</span> (bottle[i,j]&lt;=<span class="number">0.5</span>):</span><br><span class="line">            bottle[i,j]=<span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            bottle[i,j]=<span class="number">1</span></span><br><span class="line"><span class="comment">#显示二值化后的图像</span></span><br><span class="line">io.imshow(bottle)</span><br><span class="line"></span><br><span class="line"><span class="comment">#按照4:6的比例分割图像，并把下60%部分舍去，直接作为背景</span></span><br><span class="line">rows,cols=bottle.shape</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(int(rows*<span class="number">0.4</span>),rows):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(cols):</span><br><span class="line">            bottle[i,j]=<span class="number">0</span></span><br><span class="line"><span class="comment">#原图像的最上面有一条无关的白线，去除掉</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,int(rows*<span class="number">0.05</span>)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(cols):</span><br><span class="line">            bottle[i,j]=<span class="number">0</span></span><br><span class="line">io.imshow(bottle)</span><br><span class="line"></span><br><span class="line"><span class="comment">#标记连通域</span></span><br><span class="line"><span class="comment">#label返回的存储结构为 &lt;class &#x27;tuple&#x27;&gt;</span></span><br><span class="line">bottle_label=label(bottle,background =<span class="number">0</span>,return_num=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看所得到的连通域标记数组</span></span><br><span class="line">print(bottle_label)</span><br><span class="line"><span class="comment">#存储结果</span></span><br><span class="line">np.savetxt(<span class="string">&quot;bottle_label.txt&quot;</span>,bottle_label[<span class="number">0</span>],fmt=<span class="string">&quot;%s&quot;</span>,delimiter=<span class="string">&quot;,&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#按照标记统计各个连通域的面积</span></span><br><span class="line">area=np.zeros(bottle_label[<span class="number">1</span>]+<span class="number">1</span>)</span><br><span class="line">temp=np.zeros(bottle_label[<span class="number">1</span>]+<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(rows):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(cols):</span><br><span class="line">        area[int(bottle_label[<span class="number">0</span>][i][j])]+=<span class="number">1</span></span><br><span class="line">        temp[int(bottle_label[<span class="number">0</span>][i][j])]+=<span class="number">1</span></span><br><span class="line">print(area)</span><br><span class="line"></span><br><span class="line"><span class="comment">#按对连通域的面积进行从小到大排序</span></span><br><span class="line">temp=temp[<span class="number">1</span>:]</span><br><span class="line">temp.sort()</span><br><span class="line"><span class="comment">#查看排序后的数组</span></span><br><span class="line">print(temp)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将排序后前的数组顺序映射到排队后的数组</span></span><br><span class="line">area_sort_num=np.zeros(bottle_label[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range (bottle_label[<span class="number">1</span>]):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range (<span class="number">1</span>,bottle_label[<span class="number">1</span>]+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span>(int(temp[i])==int(area[j])):</span><br><span class="line">            area_sort_num[i]=j</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"><span class="comment">#查看结果</span></span><br><span class="line">print(area_sort_num)</span><br><span class="line"></span><br><span class="line"><span class="comment">#过滤无关的连通域</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#a：过滤阈值b在排序前数组中的位置映射，用来做循环</span></span><br><span class="line">a=<span class="number">-1</span></span><br><span class="line"><span class="comment">#过滤的阈值b：如果连通域面积小于b，则会被视作背景</span></span><br><span class="line">b=(<span class="number">1.2900e+02</span>)+<span class="number">1</span></span><br><span class="line"><span class="comment">#寻找b的值，赋给a</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(temp)):</span><br><span class="line">    <span class="keyword">if</span>(temp[i]&gt;b):</span><br><span class="line">        a=i;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line"><span class="comment">#过滤掉无关的连通域面积，使之成为背景</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(rows):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(cols):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(a):</span><br><span class="line">            <span class="keyword">if</span>(int(bottle_label[<span class="number">0</span>][i][j])==area_sort_num[k]):</span><br><span class="line">                bottle_label[<span class="number">0</span>][i][j]=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看过滤后的结果</span></span><br><span class="line"><span class="comment">#为不同标记的连通域赋予不同的颜色，背景依旧为黑色</span></span><br><span class="line">io.imshow(bottle_label[<span class="number">0</span>],cmap=plt.cm.hot)</span><br><span class="line"></span><br><span class="line"><span class="comment">#a：过滤阈值b在排序前数组中的位置映射，用来做循环</span></span><br><span class="line">d=[]</span><br><span class="line"><span class="comment">#过滤的阈值b：如果连通域面积小于b，则会被视作背景</span></span><br><span class="line">c=(<span class="number">1.2900e+02</span>)+<span class="number">1</span></span><br><span class="line"><span class="comment">#寻找b的值，赋给a</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(temp)):</span><br><span class="line">    <span class="keyword">if</span>(temp[i]&gt;<span class="number">1.3391e+04</span>):</span><br><span class="line">        d.append(i);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(rows):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(cols):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(a):</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(len(d)):</span><br><span class="line">                <span class="keyword">if</span>(int(bottle_label[<span class="number">0</span>][i][j])==area_sort_num[d[k]]):</span><br><span class="line">                    bottle_label[<span class="number">0</span>][i][j]=<span class="number">2</span></span><br><span class="line">                <span class="keyword">elif</span>(int(bottle_label[<span class="number">0</span>][i][j])!=<span class="number">0</span>):</span><br><span class="line">                    bottle_label[<span class="number">0</span>][i][j]=<span class="number">1</span></span><br><span class="line">io.imshow(bottle_label[<span class="number">0</span>],cmap=plt.cm.hot)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h1&gt;&lt;p&gt;包装生产线的质量检测：一家用瓶子装各种</summary>
      
    
    
    
    
    <category term="数字图像处理" scheme="http://example.com/tags/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
</feed>
