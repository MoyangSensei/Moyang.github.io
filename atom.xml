<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>MoyangSensei</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-06-16T07:17:33.407Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Fy J</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文研读：TTSR</title>
    <link href="http://example.com/2022/06/15/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ATTSR/"/>
    <id>http://example.com/2022/06/15/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ATTSR/</id>
    <published>2022-06-15T08:53:15.000Z</published>
    <updated>2022-06-16T07:17:33.407Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Learning Texture Transformer Network for Image Super-Resolution</p></blockquote><blockquote><p>CVPR 2020</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>文章指出，现有的超分辨率方法忽略了使用注意力机制从Ref图像传输高分辨率的纹理。</p><blockquote><p>使用高分辨率图像作为参考，把这个称为Ref，同SISR是两种不同的范式，abstract和introduction里提到。</p><blockquote><p>We study on image super-resolution (SR), which aims to recover realistic textures from a low-resolution (LR) image. Recent progress has been made by taking high-resolution images as references (Ref), so that relevant textures can be transferred to LR images.<br>The research on image SR is usually conducted on two paradigms, including single image super-resolution (SISR), and reference-based image super-resolution (RefSR).</p></blockquote></blockquote><blockquote><p>However, existing SR approaches neglect to use attention mechanisms to transfer high-resolution (HR) textures from Ref images, which limits these approaches in challenging cases.</p></blockquote><p>解决思路：TTSR，其中有4个模块。<strong>最大的贡献：最早将transformer体系结构引入图像生成任务。</strong></p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="Texture-Transformer"><a href="#Texture-Transformer" class="headerlink" title="Texture Transformer"></a>Texture Transformer</h2><p>用于纹理变换的transformer，也是本文的核心。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/1.png" title="Optional title"></p><h3 id="Learnable-Texture-Extractor-LTE"><a href="#Learnable-Texture-Extractor-LTE" class="headerlink" title="Learnable Texture Extractor (LTE)"></a>Learnable Texture Extractor (LTE)</h3><p><strong>过去通常使用预训练好的VGG网络提取特征</strong>，LTE是被设计用来进行特征提取的提取器，这里可以自设训练，在超分过程中训练从而更好地提取跨LR和Ref的联合特征。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/2.png" title="Optional title"></p><h3 id="Relevance-Embedding-RE"><a href="#Relevance-Embedding-RE" class="headerlink" title="Relevance Embedding (RE)"></a>Relevance Embedding (RE)</h3><p>RE的目的是通过估计Q和K之间的相似度来估计LR和REF图像之间的相关性。</p><p>LTE提取的Q和K被分成多个patch，其中每个patch可以分别表示为:</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/3.png" title="Optional title"></p><p>计算q和k的归一化内积，作为相似度：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/4.png" title="Optional title"></p><blockquote><p>R是没有出现在图上的，R是RE的结果，H和S是通过它得到的。</p></blockquote><h3 id="Hard-Attention-HA"><a href="#Hard-Attention-HA" class="headerlink" title="Hard-Attention (HA)"></a>Hard-Attention (HA)</h3><p>传统的注意力机制会直接计算qi与V的加权和，作者认为这样会造成模糊。具体原因是<strong>由于Ref图像和LR图像之间的domain gap比较大导致直接这样模型会缺乏传递HR纹理特征的能力</strong>。HA则用来传递Ref图像中纹理特征以解决这个问题。</p><p>计算HA map：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/5.png" title="Optional title"></p><p>hi是一个索引值，表示Ref图像中和LR图像中第i个patch最相关的patch的索引。根据这个值去V中索引得到最相关的纹理特征T。</p><h3 id="Soft-Attention-SA"><a href="#Soft-Attention-SA" class="headerlink" title="Soft-Attention (SA)"></a>Soft-Attention (SA)</h3><p>HA是负责找出Ref特征中和LR图像特征相关较强的部分从而建立对应关系，SA是负责将两个特征通过加权的方式更好地融合。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/6.png" title="Optional title"></p><blockquote><p>跳过的公式7就是从HA map中找到最大的（Max）那个。</p></blockquote><h2 id="Cross-Scale-Feature-Integration-CSFI"><a href="#Cross-Scale-Feature-Integration-CSFI" class="headerlink" title="Cross-Scale Feature Integration (CSFI)"></a>Cross-Scale Feature Integration (CSFI)</h2><p>通过堆叠Texture Transformer可以实现不同倍数的放大，作者提出了一个跨尺度特征集成模块（CSFI）在不同尺度的特征之间交换信息。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/7.png" title="Optional title"></p><h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>线性组合Loss，三部分组成：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/8.png" title="Optional title"></p><p>Reconstruction loss：<strong>就是L1。大家都认为这个比L2强。</strong></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/9.png" title="Optional title"></p><p>Adversarial loss：参照WGAN-GP。提出用梯度范数的惩罚来代替权重裁剪加粗样式，使得训练更加稳定，性能更好。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/10.png" title="Optional title"></p><p>Perceptual loss：SRGAN里已经详细提到过了。感知损失的关键思想是增强预测图像与目标图像在特征空间上的相似性，这里包含两部分，第一部分是传统的基于VGG19的特征。第二部分是转移感知损失（transferal perceptual loss），对应LTE提取的特征，<strong>这种传递感知损失限制了预测的SR图像与传递的纹理特征T具有相似的纹理特征，这使得作者的方法更有效地传递Ref纹理。</strong></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/11.png" title="Optional title"></p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><ul><li><p>可学习纹理提取器包含5个卷积层和2个池层，它们以三种不同的尺度输出纹理特征。为了减少时间和GPU内存的消耗，相关性嵌入只应用于最小尺度，并进一步传播到其他尺度；</p></li><li><p>鉴别器G：我们采用了SRNTT中使用的相同网络，并删除了所有BN层；</p></li><li><p>数据增广：随机水平和垂直翻转，然后随机旋转90，180和270；每个minibatch包含9块大小为40×40的LR patch，以及9块大小为160×160的HR和Ref patch；</p></li><li><p>组合Loss的权重：Lrec、Ladv和Lper的权重系数分别为1、1e-3和1e-2；</p></li><li><p>优化器：使用β1=0.9、β2=0.999和ǫ=1e-8的Adam opti-mizer；</p></li><li><p>学习率：1e-4；</p></li><li><p>训练策略：前2个epoch只用Lrec，之后将所有的Loss都用上后训练50个epoch。</p></li></ul><blockquote><p>这部分写在原文的method的最后，也就是3.4，没放在4.1去写。</p></blockquote><ul><li><p>数据集：CUFED5【41】，训练集包含11871对，每对由输入图像和Ref图像组成；测试集共有126幅测试图像，每幅图像由4幅相似程度不同的Ref图像组成。</p></li><li><p>额外数据集1：Sun80【26】包含80幅自然图像，每幅图像都有多幅Ref图像。</p></li><li><p>额外数据集2：Urban100【11】，该数据集没有Ref图像，将其LR图像视为Ref图像。由于Urban100都是具有强自相似性的建筑图像，因此这种设计可以实现显式的自相似搜索和传输过程。</p></li><li><p>额外数据集3：Manga109【20】，同样缺少Ref图像，随机抽取该数据集中的HR图像作为Ref图像。</p></li><li><p>评价指标：在YCbCr空间的Y通道上，对SR结果进行了PSNR和SSIM评估。</p></li></ul><h2 id="定量实验"><a href="#定量实验" class="headerlink" title="定量实验"></a>定量实验</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/12.png" title="Optional title"></p><h2 id="定性实验"><a href="#定性实验" class="headerlink" title="定性实验"></a>定性实验</h2><p>可视化：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/13.png" title="Optional title"></p><p>用户调研：对RCAN【39】、RSR-GAN【38】、CrossNet【43】和SRNTT【41】，10名受试者，在CUFED5测试集上收集了2520张选票。对于每个比较过程，我们为用户提供两个图像，其中包括一个TTSR图像。要求用户选择视觉质量更高的。其中Y轴上的值表示倾向于TTSR而非其他方法的用户的年龄百分比。超过90%的用户投票支持的TTSR。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/14.png" title="Optional title"></p><blockquote><p>用户调研这里很像LPIPS中的用户调研，都是同时给你两张图让你选更清楚的。</p></blockquote><h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><h3 id="Texture-Transformer中模块的影响"><a href="#Texture-Transformer中模块的影响" class="headerlink" title="Texture Transformer中模块的影响"></a>Texture Transformer中模块的影响</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/15.png" title="Optional title"></p><h3 id="CSFI的影响"><a href="#CSFI的影响" class="headerlink" title="CSFI的影响"></a>CSFI的影响</h3><p>第一行显示了仅使用TT时我们模型的性能，而第二行证明了CSFI的有效性，它使PSNR指标增加了0.17。</p><p>为了验证性能的改善不是由参数大小的增加带来的，将“Base+TT”模型的信道数增加到80和96。“Base+TT（C80）”几乎没有增长，其参数数几乎与“Base+TT+CSFI”相同。“Base+TT（C96）”将参数数量增加到9.10M也没有增长。<strong>CSFI可以以相对较小的参数大小有效地利用参考纹理信息。</strong></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/16.png" title="Optional title"></p><h3 id="Adv-loss和Transferal-per-loss的影响"><a href="#Adv-loss和Transferal-per-loss的影响" class="headerlink" title="Adv loss和Transferal per loss的影响"></a>Adv loss和Transferal per loss的影响</h3><p>两幅图，不重要。</p><h3 id="不同的Ref图像相关性的影响"><a href="#不同的Ref图像相关性的影响" class="headerlink" title="不同的Ref图像相关性的影响"></a>不同的Ref图像相关性的影响</h3><p>为了研究LR和Ref图像之间的相关性如何影响TTSR的结果，在CUFED5测试集上进行了实验。其中，“L1”至“L4”表示CUFED5测试集提供的参考图像，其中L1是最相关的级别，而L4是最不相关的级别。“LR”是指使用输入图像本身作为参考图像。使用L1作为参考图像的TTSR实现了最佳性能。当使用LR作为参考图像时，TTSR的性能仍然优于以前最先进的RefSR方法。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/17.png" title="Optional title"></p><blockquote><p>一张图像的LR难道不应该是最相关的？还是说这个数据集的组成并不是依靠两个客观指标的计算来定义参考图像是否相关？？？</p></blockquote><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>文章：<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Learning_Texture_Transformer_Network_for_Image_Super-Resolution_CVPR_2020_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Learning_Texture_Transformer_Network_for_Image_Super-Resolution_CVPR_2020_paper.pdf</a></p></li><li><p>code：<a href="https://github.com/researchmm/TTSR">https://github.com/researchmm/TTSR</a></p></li></ul><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>Transformer能够用来做很多事情，在看这篇文章之前，以为是一transformer的领域迁移为核心的创新是最大亮点，确实是，但占比重没有绝对的高，文章还设计了自己的方法流程用来解决超分辨率问题，就是CSFI。</p><p>对比实验做的非常的细，该论证到的内容都论证到了。没有提供太高倍数的模型估计是因为计算量的问题，毕竟CSFI是级联的结构，想一想就知道高倍数需要的参数量估计很大。</p><hr><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>懒得写字，直接放PPT。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/18.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/19.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/20.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/21.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/22.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/23.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/24.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/25.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/26.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/27.png" title="Optional title"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Learning Texture Transformer Network for Image Super-Resolution&lt;/p&gt;
</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>随记。</title>
    <link href="http://example.com/2022/06/08/%E9%9A%8F%E8%AE%B0%E3%80%82/"/>
    <id>http://example.com/2022/06/08/%E9%9A%8F%E8%AE%B0%E3%80%82/</id>
    <published>2022-06-08T02:41:06.000Z</published>
    <updated>2022-06-08T03:18:40.984Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，禁止转载。</p></blockquote><hr><p>我坐在工位上，想起了6年前的时光。</p><p>2016年的夏天我几乎忘干净了，只是仅剩的一点点画面还未远去。被分配到离学校有点远的校园考试，提前和妈妈去考场边上租了酒店。考前的一天晚上，告诉自己不要失眠不要失眠，却还是迷迷糊糊的没睡着。没有熬夜没有焦虑的夜晚，从早晨5点起劳累了一整天的身体，在晚上的什么时候都能入眠。时至今日我依旧羡慕当时的睡眠能力。考试的时候特意选了一件衬衣，和考前爸妈陪着去买的一件短袖，坐在考场上的时候还觉得穿件衬衣是真的很热。最后一场考英语，中午睡午觉还差点睡过了头。考完了英语，考场外碰见了英语老师，笑着问我考的怎么样。回到出租屋里，收拾行李，看着成堆的书本和卷子，舍不得扔掉但也不愿意带走。写的很好看的物理笔记和生物笔记，郑重的送给了高二年级的一个学弟。一家人开着车从市里学校回县城，我懒洋洋的躺在后座上，窗外的树荫从车窗上掠过，那一瞬间我觉得我化在了夕阳的温暖中。没有狂欢，没有激动，没有交流答案，没有估算成绩，没有立马去找寻自己将要上什么大学，没有去参加晚上的聚会，没有跟任何同龄人交流什么，但那是一种前所未有的轻松。我实在忘记了那一晚是怎么入睡的，也许是兴奋到大半夜，也许是同往日一样到头就睡。</p><p>再往前追溯，那时好像并不知道什么叫做焦虑。考前的多次模拟考试，成绩时好时坏，最后一次模拟竟然考到了50多名，要知道班里大概有70人左右。我当时是真的不以为意，好像有种天生的自信，一直捧着我，告诉我那不过是众多意外中最不起眼的那么一个罢了，心里思念的女孩子都比那模拟考试的成绩来得重要。考完之后的一段日子，大家在各种地方热烈的讨论答案，我居然也能非常心安理得的逃避，仿佛这场考试从那天起就跟自己一点关系都没有。那时妈妈在医院做完了手术，当时的我并不知道病的严重性，也是，所有人都瞒着我，生怕影响了我的考试，直到妈妈进了手术室，我才知道有这么一回事。</p><p>高中的日子是比现在苦千倍百倍的，现在想来，却也只是单纯的苦。是一种努力向上的痛苦，是成长的阵痛。彼时的单纯心思，充满幼稚但又十分干净，没有什么无奈，没有什么不甘。我并不特别怀念那段日子，但我也明白，我再也不能感受到这种纯粹的痛苦所带来的纯粹的欢愉。</p><p>我愿再一次感受那个夏天，一如那飞逝过时光的温暖夕阳。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，禁止转载。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;我坐在工位上，想起了6年前的时光。&lt;/p&gt;
&lt;p&gt;2016年的夏天我几乎忘干净了，只是仅剩的一点点画面还未远去。被分配到离学校有点远的校园考试，提前和妈妈去考场边上租了酒店。</summary>
      
    
    
    
    
    <category term="闲谈" scheme="http://example.com/tags/%E9%97%B2%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title>leetcode刷题记录（2）</title>
    <link href="http://example.com/2022/02/25/leetcode%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95%EF%BC%882%EF%BC%89/"/>
    <id>http://example.com/2022/02/25/leetcode%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95%EF%BC%882%EF%BC%89/</id>
    <published>2022-02-25T13:40:32.000Z</published>
    <updated>2022-02-26T12:48:35.615Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="统计"><a href="#统计" class="headerlink" title="统计"></a>统计</h1><ul><li><p>hard：</p></li><li><p>medium：537、</p></li><li><p>easy：35、</p></li><li><p>重难点：35、</p></li></ul><hr><h1 id="35-搜索插入位置-array-二分查找"><a href="#35-搜索插入位置-array-二分查找" class="headerlink" title="35. 搜索插入位置 [array/二分查找]"></a>35. 搜索插入位置 [array/二分查找]</h1><p>在有序数组中查找所给的target，找不到就找到其插入位置。要求是log(N)的复杂度。</p><p><img src="/images/leetcode/35.png" title="Optional title"> </p><p>关键词“有序数组”，显然用二分查找。</p><h1 id="537-复数乘法-技巧题"><a href="#537-复数乘法-技巧题" class="headerlink" title="537. 复数乘法 [技巧题]"></a>537. 复数乘法 [技巧题]</h1><p>给定两个复数，用str表示，求其乘。</p><p><img src="/images/leetcode/537.png" title="Optional title"> </p><p>用str的函数分别取出实部和虚部，按照法则运算。注意符号。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;统计&quot;&gt;&lt;a href=&quot;#统计&quot; class=&quot;headerlink&quot; title=&quot;统计&quot;&gt;&lt;/a&gt;统计&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;hard：</summary>
      
    
    
    
    
    <category term="leetcode" scheme="http://example.com/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>MODIS部分数据说明及解析</title>
    <link href="http://example.com/2022/01/11/MODIS%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E8%AF%B4%E6%98%8E%E5%8F%8A%E8%A7%A3%E6%9E%90/"/>
    <id>http://example.com/2022/01/11/MODIS%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E8%AF%B4%E6%98%8E%E5%8F%8A%E8%A7%A3%E6%9E%90/</id>
    <published>2022-01-11T07:33:38.000Z</published>
    <updated>2022-03-01T08:14:17.422Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>项目需要。记录资料。</p></blockquote><h1 id="官网"><a href="#官网" class="headerlink" title="官网"></a>官网</h1><p>官网：<code>https://modis.gsfc.nasa.gov/</code>。</p><p>Google搜关键词MODIS，第一个就是。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>About和Data页面的一些简介：</p><p><img src="/images/MODIS/1.png" title="Optional title"></p><p><img src="/images/MODIS/2.png" title="Optional title"></p><h1 id="数据下载"><a href="#数据下载" class="headerlink" title="数据下载"></a>数据下载</h1><p>数据下载和官方产品都在Data介绍页给的第一个域名：<code>http://ladsweb.nascom.nasa.gov/</code>。</p><p>里面有一个做的很详细的文件系统，可以查、下数据（主要是用最前面那个编码查）和已有产品所使用到的数据。比如MYD03/2013/001的267条数据：<code>https://ladsweb.modaps.eosdis.nasa.gov/archive/allData/61/MYD03/2013/001/</code>：</p><p><img src="/images/MODIS/3.png" title="Optional title"></p><p>下载要求注册用户和登陆。</p><h1 id="文件命名规则"><a href="#文件命名规则" class="headerlink" title="文件命名规则"></a>文件命名规则</h1><p>来自<code>https://lpdaac.usgs.gov/data/get-started-data/collection-overview/missions/modis-overview/#modis-naming-conventions</code>，内有MODIS文件命名规则（3类）、产品长名称命名规则、时空分辨率、平铺系统（Tiling Systems）、数据处理、光谱带（1-36）、元数据（Metadata）等信息。</p><p>取<code>MODIS/MYD03/2013/001/MYD03.A2013001.0000.061.2018043200701.hdf</code>：</p><table><thead><tr><th align="center">命名字段</th><th align="center">解释</th></tr></thead><tbody><tr><td align="center">MYD03</td><td align="center">产品短名，<code>MOD</code>是Terra星（上午星）数据，<code>MYD</code>是Aqua星（下午星）数据</td></tr><tr><td align="center">A2013001</td><td align="center">数据采样时间，<code>A-YYYYDDD</code>，DDD表示YYYY这一年中的第几天，与上一级文件夹命名对应，意义：该数据2013年的第一天被传感器所采集</td></tr><tr><td align="center">0000</td><td align="center">数据采样时间，<code>HHMM</code>，HH取值为[00,23]，MM取值为[00,59]，意义：该数据与00时00分（晚12时整）采集</td></tr><tr><td align="center">061</td><td align="center">数据采集版本号</td></tr><tr><td align="center">2018043200701</td><td align="center">数据生产时间，<code>YYYYDDDHHMMSS</code>，Y、D、H、M同上，SS为秒，意义：2018年第43天的20时07分01秒生产了该数据</td></tr><tr><td align="center">.hdf</td><td align="center">数据格式<code>HDF-EOS</code></td></tr></tbody></table><h1 id="tool"><a href="#tool" class="headerlink" title="tool"></a>tool</h1><h2 id="HDFView"><a href="#HDFView" class="headerlink" title="HDFView"></a>HDFView</h2><p>最新的是3.1.3版本：<code>https://portal.hdfgroup.org/display/support/HDFView+3.1.3</code>，多系统支持，mac系统的是110M，不翻墙下的很慢。</p><h2 id="HDF-Explore"><a href="#HDF-Explore" class="headerlink" title="HDF Explore"></a>HDF Explore</h2><p>1.4版本：<code>https://www.space-research.org/hdf_explorer/explorer_download.htm</code>，只支持windows，有可视化功能，比上一个好用点，需要在网页上填写一些信息才能下载，下载还需要购买使用许可，直接从百度上找破解版。</p><h1 id="数据说明"><a href="#数据说明" class="headerlink" title="数据说明"></a>数据说明</h1><h2 id="官网数据介绍"><a href="#官网数据介绍" class="headerlink" title="官网数据介绍"></a>官网数据介绍</h2><p>MYD0211KM（Level 1B Calibrated Radiances）：<code>https://ladsweb.modaps.eosdis.nasa.gov/missions-and-measurements/products/MYD021KM</code></p><p>MYD03：（Geolocation）<code>https://ladsweb.modaps.eosdis.nasa.gov/missions-and-measurements/products/MYD03</code></p><p>MYD35（Cloud Mask）：<code>https://ladsweb.modaps.eosdis.nasa.gov/missions-and-measurements/science-domain/cloud-mask/</code> </p><table><thead><tr><th align="center">数据产品名称</th><th align="center">解释</th></tr></thead><tbody><tr><td align="center">MYD021KM</td><td align="center">MODIS <code>1B</code></td></tr><tr><td align="center">MYD03</td><td align="center">MODIS数据<code>地理定位</code>文件</td></tr><tr><td align="center">MYD35</td><td align="center">大气2,3级，<code>云掩膜</code>，250m/1㎞</td></tr></tbody></table><h2 id="文件目录"><a href="#文件目录" class="headerlink" title="文件目录"></a>文件目录</h2><p>前三级目录：</p><p><img src="/images/MODIS/4.png" title="Optional title"></p><p>第四级目录，取<code>MODIS/MYD03/2013/001</code>：</p><p><img src="/images/MODIS/5.png" title="Optional title"></p><p><img src="/images/MODIS/6.png" title="Optional title"></p><p>数量：MYD03和MYD021KM都选的是2013年的相同天数，除了两个001内是267个文件，剩下的每个文件夹内都是288（60/5=12，24*12=288）个文件。</p><p>这里给的35是21年的02和03都是13年的，对不上。</p><h2 id="重点字段信息"><a href="#重点字段信息" class="headerlink" title="重点字段信息"></a>重点字段信息</h2><p>三部分产品需要联合起来用的。</p><h3 id="MYD35"><a href="#MYD35" class="headerlink" title="MYD35"></a>MYD35</h3><ul><li>Cloud_Mask：作为GT，为<code>6*2030*1354</code>矩阵。</li></ul><p><img src="/images/MODIS/7.png" title="Optional title"></p><h3 id="MYD02"><a href="#MYD02" class="headerlink" title="MYD02"></a>MYD02</h3><p>对地观测数据文件包括250米、500米和1000米分辨率的定标后的资料。</p><p>给定波段的一组探测器在沿轨道方向对齐在四个焦平面上。图象中的一条扫描线是扫描带中一个探测器的扫描观测资料。<code>1km波段包含10个探测器</code>，500m波段包含20个探测器，250m波段包含40个探测器。探测器之间的扫描间距大致分别为1km、500m和250m，在扫描方向，<code>每帧（frame）大致为1km大小</code>，MODIS仪器<code>在1km波段每帧的取样率为1</code>，500m波段为2，250m波段为4。</p><ul><li><p>EV_1KM_Emissive：热辐射波段，用来计算亮温。</p></li><li><p>EV_1KM_RefSB：太阳光反射波段，计算反射率。</p></li></ul><p><img src="/images/MODIS/8.png" title="Optional title"></p><p><img src="/images/MODIS/9.png" title="Optional title"></p><h3 id="MYD03"><a href="#MYD03" class="headerlink" title="MYD03"></a>MYD03</h3><p>暂且不用，因为不考虑特定区域划分的话，使用02和35的文件名即可对准数据。</p><h1 id="云检测实验"><a href="#云检测实验" class="headerlink" title="云检测实验"></a>云检测实验</h1><h2 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h2><ul><li><p>数据：使用2013年001天的0000、0005和0010进行实验，只取用02和35产品，不使用03产品；3个hdf文件中共有3<em>2030</em>1354=<code>8245860</code>个点</p></li><li><p>真值：使用35产品中，0通道数值的第1、2比特位</p></li><li><p>input：使用02产品中，<code>EV_1KM_Emissive的4个通道</code>和<code>EV_1KM_RefSB的4个通道</code></p></li><li><p>数据预处理：对于真值，取第一通道数据，取00状态作为有云，其他三个状态统一处理为无云，不考虑背景的冰雪、耀斑等；对于input数据，对于不同通道的处理为<code>(data+offset)*scale</code></p></li><li><p>数据划分：训练:测试=8:2，划分后的数据量为<code>6596688:1649172</code></p></li><li><p>评估指标：同真值计算<code>ACC</code></p></li></ul><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><table><thead><tr><th align="center">方法</th><th align="center">参数设置</th><th align="center">结果</th></tr></thead><tbody><tr><td align="center">PCA+逻辑回归</td><td align="center">lr的max_iter=3000，PCA缩小围度范围为8-2</td><td align="center">0.941/0.761/0.794/0.788/0.832/0.854/0.864</td></tr><tr><td align="center">SVM</td><td align="center">sklearn默认</td><td align="center">0.955（100w）</td></tr><tr><td align="center">GDBT</td><td align="center">sklearn默认</td><td align="center">0.958</td></tr><tr><td align="center">GaussianNB</td><td align="center">sklearn默认</td><td align="center">0.867</td></tr><tr><td align="center">BP神经网络</td><td align="center">三个隐藏层(50,15,5)，solver=’adam’，max_iter=1000</td><td align="center">0.969</td></tr><tr><td align="center">LSTM</td><td align="center">LSTM(100)+LSTM(50)+Dense(1)，epochs=100，batch_size=128</td><td align="center">0.919</td></tr><tr><td align="center">随机森林</td><td align="center">n_estimators=1000</td><td align="center">0.948</td></tr><tr><td align="center">RS-Net(U-Net)</td><td align="center"><a href="https://github.com/JacobJeppesen/RS-Net">https://github.com/JacobJeppesen/RS-Net</a></td><td align="center">0.935</td></tr></tbody></table><h2 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h2><ul><li><p>方法流程：有两种：一是直接将HDF文件看做图像，数据处理也当作图像那样整体处理，一般是转存为TIF或者是HDF5，用图像的方法进行云检测，是部分深度学习方法的思路；另一种是把这些离散数据取出，将多通道的数据所对应的检测结果当成一个个像素点，用分类的方法进行拟合，偏向于传统机器学习的思路。<code>不知道到底哪种思路适合这个工程</code>。现状：两种方法都有应用；写论文的话，业内人士会尽量避免深度学习的方法，因为卫星数据的物理因素比较多的情况下进行数据处理会引入很大的误差；模型速度上深度方法更适合大量数据，看做图像的话要使用较多的显卡。</p></li><li><p>训练数据不同种类下垫面划分：做了的话肯定有助于提高性能，属于是标数据的一部分。但对于国产数据来说，没有固定的标准，需要制定划分标准（这里可以使用Modis的），试验前做划分，且如果做了划分，就肯定要训练多个模型。</p></li><li><p>算法性能评估指标：简单的使用了ACC，是否可信未知。</p></li><li><p>算法性能比较：推荐的是<code>随机森林</code>、<code>朴素贝叶斯</code>和<code>U-Net</code>。实际进行下来，决策树类算法、BP的性能是第一档；U-Net性能稍差一点但是也能看，估计是因为数据量的原因，上面的数据量对于其他算法来说够了，但是对于U-Net来说肯定是少；基于高斯分布的朴素贝叶斯网络性能就一般；<code>LSTM</code>的性能也算做中档，但实际上LSTM并不适合这个场景，因为数据并没有呈现序列性；SVM就不考虑了，数据量一大的话，效率太低。</p></li><li><p>国产数据与MODIS数据的对齐问题：raw data经纬度对不齐。</p></li><li><p>国产数据的评估问题：真值、指标。</p></li></ul><h2 id="接下来需要做的"><a href="#接下来需要做的" class="headerlink" title="接下来需要做的"></a>接下来需要做的</h2><ul><li><p>可以补一下Deep CNN系的方法，简单粗暴，上面说的两种思路都可以做，但主要的可能还是数值拟合。</p></li><li><p>使用MODIS的数训练FY4的，主要需要解决数据对齐、数据选择（主要是通道选择，推荐通道还需要再缩小一下提高性能）、评估，尽量上深度方法。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;项目需要。记录资料。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;官网&quot;&gt;&lt;a href=&quot;#官网&quot; class=&quot;headerlink&quot; title=&quot;官网&quot;&gt;&lt;/a&gt;官网&lt;/h1&gt;&lt;p&gt;官网：&lt;code&gt;https://modis.gs</summary>
      
    
    
    
    
    <category term="MODIS" scheme="http://example.com/tags/MODIS/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：LPIPS</title>
    <link href="http://example.com/2022/01/07/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ALPIPS/"/>
    <id>http://example.com/2022/01/07/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ALPIPS/</id>
    <published>2022-01-07T12:51:01.000Z</published>
    <updated>2022-06-15T13:29:58.289Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</p></blockquote><blockquote><p>CVPR 2018</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>在计算机科学的许多领域中，比较数据都不会构成太大的困难：比如汉明距离、欧几里得距离等，但是视觉相似性（visual similarity）的概念往往是很主观的，旨在模仿人类的视觉感知。</p><p><strong>期望得到一个真正的“感知距离”（也就是能明确证明这个感知指标是接近人的主观视觉感知的）</strong>，已有一些依据感知的距离度量，比如SSIM[58]、MSSIM[60]、FSIM[62]和HDR-VDP[34]。</p><blockquote><p>[58] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 2004. 1, 2, 8, 12, 14<br>[60] Z. Wang, E. P. Simoncelli, and A. C. Bovik. Multiscale struc- tural similarity for image quality assessment. In Signals, Systems and Computers. IEEE, 2004. 1<br>[62] L. Zhang, L. Zhang, X. Mou, and D. Zhang. Fsim: A feature similarity index for image quality assessment. TIP, 2011. 1, 2, 12, 14<br>[34] R.Mantiuk,K.J.Kim,A.G.Rempel,andW.Heidrich.Hdr-<br>vdp-2: A calibrated visual metric for visibility and quality predictions in all luminance conditions. In ACM Transac- tions on Graphics (TOG), 2011. 1, 12</p></blockquote><p>根据以上背景，文章提出问题：</p><ul><li><p>如何得到一个”感知距离(Perceptual Distance)“，以符合人类判断的方式衡量两个图像的相似程度？</p></li><li><p>这些”感知损失“与人类的视觉到底有多对应？</p></li><li><p>它们与网络架构是否有关？</p></li></ul><blockquote><p>怎样以“感知”的形式衡量图像的相似程度？怎么证明或者评估这种衡量指标的性能？这种衡量指标可否在不同网络架构下通用？</p></blockquote><h1 id="Berkeley-Adobe-Perceptual-Patch-Similarity-BAPPS-Dataset"><a href="#Berkeley-Adobe-Perceptual-Patch-Similarity-BAPPS-Dataset" class="headerlink" title="Berkeley-Adobe Perceptual Patch Similarity (BAPPS) Dataset"></a>Berkeley-Adobe Perceptual Patch Similarity (BAPPS) Dataset</h1><p>为了评估不同感知方法的性能，首先对已有方法造成的图像扭曲进行分类和收集，然后使用两种方法收集了一个大规模的高度不同的认知判断数据集，称为BAPPS。</p><h2 id="Distortions"><a href="#Distortions" class="headerlink" title="Distortions"></a>Distortions</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LPIPS/1.png" title="Optional title"></p><h3 id="Traditional-distortions"><a href="#Traditional-distortions" class="headerlink" title="Traditional distortions"></a>Traditional distortions</h3><p>使用了亮度失真(photometric distortions)，随机噪声(random noise)，模糊(blurring)，空间移位(spatial shifts)，损坏(corruptions)，压缩伪影(compression artifacts)等20种传统方法进行顺序组合成308种顺序的失真。</p><p><strong>这里的“扭曲”，指的是人为添加的失真</strong>，也就是直接用图像处理算法对每张图像进行处理。可以看作是数据增广的概念和操作。</p><blockquote><p>Our traditional distortions (left) are performed by basic low-level image editing operations.</p></blockquote><h3 id="CNN-based-distortions"><a href="#CNN-based-distortions" class="headerlink" title="CNN-based distortions"></a>CNN-based distortions</h3><p><strong>通过探索各种图像任务、网络结构和损失函数来模拟可能的算法输出</strong>，比如编码(autoencoding)、去噪(Denoising)、着色(Colorization)、超分(Super Resolution)、去模糊(Video Deblurring)、帧插值(Frame Interpolation)等。总成生成了96个去噪自编码器(denoising autoencoders)，每个网络的目标不是本身解决任务，而是探索困扰基于深度学习方法输出的常见工件。</p><blockquote><p>Our CNN-based distortions (right) are formed by randomly varying parameters such as task, network architecture, and learning parameters. The goal of the distortions is to mimic plausible distortions seen in real algorithm outputs.</p></blockquote><h3 id="超分部分的工作内容"><a href="#超分部分的工作内容" class="headerlink" title="超分部分的工作内容"></a>超分部分的工作内容</h3><p>传统扭曲方法：评估了NTIRE 2017 workshop的结果，使用×2、×3、×4上采样率，使用“未知”下采样创建输入图像，每个上采样倍数都有大约20个算法提交。</p><p>基于CNN的扭曲方法：包括双三次上采样和四种性能最好的深度超分辨率方法[24、59、31、48]。结果比较：从Div2K数据集中随机位置的图像中随机抽取64×64 patch进行比较。</p><blockquote><p>原文中没做划分，但应该就是按照之前的两个区分标准进行的扭曲方法。给的四个“效果最好”的模型只看过SRGAN，其他三个都见过。<br>[24] J.Kim,J.KwonLee,andK.MuLee.Accurateimagesuper- resolution using very deep convolutional networks. In CVPR,<br>pages 1646–1654, 2016. 4<br>[59] Z. Wang, D. Liu, J. Yang, W. Han, and T. Huang. Deep networks for image super-resolution with sparse prior. In ICCV, 2015. 4<br>[31] SRGAN<br>[48] M. S. Sajjadi, B. Scho ̈lkopf, and M. Hirsch. Enhancenet: Single image super-resolution through automated texture synthesis. ICCV, 2017. 4</p></blockquote><h2 id="Psychophysical-Similarity-Measurements"><a href="#Psychophysical-Similarity-Measurements" class="headerlink" title="Psychophysical Similarity Measurements"></a>Psychophysical Similarity Measurements</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LPIPS/2.png" title="Optional title"></p><h3 id="2AFC-similarity-judgments"><a href="#2AFC-similarity-judgments" class="headerlink" title="2AFC similarity judgments"></a>2AFC similarity judgments</h3><p>随机选取一个图片x，使用两种失真（变换）方法生成两张图片x0和x1，让人来判断哪一个更接近原图，记录结果h，最终这一组实验数据表示为<code>T(x,x0,x1,h)</code>。</p><p>把每一个测试样本称为一个双向强迫选择（2AFC），人平均每次判断花费3秒钟。收集了训练样本151.4K个。</p><h3 id="Just-noticeable-differences-JND"><a href="#Just-noticeable-differences-JND" class="headerlink" title="Just noticeable differences (JND)"></a>Just noticeable differences (JND)</h3><p><strong>2AFC的一个缺点是它是”cognitively penetrable(可认知渗透的)“，参与者可以有意识地选择他们在完成任务时选择关注的相似性方面，这就向判断中引入了主观性。</strong></p><p>为了反应客观、有意义的内容，还需要收集JND的判断：只给出两张图像，先给出一个参考图像(reference image，2AFC中的x0)，然后再给出一张随机失真的图像，询问人图像是相同的还是不同的。</p><p>两张图像分别显示1s，间隔为250ms。收集了9.6K个样本。</p><h1 id="Deep-Feature-Spaces"><a href="#Deep-Feature-Spaces" class="headerlink" title="Deep Feature Spaces"></a>Deep Feature Spaces</h1><p>在不同网络的深度特征空间中评估特征的距离。</p><h2 id="Network-architectures"><a href="#Network-architectures" class="headerlink" title="Network architectures"></a>Network architectures</h2><ul><li><p>评估了Squeezenet、AlexNet、VGG网络。使用了VGG网络的5个conv层，还使用了更接近人类视觉皮层结构的较浅的AlexNet以及具有与AlexNet相似性能的轻量SqueezeNet架构。</p></li><li><p>评估了自监督的方法，包括解谜(puzzle-solving)、跨通道预测(cross-channel prediction)、视频学习(learning from video)和生成建模(generative modeling)。</p></li></ul><h2 id="Network-activations-to-distance"><a href="#Network-activations-to-distance" class="headerlink" title="Network activations to distance"></a>Network activations to distance</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LPIPS/3.png" title="Optional title"></p><p>上图左半部分说明了如何用网络计算<code>原图x</code>和<code>失真图像x0</code>之间的距离：<strong>从L层中提取特征堆栈，并在通道维度中进行单元归一化(unit-normalize in the channel dimension)。</strong></p><p>对于<code>l层</code>，将得到的结果记为<code>y^l,y^l0(Hl*wl*cl)</code>。利用向量<code>wl</code>缩放激活通道并计算<code>l2距离</code>，最后在空间上求平均值，在信道上求和。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LPIPS/4.png" title="Optional title"></p><blockquote><p>Note that using <code>wl = 1∀l</code> is equivalent to computing <code>cosine distance</code>.</p></blockquote><p>上图的右半部分用于映射<code>得分h</code>的小网络G使用了 2个32通道的FC-ReLU层、单通道FC层和sigmoid层。</p><p>相似性度量的损失函数为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LPIPS/5.png" title="Optional title"> </p><blockquote><p>来自Appendix部分的B. Model Training Details，正文中没有提到。</p></blockquote><h2 id="Training-on-our-data"><a href="#Training-on-our-data" class="headerlink" title="Training on our data"></a>Training on our data</h2><p><strong>考虑一些不同的训练方式来进行感知判断：lin，tune，scratch。</strong>三个变种统称为Learned Perceptual Image Patch Similarity (LPIPS) metric。</p><ul><li><p>lin：保持预训练网络权重不变，在顶部学习线性权重w，这构成了已有特征空间中一些参数的”感知校准(perceptual calibration)；</p></li><li><p>tune：从预先训练的分类模型初始化，并允许对网络的所有权重进行微调；</p></li><li><p>scratch：从随机高斯分布进行权重初始化，并完全根据前文判断对其进行训练。</p></li></ul><blockquote><p>这里的训练策略，lin没有查到；tune相关的词是fine-tune；scratch是有的，但具体所指意义不明。对于这三个训练策略，原文中也没有引用什么论文加以说明。</p></blockquote><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>部分实验结果：图4、5、6（11）；表5。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LPIPS/6.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LPIPS/7.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LPIPS/8.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LPIPS/9.png" title="Optional title"></p><p>以问答形式解释了实验结果，部分重点如下：</p><ol><li><p>low-level metrics和Classification networks的性能如何？<br>答：<code>图4a，红色同黑色的比较和其他颜色同黑色的比较；表5（Appendix中A. Quantitative Results）</code>。Classification networks的表现要明显优于low-level metrics。</p></li><li><p>网络是否必须是Classification的任务？<br>答：<code>图4a，绿色右1、2和蓝色左1</code>。网络不一定要在Classification任务上训练。BiGAN，Puzzle，Splitbrain等无监督和自监督模型特征和监督模型差距不大。</p></li><li><p>不同的感知任务之间是否存在相关性？<br>答：<code>图5</code>。没有说明相关的程度，只给了结果。</p></li><li><p>我们可以针对传统的和基于CNN的失真训练一个指标吗？（这个问题的问法和回答有点对不上，因该是问题叙述没叙述清楚，应该是<code>是否可以用第二部分收集到的数据训练出一个可以用来评估的感知指标，该指标的效果如何？</code>）<br>答：<code>图4a，右1-9，紫、粉、棕黄色</code>。通过实验证实了更高容量的网络VGG比低容量的SqueezeNet和AlexNet架构表现更好，也就证实了网络确实可以从感知判断中学习。</p></li><li><p>关于传统和基于CNN的扭曲的训练是否转移到了real-world场景中？（<code>迁移到真实算法中是否能提高性能？</code>）<br>答：<code>图4b，右1-9，紫、粉、棕黄色</code>。学习线性分类器（紫）可以提高所有网络的性能。从零开始训练一个网络（粉），AlexNet的性能略低，而VGG的性能略高于线性校准。<strong>总结：使用数据“校准”预先存在的表征的激活是实现性能小幅提升的安全方法（分别为1.1%、0.3%和1.5%）。</strong></p></li><li><p>deep metrics和low-level metrics在哪里不一致?（<code>扭曲的定性比较？</code>）<br>答：<code>图6；图11（Appendix中B. Model Training Details，图6的拓展，和图6性质一样）</code>。使用BiGAN进行对比实验，认为相关噪声模式比SSIM失真更小。</p></li></ol><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>文章：<a href="https://arxiv.org/abs/1801.03924">https://arxiv.org/abs/1801.03924</a></p><p>code：<a href="https://www.github.com/richzhang/PerceptualSimilarity">https://www.github.com/richzhang/PerceptualSimilarity</a></p><p>如何使用：<a href="https://blog.csdn.net/Magic_o/article/details/106770317">https://blog.csdn.net/Magic_o/article/details/106770317</a></p><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>LPIPS这个方法本身，从数据收集到网络训练，难度并不高，但非常流畅。对实验结果做了很详细的解释，这一点是非常好的，因为如果只是将提高的性能作为结果摆上去，说服力也就那样，但针对不同位置进行的解释，就给了这个方法极大的可用性和拓展性。</p><p>估计是受限于篇幅，并没有对每个算法之间的指标进行逻辑上的比较，算是这篇文章没有完成的内容，可以作为自己写论文过程中指标对比的一个论点去写。</p><p>使用过程中发现LPIPS计算图像距离，速度并不是特别快，想要将这个当作新的SR指标，还是需要考虑时间成本的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;The Unreasonable Effectiveness of Deep Features as a Perceptual Metr</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>SR Baseline Model code detail</title>
    <link href="http://example.com/2021/12/22/SR-Baseline-Model-code-detail/"/>
    <id>http://example.com/2021/12/22/SR-Baseline-Model-code-detail/</id>
    <published>2021-12-22T09:49:20.000Z</published>
    <updated>2022-03-11T15:09:54.445Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据集WSI"><a href="#数据集WSI" class="headerlink" title="数据集WSI"></a>数据集WSI</h1><table><thead><tr><th>info</th><th align="left">command</th></tr></thead><tbody><tr><td>home</td><td align="left">/home/jfy/project/data/WSI</td></tr><tr><td>10训2验2测_训</td><td align="left">/home/jfy/project/data/WSI/lv0_small_10_2_2/train</td></tr><tr><td>10训2验2测_验</td><td align="left">/home/jfy/project/data/WSI/lv0_small_10_2_2/val</td></tr><tr><td>10训2验2测_测</td><td align="left">/home/jfy/project/data/WSI/lv0_small_10_2_2/test</td></tr><tr><td>1000训100验100测_训</td><td align="left">/home/jfy/project/data/WSI/lv0_medium_1000_100_100/train</td></tr><tr><td>1000训100验100测_验</td><td align="left">/home/jfy/project/data/WSI/lv0_medium_1000_100_100/val</td></tr><tr><td>1000训100验100测_测</td><td align="left">/home/jfy/project/data/WSI/lv0_medium_1000_100_100/test</td></tr></tbody></table><h1 id="SRCNN"><a href="#SRCNN" class="headerlink" title="SRCNN"></a>SRCNN</h1><p>code：<a href="https://github.com/yjn870/SRCNN-pytorch">https://github.com/yjn870/SRCNN-pytorch</a></p><table><thead><tr><th>info</th><th align="left">command</th></tr></thead><tbody><tr><td>home</td><td align="left">/home/jfy/project/SRCNN-pytorch-master</td></tr><tr><td>prepare</td><td align="left">python prepare.py --images-dir <code>xxx</code> --output-path result/2x/lv0_0_9.h5 --patch-size 32 --stride 14 --scale 2</td></tr><tr><td>train</td><td align="left">python train.py --train-file result/2x/lv0_0_9.h5 --eval-file result/2x/lv0_11_12.h5 --outputs-dir result/2x/little --num-epochs 2 --batch-size 16 --seed 11</td></tr></tbody></table><p><code>更改成个人数据集</code>：用prepare.py分别生成测试集和训练集的h5文件，train的时候输入两个h5文件即可。</p><p>仅有PSNR指标，Y通道计算，utils.py中<code>def calc_psnr(img1, img2)</code>传的是tensor。</p><p><code>添加SSIM指标：utils.py添加函数def calc_ssim(img1, img2)</code>，代码来自IGNN，需要将tensor先转成array。</p><p><code>添加LPIPS指标：utils.py添加函数def calc_lpips(img1, img2)</code>，需要pip install lpips并在最上面import，调用需要统一将tensor转到device=cpu，并将Y通道重复三次。</p><p>上两周跑起来没问题，这周开始报h5的错误了，暂且放下。</p><h1 id="SRGAN"><a href="#SRGAN" class="headerlink" title="SRGAN"></a>SRGAN</h1><p>code：<a href="https://github.com/leftthomas/SRGAN">https://github.com/leftthomas/SRGAN</a></p><table><thead><tr><th>info</th><th align="left">command</th></tr></thead><tbody><tr><td>home</td><td align="left">/home/jfy/project/SRGAN-master</td></tr><tr><td>train</td><td align="left">python train.py –train_set_path ../data/WSI/lv0_medium_1000_100_100/train –val_set_path ../data/WSI/lv0_medium_1000_100_100/test –upscale_factor 2 –num_epochs 100 –train_bs 128</td></tr></tbody></table><h1 id="EDSR"><a href="#EDSR" class="headerlink" title="EDSR"></a>EDSR</h1><p>code：<a href="https://github.com/sanghyun-son/EDSR-PyTorch">https://github.com/sanghyun-son/EDSR-PyTorch</a></p><table><thead><tr><th>info</th><th align="left">command</th></tr></thead><tbody><tr><td>home</td><td align="left">/home/jfy/project/local/EDSR-PyTorch-master/src</td></tr><tr><td>train</td><td align="left">sh wsi3.sh</td></tr></tbody></table><p><code>更改成个人数据集</code>：仿照DIV2K，步骤：按照DIV2K的文件目录制作数据集的文件目录（尤其是图片命名）、修改option中参数（dir_data、data_train、data_test，其他的写到sh文件内）、仿照./EDSR-PyTorch-master/src/data/div2k.py写一个wsi.py。三个步骤缺一不可。</p><p>使用medium的测试结果：</p><table><thead><tr><th>参数设置</th><th align="left">结果</th></tr></thead><tbody><tr><td>lr=1e-3,bs=32</td><td align="left">loss曲线不收敛，psnr从38开始到40多</td></tr><tr><td>lr=1e-4,bs=16,n_resblocks=8,n_feats=32</td><td align="left">收敛，但300epoch的loss2.2左右</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;数据集WSI&quot;&gt;&lt;a href=&quot;#数据集WSI&quot; class=&quot;headerlink&quot; title=&quot;数据集WSI&quot;&gt;&lt;/a&gt;数据集WSI&lt;/h1&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;info&lt;/th&gt;
&lt;th align=&quot;left&quot;&gt;comma</summary>
      
    
    
    
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>leetcode刷题记录</title>
    <link href="http://example.com/2021/11/20/leetcode%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    <id>http://example.com/2021/11/20/leetcode%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/</id>
    <published>2021-11-20T06:11:06.000Z</published>
    <updated>2021-12-09T11:20:09.933Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="统计"><a href="#统计" class="headerlink" title="统计"></a>统计</h1><ul><li><p>hard：4、458</p></li><li><p>medium：2、3、5、6、11、12、15、17、56、148、165、189、215</p></li><li><p>easy：*1、7、9、20、21、26、27、53、66、70、88、*118/119、141、202、206、*231、283、*461、*2073</p></li><li><p>重难点：2、3、^4、5、15、20、21、53、70、141、148、206、215</p></li></ul><hr><h1 id="2-两数相加-LinkList"><a href="#2-两数相加-LinkList" class="headerlink" title="2. 两数相加 [LinkList]"></a>2. 两数相加 [LinkList]</h1><p><img src="/images/leetcode/2_1.png" title="Optional title"> </p><p><img src="/images/leetcode/2_2.png" title="Optional title"> </p><p>按顺序加，主要就是保持一个进位flag，以及如果最后有进位，需要再新设一个node，值为1。</p><h1 id="3-无重复字符的最长子串-array"><a href="#3-无重复字符的最长子串-array" class="headerlink" title="3. 无重复字符的最长子串 [array]"></a>3. 无重复字符的最长子串 [array]</h1><p><img src="/images/leetcode/3.png" title="Optional title"></p><p>设两个指针head和tail，分别从0和1开始，tail每次向下走一个，head走到在[head（包）,tail（不包）]区间内跟当前tail重复的位置，显然这个重复位置如果有的话那么就只有一个，如果无重复，那么head不动即可。每次tail走动一下后，将其区间长度的最大值保留即可。tail从头到尾走一遍就可以得到答案。</p><p>题解有提示到查看是否重复和以用到一些容器，比如python中的set和cpp中的map。上面解法时间太慢也是因为判断是否重复我没有用这类容器，但实际上也省下了空间，时间换空间属于是。</p><h1 id="4-寻找两个正序数组的中位数-array-num-技巧题"><a href="#4-寻找两个正序数组的中位数-array-num-技巧题" class="headerlink" title="4. 寻找两个正序数组的中位数 [array/num/技巧题]"></a>4. 寻找两个正序数组的中位数 [array/num/技巧题]</h1><p><img src="/images/leetcode/4.png" title="Optional title"> </p><p>把两个数组放到一个里面，然后返回中位数即可。放的时候都从头循环，谁小就放谁。其中一个放空之后，把另一个剩下的所有跟上前面，就能保证新的依旧是有序的。</p><p><code>这题作为hard是因为要求的时间为O(log(m+n))</code>，这样的话就需要二分查找来解决，说实话给的解析确实没看懂。</p><h1 id="5-最长回文子串-array-DP-技巧题"><a href="#5-最长回文子串-array-DP-技巧题" class="headerlink" title="5. 最长回文子串 [array/DP/技巧题]"></a>5. 最长回文子串 [array/DP/技巧题]</h1><p><img src="/images/leetcode/5_1.png" title="Optional title"></p><p>首先是好理解的方法，就是将这个<code>倒置串</code>，寻找本体和其倒置的 <code>最长公共</code> <code>回文</code> 子串。该做法的时间和空间都是0(n*n)。对于这两个操作分两部分走：</p><ul><li><p>最长公共子串就是设置一个len*len的数组arr，初始全部置0，从头循环，如果两个指针i、j指的字符相同，那么其状态就是他们的前面的最大子串长度+1，也就是arr[i][j]=arr[i-1][j-1]+1。这里需要注意的是当i和j是0的时候要单独判断。设置2个容器记录这个长度和末尾位置，每当当前的长度（arr[i][j]）大于记录的长度，那么就以当前的长度和i替换这两个记录容器。</p></li><li><p>在上述进行两个容器的记录之前需要判断这一段是否是回文串，具体做法有两种，一种就是把这一段整体取下来整体判断，另一种就是判断最末尾一位的下标。</p></li></ul><p>在这里，对最长公共子串的求法是DP，而对这个题目本身有DP的做法，这两者的状态转移是完全不一样的，要区分好。</p><p><img src="/images/leetcode/5_2.png" title="Optional title"></p><p><img src="/images/leetcode/5_3.png" title="Optional title"></p><p>其次，还是要了解本题的<code>扩展中心法</code>和<code>Manacher算法</code></p><h1 id="6-Z字形变换-array-num-技巧题"><a href="#6-Z字形变换-array-num-技巧题" class="headerlink" title="6. Z字形变换 [array/num/技巧题]"></a>6. Z字形变换 [array/num/技巧题]</h1><p><img src="/images/leetcode/6.png" title="Optional title"> </p><p>没啥好说的，几行就设置几个str，按顺序往里存，最后从上到下排成一行。也可以找规律，每一行都是从某一位开始，间隔多少个数取（这个数肯定是不一样的）。</p><h1 id="7-整数反转-array-num"><a href="#7-整数反转-array-num" class="headerlink" title="7. 整数反转 [array/num]"></a>7. 整数反转 [array/num]</h1><p>反转有符号整数，超过[−2<strong>31,  2</strong>31 − 1]就返回0。</p><p><img src="/images/leetcode/7.png" title="Optional title"> </p><p>常规方法设置一个n，每次对x取10的余数，再加上10倍的n（相当于前一位的进位）。</p><p>使用python的切片可以反转str，将int保留符号，转成str，然后再添上符号。</p><p>一定要注意反转后的数字有可能超过范围。</p><h1 id="9-回文数-array"><a href="#9-回文数-array" class="headerlink" title="9. 回文数 [array]"></a>9. 回文数 [array]</h1><p>判断一个数从前向后读和从后向前读是不是一样的，是就称为回文数。</p><p>常规思路就是和<code>7. 整数反转</code>，一样的操作，把这个整数反转然后看他们是否相等即可。偷鸡思路还是使用python切片反转str。</p><p>注意，负数一定不会是回文数，因为多个符号。</p><h1 id="11-Container-With-Most-Water-array"><a href="#11-Container-With-Most-Water-array" class="headerlink" title="11. Container With Most Water [array]"></a>11. Container With Most Water [array]</h1><p><img src="/images/leetcode/11.png" title="Optional title"></p><p>直接两层循环会TLE，评论做法是<code>从头和尾开始相对着走</code>，实际上就是考虑了坐标轴长度，<code>保留最大的h</code>然后逐渐缩小坐标轴长度，时间O(1)。</p><h1 id="12-整数转罗马数字-num-技巧题"><a href="#12-整数转罗马数字-num-技巧题" class="headerlink" title="12. 整数转罗马数字 [num/技巧题]"></a>12. 整数转罗马数字 [num/技巧题]</h1><p><img src="/images/leetcode/12.png" title="Optional title"></p><p>写好个位数，然后替换即可，替换规则都是相通的。</p><p>题解里有提到用贪心，就是写好1000、<code>900</code>、500、<code>400</code>、100、…，以9和4的开头是需要用减法构造的，单独写出来，剩下的所有内容都可以用加法构造，所以就每次用num减去最大的，说是贪心，但这是贪心的意义么，后面写到这类题再说。</p><h1 id="15-三数之和-num-技巧题"><a href="#15-三数之和-num-技巧题" class="headerlink" title="15. 三数之和 [num/技巧题]"></a>15. 三数之和 [num/技巧题]</h1><p><img src="/images/leetcode/15.png" title="Optional title"></p><p>面试重点。</p><p>首先，思路方面，<code>三重循环肯定是TLE</code>，那么就力争变成二重循环。最外面的一层循环肯定不能省略，那么就要对里面的两层动心思。做法就是先排序，然后对内层的[i+1,n]区间的循环，设置<code>双指针</code>，j从i+1向后走，k从n向前走。能这么的核心就是因为排过序，前后操作是对应的。判断的时候，如果是0，那么记录结果，如果不是，就看是比0大还是比0小，结果比0大证明后面加的两个数过大了，那么k就往小变；同理，结果小了证明所加的数不够大，那么j就往大变。</p><p>其次，操作方面，比较重要的就是<code>去重</code>，对于ijk共同来说，就是要跳过相同的数，因为相同的数判断过会出现相同的结果，这里同时做到了去重和优化循环；对i来说，单独有一个优化，就是排了序之后，大于0的i，j和k都会大于，三个大于0的数相加必大于0。</p><p>最后整体做下来，排序用的是O(logN)，循环遍历用的是<code>O(N*N)</code>。</p><p>以及并不能为了去重而对nums上来就做去重，写题的时候就一直疑惑，写总结的时候突然发现确实不能这么写，不解释，懂得都懂。</p><h1 id="17-电话号码的字母组合-DP"><a href="#17-电话号码的字母组合-DP" class="headerlink" title="17. 电话号码的字母组合 [DP]"></a>17. 电话号码的字母组合 [DP]</h1><p><img src="/images/leetcode/17.png" title="Optional title"></p><p>并不算难，实际上是最简单的那种DP：<code>把之前的内容，每一个都重新加上新加的这个就行</code>。就是要记得对每个老内容，添加3或4个新内容之后，把老内容删了。</p><p>上述写法模拟了一个队列的操作，每次都操作队头的那个就行，count则控制每新添加一个数之前一共有多少老内容。</p><h1 id="20-有效的括号-技巧题"><a href="#20-有效的括号-技巧题" class="headerlink" title="20. 有效的括号 [技巧题]"></a>20. 有效的括号 [技巧题]</h1><p><img src="/images/leetcode/20.png" title="Optional title"> </p><p>模拟<code>栈（先进先出）</code>的存储方式匹配括号序列。</p><h1 id="21-合并两个有序链表-LinkList"><a href="#21-合并两个有序链表-LinkList" class="headerlink" title="21. 合并两个有序链表 [LinkList]"></a>21. 合并两个有序链表 [LinkList]</h1><p><img src="/images/leetcode/21.png" title="Optional title"> </p><p>新设一个头和操作这个头的操作指针，两个链表从头遍历有两个操作指针，当前两个指针哪个指的值小，就把这个node接上，该node后面断掉，指针向前走一个。最后一定记得把两个表中剩下的部分接上。</p><h1 id="26-Remove-Duplicates-from-Sorted-Array-array"><a href="#26-Remove-Duplicates-from-Sorted-Array-array" class="headerlink" title="26. Remove Duplicates from Sorted Array [array]"></a>26. Remove Duplicates from Sorted Array [array]</h1><p>给定按非降序排序的整数数组nums，请删除重复项，以便每个唯一元素只显示一次。元素的相对顺序应保持不变。</p><p><img src="/images/leetcode/26.png" title="Optional title"></p><p>这题返回值只有k，但检测结果还是操作num变成前k个不重复的值。</p><h1 id="27-Remove-Element-array"><a href="#27-Remove-Element-array" class="headerlink" title="27. Remove Element [array]"></a>27. Remove Element [array]</h1><p>将所有非val的数挪到list最前面，不论顺序，返回非val的个数。</p><p><img src="/images/leetcode/27.png" title="Optional title"> </p><p>与<code>283. Move Zeroes</code>同一个原理，就是把0换成是val然后做一个反向问题，代码都是一个逻辑，也是需要注意list长度是0或者list内没有要操作的数。</p><p>细细看了下，与<code>26. Remove Duplicates from Sorted Array</code>应该也是同理，在26的<code>去重</code>中，无非就是把k=0，k=val换成了动态的k，就是当前的重复的那个值。当然，能这么想只存在于<code>26是有序</code>的，无序的话就不是这个道理了。</p><p>时间O(1)，循环指针每次变动到新的不重复的值上，然后交换到当前k的位置，两件事情分开做就行。</p><h1 id="53-最大子数组和-num-DP"><a href="#53-最大子数组和-num-DP" class="headerlink" title="53. 最大子数组和 [num/DP]"></a>53. 最大子数组和 [num/DP]</h1><p>求在一个整数数组内，和最大的子序列的这个和。</p><p><img src="/images/leetcode/53_1.png" title="Optional title"></p><p>屏蔽掉的是暴力求解，肯定TLE（亏我还想了半天，用矩阵存储了一部分结果，把每个子列求和的N省略了，以为从O(N^3)变成O(N^2)，TLE之后再一寻思，nmd用切片和sum函数求和好像也是线性，人家本来就是O(N^2)，我优化了个寂寞，我是个逆天）。</p><p>上面代码确实没体现出DP，好像就是用了常规的规律，但确实是DP的思想。</p><p><img src="/images/leetcode/53_2.png" title="Optional title"></p><p>以及，这题是easy就多少有点说不过去了。</p><h1 id="56-Merge-Intervals-array"><a href="#56-Merge-Intervals-array" class="headerlink" title="56. Merge Intervals [array]"></a>56. Merge Intervals [array]</h1><p>融合所给区间，区间相交取交集。</p><p>自己想的方法：先求从第几个位置到第几个位置需要融合，然后融合了放进新的result，做得很复杂，要考虑开头结尾长度啥的，搞了半天还写的不对。</p><p><img src="/images/leetcode/56_1.png" title="Optional title"> </p><p>看评论区，实际上这题的思路非常简单，就是从头遍历，设置一个指针i，从0开始，看i和i+1，左区间是i[0]，右区间是max(i[1],i+1[1])。<code>如果有融合，那么指针不动，没有做融合操作，指针再向前走</code>。</p><p><img src="/images/leetcode/56_2.png" title="Optional title"> </p><h1 id="66-Plus-One-array"><a href="#66-Plus-One-array" class="headerlink" title="66. Plus One [array]"></a>66. Plus One [array]</h1><p><img src="/images/leetcode/66.png" title="Optional title"></p><p>设置一个进位flag。时间O(1)循环。</p><p>注意最后一位如果进位的话，要在第一个位置多加一个1.</p><h1 id="70-Climbing-Stairs-DP-array"><a href="#70-Climbing-Stairs-DP-array" class="headerlink" title="70. Climbing Stairs [DP/array]"></a>70. Climbing Stairs [DP/array]</h1><p><img src="/images/leetcode/70_1.png" title="Optional title"></p><p>dp问题，本质上是fib，重点。</p><p>最高赞解释：</p><p><img src="/images/leetcode/70_2.png" title="Optional title"></p><p><code>递归fib肯定会TLE</code>，两种做法，一种是设置n个空间，一种是就用两个空间做连续数组存放。</p><p>这题也能看作是DFS，左子树是走一步，右子树是走两步，结果就是这棵树的子节点数量。</p><h1 id="88-Merge-Sorted-Array-array"><a href="#88-Merge-Sorted-Array-array" class="headerlink" title="88. Merge Sorted Array [array]"></a>88. Merge Sorted Array [array]</h1><p>两个升序数组融合，要求在nums1上修改。</p><p><img src="/images/leetcode/88_1.png" title="Optional title"></p><p><img src="/images/leetcode/88_2.png" title="Optional title"></p><p><img src="/images/leetcode/88_3.png" title="Optional title"></p><p>这题有几个坑，首先如果用python的话，直接使用<code>nums1=</code>这种赋值方法，会<code>改变内存空间</code>，导致最后的结果系统不认；其次，需要考虑m和n是0的情况（图2），在考虑这个情况的时候同样要考虑前者。</p><p>python可以有两个做法，偷懒就是用切片把两个list合起来，然后sort()（图1）；其次就是常规做法，在时间O(m+n)下，<code>倒着遍历两个list</code>，谁比较大就放到nums1的最后，收尾工作就是把最小的那些挨着放到nums1的最前面即可（图3）。</p><p>这题逆大天，一定一定要注意python的赋值，这个以前从来没考虑过。</p><h1 id="141-环形链表-LinkList"><a href="#141-环形链表-LinkList" class="headerlink" title="141. 环形链表 [LinkList]"></a>141. 环形链表 [LinkList]</h1><p>检查是不是环形链表。</p><p><img src="/images/leetcode/141.png" title="Optional title"></p><p>必须要遵从链表思想的话是做法是<code>快慢指针</code>：两个指针，一个每次前进1步，一个每次前进2步，如果有环的话它们必定相遇。此时内存用的是O(1)。</p><p>常规做法就是每过一个内存地址，就存储，如果得到一个重复的，证明有环，指针遇到末尾了证明无环。此时用的内存是O(N)。检查重复自然需要用到一些数据结构比如hash和set等，实际上还是第一个方法简单。</p><h1 id="148-Sort-List-Sort-LinkList"><a href="#148-Sort-List-Sort-LinkList" class="headerlink" title="148. Sort List [Sort/LinkList]"></a>148. Sort List [Sort/LinkList]</h1><p>链表排序。</p><p><img src="/images/leetcode/148_1.png" title="Optional title"> </p><p><img src="/images/leetcode/148_2.png" title="Optional title"> </p><p><img src="/images/leetcode/148_3.png" title="Optional title"> </p><p>三种方法：</p><p>第一种是只对每个node的value进行冒泡排序，不动next，这样做显然会TLE。</p><p>第二种是偷鸡做法，就是把所有value取到一个list里，直接对list进行排序，这时的排序就是线性表的排序而不是链表的排序，最后把排好序的内容从链表头开始依次填进去，这样做面试估计不给过。</p><p>第三种是正常做法，就是<code>归并排序</code>。</p><p>对于链表而言最好的排序方法就是归并。要牢牢记住第三种方法的流程：<code>共三个函数</code>，<code>主函数</code>返回head的归并排序调用；<code>归并排序函数</code>就是将当前表头一分为二，并且递归的归并排左和右，最后返回的左右两部分的merge函数调用；<code>merge函数</code>就是设置一个新的暂时链表头和两个指针，把两个子链表，用操作指针从头遍历融合，返回的是暂时链表表头的指针next。</p><p><strong>三种O(logn)的排序算法：快排、堆排序、归并排序。</strong></p><h1 id="165-比较版本号"><a href="#165-比较版本号" class="headerlink" title="165. 比较版本号"></a>165. 比较版本号</h1><p>题目比较拗口，但是不难，中等就这？</p><p><img src="/images/leetcode/165.png" title="Optional title"></p><p>主要用到python的<code>split</code>和<code>int强制类型转换时会忽略前导0</code>。</p><h1 id="189-Rotate-Array-array"><a href="#189-Rotate-Array-array" class="headerlink" title="189. Rotate Array [array]"></a>189. Rotate Array [array]</h1><p>根据题意，就是将list中的后k个挪到前面来，就是尾出头进k次，每次一个人，之后的队列顺序。</p><p><img src="/images/leetcode/189.png" title="Optional title"></p><p>使用<code>切片</code>做即可。注意循环次数超过了队列长度，意味着要回归原位一次，那么<code>先用k对list长度取余数</code>即可。</p><h1 id="206-反转链表-LinkList"><a href="#206-反转链表-LinkList" class="headerlink" title="206. 反转链表 [LinkList]"></a>206. 反转链表 [LinkList]</h1><p><img src="/images/leetcode/206.png" title="Optional title"></p><p>在我看来还是递归比较舒服一点：从头开始循环，两个指针分别是头p和头的下一个p_next，如果p_next的下一个，也就是p_next-&gt;next不为空，就递归传入p_next和p_next-&gt;next，p_next-&gt;next为空就证明循环到了最后俩，此时p_next-&gt;next指向p，也就是反转操作，p的下一个也就是p-&gt;next指空，也就是改递归边界。</p><p>不递归纯循环的话就从头设立两个指针p和p_next，初始化为head和head-&gt;next，然后翻转，反转之后根据p_next的定位，挪p，然后p_next向下指。</p><p>两种方法从<code>操作顺序</code>上来说，<code>递归是从尾部开始反转，迭代就是从头部开始循环</code>。</p><p>非常规方法就是设其他空间去记录一部分信息，比如将值信息按顺序取下来，反着再装到这个表里；再或者用栈记录从头到尾的位置信息，然后每次反转栈头的两个。</p><h1 id="215-数组中的第K个最大元素-quick-sort"><a href="#215-数组中的第K个最大元素-quick-sort" class="headerlink" title="215. 数组中的第K个最大元素 [quick sort]"></a>215. 数组中的第K个最大元素 [quick sort]</h1><p><img src="/images/leetcode/215.png" title="Optional title"></p><p>主要是<code>快排</code>的代码，背下来。</p><h1 id="283-Move-Zeroes-array"><a href="#283-Move-Zeroes-array" class="headerlink" title="283. Move Zeroes [array]"></a>283. Move Zeroes [array]</h1><p>将数组内的所有0挪到最后。</p><p><img src="/images/leetcode/283.png" title="Optional title"></p><p>从头开始遍历，设置一个flag=0，非0的就放到flag位置，flag++，也统计了非0的个数，最后把从第flag位置到最后设置成0即可，注意list长度是0或者list内没有要操作的数这两个问题。</p><h1 id="458-可怜的小猪-技巧题-num"><a href="#458-可怜的小猪-技巧题-num" class="headerlink" title="458. 可怜的小猪 [技巧题/num]"></a>458. 可怜的小猪 [技巧题/num]</h1><p><img src="/images/leetcode/458.png" title="Optional title"> </p><p>比传统的那个用2进制做的题多了一个测试次数。使用测试次数可以扩展维度，即从<code>2进制（测试次数=1，2=1+1）变为测试次数+1进制</code>。</p><p>可以以一个小时时长为例，每次15min，那么对于每个猪，就能测试4次。</p><p>当只有猪1时，测试4次就意味着<code>猪1能断言一共5桶水的情况</code>：如果前4次没死，有毒的就是第5桶，如果前4次死了的话那就是死了那次的那一桶。</p><p>两只猪的话，<code>把25个桶排列成5*5，猪1每次喝列5桶，猪2每次喝行5桶，交叉死的就是有毒的</code>；</p><p>三只猪的话，<code>把125个桶排列成5*5*5</code>，猪1每次喝x面的25桶，猪2每次喝y面的25桶，猪3每次喝z面的25桶；</p><p>四只猪的话，<code>排5组的5*5*5</code>，<code>猪1每次喝这5组的x面共5*25=125桶</code>，猪2、3同理，<code>猪4每次喝一组的5*5*5共125桶</code>；</p><p>五只猪的话，<code>排列5*5=25组的5*5*5</code>，猪4每次喝一列的5<em>5</em>5，猪5每次喝1行的5<em>5</em>5…</p><p>注意上述过程中的乘法，有助于理解，因为我们只有3维，猪喝水喝到4维之后就是3维又3维的叠加。</p><hr><h1 id="代码技巧"><a href="#代码技巧" class="headerlink" title="代码技巧"></a>代码技巧</h1><blockquote><p>都是python和cpp</p></blockquote><ul><li>python可以函数里面写函数；</li><li>python <code>max和min、count</code>能用，list的许多操作可用，主要是append和extend；</li><li>cpp中vector常用的函数：size()，可按数组循环；insert(locat,num,value)指定位置插入指定个数的指定元素；</li><li>python的list切片，记住[:a]和[a:]的区别；</li><li>一些要求在原list上修改，不返回值的题，python操作数据赋值一定要用切片，不然会改变内存位置，导致过不去；</li><li>【56】<code>python如果想写变循环上限的循环，一定不要用for，不管用</code>：就是说用i当循环变量写for，如果在for内部修改了这个i，对于循环来说i的值是没有变动的，这一点非常重要，和c语言的循环是不一样的，如果想写，可采用外设循环i，然后用while当循环，在while内部修改i的值，这样才行；</li><li>python <code>str用切片反转：a -&gt; a[::-1]</code>（两个冒号）；</li><li><code>负数取余数和正数不一样</code>；</li><li>cpp链表，新设节点最好用<code>ListNode *temp = new ListNode(val,next)</code>，两个参数建议为-1和nullptr，用的时候<code>直接用 -&gt; 取元素</code>即可；</li><li><code>python str replace</code>的话，一定记得<code>用变量去接一下结果</code>，直接调用不接的话，原本的str是不会变的，会导致误判，觉得replace没生效或者写错了啥的；</li><li>python 用0初始化m<em>n二维数组：<code>arr=[[0]*m for _ in range(n)]</code>；不能用arr=[[0]\</em>m]*n，这样做修改某一行的某个位置会导致所有行的该位置做同样修改，意思就是把第一行的内存存了n遍；</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;统计&quot;&gt;&lt;a href=&quot;#统计&quot; class=&quot;headerlink&quot; title=&quot;统计&quot;&gt;&lt;/a&gt;统计&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;hard：</summary>
      
    
    
    
    
    <category term="leetcode" scheme="http://example.com/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：RealVSR</title>
    <link href="http://example.com/2021/11/02/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ARealVSR/"/>
    <id>http://example.com/2021/11/02/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ARealVSR/</id>
    <published>2021-11-02T07:59:53.000Z</published>
    <updated>2021-12-21T08:37:15.232Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Real-world Video Super-resolution: A Benchmark Dataset and A Decomposition based Learning Scheme</p></blockquote><blockquote><p>ICCV 2021</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>针对的问题是视频超分在<strong>下采样、退化</strong>时候产生的误差问题，认为使用简单的退化方法所得到的模型并不能很好的在实际应用中发挥作用。以及所使用的VSR数据集大多都是合成的（与SISR是一样的痛点）</p><p>为了解决上述问题，本文首先使用iPhone 11 Pro Max的多摄像头系统捕获成对的LR-HR视频序列，构建了一个真实世界的视频超分辨率（RealVSR）数据集。</p><p>接下来分析指出，由于LR-HR视频对由两个单独的摄像头捕获，因此它们之间不可避免地存在一定的错位和亮度/颜色差异。为了更稳健地训练VSR模型并从LR输入中恢复更多细节，将LR-HR视频转换为YCbCr空间，并将亮度通道分解为拉普拉斯金字塔，然后对不同的分量应用不同的损失函数。</p><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><blockquote><p>因为做的是比较有意义的新数据集，所以这部分有必要介绍一下。</p></blockquote><ul><li><p>常用的VSR数据集：Vimeo-90k[31]、REDS[23]、一些私有数据集[25]。</p></li><li><p>真实世界的SISR数据集：[6][33][4][30]。</p></li><li><p>在这篇文章之前，没有公开的真实世界VSR数据集。</p></li></ul><blockquote><p>[31] Video enhancement with task-oriented flow<br>[23] Ntire 2019 challenge on video deblurring and super-resolution: Dataset and study<br>[25] Detail-revealing deep video super-resolution</p></blockquote><blockquote><p>[6] Camera lens super-resolution<br>[33] Zoom to learn, learn to zoom<br>[4] Toward real-world single image super-resolution: A new benchmark and a new mode<br>[30] Component divide-and-conquer for real-world image super-resolution<br>上述文章中，33是之前读过的一篇；30是本文章的SISR的工作，ICCV 2019，后面可能会读。</p></blockquote><h1 id="The-Real-world-VSR-Dataset"><a href="#The-Real-world-VSR-Dataset" class="headerlink" title="The Real-world VSR Dataset"></a>The Real-world VSR Dataset</h1><p>使用<code>iPhoen 11 Pro Max + DoubleTake</code>制作该数据集。其中，ip11pm有3个摄像头，分别是13mm超宽摄像头、26mm宽摄像头和52mm长焦摄像头，每个摄像头都可以拍1200万像素的照片。DoubleTake可以通过具有不同焦距的两个摄像机以不同的比例捕获两个近似同步的视频。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-RealVSR/1.png" title="Optional title"></p><p>所构建数据集的一些主要信息：</p><ul><li><p>考虑到13mm镜头（也就是广角）有较为严重的失真，<strong>采用26mm镜头和52mm镜头构建数据集</strong>；</p></li><li><p><strong>使用52mm镜头拍摄的视频作为GT，也就是HR；使用26mm镜头拍摄的视频作为LR，从而生成×2的数据集，文中提到2倍就是VSR的主要需求</strong>；</p></li><li><p>前期制作总共<code>做了700个</code>视频对，每对是两个<code>帧速率30fps</code>和<code>分辨率1080P</code>的<code>近似同步</code>视频成，场景包括室内外、白天黑夜、静止场景和运动场景（包括摄像机运动和对象运动）等，文中提到具有丰富纹理的场景是首选场景；</p></li><li><p>数据收集后进行后处理，首先筛掉约200个质量较差的视频，例如严重模糊、噪声、过度曝光或曝光不足、严重对齐错误等，最终<code>保留500个</code>序列对。</p></li><li><p>后处理第二步是对齐，对齐算法来自[4]，考虑到相邻帧之间可能存在一些小的配准漂移，扩展了[4]中的配准算法，使用五个相邻帧作为输入来计算中心帧的配准矩阵。</p></li><li><p>后处理第三步是切割，对齐后在<code>1024×512</code>大小的中心区域裁剪对齐每一对序列，并将所有序列切割为<code>50帧</code>长度。</p></li></ul><p>综上，最终的数据集结果为<code>多场景/500个序列对/每个序列对有HR和LR2个序列/每个序列50帧/帧大小1024×512</code>。</p><blockquote><p>[4] Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. Toward real-world single image super-resolution: A new benchmark and a new model. In Proceedings of the IEEE International Conference on Computer Vision, pages 3086–3095, 2019. 2, 3, 4</p></blockquote><p>App Store内的DoubleTake软件页面：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-RealVSR/2.png" title="Optional title"></p><h1 id="VSR-Model-Learning"><a href="#VSR-Model-Learning" class="headerlink" title="VSR Model Learning"></a>VSR Model Learning</h1><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>文章：<a href="https://www4.comp.polyu.edu.hk/~cslzhang/paper/ICCV21_RealVSR.pdf">https://www4.comp.polyu.edu.hk/~cslzhang/paper/ICCV21_RealVSR.pdf</a></p></li><li><p>code：<a href="https://github.com/IanYeung/RealVSR">https://github.com/IanYeung/RealVSR</a>.</p></li></ul><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Real-world Video Super-resolution: A Benchmark Dataset and A Decompo</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：LIIF</title>
    <link href="http://example.com/2021/09/01/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ALIIF/"/>
    <id>http://example.com/2021/09/01/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ALIIF/</id>
    <published>2021-09-01T11:20:43.000Z</published>
    <updated>2021-09-10T13:53:04.727Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Learning Continuous Image Representation with Local Implicit Image Function</p></blockquote><blockquote><p>cvpr 2021</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>现实世界的图像是连续的，但计算机和采样设备得到的图像是非连续的。将现实世界的图像存储到计算机中，所使用的方法是基于像素（pixel）的2D矩阵。</p><p>文章受到了<strong>隐式神经表征（implicit neural representation）</strong>的启发，希望将图像表示为连续形式，提出了局部隐式图像函数（LIIF），用于以连续方式表示自然图像和复杂图像。</p><blockquote><p>Inspired by the recent progress in 3D reconstruc- tion with implicit neural representation, we propose Local Implicit Image Function (LIIF), which takes an image co- ordinate and the 2D deep features around the coordinate as inputs, predicts the RGB value at a given coordinate as an output. Since the coordinates are continuous, LIIF can be presented in arbitrary resolution. To generate the contin- uous representation for images, we train an encoder with LIIF representation via a self-supervised task with super- resolution. The learned continuous representation can be presented in arbitrary resolution even extrapolate to ×30 higher resolution, where the training tasks are not provided.</p></blockquote><h1 id="Implicit-Neural-Representation"><a href="#Implicit-Neural-Representation" class="headerlink" title="Implicit Neural Representation"></a>Implicit Neural Representation</h1><h2 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h2><p>来自论文SIREN。</p><blockquote><p>Implicit Neural Representations with Periodic Activation Functions<br><a href="https://github.com/vsitzmann/siren">https://github.com/vsitzmann/siren</a><br><a href="https://vsitzmann.github.io/siren/">https://vsitzmann.github.io/siren/</a></p></blockquote><p>作者本人对该方法的论文做了综述。</p><blockquote><p><a href="https://github.com/vsitzmann/awesome-implicit-representations">https://github.com/vsitzmann/awesome-implicit-representations</a></p></blockquote><p>比较著名的有NeRF等。</p><h2 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h2><p>以图像为例，其最常见的表示方式为二维空间上的离散像素点。</p><p>但是，在真实世界中，我们观察到的世界可以认为是连续的，或者近似连续。于是，<strong>可以考虑使用一个连续函数来表示图像的真实状态（本篇的思路）</strong>。</p><p>然而我们无从得知这个连续函数的准确形式，因此有人提出<strong>用神经网络来逼近这个连续函数</strong>，这种表示方法被称为“隐式神经表示“ （Implicit Neural Representation，INR）。</p><p>对于图像，INR函数将二维坐标映射到rgb值。对于视频，INR函数将时刻t以及图像二维坐标xy映射到rgb值。对于一个三维形状，INR函数将三维坐标xyz映射到0或1，表示空间中的某一位置处于物体内部还是外部。当然还有其他形式，如NERF将xyz映射到rgb和sigma。<strong>总而言之，这个函数就是将坐标映射到目标值。一旦该函数确定，那么一个图像/视频/体素就确定了。</strong></p><p><strong>INR是一个连续的函数，函数（网络）的复杂程度和信号的复杂程度成正比，但是和信号的分辨率无关。比如一个16<em>16的图像，和一个32</em>32的图像，如果内容一样，那么INR就会一样。</strong></p><blockquote><p>Implicit Neural Representation 隐式神经表示 - 知乎专栏<br><a href="https://zhuanlan.zhihu.com/p/372338398">https://zhuanlan.zhihu.com/p/372338398</a></p></blockquote><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="Local-Implicit-Image-Function"><a href="#Local-Implicit-Image-Function" class="headerlink" title="Local Implicit Image Function"></a>Local Implicit Image Function</h2><blockquote><p>局部隐式图像函数</p></blockquote><p>在LIIF表示中，设每个连续图像<code>Ii</code>表示为2D特征映射<code>M(I) ∈ R_H×W×D</code>。解码函数<code>fθ</code>（以θ为参数）由所有图像共享，其形式如下：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/1.png" title="Optional title"></p><p>其中，<code>z</code>是一个向量，<code>x</code>是连续图像域中的被查询的二维坐标，<code>s</code>是预测信号，即RGB值。</p><p>假设<code>M</code>（latent codes，潜在代码）均匀分布在连续图像域（图2中的蓝色圆圈）的2D空间中，根据<code>f</code>，对任意位置<code>xq</code>，其重构的RGB值为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/2.png" title="Optional title"></p><p>其中，<code>z*</code>为离<code>xq</code>最近（欧几里得距离）的特征向量（<code>M</code>中的一部分），<code>v*</code>为<code>z*</code>对应的坐标。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/3.png" title="Optional title"></p><blockquote><p><code>z*</code>表示特征向量，它可以看作代表了一整块的像素；<code>角标00、01、10、11</code>分别是指离<code>xq</code>最近的左上、右上、左下、右下四个<code>z*</code>。所有图像共享上述解码函数<code>fθ</code>。<br>以图2为例，<code>z*11</code>是距离<code>xq</code>最近的（欧氏距离）潜码（xq在z*11的块内），<code>v*</code>就是潜码<code>z*11</code>的二维坐标。</p></blockquote><h2 id="Feature-unfolding"><a href="#Feature-unfolding" class="headerlink" title="Feature unfolding"></a>Feature unfolding</h2><blockquote><p>特征展开</p></blockquote><p>为了丰富潜码<code>M</code>，对<code>M</code>进行特征展开，称为得到<code>M^</code>：<code>M^</code>中的潜码是<code>M</code>中3×3个相邻潜码的<code>级联（Concat）</code>，形式为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/4.png" title="Optional title"></p><p>其中，<code>级联</code>指的是一组向量的连接，<code>M</code>在其边界外用零向量填充。</p><p>后面为了简洁，直接使用<code>M</code>表示这一步操作。</p><h2 id="Local-ensemble"><a href="#Local-ensemble" class="headerlink" title="Local ensemble"></a>Local ensemble</h2><blockquote><p>局部集合</p></blockquote><p><strong>式(2)是不连续预测</strong>：<code>xq</code>处的信号预测是通过查询最近的潜码<code>z*</code>来完成的，在<code>M</code>中，当<code>xq</code>在2D域中移动时，不同<code>z*</code>的边界是不连续的，会发生突变。<strong>这会导致坐标上无限接近的点，所选择的<code>z*</code>可能非常不同。</strong></p><p>例如，<code>xq</code>穿过图2中的边界虚线（或者是极其接近边界位置），在这些坐标周围选择<code>z*</code>，两个无限接近坐标的信号将由不同的<code>z*</code>进行预测。只要学习到的函数<code>fθ</code>不是完美的，这些边界就会出现不连续的图案。</p><p>为了解决这个问题，扩展式(2)为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/5.png" title="Optional title"></p><p>其中，<code>St</code>是指<code>xq</code>与<code>z*u</code>所围成的矩形的面积，<code>S</code>为四块矩形面积之和。</p><blockquote><p>记u为t的对角特征，即t=0-&gt;u=1。</p></blockquote><blockquote><p>这里采用面积之比作为权重，是为了维持在四个特征向量之间任何一点的总权重是相等的，如果采用距离之比则不能达到这一点。</p></blockquote><h2 id="Cell-decoding"><a href="#Cell-decoding" class="headerlink" title="Cell decoding"></a>Cell decoding</h2><blockquote><p>单元解码</p></blockquote><p>根据上述步骤，则可以在任意分辨率下使用LIIF对图像进行表示。</p><p>对于给定的分辨率，最直接的方式就是根据像素点中心坐标求得对应的RGB值。但这样的方式是独立于size的，也就是说像素点包围的位置中的其他信息都丢失了。</p><p>为此，采用Cell decoding的策略，表示为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/6.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/7.png" title="Optional title"></p><p>其中,<code>c=[ch，cw]</code>包含两个指定查询像素高度和宽度的值，<code>[x，c]</code>表示x和c的串联(concatenation)。</p><p><code>fcell（z，[x，c]）</code>的含义可以解释为：如果用<code>形状c</code>渲染以坐标x为中心的像素，RGB值应该是什么。在给定分辨率下呈现连续表示时，拥有额外的输入c是有益的。</p><blockquote><p>逻辑上，当c -&gt; 0时，<code>f(z，x) = fcell(z，[x，c])</code>，即：连续图像可以被视为具有无限小像素的图像。</p><blockquote><p>The meaning of fcell(z, [x, c]) can be interpreted as: what the RGB value should be, if we ren- der a pixel centered at coordinate x with shape c. As we will show in the experiments, having an extra input c can be beneficial when presenting the continuous representation in a given resolution.</p></blockquote></blockquote><h2 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/8.png" title="Optional title"></p><p>对于给定的一张训练图片，以随机的scale下采样作为input。对应的ground-truth表示为<code>xhr</code>、<code>shr</code>，其中，<code>xhr</code>是中心坐标，<code>shr</code>是对应的RGB值。</p><p><code>Eϕ</code>将input映射为二维特征图作为LIIF表示，使用<code>xhr</code>进行query，<code>fθ</code>会预测对应RGB值<code>Spred</code>，与<code>shr</code>计算loss。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><ul><li><p><code>Eϕ</code>是EDSR或RDN去掉upsampler部分。</p></li><li><p>选用的loss是L1 loss。</p></li><li><p>训练：训练集：DIV2K，1000幅中的800幅；下采样：比例为×2、×3、×4的低分辨率对应图像，由Matlab中的imresize函数生成，默认设置为双三次插值；训练细节：初始学习率为1*1^-4的Adam优化器（每200个阶段衰减0.5倍）、训练时间为1000个epoch、bs=16、MetaSR的实验设置与LIIF相同，只是将LIIF表示替换为其元解码器。</p></li><li><p>测试：DIV2K、Set5、Set14、B100、Urban100。</p></li><li><p>平台：Pytorch。</p></li></ul><h2 id="定量实验"><a href="#定量实验" class="headerlink" title="定量实验"></a>定量实验</h2><p>训练时在1倍到4倍之间均匀采样，在测试时对6倍到30倍都进行了验证。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/9.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/10.png" title="Optional title"></p><blockquote><p>由于针对如此大倍数的SR方法很少，这里实际上的SOTA就只有MetaSR。</p></blockquote><h2 id="定量实验-1"><a href="#定量实验-1" class="headerlink" title="定量实验"></a>定量实验</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/11.png" title="Optional title"></p><h2 id="对比实验"><a href="#对比实验" class="headerlink" title="对比实验"></a>对比实验</h2><ul><li><p>cell decoding似乎会影响out-of-distribution high resolution时的PSNR值。实验结果为：可以看到针对30倍的任务，cell-1/30明显好于其他设定。如果decoding cell大于实际的像素大小，这就类似于用一个比较大的平均核对图像进行处理。结论是，使用cell decoding有助于in-distribution scales，当scale过大时可能会影响PSNR但是仍然可以提升视觉质量。（图7）</p></li><li><p>实验在训练时使用固定的scale，虽然可以提升该scale的结果，但是对于其他scale效果不好。（表4）</p></li><li><p>针对size-varied ground-truth问题进行了实验。</p></li></ul><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>文章：<a href="https://arxiv.org/pdf/2012.09161.pdf">https://arxiv.org/pdf/2012.09161.pdf</a></p><p>code/home：<a href="https://yinboc.github.io/liif/">https://yinboc.github.io/liif/</a></p><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>新的点的思路之一！就是用连续进行表示！如果真的出来的话还是非常nice的！</p><hr><blockquote><p>截出来的图片糊的一，以后再也不在副屏上截图了，或者等有钱换个好一点的副屏……</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Learning Continuous Image Representation with Local Implicit Image F</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：IGNN</title>
    <link href="http://example.com/2021/06/21/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AIGNN/"/>
    <id>http://example.com/2021/06/21/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AIGNN/</id>
    <published>2021-06-21T08:40:41.000Z</published>
    <updated>2021-06-29T08:45:47.839Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Cross-Scale Internal Graph Neural Network for Image Super-Resolution</p></blockquote><blockquote><p>Nips 2020</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>和CS-NL那篇相似，都是利用一张图片中的相似patch做超分。</p><p>解决了两个问题：</p><ul><li><p>怎么准确地找到这些相似的patch？</p></li><li><p>怎么合理地融合这些patch？</p></li></ul><h1 id="Non-local"><a href="#Non-local" class="headerlink" title="Non-local"></a>Non-local</h1><h2 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h2><p>Non-local在图像重建中用的很多。许多经典的方法如非局部均值和BM3D，将相似的patch聚集起来进行图像去噪。</p><p>相似patch聚集的过程可表述为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-IGNN/1.png" title="Optional title"></p><p>其中，<code>Xi</code>和<code>Yi</code>是第i个位置（聚集中心）的输入和输出patch；<code>Xj</code>是第i个位置的相邻特征patch集<code>Si</code>中包含的第j个邻居；<code>Q(·)</code>将输入X变换到另一个特征空间；<code>C(·,·)</code>根据Xj和Xi的相关性得到的权重，越相关权重越大；<code>δi(X)</code>给前面的和项做归一化。</p><p><strong>将patch和权重看作图结构的顶点和边，那么上面的过程可以看作是一个GNN。</strong></p><blockquote><p>The above aggregation can be treated as a GNN if we treat the feature patches and weighted connections as vertices and edges respectively. The non-local neural networks actually model a fully-connected self-similarity graph. They estimate the aggregation weights between the query item Xi and all the spatially nearby patches Xj in a d × d window (or within the whole features). To reduce the memory and computational costs introduced by the above dense connection, some k-nearest neighbor based networks, e.g., GCDN and N3Net, only consider k (k ≪ d2) most similar feature patches for aggregation and treat them as the neighbors in Si for every query Xi. For all the above mentioned non-local methods, the aggregated neighboring patches are all in the same scale of the query and no HR information is incorporated, thus leading to a limited performance improvement for SISR.</p></blockquote><h2 id="一般化图像是否泛用？"><a href="#一般化图像是否泛用？" class="headerlink" title="一般化图像是否泛用？"></a>一般化图像是否泛用？</h2><p>论文中给出的图片是比较特殊的一类。实际拍摄图像中往往不会特别多重复的图像块（一栋大楼上都是一模一样的窗户，但拍大楼的图片只是极狭窄的题材）。</p><p><strong>真实算法中考虑的图像块非常小，往往只是很小的重复纹理repetitive texture、边edge或角corner，所以Non-local patch复现的性质对于任何一般图片都是成立的。</strong></p><blockquote><p>Zontak and Irani, Internal statistics of a single natural image<br>CVPR 2011</p></blockquote><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-IGNN/2.png" title="Optional title"></p><h2 id="Graph-Construction"><a href="#Graph-Construction" class="headerlink" title="Graph Construction"></a>Graph Construction</h2><p>这部分的过程如下：</p><ul><li><p>对LR下采样（指向左边的黑色箭头）：先使用双三次将输入的LR图像<code>IL</code>下采样，倍率为<code>s</code>，表示为<code>IL↓s</code>。s的取值所需的上采样的倍率。且文章指出对于上采样，s=2比s=4好。</p></li><li><p>特征提取：用VGG19对<code>IL</code>和<code>IL↓s</code>进行特征提取得到对应特征图。</p></li><li><p>查找相似patch：对于<code>EL</code>中要搜索的一个patch，<strong>在下采样特征图<code>EL↓s</code>中以MSE作为metric，搜索到k个相似patch，然后按位置对应回EL（图中红色虚线Vertex Mapping）</strong>，这样对于一个搜索patch就得到了k个EL中的相似patch。</p></li></ul><p>要搜索的patch和得到的k个相似patch是图的节点，搜索patch和相似patch的差是图的边，逐个patch进行搜索完成，图就构建完毕了。</p><p>根据对图片的统计验证，图像的相似patch很多。<strong>也就是说Graph Construction这部分如果按照最朴素的想法来做的话，计算量会很大。</strong></p><p><strong>设定k和s就是为了保证模型速度。</strong></p><ul><li><p>s：选定在IL下采样s倍的特征图上做搜索，而不是IL本身对应的特征图中去搜索；</p></li><li><p>k：只选k个相似的patch，减少后面聚合的计算量。</p></li></ul><h2 id="Patch-Aggregation"><a href="#Patch-Aggregation" class="headerlink" title="Patch Aggregation"></a>Patch Aggregation</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-IGNN/3.png" title="Optional title"></p><p>这部分是对前面获得的相似patch信息的应用。本质和一开始列的那个non-local聚合公式是一样的，都是加权，只不过形式复杂一点：这里是对于要搜索patch的对应k个相似patch进行加权，权重就是前面说到的图结构的边，将边值送至网络的输出取对数作为权。</p><h2 id="跳转连接"><a href="#跳转连接" class="headerlink" title="跳转连接"></a>跳转连接</h2><p>通过跨不同规模的跳转连接，聚合HR功能中的丰富HR信息↑s在网络中直接从中间位置传递到后面位置。这种机制允许HR信息帮助网络生成更详细的输出。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><ul><li><p>骨干网络：为了证明GraphAgg模块的有效性，选择了<code>EDSR</code>作为骨干网络，它包含32个ResBlock。所提出的GraphAgg模块在<code>IGNN中只使用一次</code>，并且<code>插入在第16个ResBlock之后</code>。</p></li><li><p>图构造：使用VGG19的前三层和固定的预训练参数来嵌入图像IL和IL↓s到EL和EL↓s。</p></li><li><p>训练集：800 high-quality (2K resolution) images from DIV2K dataset</p></li><li><p>测试集：Set5, Set14, BSD100, Urban100 and Manga109 in three upscaling factors: ×2, ×3 and ×4. </p></li></ul><h2 id="定量实验"><a href="#定量实验" class="headerlink" title="定量实验"></a>定量实验</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-IGNN/4.png" title="Optional title"></p><h2 id="定性试验"><a href="#定性试验" class="headerlink" title="定性试验"></a>定性试验</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-IGNN/5.png" title="Optional title"></p><h2 id="window-size-d-和-neighbor-k-的取值"><a href="#window-size-d-和-neighbor-k-的取值" class="headerlink" title="window size d 和 neighbor k 的取值"></a>window size d 和 neighbor k 的取值</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-IGNN/6.png" title="Optional title"></p><blockquote><p>windos size 实验中固定为60 × 60。</p></blockquote><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>文章：<a href="https://arxiv.org/pdf/2006.16673.pdf">https://arxiv.org/pdf/2006.16673.pdf</a></p></li><li><p>code：<a href="https://github.com/sczhou/IGNN">https://github.com/sczhou/IGNN</a></p></li></ul><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>看这篇是因为跟CS-NL那篇的思想相似，想做一对儿文章来讲。但实际上看的顺序反了，应该先看这篇。</p><p>IGNN这篇对于我来说，最大的好处就是<strong>解释清楚了Non-local的前世今生</strong>。这一点CS-NL没太解释也没太重视。</p><p>思路方面的评价是一样的，Non-local这个思路的有效性之前就说过。<strong>这类通过对LR-HR exemplars的挖掘，可以在一定程度上缓解超分任务中ill-posed的问题。</strong></p><p>以及还是比较在意效率问题，这点文章没提，估计也是效率方面差，扬长避短了。虽然在Graph Construction那边做了限制，但以图结构的复杂度，还是平方级别的计算量增长。这一点好像避不开，那么后面的优化就需要多做一做了。</p><p>结构方面，这一篇做的就不如CS-NL的解释性好了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Cross-Scale Internal Graph Neural Network for Image Super-Resolution</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：CSNLA</title>
    <link href="http://example.com/2021/06/13/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ACSNLA/"/>
    <id>http://example.com/2021/06/13/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ACSNLA/</id>
    <published>2021-06-13T07:46:31.000Z</published>
    <updated>2021-06-21T08:38:36.976Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining</p></blockquote><blockquote><p>CVPR 2020</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>文章指出，传统SISR方法着眼于LR图像，主要集中在局部先验和非局部先验的匹配和重建上。特别是基于局部先验（双线性、双三次插值）的方法，仅仅通过相邻像素的加权和来重建像素。</p><p><strong>自然图像中广泛存在跨尺度的块相似性。</strong></p><p>基于上述两点，<strong>为了突破局部限制，考虑基于非局部均值滤波的方法：即整个LR图像上全局搜索相似的块。</strong>也就是说，除了非局部像素对像素的匹配外，像素还可以与较大的图像块进行匹配。</p><p><strong>对于SISR任务来说，由于自然的跨尺度特征对应关系，可以直接从LR图像中搜索高频细节。</strong></p><p>论文主要有以下三点贡献：</p><ul><li><p>提出了用于SISR任务的第一个跨尺度非局部（CS-NL）注意力模块，计算图像内部的像素到块以及块到块的相似性。</p></li><li><p>提出了一个强大的SEM单元，在单元内部，通过结合<code>局部</code>、<code>尺度内非局部</code>和<code>跨尺度非局部</code>特征相关性，尽可能挖掘更多的的先验信息。</p></li><li><p>该网络在多个图像基准数据集上达到了最佳性能。</p></li></ul><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/1.png" title="Optional title"></p><h2 id="In-Scale-Non-Local-IS-NL-Attention"><a href="#In-Scale-Non-Local-IS-NL-Attention" class="headerlink" title="In-Scale Non-Local (IS-NL) Attention"></a>In-Scale Non-Local (IS-NL) Attention</h2><p>非局部注意可以通过从整体图像中总结相关特征来探索自我样本。形式上，给定图像特征映射X，非局部注意定义为</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/2.png" title="Optional title"></p><p>其中，<code>(i，j)、(g，h)和(u，v)</code>是X的坐标对，<code>ψ(·)</code>是特征变换函数，<code>φ(·,·)</code> 是用来衡量相似性的相关函数，定义为</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/3.png" title="Optional title"></p><p>其中，<code>θ(·)、δ(·)</code>是特征变换。</p><h2 id="Cross-Scale-Non-Local-CS-NL-Attention"><a href="#Cross-Scale-Non-Local-CS-NL-Attention" class="headerlink" title="Cross-Scale Non-Local (CS-NL) Attention"></a>Cross-Scale Non-Local (CS-NL) Attention</h2><p><strong>CS-NL注意是建立在IS-NL注意的基础上的，主要做法是：在根据比例<code>s</code>下采样过的图像中寻找特征Y=X中的候选特征。下采样操作是双线性插值。</strong></p><p>这样做的原因是，由于空间维度的差异，使用公共相似性度量直接将像素与patch匹配是不可行的。因此，只需对特征进行降采样，将patch表示为像素，并测量其距离（affinity）。</p><blockquote><p>The reason to do so is because directly matching pixels with patches using common similarity measurement is infeasible due to spatial dimension difference. So we simply downsample the features to represent the patch as pixel and measure the affinity.</p></blockquote><p>IS-NL公式扩展到CS-NL版本：设缩放比例为<code>s</code>为了计算像素和patch的相似性，首先需要从<code>X(w×h)</code>到<code>Y(w/s×h/s)</code>进行下采样，找到X和Y的pixel-wise相似性，最后使用相应的<code>s×s patch</code>重建X，输出是<code>Z(sh×sh)</code>。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/4.png" title="Optional title"></p><p>CS-NL attention module的流程：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/5.png" title="Optional title"></p><h2 id="Patch-Based-Cross-Scale-Non-Local-Attention"><a href="#Patch-Based-Cross-Scale-Non-Local-Attention" class="headerlink" title="Patch-Based Cross-Scale Non-Local Attention"></a>Patch-Based Cross-Scale Non-Local Attention</h2><p>CS-NL attention 的距离（affinity）度量可能存在问题。</p><blockquote><p>First, high-level features are robust to transformations and distortions, that is rotated/distorted low-level patches may yield same high-level features.<br>Besides, adjacent target regions are generated in a non-overlapping fashion, possibly creating discontinuous region boundaries artifacts.</p></blockquote><p>给出新的公式：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/6.png" title="Optional title"></p><p>p×p给出了非1的patch大小，当p=1的时候就回归了CS-NL。</p><blockquote><p>解释原理是上一个公式，代码里用的是这一个公式。</p></blockquote><h2 id="Self-Exemplars-Mining-SEM-Cell"><a href="#Self-Exemplars-Mining-SEM-Cell" class="headerlink" title="Self-Exemplars Mining (SEM) Cell"></a>Self-Exemplars Mining (SEM) Cell</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/7.png" title="Optional title"></p><h3 id="Multi-Branch-Exemplars"><a href="#Multi-Branch-Exemplars" class="headerlink" title="Multi-Branch Exemplars"></a>Multi-Branch Exemplars</h3><p>Local Branch不对Li-1做任何操作，直接搬过来；Cross-Scale NL Attention前面提过；In-Scale NL Attention计算的是特征图内像素间的非局部相似性。</p><h3 id="Mutual-Projected-Fusion"><a href="#Mutual-Projected-Fusion" class="headerlink" title="Mutual-Projected Fusion"></a>Mutual-Projected Fusion</h3><p>将三部分的特征进行融合：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/8.png" title="Optional title"></p><blockquote><p>式(5)-(8)</p></blockquote><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><ul><li><p>训练集：DIV2K（800 images）</p></li><li><p>测试集：Set5, Set14, B100, Urban100, and Manga109</p></li><li><p>SEM：12</p></li><li><p>p = 3</p></li></ul><h2 id="定量试验"><a href="#定量试验" class="headerlink" title="定量试验"></a>定量试验</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/9.png" title="Optional title"></p><h2 id="定性试验"><a href="#定性试验" class="headerlink" title="定性试验"></a>定性试验</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/10.png" title="Optional title"></p><h2 id="branch效果对比"><a href="#branch效果对比" class="headerlink" title="branch效果对比"></a>branch效果对比</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/11.png" title="Optional title"></p><h2 id="branch融合方式对比"><a href="#branch融合方式对比" class="headerlink" title="branch融合方式对比"></a>branch融合方式对比</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/12.png" title="Optional title"></p><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>文章：<a href="https://arxiv.org/pdf/2006.01424.pdf">https://arxiv.org/pdf/2006.01424.pdf</a></p></li><li><p>code：<a href="https://github.com/SHI-Labs/Cross-ScaleNon-Local-Attention">https://github.com/SHI-Labs/Cross-ScaleNon-Local-Attention</a></p></li></ul><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>从文章的结果来看，用相似块来做SR的这个思路是非常成功的，也符合自然的想法，合理是非常合理的总之。</p><p>实验做的很细，但感觉少了一个非常重要的内容，就是SEM的数量问题。p的大小是做了实验的，上面没放。</p><p>最终的对比结果里，用了注意力机制的都是效果不错的，还是要多关注Attention。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Image Super-Resolution with Cross-Scale Non-Local Attention and Exha</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：AdderNet/AdderSR</title>
    <link href="http://example.com/2021/05/28/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AAdderNet:AdderSR/"/>
    <id>http://example.com/2021/05/28/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AAdderNet:AdderSR/</id>
    <published>2021-05-28T06:47:01.000Z</published>
    <updated>2021-06-02T12:48:12.361Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="AdderNet"><a href="#AdderNet" class="headerlink" title="AdderNet"></a>AdderNet</h1><blockquote><p>AdderNet: Do We Really Need Multiplications in Deep Learning?</p></blockquote><blockquote><p>CVPR 2019</p></blockquote><h2 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h2><p>在深度学习算法中，卷积操作被广泛的用于度量输入特征和卷积滤波器之间的相似性。</p><p>GPU+深度卷积神经网络，大大加快了计算机视觉领域的发展。但GPU有着显而易见的缺陷，主要是物理因素所导致的。第一个是功耗问题，再一个是体积问题。这两个问题导致了现有的多数深度学习方法只能运行在装有大型GPU的机器上。人们希望这些算法能在小巧的移动设备上实现。因此，目的就是降低算法的运行成本，使得在硬件配置不够的情况下也能高效的运行这些深度算法。</p><p><strong>在正向推理过程中，深层神经网络的计算大多是浮值权和浮值激活的乘法运算。显然，乘法运算比加法运算要慢。</strong></p><p><strong>本文提出的AdderNet，就是在放弃卷积运算的同时，利用l1距离，最大限度地描述和利用加法，逼近卷积操作的精度。为了保证模板的充分更新和网络的收敛性，设计了一种改进的梯度正则化反向传播算法。</strong></p><h2 id="数学描述"><a href="#数学描述" class="headerlink" title="数学描述"></a>数学描述</h2><h3 id="Adder-Network"><a href="#Adder-Network" class="headerlink" title="Adder Network"></a>Adder Network</h3><p>对于CNN中的卷积运算，假定输入特征<code>X</code>，filter表示为<code>F</code>，卷积后输出的是二者的相似性度量，表述如下面公式：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/1.png" title="Optional title"></p><p>其中，<code>d</code>为kernel size，<code>c_in</code>和<code>c_out</code>分别为输入、输出通道数；卷积核<code>F</code>的大小则为<code>d × d × c_in × c_out</code>；H和W为输入特征的长和宽，输入特征<code>X</code>的大小为<code>H × W × c_in</code>。</p><p>以及，<code>S(·,·)</code>是相似性度量，也就是距离度量函数。如果将互相关（cross-correlation）作为距离的度量，即<code>S(x,y)= x × y</code>，这时式（1）成为卷积运算。式（1）还可以表示当d=1时计算完全连接的层。</p><p>还有许多其他度量来测量滤波器和输入特征之间的距离，但这些度量大多涉及乘法。</p><p>于是，希望在式（1）中用加法来代替乘法。<strong>L1距离仅涉及到两个向量差的绝对值</strong>，不包含乘法。因此，通过计算滤波器和输入特征之间的L1距离，式（1）可以重新表示为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/2.png" title="Optional title"></p><p>不论是使用互相关，还是L1距离，都可以完成相似性度量，但二者的输出结果还是有一些差别的：通过卷积核完成输入特征图谱的加权和计算，结果可正可负；<strong>但adder filter输出的结果恒为负，为此作者引入了batch normalization将结果归一化到一定范围区间内，从而保证传统CNN使用的激活函数在此依旧可以正常使用。</strong></p><blockquote><p>用式（3）计算的特征都是负数，因此在特征计算后，采用BN层来归一化，之后再用激活函数提高特征非线性。</p></blockquote><h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><p>神经网络利用<strong>反向传播</strong>来计算滤波器的梯度和<strong>随机梯度下降</strong>来更新参数。</p><p>在CNN中，输出特征Y相对于滤波器F的偏导数被计算为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/3.png" title="Optional title"></p><p>在AdderNets中，Y相对于F的偏导数是：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/4.png" title="Optional title"></p><blockquote><p>sgn(·)是阶跃函数，取-1、0、1。</p></blockquote><p>式（4）中等号右边的是<code>signSGD</code>，<strong>signSGD被证明的缺陷是几乎不会选择到最陡的方向，而且随着维度增加效果会更差。</strong></p><p>因此AdderNet使用如下公式进行梯度更新：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/5.png" title="Optional title"></p><p>此外，如果使用全精度梯度的更新方法，由于涉及到前层的梯度值，很容易导致梯度爆炸。这里使用<code>HardTanh</code>将输出限定在[-1,1]范围内：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/6.png" title="Optional title"></p><blockquote><p>梯度计算也好了，接下来的问题是：加法层的方差会很大。</p></blockquote><h3 id="Adaptive-Learning-Rate-Scaling"><a href="#Adaptive-Learning-Rate-Scaling" class="headerlink" title="Adaptive Learning Rate Scaling"></a>Adaptive Learning Rate Scaling</h3><p>在传统的CNN中，假设权值和输入特征是独立的，服从正态分布，输出的方差大致可以估计为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/7.png" title="Optional title"></p><p>对于AdderNet，输出的方差可以近似为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/8.png" title="Optional title"></p><p><strong>实际上，权重var[F]的方差通常非常小（10^−3～10^-4）。因此，式（9）中的加法运算会比式（8）带来更大的输出方差。</strong></p><blockquote><p>式（8）:var[X] × var[F]<br>式（9）:var[X] + var[F]</p></blockquote><p>AdderNet的输出具有较大方差，在更新时根据常规的链式法则会导致梯度比常规CNN更小，从而导致参数更新过慢。</p><p>这里，最容易想到的解决方法就是：使用大的learning rate。这样虽然可以提高学习速度，但是文中又指出，<strong>不同层其权重梯度变化很大</strong>：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/9.png" title="Optional title"></p><p><strong>这表明每一层的参数需要不同。</strong>因此作者给<strong>每一个adder layer</strong>设计了不同学习率：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/10.png" title="Optional title"></p><p>其中，<code>γ</code>是整个神经网络的全局学习速率，<code>∆L(Fl)</code>是层L中过滤器的梯度，<code>αl</code>是其相应的局部学习率。</p><blockquote><p>输入是经过BN的，也就是说在AdderNet中，filters的值也是经过了归一化。</p></blockquote><p>局部学习率<code>αl</code>定义为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/11.png" title="Optional title"></p><p>其中，<code>η</code>是一个控制adder filters学习率的超参数，<code>k</code>是Fl中参数的个数。</p><h3 id="Adder-Network的前馈和反传流程"><a href="#Adder-Network的前馈和反传流程" class="headerlink" title="Adder Network的前馈和反传流程"></a>Adder Network的前馈和反传流程</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/12.png" title="Optional title"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h3><ul><li><p>NVIDIA Tesla V100 GPU：2017年中发布、16/32GB（文中没提用的是什么显存的版本）</p></li><li><p>PyTorch</p></li></ul><h3 id="MINST-LeNet-5-BN-AddNN-CNN"><a href="#MINST-LeNet-5-BN-AddNN-CNN" class="headerlink" title="MINST + LeNet-5-BN + AddNN/CNN"></a>MINST + LeNet-5-BN + AddNN/CNN</h3><p>结果：</p><ul><li><p>CNN的准确率为99.4%∼435K乘法。</p></li><li><p><strong>通过将卷积运算中的乘法替换为加法运算，该加法网的准确率达到99.4%∼870K加法，几乎没有乘法。</strong></p></li></ul><p>训练设置：</p><ul><li>图像大小：调整为32×32；</li><li>优化：Nesterov加速梯度法（NAG）；</li><li>weight decay/momentum：5×10^−4/0.9；</li><li>初始lr为0.1、cosine learning rate decay（余弦学习率衰减）；</li><li><strong>用加法器滤波器代替LeNet-5-BN中的卷积滤波器、将全连接层中的乘法替换为减法；</strong></li><li>50个epochs的训练。</li></ul><h3 id="CIFAR-10-100-VGG-ResNet-BNN-AddNN-CNN"><a href="#CIFAR-10-100-VGG-ResNet-BNN-AddNN-CNN" class="headerlink" title="CIFAR-10/100 + VGG/ResNet + BNN/AddNN/CNN"></a>CIFAR-10/100 + VGG/ResNet + BNN/AddNN/CNN</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/13.png" title="Optional title"></p><p>结果：</p><ul><li><p>对于VGG，AddNN与CNNs（CIFAR-10为93.80%，CIFAR-100为72.73%）的结果基本相同（CIFAR-10为93.72%，CIFAR-100为72.64%）；BNN的模型尺寸比AdderNet和CNN小得多，精度也低得多（CIFAR-10为89.80%，CIFAR-100为65.41%）；</p></li><li><p>对于ResNet-20，CNNs的精确度最高（CIFAR10为92.25%，CIFAR-100为68.14%），但乘法次数较多（41.17M），AddNN少了0.5个点左右（CIFAR-10和CIFAR-100中的无乘法运算精度分别为91.84%和67.60%）。</p></li></ul><blockquote><p>由于二进制神经网络BNN可以使用XNOR运算来代替乘法，所以还比较了BNN的结果。</p><blockquote><p>XNOR：同或。（1 XNOR 1 = 0 XNOR 0 = 1，1 XNOR 0 = 0 XNOR 1 = 0）</p></blockquote></blockquote><p>训练设置：</p><ul><li>图像大小：32×32；</li><li>与何凯明大佬2016年残差那篇相同的DA和处理方法；</li><li>初始lr为0.1、后续遵循多项式学习率时间表；</li><li>bs=256；</li><li>BNN中，将第一层和最后一层设置为全精度卷积层；</li><li>400个epochs的训练。</li></ul><h3 id="ImageNet-ResNet-BNN-AddNN-CNN"><a href="#ImageNet-ResNet-BNN-AddNN-CNN" class="headerlink" title="ImageNet + ResNet + BNN/AddNN/CNN"></a>ImageNet + ResNet + BNN/AddNN/CNN</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/14.png" title="Optional title"></p><h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><ul><li><p>文章：<a href="https://arxiv.org/pdf/1912.13200.pdf">https://arxiv.org/pdf/1912.13200.pdf</a></p></li><li><p>code：<a href="https://github.com/huawei-noah/AdderNet">https://github.com/huawei-noah/AdderNet</a></p></li></ul><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>首先，文章思路很不错。相似的思路之前也提过，这一篇的内容做的挺多的，不管是数学方面的论证还是实验部分做的都还不错，比较清晰。</p><p>以及看了知乎上这篇文章的团队内成员的答复和一些网友的评论，整体的评价也还不错，去年6月份的时候，大部分都还在求硬件加速之类的，官方也说会给出预训练模型，接下来的一篇文章也是继承了这个工作，说明这个工作持续的在做。这是比较nice的。</p><p>缺陷也有，最主要的一点就是<strong>为什么最后的实验不去对比GPU下的速度？</strong>显得有点文不对题，故意隐瞒，不是特别的convincing。以及实验主要做的是图像分类的问题，但也没提修改了卷积操作之后，在别的领域的应用，也没特别说明只是针对图像分类问题做的。</p><p>最后，有一点没看懂，就是做的是简化网络，从GPU转移到CPU，但是最终部署的时候用的还是GPU，且没给CPU的主频信息等，包括文章中没有特别提到的硬件方面的东西，包括cuda编程等。这些都是没说清楚的点，也许不重要？但是这要不说清楚，代码都不怎么看得懂，哪有心思跑它。</p><hr><h1 id="AdderSR"><a href="#AdderSR" class="headerlink" title="AdderSR"></a>AdderSR</h1><blockquote><p>AdderSR: Towards Energy Efficient Image Super-Resolution</p></blockquote><blockquote><p>CVPR 2021</p></blockquote><h2 id="立意-1"><a href="#立意-1" class="headerlink" title="立意"></a>立意</h2><p>AdderNet用于SISR问题中会出现两个问题：</p><ul><li><p>加法器不能很容易的学习身份映射（identity mapping）；</p></li><li><p>加法器不能实现高通滤波。</p></li></ul><blockquote><p>Specifically, the adder operation cannot easily learn the identity mapping, which is essential for image processing tasks. In addition, the functionality of high-pass filters cannot be ensured by AdderNets.</p></blockquote><p>换言之，在SISR中，AdderNet需要保证两个重要特性：<strong>各卷积层输入输出特征的相似性和高频信息的细节增强</strong>。</p><blockquote><p>In SISR, there are two important properties that should be ensured by adder neural networks: the similarity between the input and output features of each convolutional layer, and the enhancement of details w.r.t. high frequency information.</p></blockquote><p>VDSR中输入图像“蝴蝶”的不同层的输出特征映射。<strong>任何两个相邻层之间的差异在纹理和颜色信息上都非常相似。高频信息的细节随着深度的增加而增强。</strong>这两个重要性质需要加法神经网络来保证。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/15.png" title="Optional title"></p><p>这篇文章分别设计了模块去解决了上述问题，最终使AdderNet的SISR模型逼近CNN性能。</p><blockquote><p>To maximally excavate the potential for exploiting AdderNets to establish SISR models, we first analyze the theoretical difficulties for applying the additions into SISR tasks. Specifically, input and output features in any two neighbor layers in SISR models are very close with the similar global texture and color information as shown in Figure 2. However, the identify mapping cannot be learned by a one-layer adder network. Thus, we suggest to insert self-shortcuts and formulate new adder models for the SISR task. Moreover, we find that the high-pass filter is also hard to approximate by adder units. We then develop a learnable power activation. By exploiting these two techniques, we replace the conventional convolution filters in modern SISR networks by adder filters and establish AdderSR models accordingly. </p></blockquote><h2 id="数学描述-1"><a href="#数学描述-1" class="headerlink" title="数学描述"></a>数学描述</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>一般的SISR任务的目标函数可以表示为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/16.png" title="Optional title"></p><p>其中，其中<code>Iy</code>是观测数据，即低分，<code>Ix</code>是期望的高分辨率图像，<code>R(·)</code>表示所使用的先验信息，例如平滑和加性噪声，<code>λ</code>是权重。</p><p>式（2）又提了一遍加法器模型，就是上一篇的式（2）。</p><blockquote><p>本篇的式（2）跟上一篇的式（2）有一点点不一样，就是求和号的角标都是从1开始的，上一篇是从0；以及换了两个字母表示。不过应该不是什么问题。</p></blockquote><h3 id="Learning-Identity-Mapping-using-AdderNet"><a href="#Learning-Identity-Mapping-using-AdderNet" class="headerlink" title="Learning Identity Mapping using AdderNet"></a>Learning Identity Mapping using AdderNet</h3><blockquote><p>一上来，这个恒等映射的作用就没太看懂，查只能查到何大佬那篇文章，但也没有引，公式是一模一样的，不知道说的是不是一个事情。</p></blockquote><p>图2表明，在使用深度学习方法的SISR任务中，恒等映射<code>Iy=F(Iy)</code>是非常必要的。</p><p><strong>对于传统的卷积网络，当权值为单位矩阵时，恒等映射很容易学习。但AdderNet使用的是l1范数，虽然l1距离可以很好地完成图像分类任务，但是不能逼近恒等映射。</strong></p><p>使用多个Adder Layer叠加可以逼近，但是会显著的增加参数量。</p><h4 id="Adder-Layer为什么无法逼近恒等映射？"><a href="#Adder-Layer为什么无法逼近恒等映射？" class="headerlink" title="Adder Layer为什么无法逼近恒等映射？"></a>Adder Layer为什么无法逼近恒等映射？</h4><blockquote><p>文中的Theorem 1。</p></blockquote><p>描述低分图像<code>Iy</code>和带权重的Adder Layer<code>W</code>的恒等映射：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/17.png" title="Optional title"></p><p><strong>这个等式是<span style="border-bottom:2px dashed red;">不成立</span>的</strong>，是因为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/18.png" title="Optional title"></p><h4 id="如何实现恒等映射？"><a href="#如何实现恒等映射？" class="headerlink" title="如何实现恒等映射？"></a>如何实现恒等映射？</h4><p>改进Adder Layer：为每个Adder Layer增加<code>自快捷操作（self-shortcut operation）</code>：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/19.png" title="Optional title"></p><p>其中<code>Wl</code>是第l层中加法器滤波器的权重，<code>Xl</code>是输入，<code>Yl</code>是输出。</p><p><strong>根据式（7），<code>Yl</code>包含<code>Xl</code>本身，则可以通过减小<code>Wl</code>，使等号右边近似等号左边。</strong></p><p>与图像识别任务不同的是，大多数SISR问题的特征应该保持一个固定的大小，即Xl和Yl的宽度和高度完全相同。因此，式（7）可嵌入大多数常规SISR模型中。</p><h3 id="Learnable-Power-Activation"><a href="#Learnable-Power-Activation" class="headerlink" title="Learnable Power Activation"></a>Learnable Power Activation</h3><p>自然图像通常由不同的频率信息组成。例如，背景和大面积的草都是低频信息，其中大部分相邻像素非常接近。与此相反，物体和某些建筑物的边缘对于给定的整个图像来说，都是精确的高频信息。</p><p>理想的高通滤波器<code>Φ(·)</code>可定义为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/20.png" title="Optional title"></p><p>对于卷积操作，高通滤波的实现也很简单，比如：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/21.png" title="Optional title"></p><p>但是Adder Layer无法实现高通滤波。</p><h4 id="Adder-Layer为什么无法实现高通滤波？"><a href="#Adder-Layer为什么无法实现高通滤波？" class="headerlink" title="Adder Layer为什么无法实现高通滤波？"></a>Adder Layer为什么无法实现高通滤波？</h4><blockquote><p>文中的Theorem 2。</p></blockquote><p>令<code>E ∈ R_d×d</code> 是给定SR模型的输入，其中每个元素等于1。<code>W</code>表示任意Adder Layer的权重。<code>s ∈ R</code>。则<span style="border-bottom:2px dashed red;">不存在</span><code>W</code>和常数<code>a ∈ R</code>满足以下等式：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/22.png" title="Optional title"></p><p>是因为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/23.png" title="Optional title"></p><h4 id="如何实现高通滤波？"><a href="#如何实现高通滤波？" class="headerlink" title="如何实现高通滤波？"></a>如何实现高通滤波？</h4><p>提出了<code>Learnable Power Activation</code>函数来解决Adder Layer的高通滤波问题和细化输出图像：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/24.png" title="Optional title"></p><p>其中，<code>Y</code>为输出特征，<code>sgn(·)</code>为符号函数。</p><p><strong><code>α &gt; 0</code>是一个可学习的参数，用于调整信息和分布。当<code>α &gt; 1</code>，式（11）可以增强输出图像的对比度，强调高频信息；当<code>0 &lt; α &lt; 1</code>，式（11）可以平滑输出图像中的所有信号并去除伪影和噪声。</strong></p><p>式（11）可以很容易地嵌入到任何SISR模型的常规ReLU中。</p><blockquote><p>式（11）的背景：Box-Cox变换应用于图像去噪任务中、Box-Cox变换在图像超分辨率中的功能。没有过多解释和证明。</p><blockquote><p>Fortunately, Sharabati and Xi [29] applied the Box-Cox transformation [28] in the image denoising task and found that this transformation can achieve the similar functionality to that of the high-pass filters without adding massive parameters and calculations. Oliveira et al. [26] further discussed the functionality of Box-Cox transformation in image super- resolution. In addition, a sign-preserving power law point transformation is also explored for emphasizing the areas with abundant details in the input image [27]. </p></blockquote></blockquote><h2 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h2><p>实验具体细节不放了，直接放数据：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/25.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/26.png" title="Optional title"></p><p>做出来的3倍的效果：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/27.png" title="Optional title"></p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul><li><p>文章：<a href="https://arxiv.org/pdf/2009.08891.pdf">https://arxiv.org/pdf/2009.08891.pdf</a></p></li><li><p>code：上一个工作的拓展，所以和上一篇的code放在一起了。</p></li></ul><h2 id="感想-1"><a href="#感想-1" class="headerlink" title="感想"></a>感想</h2><p>最大的问题还是实验部分没有提速度，这本来是一个加速的工作，不提速度只说工作量，确实有点问题。</p><p>评价指标结果相当相当接近CNN了，这方面真的很厉害。</p><hr><h1 id="其他-1"><a href="#其他-1" class="headerlink" title="其他"></a>其他</h1><h2 id="卷积、互相关、自相关"><a href="#卷积、互相关、自相关" class="headerlink" title="卷积、互相关、自相关"></a>卷积、互相关、自相关</h2><p>卷积的数学表示：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/28.png" title="Optional title"></p><p>互相关的数学表示：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/29.png" title="Optional title"></p><p>互相关的公式中，变量和积分对象不再是t，而是<code>tao</code>。也就是说互相关研究室的是两个对象产生一段时间差<code>tao</code>之后的关系，也就是相不相关，如果平移之后两个函数正交，那么他们就没有相互关系，互相关为0。</p><p>如果<code>tao</code>是偶函数的话，互相关与卷积的结果是一样的。</p><blockquote><p>卷积、互相关与自相关 - 知乎专栏<br><a href="https://zhuanlan.zhihu.com/p/62292503">https://zhuanlan.zhihu.com/p/62292503</a></p></blockquote><h2 id="L1距离-L1-norm"><a href="#L1距离-L1-norm" class="headerlink" title="L1距离/L1-norm"></a>L1距离/L1-norm</h2><p><code>L1-norm</code>又叫做<code>taxicab-norm</code>或者 <code>Manhattan-norm</code>，可<code>用在曼哈顿区坐出租车来做比喻</code>。图中绿线是两个黑点的L2距离，而其他几根是L1距离。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/30.png" title="Optional title"></p><blockquote><p>理解L1，L2 范数在机器学习中应用 - 简书<br><a href="https://www.jianshu.com/p/6cf5d60db634">https://www.jianshu.com/p/6cf5d60db634</a></p></blockquote><h2 id="signSGD"><a href="#signSGD" class="headerlink" title="signSGD"></a>signSGD</h2><blockquote><p>SIGNSGD: compressed optimisation for non-convex problems</p></blockquote><p>文章核心是：SGD里面，梯度真正有用的是方向而不是大小。所以，即使只保留梯度的符号来对模型进行更新，也能得到收敛的效果。甚至有些情况下，这么做能减少梯度的噪声，使得收敛速度更快。</p><p>根据上面的结论，进而衍生出了三种算法。<strong>SignSGD是第一种：直接把gradient求sign：</strong></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/31.png" title="Optional title"></p><blockquote><p>SignSGD 及其 MXNet 实现解读 - 知乎专栏<br><a href="https://zhuanlan.zhihu.com/p/112346480">https://zhuanlan.zhihu.com/p/112346480</a></p></blockquote><h2 id="HardTanh"><a href="#HardTanh" class="headerlink" title="HardTanh"></a>HardTanh</h2><p>HardTanh比Tanh计算开销小，缺点是和dying relu类似，部分参数可能不更新。</p><p>文章中的公式：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/32.png" title="Optional title"></p><p>图像：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/33.png" title="Optional title"></p><h2 id="Identity-Mapping"><a href="#Identity-Mapping" class="headerlink" title="Identity Mapping"></a>Identity Mapping</h2><blockquote><p>Identity Mappings in Deep Residual Networks</p></blockquote><blockquote><p>2016 ECCV</p></blockquote><p>这个关键词何凯明大佬为了残差网络而提到的。这篇文章的核心是：分析了残差模块的传播方式，能过解释为什么使用identity mapping作为跳跃连接和加和的激活项，能使得前向和反向的信号能直接在模块之间传播。</p><h2 id="Box-Cox变换"><a href="#Box-Cox变换" class="headerlink" title="Box-Cox变换"></a>Box-Cox变换</h2><p>是Box和Cox在1964年提出的一种广义幂变换方法，是统计建模中常用的一种数据变换，用于连续的响应变量不满足正态分布的情况。</p><p>Box-Cox变换之后，可以一定程度上减小不可观测的误差和预测变量的相关性。</p><blockquote><p>box-cox变换 - 百度百科<br><a href="https://baike.baidu.com/item/box-cox%E5%8F%98%E6%8D%A2/10278422?fr=aladdin">https://baike.baidu.com/item/box-cox变换/10278422?fr=aladdin</a></p></blockquote><blockquote><p><a href="https://onlinestatbook.com/2/transformations/box-cox.html">https://onlinestatbook.com/2/transformations/box-cox.html</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;AdderNet&quot;&gt;&lt;a href=&quot;#AdderNet&quot; class=&quot;headerlink&quot; title=&quot;AdderNet&quot;&gt;&lt;/a&gt;AdderN</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04虚拟机+Hadoop：伪分布式</title>
    <link href="http://example.com/2021/05/28/Ubuntu16-04%E8%99%9A%E6%8B%9F%E6%9C%BA-Hadoop%EF%BC%9A%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    <id>http://example.com/2021/05/28/Ubuntu16-04%E8%99%9A%E6%8B%9F%E6%9C%BA-Hadoop%EF%BC%9A%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F/</id>
    <published>2021-05-28T06:17:20.000Z</published>
    <updated>2021-05-28T06:42:30.529Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>Hadoop是一个由Apache基金会所开发的分布式系统基础架构。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力进行高速运算和存储。</p><p>Hadoop实现了一个分布式文件系统（Distributed File System），其中一个组件是HDFS（Hadoop Distributed File System）。HDFS有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。HDFS放宽了（relax）POSIX的要求，可以以流的形式访问（streaming access）文件系统中的数据。</p><p>Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算。</p><p>Hadoop一共有三种部署方式，分别是<strong>本地部署、伪分布部署、集群部署</strong>。Hadoop默认模式为非分布式模式（本地模式），无需进行其他配置即可运行。非分布式即单Java进程，方便进行调试。而Hadoop伪分布模式的工作原理和集群模式的工作原理一样。</p><h1 id="Hadoop伪分布式"><a href="#Hadoop伪分布式" class="headerlink" title="Hadoop伪分布式"></a>Hadoop伪分布式</h1><p><strong>使用64位的Ubuntu 16.04虚拟机以及Hadoop 2.7.2 (stable)版本进行Hadoop伪分布式配置和测试。</strong></p><h2 id="创建hadoop用户"><a href="#创建hadoop用户" class="headerlink" title="创建hadoop用户"></a>创建hadoop用户</h2><p>需要使用名为hadoop的用户来进行接下来的步骤。</p><p>在原用户的terminal中进行如下操作：</p><p><img src="/images/Hadoop/1.png" title="Optional title"></p><h2 id="SSH"><a href="#SSH" class="headerlink" title="SSH"></a>SSH</h2><p>切换到新用户hadoop，准备工作就是做一下apt-get的更新以及需要用到的文本编辑器的安装。</p><p>集群、单节点模式都需要用到SSH登陆，需要安装SSH server：</p><p><img src="/images/Hadoop/2.png" title="Optional title"></p><p><img src="/images/Hadoop/3.png" title="Optional title"></p><p>SSH server安装完成后，使用ssh localhost即可登录本机。但这样登陆需要每次输入密码。再将SSH配置成无密码登陆：</p><p><img src="/images/Hadoop/4.png" title="Optional title"></p><p>进行上图命令之前，需要先登陆一次本地SSH，exit后再操作。</p><h2 id="Java环境"><a href="#Java环境" class="headerlink" title="Java环境"></a>Java环境</h2><p>Hadoop需要Java环境。使用如下命令安装JDK：</p><p><img src="/images/Hadoop/5.png" title="Optional title"></p><p><img src="/images/Hadoop/6.png" title="Optional title"></p><p>JDK安装结束以后，需要配置JAVA_HOME环境变量：</p><p><img src="/images/Hadoop/7.png" title="Optional title"></p><p>更新环境变量之后，检验Java环境：</p><p><img src="/images/Hadoop/8.png" title="Optional title"></p><p>上图中信息证明JDK已成功安装。</p><h2 id="Hadoop安装及非分布式实例测试"><a href="#Hadoop安装及非分布式实例测试" class="headerlink" title="Hadoop安装及非分布式实例测试"></a>Hadoop安装及非分布式实例测试</h2><p>在官网中选择需要的版本进行Hadoop下载，为压缩文件格式：</p><p><img src="/images/Hadoop/9.png" title="Optional title"></p><p><img src="/images/Hadoop/10.png" title="Optional title"></p><p>Hadoop解压后即可使用。检查Hadoop环境：</p><p><img src="/images/Hadoop/11.png" title="Optional title"></p><p>上图中成功输出了Hadoop版本信息，说明Hadoop安装成功。</p><p>Hadoop附带了丰富的例子，见命令<code>./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar</code>。</p><p>选择运行<code>grep</code>例子来测试Hadoop非分布式，步骤如下：</p><ul><li><p>给出input目录；</p></li><li><p>复制hadoop配置文件至input目录，用作grep例子的输入；</p></li><li><p>将input中的所有文件作为输入，筛选当中符合正则表达式 dfs[a-z.]+ 的单词并统计出现的次数，最后输出结果到output文件夹中；</p></li></ul><p><img src="/images/Hadoop/12.png" title="Optional title"></p><ul><li>执行成功后，查看结果：</li></ul><p><img src="/images/Hadoop/13.png" title="Optional title"></p><p>可见，执行成功后，输出的结果是符合正则的单词dfsadmin，该词出现了1次。</p><h2 id="伪分布式配置及实例测试"><a href="#伪分布式配置及实例测试" class="headerlink" title="伪分布式配置及实例测试"></a>伪分布式配置及实例测试</h2><p>Hadoop的配置文件是xml格式，每个配置以声明property的name和value的方式来实现，位于<code>/usr/local/hadoop/etc/hadoop/</code>中。</p><p>伪分布式需要修改2个配置文件<code>core-site.xml</code>和<code>hdfs-site.xml</code>：</p><p><img src="/images/Hadoop/14.png" title="Optional title"></p><p><img src="/images/Hadoop/15.png" title="Optional title"></p><p>修改完配置文件之后，进行NameNode格式化：</p><p><img src="/images/Hadoop/16.png" title="Optional title"></p><p><img src="/images/Hadoop/17.png" title="Optional title"></p><p><code>successfully formatted</code>、<code>Exitting with status 0</code>证明NameNode格式化操作成功。</p><p>然后开启NameNode和DataNode守护进程。可以使用<code>jps</code>命令和Web界面<code>http://localhost:50070</code>检查是否成功：</p><p><img src="/images/Hadoop/18.png" title="Optional title"></p><p>按照如下步骤进行Hadoop伪分布式实例测试：</p><ul><li><p>单机模式的grep例子读取的是本地数据，伪分布式读取的则是HDFS上的数据，使用HDFS需要在HDFS中创建用户目录；</p></li><li><p>给出input目录；</p></li><li><p>将./etc/hadoop中的xml文件作为输入文件复制到分布式文件系统中，即将/usr/local/hadoop/etc/hadoop复制到分布式文件系统中的/user/hadoop/input中；</p></li><li><p>同样将input中的所有文件作为输入，筛选当中符合正则表达式dfs[a-z.]+的单词并统计出现的次数，最后输出结果到output文件夹中（伪分布式运行 MapReduce 作业的方式跟单机模式相同，区别在于伪分布式读取的是HDFS中的文件）；</p></li></ul><p><img src="/images/Hadoop/19.png" title="Optional title"></p><ul><li>查看位于HDFS中的输出结果：</li></ul><p><img src="/images/Hadoop/20.png" title="Optional title"></p><p>输出结果有4条内容。因为修改了配置文件，所以这里的运行结果与本地模式下的运行结果不同。</p><hr><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><blockquote><p>Hadoop安装教程_单机/伪分布式配置_Hadoop2.6.0(2.7.1)/Ubuntu14.04(16.04)<br><a href="http://dblab.xmu.edu.cn/blog/install-hadoop/">http://dblab.xmu.edu.cn/blog/install-hadoop/</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;Hadoop是一个由Apac</summary>
      
    
    
    
    
    <category term="Hadoop" scheme="http://example.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>社会媒体计算：知乎问答信息挖掘</title>
    <link href="http://example.com/2021/04/14/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97%EF%BC%9A%E7%9F%A5%E4%B9%8E%E9%97%AE%E7%AD%94%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98/"/>
    <id>http://example.com/2021/04/14/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97%EF%BC%9A%E7%9F%A5%E4%B9%8E%E9%97%AE%E7%AD%94%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98/</id>
    <published>2021-04-14T07:33:25.000Z</published>
    <updated>2021-04-21T01:28:30.299Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>刚好在摸鱼的时候比较喜欢刷知乎，又碰到了这个课程，所以就尝试了做这个内容。以前从来没有接触过文本分析和爬虫这类的技术，就当学新技术了。以及这些在去年12月就写完了，现在闲了才想起搬到这里来。</p><p>依稀记得那是为数不多的有心思敲代码的时间，会珍惜的。</p><hr><h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><h2 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h2><p>从互联网上采集数据，如从 Tewitter、Facebook、新浪微博、B站等采集数据，包括用户基本信息、互相浏览、互相关注等信息，以及对应某一段时间发布的文本内容信息。</p><p>对上述数据进行预处理，要求用程序进行预处理。</p><p>对上述处理数据进行社团挖掘，包括基本统计信息、社团发现等，对预处理的文本进行情感分析、主题挖掘、分类或聚类等研究。要求用到社会网络计算、文本挖掘等技术。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>知乎是一个网络问答社区，用户彼此分享知识、经验和见解，围绕着某一感兴趣的话题进行相关的讨论，同时也可以关注兴趣一致的人。</p><p>知乎的基本模式是：用户提问，每一个问题都会有一个独有的id。其他对此问题感兴趣的用户在此问题下进行回答，每一个此问题下的回答也会有一个id。用户可对回答进行点赞、点踩、评论等操作。</p><p>选择从知乎采集要用到的数据。所选取的问题是：你打算在 12 月 31 日发什么朋友圈跨年文案？</p><p>链接：<a href="https://www.zhihu.com/question/360940960">https://www.zhihu.com/question/360940960</a></p><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/1.png" title="Optional title"></p><h2 id="编程环境"><a href="#编程环境" class="headerlink" title="编程环境"></a>编程环境</h2><p>Mac OS 10.14.6 + Jupyter notebook + Python 3.6.5</p><h1 id="知乎数据采集"><a href="#知乎数据采集" class="headerlink" title="知乎数据采集"></a>知乎数据采集</h1><h2 id="数据爬虫"><a href="#数据爬虫" class="headerlink" title="数据爬虫"></a>数据爬虫</h2><p>这部分爬虫功能的主要作用就是从知乎的网页上拿数据。</p><p><strong>这部分的原理参考最下面唯一的一条引用，包括从网络请求找到数据存放位置、分析请求头的格式和构造代码中所需要的新的请求头。</strong></p><p>def crawler(question_num)是该部分功能的组织函数，其他4个函数的具体功能见注释。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span>(<span class="params">url</span>):</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：访问 url 的网页，获取网页内容并返回</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         url ：目标网页的 url</span></span><br><span class="line"><span class="comment">#     返回：目标网页的 html 内容</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 设置多个用户代理</span></span><br><span class="line">    agent=[<span class="string">&#x27;Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36&#x27;</span>,  </span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.80 Safari/537.36&#x27;</span>,  </span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:30.0) Gecko/20100101 Firefox/30.0&#x27;</span>,  </span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/537.75.14&#x27;</span>,  </span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Win64; x64; Trident/6.0)&#x27;</span> ]</span><br><span class="line">    <span class="comment"># 每次随机抽取一个，防止被封ip</span></span><br><span class="line">    randdom_agent=random.choice(agent)</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;accept&#x27;</span>: <span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;user-agent&#x27;</span>: randdom_agent</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 爬取数据</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url, headers=headers)</span><br><span class="line">        r.encoding = <span class="string">&#x27;UTF8&#x27;</span></span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span> requests.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">        print(<span class="string">&quot;HTTPError&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> requests.RequestException <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">&quot;Unknown Error !&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_data</span>(<span class="params">html</span>):</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：提取 html 页面信息中的关键信息，并整合一个数组并返回</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         html：根据 url 获取到的网页内容</span></span><br><span class="line"><span class="comment">#     返回：存储有 html 中提取出的关键信息的数组</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    json_data = json.loads(html)[<span class="string">&#x27;data&#x27;</span>]</span><br><span class="line">    comments = []</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> json_data:</span><br><span class="line">            comment = []</span><br><span class="line">            comment.append(item[<span class="string">&#x27;author&#x27;</span>][<span class="string">&#x27;name&#x27;</span>]) <span class="comment"># 姓名</span></span><br><span class="line">            comment.append(item[<span class="string">&#x27;author&#x27;</span>][<span class="string">&#x27;gender&#x27;</span>]) <span class="comment"># 性别</span></span><br><span class="line">            comment.append(item[<span class="string">&#x27;voteup_count&#x27;</span>]) <span class="comment"># 点赞数</span></span><br><span class="line">            comment.append(item[<span class="string">&#x27;comment_count&#x27;</span>])  <span class="comment"># 评论数</span></span><br><span class="line">            comment.append(item[<span class="string">&#x27;url&#x27;</span>])  <span class="comment"># 回答链接</span></span><br><span class="line">            comment.append(item[<span class="string">&#x27;created_time&#x27;</span>])  <span class="comment"># 回答时间</span></span><br><span class="line">            comment.append(item[<span class="string">&#x27;content&#x27;</span>]) <span class="comment"># 回答时间</span></span><br><span class="line">            comments.append(comment)   </span><br><span class="line">        <span class="keyword">return</span> comments    </span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(comment)</span><br><span class="line">        print(e)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_file</span>(<span class="params">question_num</span>):</span></span><br><span class="line"><span class="comment">#      &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：根据知乎的问题id构造文件路径和文件名，csv、png都会用到</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         question_num：知乎提问的问题id</span></span><br><span class="line"><span class="comment">#     返回：csv文件路径、csv文件名</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    current_path = os.getcwd()</span><br><span class="line">    file_name=str(question_num)+<span class="string">&#x27;_&#x27;</span>+str(datetime.datetime.now())+<span class="string">&#x27;.csv&#x27;</span></span><br><span class="line">    path = current_path+<span class="string">&#x27;/&#x27;</span>+file_name</span><br><span class="line">    print(<span class="string">&quot;文件：&quot;</span>+path)</span><br><span class="line">    <span class="keyword">return</span> path,file_name</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_data</span>(<span class="params">comments,header,path</span>):</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：将comments中的信息输出到文件/数据库中</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         comments：将要保存的数据  </span></span><br><span class="line"><span class="comment">#     返回：无</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    dataframe = pd.DataFrame(comments)</span><br><span class="line">    <span class="keyword">if</span> header:</span><br><span class="line">        dataframe.to_csv(path, mode=<span class="string">&#x27;a&#x27;</span>, index=<span class="literal">False</span>, sep=<span class="string">&#x27;,&#x27;</span>, header=[<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;gender&#x27;</span>,<span class="string">&#x27;voteup&#x27;</span>,<span class="string">&#x27;cmt_count&#x27;</span>,<span class="string">&#x27;ans_url&#x27;</span>,<span class="string">&#x27;ans_time&#x27;</span>,<span class="string">&#x27;ans_content&#x27;</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dataframe.to_csv(path, mode=<span class="string">&#x27;a&#x27;</span>, index=<span class="literal">False</span>, sep=<span class="string">&#x27;,&#x27;</span>, header=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crawler</span>(<span class="params">question_num</span>):</span></span><br><span class="line"><span class="comment">#      &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：爬虫功能的组织函数，爬取数据并存储到csv文件中</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         question_num：知乎提问的问题id</span></span><br><span class="line"><span class="comment">#     返回：文件名</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    url_start = <span class="string">&#x27;https://www.zhihu.com/api/v4/questions/&#x27;</span>+str(question_num)+<span class="string">&#x27;/answers?include=data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cis_labeled%3Bdata%5B%2A%5D.mark_infos%5B%2A%5D.url%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%2A%5D.topics&amp;limit=5&amp;offset=&#x27;</span></span><br><span class="line">    url_end=<span class="string">&#x27;&amp;platform=desktop&amp;sort_by=default&#x27;</span></span><br><span class="line">    html=get_data(url_start+str(<span class="number">5</span>)+url_end)</span><br><span class="line">    totals=json.loads(html)[<span class="string">&#x27;paging&#x27;</span>][<span class="string">&#x27;totals&#x27;</span>]</span><br><span class="line">    path,file_name=get_file(question_num)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">&quot;总回答数：&quot;</span>+str(totals))</span><br><span class="line">    page = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span>(page &lt; totals):</span><br><span class="line">        url = url_start+str(page) +url_end</span><br><span class="line"> </span><br><span class="line">        html = get_data(url)</span><br><span class="line">        comments = parse_data(html)</span><br><span class="line">        <span class="keyword">if</span> page==<span class="number">0</span>:</span><br><span class="line">            save_data(comments,<span class="literal">True</span>,path)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            save_data(comments,<span class="literal">False</span>,path)</span><br><span class="line">        page += <span class="number">5</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> file_name</span><br></pre></td></tr></table></figure><h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><p>def save_data(comments,header,path)将爬到的数据存到了.CSV中，数据读取函数则将CSV中的内容读到内存里供。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_csv</span>(<span class="params">file_name</span>):</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：读取爬虫函数得到的csv文件</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         file_name：文件名  </span></span><br><span class="line"><span class="comment">#     返回：csv文件的list</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> open(file_name, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        reader = csv.reader(f)</span><br><span class="line">        result = list(reader)</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>爬取下来的数据需要预处理。<strong>主要是因为所关注的“ans_content”内，除了中文之外，还有一些奇奇怪怪的东西</strong>：</p><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/2.png" title="Optional title"></p><p>可以看到，<strong>有emoji、有非中文的特殊字符、有h5的标签</strong>（由&lt;&gt;括起的部分，主要由于回答中的图片的网页链接需要用这种方式在网页源码内引用）。<strong>使用正则表达式去掉这些对中文文本分析没有用的内容。</strong></p><p>其他的一些处理还包括<strong>秒级时间戳的转换和知乎用户系统中的“匿名用户”进行编号等</strong>。这些无关紧要，只是为了看着舒服。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">date_pre_treatment</span>(<span class="params">csv_data</span>):</span></span><br><span class="line"><span class="comment">#      &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：数据预处理</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         csv_data：已经读取好的list  </span></span><br><span class="line"><span class="comment">#     返回：经过了预处理的list</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    niming_num=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(csv_data[<span class="number">0</span>])):</span><br><span class="line">        <span class="keyword">if</span> csv_data[<span class="number">0</span>][j]==<span class="string">&#x27;name&#x27;</span>:</span><br><span class="line">            <span class="keyword">for</span> i1 <span class="keyword">in</span> range(len(csv_data)):</span><br><span class="line">                <span class="comment"># 将知乎用户、匿名用户的名称统一，加上编号做区分</span></span><br><span class="line">                <span class="keyword">if</span> csv_data[i1][j]==<span class="string">&#x27;知乎用户&#x27;</span> <span class="keyword">or</span> csv_data[i1][j]==<span class="string">&#x27;匿名用户&#x27;</span>:</span><br><span class="line">                    csv_data[i1][j]=<span class="string">&#x27;匿名用户&#x27;</span>+str(++niming_num)</span><br><span class="line">        <span class="keyword">if</span> csv_data[<span class="number">0</span>][j]==<span class="string">&#x27;ans_time&#x27;</span>:</span><br><span class="line">            <span class="keyword">for</span> i2 <span class="keyword">in</span> range(len(csv_data)<span class="number">-1</span>):</span><br><span class="line">                ltime = time.localtime(int(csv_data[i2+<span class="number">1</span>][j]))</span><br><span class="line">                <span class="comment"># 将秒级时间戳转化为&quot;年月日时分秒&quot;格式</span></span><br><span class="line">                csv_data[i2+<span class="number">1</span>][j]=time.strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>,ltime)</span><br><span class="line">        <span class="keyword">if</span> csv_data[<span class="number">0</span>][j]==<span class="string">&#x27;ans_content&#x27;</span>:</span><br><span class="line">            <span class="keyword">for</span> i3 <span class="keyword">in</span> range(len(csv_data)):</span><br><span class="line">                <span class="comment"># 去除文本信息中的h5标签</span></span><br><span class="line">                csv_data[i3][j] = re.sub(<span class="string">u&quot;\\&lt;.*?\\&gt;&quot;</span>, <span class="string">&quot; &quot;</span>, csv_data[i3][j])</span><br><span class="line">                <span class="comment"># 去除文本信息除中文、英文、ascll字符、常用标点以外的所有内容</span></span><br><span class="line">                csv_data[i3][j] = re.sub(<span class="string">&quot;[^\u4e00-\u9fa5 ^a-z ^A-Z ^0-9 ^~!@#$%&amp;*()_+-=:;,.^～！，。？、《》]&quot;</span>,<span class="string">&#x27;&#x27;</span>, csv_data[i3][j])</span><br><span class="line">        <span class="keyword">if</span> csv_data[<span class="number">0</span>][j]==<span class="string">&#x27;voteup&#x27;</span>:</span><br><span class="line">            <span class="keyword">for</span> i4 <span class="keyword">in</span> range(len(csv_data)<span class="number">-1</span>):</span><br><span class="line">                <span class="comment"># 将投票数转化为int（读取的时候会存储为str）</span></span><br><span class="line">                csv_data[i4+<span class="number">1</span>][j] = int(float(csv_data[i4+<span class="number">1</span>][j]))</span><br><span class="line">    <span class="keyword">return</span> csv_data</span><br></pre></td></tr></table></figure><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/3.png" title="Optional title"></p><h1 id="词云"><a href="#词云" class="headerlink" title="词云"></a>词云</h1><p>使用wordcloud包进行所有回答文本的词云的绘制，并存储为png图片文件。用到的功能为WordCloud，参数和说明见代码注释。</p><p><strong>词云的图如果想做的有意义且好看，最主要的参数是stopwords、max_words。</strong></p><p>其中，stopwords指的是词云里什么词不能出现，默认为空。毕竟不管是中文还是英文，文本里都有许多没有意义的词，比如语气词“嗯”、“啊”、“呢”这种类似的。所以如果不加限制的话，词云里最大的那个词一定是没什么意义的词（知乎作为一个网络社区，大家的回答肯定是偏向口头语的，那么无意义的词就会多起来，毕竟没人在知乎上写论文是吧）。这里的stopwords是通过txt导入的。stopwords网上能找到许多，我是在GitHub上找到了一份中文停用词，直接拿过来用了。后期又在里面填了一些自己不想看见的词。</p><p>max_words就是图片上最多出现多少个词，个人感觉还是多一些好看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud</span><br><span class="line">now_time= datetime.datetime.now()</span><br><span class="line">print(<span class="string">&quot;2.词云&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置数据，将所有文本放到一个list里方便处理</span></span><br><span class="line">text = <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(csv_data)<span class="number">-1</span>):</span><br><span class="line">    text+=csv_data[i+<span class="number">1</span>][<span class="number">6</span>]+<span class="string">&#x27; &#x27;</span></span><br><span class="line"><span class="comment"># 结巴中文分词，生成字符串，默认精确模式，如果不通过分词，无法直接生成正确的中文词云</span></span><br><span class="line">cut_text = jieba.cut(text)</span><br><span class="line"><span class="comment"># 必须给个符号分隔开分词结果来形成字符串,否则不能绘制词云</span></span><br><span class="line">result = <span class="string">&quot; &quot;</span>.join(cut_text)</span><br><span class="line"><span class="comment"># 从文件中获取停用词</span></span><br><span class="line">stopwords=get_stopwords()</span><br><span class="line"><span class="comment"># 词云设置</span></span><br><span class="line">wc = WordCloud(     </span><br><span class="line">        font_path=<span class="string">&quot;仿宋_GB2312.ttf&quot;</span>,<span class="comment"># 设置字体（不指定就会出现乱码）</span></span><br><span class="line">        background_color=<span class="string">&#x27;white&#x27;</span>,<span class="comment"># 设置背景色</span></span><br><span class="line">        width=<span class="number">1500</span>, height=<span class="number">900</span>,<span class="comment"># 设置背景宽、高</span></span><br><span class="line">        max_font_size=<span class="number">400</span>, min_font_size=<span class="number">20</span>,<span class="comment"># 设置字体大小上下限</span></span><br><span class="line">        stopwords = stopwords,<span class="comment"># 停用词</span></span><br><span class="line">        max_words=<span class="number">150</span> <span class="comment"># 图片中显示词的最大数量</span></span><br><span class="line">        )</span><br><span class="line"><span class="comment"># 产生词云</span></span><br><span class="line">wc.generate(result)</span><br><span class="line"><span class="comment"># 保存图片</span></span><br><span class="line">pic_name=re.sub(<span class="string">&#x27;.csv&#x27;</span>,<span class="string">&#x27;&#x27;</span>,file_name)+<span class="string">&#x27;_wordcloud.png&#x27;</span></span><br><span class="line">wc.to_file(pic_name) </span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">10</span>))</span><br><span class="line">plt.imshow(wc)</span><br><span class="line">plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Runtime:%d s&#x27;</span>%(datetime.datetime.now()-now_time).seconds)</span><br></pre></td></tr></table></figure><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/4.png" title="Optional title"></p><h1 id="keywords"><a href="#keywords" class="headerlink" title="keywords"></a>keywords</h1><p>使用jieba包进行关键词提取。</p><p>Jieba提供了两种关键词的提取算法，分别是：</p><ul><li><p>基于TF-IDF（term frequency–inverse document frequency）算法的关键词抽取。函数参数如下：sentence：待提取的文本；topK：返回topK个 TF/IDF 权重最大的关键词；withWeight：是否一并返回关键词权重值，默认值为False；allowPOS：仅包括指定词性的词，默认值为空，即不筛选。</p></li><li><p>基于TextRank算法的关键词抽取。函数接口同TF-IDF相同。不同的是allowPOS默认指定了一些词性的词。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba.analyse</span><br><span class="line">now_time= datetime.datetime.now()</span><br><span class="line">print(<span class="string">&quot;3.keywords&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;3.1. TF-IDF&quot;</span>)</span><br><span class="line">keywords1=jieba.analyse.extract_tags(text, topK=<span class="number">20</span>, withWeight=<span class="literal">True</span>, allowPOS=(<span class="string">&#x27;ns&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;vn&#x27;</span>, <span class="string">&#x27;v&#x27;</span>))</span><br><span class="line">print(keywords1)</span><br><span class="line">print(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;3.2. textrank&quot;</span>)</span><br><span class="line">keywords2=jieba.analyse.textrank(text, topK=<span class="number">20</span>, withWeight=<span class="literal">True</span>, allowPOS=(<span class="string">&#x27;ns&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;vn&#x27;</span>, <span class="string">&#x27;v&#x27;</span>)) </span><br><span class="line">print(keywords2)</span><br><span class="line">print(<span class="string">&#x27;Runtime:%d s&#x27;</span>%(datetime.datetime.now()-now_time).seconds)</span><br></pre></td></tr></table></figure><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/5.png" title="Optional title"></p><h1 id="情感分析"><a href="#情感分析" class="headerlink" title="情感分析"></a>情感分析</h1><p>使用snownlp进行情感分析。</p><p>snownlp是一个python类库，可以方便的处理中文文本内容，是受到了TextBlob的启发而写的，并且和TextBlob不同的是，这里没有用NLTK，所有的算法都是重新实现的，并且自带了一些训练好的字典。</p><blockquote><p>snownlp: <a href="https://github.com/isnowfy/snownlp">https://github.com/isnowfy/snownlp</a><br>TextBlob: <a href="https://github.com/sloria/TextBlob">https://github.com/sloria/TextBlob</a></p></blockquote><p><strong>snownlp给出了基于贝叶斯分类的情感分析函数SnowNLP。对于每一条文本，该函数会给出一个0-1之间的评分，越接近1代表积极情绪占比越高。</strong></p><p>使用SnowNLP对每一条回答文本进行情感分析，并以评分达到<strong>0.6、0.5认定为是积极文本</strong>，分别给出积极回答文本在所有文本中的比重。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> snownlp <span class="keyword">import</span> SnowNLP</span><br><span class="line">now_time= datetime.datetime.now()</span><br><span class="line">print(<span class="string">&quot;4.情感分析&quot;</span>)</span><br><span class="line">s=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(csv_data)<span class="number">-1</span>):</span><br><span class="line">    a=SnowNLP(csv_data[i+<span class="number">1</span>][<span class="number">6</span>])</span><br><span class="line">    s.append(round(a.sentiments,<span class="number">3</span>))</span><br><span class="line">s.sort(reverse=<span class="literal">True</span>)</span><br><span class="line">flag06=<span class="number">-1</span></span><br><span class="line">flag05=<span class="number">-1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(s)):</span><br><span class="line">    <span class="keyword">if</span> s[i]&lt;<span class="number">0.5</span> :</span><br><span class="line">        flag05=i</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(s)):</span><br><span class="line">    <span class="keyword">if</span> s[i]&lt;<span class="number">0.6</span> :</span><br><span class="line">        flag06=i</span><br><span class="line">        <span class="keyword">break</span>    </span><br><span class="line">print(<span class="string">&quot;积极（60%）百分比：&quot;</span>,round(flag06/(len(s)),<span class="number">2</span>)) </span><br><span class="line">print(<span class="string">&quot;积极（50%）百分比：&quot;</span>,round(flag05/(len(s)),<span class="number">2</span>))         </span><br><span class="line">print(<span class="string">&#x27;Runtime:%d s&#x27;</span>%(datetime.datetime.now()-now_time).seconds)</span><br></pre></td></tr></table></figure><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/6.png" title="Optional title"></p><p><strong>两个评分阈值下的大部分的回答都被认为是积极的，这也符合这个问题的背景：新年。</strong></p><h1 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h1><p>对点赞数最多的100条数据进行聚类。</p><p>进行涉及文本的向量化表示。sklearn提供了传统的词袋模型。使用sklearn中的TfidfVectorizer计算tf-idf矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line">print(<span class="string">&#x27;5. 文本聚类&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;5.1. 准备点赞数最多的100条数据&#x27;</span>)</span><br><span class="line">user_id=[]</span><br><span class="line">content=[]</span><br><span class="line">csv_data_sorted=csv_data</span><br><span class="line"><span class="keyword">del</span>(csv_data_sorted[<span class="number">0</span>])</span><br><span class="line">csv_data_sorted=sorted(csv_data,key=itemgetter(<span class="number">2</span>))</span><br><span class="line"><span class="comment"># print(csv_data[:10])</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    user_id.append(csv_data_sorted[i][<span class="number">0</span>])</span><br><span class="line">    content.append(csv_data_sorted[i][<span class="number">6</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line">now_time= datetime.datetime.now()</span><br><span class="line">print(<span class="string">&#x27;5.2. tfidf matrix&#x27;</span>)</span><br><span class="line"><span class="comment">#max_df: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.</span></span><br><span class="line"><span class="comment">#min_df: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.</span></span><br><span class="line">tfidf_vectorizer = TfidfVectorizer(max_df=<span class="number">0.9</span>, max_features=<span class="number">200000</span>,min_df=<span class="number">0.1</span>, stop_words=<span class="string">&#x27;english&#x27;</span>,use_idf=<span class="literal">True</span>, tokenizer=segment)</span><br><span class="line">tfidf_matrix = tfidf_vectorizer.fit_transform(content) <span class="comment">#fit the vectorizer to synopses</span></span><br><span class="line">print(tfidf_matrix.shape)</span><br><span class="line">print(<span class="string">&#x27;Runtime:%d s&#x27;</span>%(datetime.datetime.now()-now_time).seconds)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scipy.cluster.hierarchy <span class="keyword">import</span> ward, dendrogram, linkage</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line">now_time= datetime.datetime.now()</span><br><span class="line">print(<span class="string">&#x27;5.3. linkage matrix&#x27;</span>)</span><br><span class="line">dist = <span class="number">1</span> - cosine_similarity(tfidf_matrix)</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>]=[<span class="string">&#x27;Microsoft YaHei&#x27;</span>] <span class="comment">#用来正常显示中文标签</span></span><br><span class="line">linkage_matrix = linkage(dist, method=<span class="string">&#x27;ward&#x27;</span>, metric=<span class="string">&#x27;euclidean&#x27;</span>, optimal_ordering=<span class="literal">False</span>)</span><br><span class="line">print(<span class="string">&#x27;Runtime:%d s&#x27;</span>%(datetime.datetime.now()-now_time).seconds)</span><br><span class="line">print(linkage_matrix)</span><br></pre></td></tr></table></figure><p>由于选取了100条回答，tf-idf矩阵的第一维为100。</p><p>然后根据tf-idf矩阵进行层次聚类，给出linkage矩阵。使用函数：scipy.cluster.hierarchy.linkage(y, method=’single’, metric=’euclidean’, optimal_ordering=False)。其中，计算新形成的聚类簇u和v之间距离的方法是用的是single，即最近邻点算法。</p><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/7.png" title="Optional title"></p><p>对上述矩阵进行可视化，并将结果保存为png文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">now_time= datetime.datetime.now()</span><br><span class="line">print(<span class="string">&#x27;5.4. linkage matrix可视化&#x27;</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">40</span>, <span class="number">15</span>))</span><br><span class="line">plt.title(<span class="string">&#x27;层次聚类树状图&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;知乎用户名称&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;距离（越低表示文本越类似）&#x27;</span>)</span><br><span class="line">dendrogram(</span><br><span class="line">    linkage_matrix,</span><br><span class="line">    labels=user_id, </span><br><span class="line">    leaf_rotation=<span class="number">-70</span>,  <span class="comment"># rotates the x axis labels</span></span><br><span class="line">    leaf_font_size=<span class="number">12</span>  <span class="comment"># font size for the x axis labels</span></span><br><span class="line">)</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>]=[<span class="string">&#x27;SimHei&#x27;</span>] <span class="comment">#用来正常显示中文标签</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span> <span class="comment">#用来正常显示负号</span></span><br><span class="line">plt.savefig(re.sub(<span class="string">&#x27;.csv&#x27;</span>,<span class="string">&#x27;&#x27;</span>,file_name)+<span class="string">&#x27;_linkage.png&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;Runtime:%d s&#x27;</span>%(datetime.datetime.now()-now_time).seconds)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/8.png" title="Optional title"></p><p>其中，横坐标为知乎用户名，每种的线连起来的用户名代表这些用户的回答可被分为相似的一类。</p><p>（看到这里明白为什么只选100个了吧，因为选多了图就画不下了）</p><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>在代码中更改需要爬取的知乎问题的问题id，就可以实现对任意知乎问题下的所有回答内容的上述操作。</p><p>代码每次运行都会按照以“问题id+时间”为文件名进行各项文件的保存，不会覆盖之前运行所保留的文件。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><blockquote><p>Python网络爬虫实战：爬取知乎话题下 18934 条回答数据-csdn<br><a href="https://blog.csdn.net/wenxuhonghe/article/details/86515558">https://blog.csdn.net/wenxuhonghe/article/details/86515558</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;刚好在摸鱼的时候比较喜欢刷知</summary>
      
    
    
    
    
    <category term="社会媒体计算" scheme="http://example.com/tags/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>想分享给别人看的一些影像</title>
    <link href="http://example.com/2021/04/14/%E6%83%B3%E5%88%86%E4%BA%AB%E7%BB%99%E5%88%AB%E4%BA%BA%E7%9C%8B%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BD%B1%E5%83%8F/"/>
    <id>http://example.com/2021/04/14/%E6%83%B3%E5%88%86%E4%BA%AB%E7%BB%99%E5%88%AB%E4%BA%BA%E7%9C%8B%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BD%B1%E5%83%8F/</id>
    <published>2021-04-14T07:23:04.000Z</published>
    <updated>2021-11-15T10:33:35.208Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创内容。如果喜欢，告知后自取即可。</p></blockquote><hr><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>仪式感是很重要的东西。喜欢把生活中的内容留个纪念，这里就用来留一些想给别人看的图片。</p><p>不定期更新。</p><hr><blockquote><p>2021.4.14更新。</p></blockquote><p><img src="/images/%E7%9B%B8%E5%86%8C/1.jpg" title="打羽毛球"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/2.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/3.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/4.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/5.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/6.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/7.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/8.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/9.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/10.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/11.jpg" title="打羽毛球"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/12.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/13.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/14.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/15.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/16.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/17.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/18.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/19.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/20.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/21.jpg" title="打羽毛球"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/22.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/23.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/24.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/25.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/26.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/27.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/28.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/29.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/30.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/31.jpg" title="打羽毛球"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/32.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/33.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/34.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/35.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/36.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/37.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/38.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/39.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/40.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/41.jpg" title="打羽毛球"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/42.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/43.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/44.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/45.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/46.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/47.jpg" title="Optional title"></p><!-- ![](/images/相册/48.jpg "Optional title") --><p><img src="/images/%E7%9B%B8%E5%86%8C/49.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/50.JPG" title="Optional title"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创内容。如果喜欢，告知后自取即可。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;仪式感是很重要的东西。喜</summary>
      
    
    
    
    
    <category term="闲谈" scheme="http://example.com/tags/%E9%97%B2%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：Zoom to learn, learn to zoom</title>
    <link href="http://example.com/2021/04/12/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AZoom-to-learn-learn-to-zoom/"/>
    <id>http://example.com/2021/04/12/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AZoom-to-learn-learn-to-zoom/</id>
    <published>2021-04-12T02:29:43.000Z</published>
    <updated>2021-04-14T07:23:41.397Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>变焦学习，学习变焦</p></blockquote><blockquote><p>CVPR 2019</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>变焦功能是当今手机和相机的必备功能。</p><p>光学变焦是图像变焦的最佳选择，可以保持较高的图像质量。但变焦镜头价格昂贵且其物理组件比较笨重。数码变焦方法是一种降低成本的选择，但数码变焦只是简单地向上采样相机传感器输入的裁剪区域，产生模糊的输出。</p><p>文章提出，常规的SISR方法有以下两个限制：</p><ul><li><p>现有的大多数方法使用的是合成方法来逼近真实变焦，即其中输入图像是高分辨率图像的下采样版本。这种方法间接降低了输入中的噪声水平，但实际上，由于在曝光时间内进入光圈的光子更少，遥远物体的区域往往包含更多的噪声。</p></li><li><p>其次，现有的大多数方法都是从8位RGB图像开始的，该图像由相机图像信号处理器（ISP）处理，ISP将高位原始传感器数据中的高频信号用于其他目标（例如降噪）。</p></li></ul><blockquote><p>第二点的解读：本文训练的数据均是Raw Data，这是专业单反拍摄的格式，而RGB图片是Raw Data经过图像处理器（Image SIgnal Processer, ISP）制作的，在某种程度上来说，RGB图片也是有损的。</p></blockquote><p>基于上述限制，文章做了以下内容：</p><ul><li><p>使用真实的高位传感器数据进行计算变焦的实用性，而不是处理8位RGB图像或合成传感器模型。</p></li><li><p>新的数据集SR-RAW，它是第一个具有光学ground truth的超分辨率原始数据集。SR-RAW是用变焦镜头拍摄的。对于焦距较短的图像，长焦距图像作为光学ground truth。</p></li><li><p>提出了一种新的上下文双边损失（CoBi）处理稍微失调的图像对。</p></li></ul><h1 id="SR-RAW"><a href="#SR-RAW" class="headerlink" title="SR-RAW"></a>SR-RAW</h1><h2 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h2><p>使用24-240毫米变焦镜头来收集不同光学变焦水平的原始图像对。</p><p>采集过程中，在每个场景的7个光学变焦设置下采集了7幅图像。来自7幅图像序列的每一对图像形成一个数据对。总共500个室内外场景，ISO从100到400。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/1.png" title="Optional title"></p><p><strong>在训练模型中，以短焦距原始传感器图像作为输入，以长焦距RGB图像作为超分辨率的基础。</strong></p><blockquote><p>例如，使用70mm焦距拍摄的RGB图像作为使用35mm焦距拍摄的原始传感器数据的2X缩放地面真相。</p></blockquote><p>相机有特殊设置，来自原文：</p><ul><li><p>景深（DOF）随着焦距的变化而变化，调整孔径大小使每个焦距的DOF相同是不现实的。选择一个小的光圈尺寸（至少f/20）来最小化DOF差异（在图2的B2中仍然可以看到），使用一个三脚架来捕捉长时间曝光的室内场景。</p></li><li><p>其次，使用相同的曝光时间的所有图像在一个序列，使噪音水平不受焦距变化的影响。但是仍然观察到由于快门和物理光瞳是机械的并且涉及到动作变化而引起的明显的光照变化。这种颜色的变化是避免使用像素对像素的损失进行训练的另一个动机。</p></li><li><p>第三，虽然透视不随焦距的变化而变化，但当镜头放大或缩小时，在投影中心存在微小的变化（镜头的长度），在不同深度的物体之间产生明显的透视变化（图2 B1）。使用的Sony FE 24-240mm镜头，需要与被摄对象至少56.4米的距离，才能在相距5米的物体之间产生小于1像素的透视位移。因此，避免捕获非常接近的对象，但允许在数据集中进行这样的透视图转换。</p></li></ul><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>对于任意一对训练图像，用RGB-L表示低分，用RAW-L表示高分，也就是相机的传感器的数据，GT。</p><p>高分GT，使用RGB-H和RAW-H。首先匹配RAW-H和RGB-H之间的视图(FOV)。然后计算RGB-L和RGB-H之间的对齐（手动缩放相机以调整焦距所引起的相机轻微移动）。</p><p>使用一个欧几里德运动模型，通过增强相关系数最小化来实现图像的旋转和平移。</p><p>训练时，将匹配FOV的RAW-L作为输入，它的GT是RGB-H，与RAW-L对齐并具有相同的FOV。如果光学变焦与目标变焦比不完全匹配，则对图像应用比例偏移。</p><blockquote><p>例如，如果使用（35mm，150mm）训练一个4X变焦模型，那么目标图像的偏移量为1.07。</p></blockquote><h2 id="非对齐分析"><a href="#非对齐分析" class="headerlink" title="非对齐分析"></a>非对齐分析</h2><p>预处理步骤很难消除偏差。由于捕捉的数据焦距不同，视角的变化会导致视角的不对齐。此外，当对不同分辨率的图像进行对齐时，高分辨率图像中的锐边不能与低分辨率图像中的模糊边精确对齐(图2 B3)。</p><blockquote><p>SR-RAW中描述的失调通常会导致800万像素图像对中的40-80像素偏移。</p></blockquote><blockquote><p>该数据集中的HR和LR是不是对齐的，这也是后面给出的算法所解决的问题之一，非常重要。</p></blockquote><h1 id="Contextual-Bilateral-Loss"><a href="#Contextual-Bilateral-Loss" class="headerlink" title="Contextual Bilateral Loss"></a>Contextual Bilateral Loss</h1><h2 id="Contextual-Loss"><a href="#Contextual-Loss" class="headerlink" title="Contextual Loss"></a>Contextual Loss</h2><p>CoBi Loss来自Contextual Loss。Contextual Loss的原文中的叙述为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/2.png" title="Optional title"></p><blockquote><p>Contextual Loss来自《The Contextual Loss for Image Transformation with Non-Aligned Data》，ECCV 2018。<br><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Roey_Mechrez_The_Contextual_Loss_ECCV_2018_paper.pdf">https://openaccess.thecvf.com/content_ECCV_2018/papers/Roey_Mechrez_The_Contextual_Loss_ECCV_2018_paper.pdf</a><br><a href="https://www.github.com/roimehrez/contextualLoss">https://www.github.com/roimehrez/contextualLoss</a></p></blockquote><blockquote><p>Contextual Loss的原文中是跟Perceptual Loss进行对比的。<br>Perceptual Loss来自《Perceptual Losses for Real-Time Style Transfer and Super-Resolution》，ECCV 2016。<br>SRGAN的Loss就是从这篇文章来的。</p></blockquote><p>本篇的Contextual Loss的公式：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/3.png" title="Optional title"></p><h2 id="CoBi-Loss"><a href="#CoBi-Loss" class="headerlink" title="CoBi Loss"></a>CoBi Loss</h2><p>作者用这个Contextual Loss去训练模型发现会出现很多artifacts。作者认为这是由于CX损失函数中不准确的特征匹配造成的。<br>受到保边滤波器的启发，作者将空间区域也加入到损失函数中：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/4.png" title="Optional title"></p><p>又借鉴了Perceptual Loss，引入VGG loss。本文最终的Loss为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/5.png" title="Optional title"></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><h2 id="实验步骤"><a href="#实验步骤" class="headerlink" title="实验步骤"></a>实验步骤</h2><p>使用来自SR-RAW的图像来训练一个4X模型和一个8X模型。<strong>采用了一个16层的ResNet架构，然后是log_2( N + 1) 上卷积层，其中N是缩放因子。</strong></p><blockquote><p>文章所使用的模型没有图和其他说明，只有这一句文字叙述。</p></blockquote><p>将SR-RAW中的500个序列按8:1:1的比例分割为训练、验证和测试集。</p><p>对于4X变焦模型，每个序列有3对数据对用于训练。对于8X变焦模型，每个序列有1对数据。</p><p>每对包含一个全分辨率（800万像素）Bayer mosaic图像及其相应的全分辨率光学放大RGB图像。随机从一个全分辨率的Bayer mosaic中裁剪64个patch作为训练的输入。</p><blockquote><p><a href="https://petapixel.com/2017/03/03/x-trans-vs-bayer-sensors-fantastic-claims-test/">https://petapixel.com/2017/03/03/x-trans-vs-bayer-sensors-fantastic-claims-test/</a></p></blockquote><p>选择了几种具有代表性的超分辨方法进行比较：SRGAN、SRResnet、LapSRN、Johnson等人提出的Perceptual Loss的模型以及ESRGAN。</p><p>使用公共的预训练模型。首先采用原文献中的标准设置尝试在SR-RAW上微调模型，对比模型的下采样方式是双三次。但与未经微调的预训练模型相比，平均性能差异不大(SSIM &lt; 0.04, PSNR &lt; 0.05, lpip &lt; 0.025)，所以原文直接采用了没有微调的原模型。对于没有预先训练模型的方法，在SR-RAW上从零开始训练它们的模型。</p><h2 id="定量分析"><a href="#定量分析" class="headerlink" title="定量分析"></a>定量分析</h2><h3 id="不同模型使用SR-RAW进行训练"><a href="#不同模型使用SR-RAW进行训练" class="headerlink" title="不同模型使用SR-RAW进行训练"></a>不同模型使用SR-RAW进行训练</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/6.png" title="Optional title"></p><p>结果很明显，现有的超分模型在这个数据集上的表现比较差。</p><h3 id="不同训练策略"><a href="#不同训练策略" class="headerlink" title="不同训练策略"></a>不同训练策略</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/7.png" title="Optional title"></p><p>依旧证明了本文提出的模型和数据是比较契合的。</p><blockquote><p>Ours-png：为了进行比较，使用经过8位处理的RGB图像来训练模型的副本(our-png)，以评估拥有真实原始传感器数据的好处。与5.1节中描述的合成设置不同，没有使用向下采样的RGB图像作为输入，而是使用较短焦距拍摄的RGB图像作为输入。用较长焦距拍摄的RGB图像作为地面真实值。</p></blockquote><blockquote><p>Ours-syn-raw：测试合成的原始数据是否可以代替训练的感知真实数据。采用Gharbi等人描述的标准传感器合成模型[9]代替真实的传感器数据进行训练，从8位RGB图像中生成合成的Bayer马赛克。简而言之，我们根据白平衡、gammacorrected sRGB图像的Bayer镶嵌模式，每个像素保留一个颜色通道，并引入随机方差高斯噪声。在这些合成传感器数据上训练我们的模型的一个副本，并在经过白平衡和伽玛校正的真实传感器数据上进行测试。</p></blockquote><h2 id="定性分析"><a href="#定性分析" class="headerlink" title="定性分析"></a>定性分析</h2><p>视觉效果：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/8.png" title="Optional title"></p><p>可以看到，给出的图示在色块交界区域的效果确实不错。</p><p>以及在Amazon Mechanical Turk上进行感知实验来评估生成图像的感知质量。有50人参加了这个测试：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/9.png" title="Optional title"></p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><ul><li><p>使用RAW进行超分，相比经过ISP处理的JPG，RAW有丰富的信息。</p></li><li><p>提出CoBi Loss解决不对齐的问题。</p></li></ul><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>文章：<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8954193&amp;tag=1">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8954193&amp;tag=1</a></p></li><li><p>补充材料：<a href="https://ceciliavision.github.io/files/Zoom_Learn_Supplementary_Material_CVPR2019.pdf">https://ceciliavision.github.io/files/Zoom_Learn_Supplementary_Material_CVPR2019.pdf</a></p></li></ul><blockquote><p>补充材料还挺长，11页pdf。</p></blockquote><ul><li>code：<a href="https://github.com/ceciliavision/zoom-learn-zoom">https://github.com/ceciliavision/zoom-learn-zoom</a></li></ul><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>看上一篇的时候，看到第三部分的那个数据集，顺手去看了一下那个reference，然后顺手找到了这篇文章。看完之后觉得还是很有意义的。</p><p>说句实话这个方法初看下来也算那种大神级别的了。感觉以后用不对齐做超分好像也不是难事情？也许还是我们思路不够开阔。还是要敢想啊。</p><p>但是再一细看，局限性感觉也是很大。</p><p>首先一点就是第一部分针对不同模型的对比实验，感觉不是很公平。毕竟是从Loss方面做的改变，直接就比PSNR和SSIM，感觉有点流氓，意义不大。</p><p>第二个就是文章中也没特别解释他们用的网络模型和原因，读着有点难受。</p><p>第三个就是感觉不对齐这个问题还是太广了，这个好像只是水平偏移，如果偏移的更多样的话，不知道会有怎么样的解决思路。感觉这一点能用到光学显微镜成像的那里。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;变焦学习，学习变焦&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;CVPR 2019&lt;/p&gt;
&lt;/blockquo</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>能够提高个人体验的计算机软硬件问题合集</title>
    <link href="http://example.com/2021/04/03/%E8%83%BD%E5%A4%9F%E6%8F%90%E9%AB%98%E4%B8%AA%E4%BA%BA%E4%BD%93%E9%AA%8C%E7%9A%84%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/"/>
    <id>http://example.com/2021/04/03/%E8%83%BD%E5%A4%9F%E6%8F%90%E9%AB%98%E4%B8%AA%E4%BA%BA%E4%BD%93%E9%AA%8C%E7%9A%84%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/</id>
    <published>2021-04-03T11:20:56.000Z</published>
    <updated>2022-03-04T10:45:21.028Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>经验内容，欢迎转载。</p></blockquote><hr><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>一个新坑。这个坑实际上好久之前就想开了（没错，就是在根目录上给777权限的那次）。</p><p>自诩是一个电脑维修铺子的老板，软硬件什么的也是天天都在摸索和学习。科班只教了如何搞软件，现在大家也都在一股脑的搞软件。但是碰到了硬件问题，好像大家都没那么感兴趣？只是作为一个喜欢搞机和经常胡乱搞机的人，常常会碰到这类问题。从硬件购买安装到操作系统层面的软件配置问题，每一次遇到了，每一次去解决，都是新的尝试和学习。恰好这段时间从zkyzdhs那边搞到了一台主机，有条件搞机了，因此借这个机会开新坑。</p><p>这里将记录在搞机过程中遇到的一切非专业但有意义的问题。随时更新，标题间几乎无关联。</p><p>实际上，遇到问题之后，基本的解决方法就是百度，然后选择靠谱的方法自己进行测试。唯一不同的是，我将会在这里更加详细的记录细节，尤其是硬件的型号或者时间或者是具体的软硬件版本这类的内容，毕竟自己也是一个怕出错的人，我要是查百度，对于具体的细节查不到，是会非常恼火的。</p><hr><h1 id="windosw-10基础上安装Ubuntu-18-04"><a href="#windosw-10基础上安装Ubuntu-18-04" class="headerlink" title="windosw 10基础上安装Ubuntu 18.04"></a>windosw 10基础上安装Ubuntu 18.04</h1><blockquote><p>2021.4.3更新。</p></blockquote><blockquote><p>新搞来的主机，原本是带着Ubuntu 18的。结果我在装win 10双系统的时候，没看清硬盘名字，直接用win 10给覆盖上了。Ubuntu肯定是得有，所以得重新装一下，此为问题背景。以及学校的校园网，有线连接不支持Ubuntu，机箱内也无无线网卡，需要解决这个问题。</p></blockquote><p>直接上图：</p><h2 id="win10上的操作"><a href="#win10上的操作" class="headerlink" title="win10上的操作"></a>win10上的操作</h2><p>主要就是使用Rufus做系统盘。下载好iso，u盘准备好，按照提示做。</p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/1.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/2.png" title="Optional title"></p><p>出现这个是因为用的那个u盘之前被我分了两个盘，在做系统盘的时候是要合并成一个的。无脑点确定就完事了。</p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/3.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/4.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/5.png" title="Optional title"></p><p>以及看一下这个机箱的硬盘情况。就是手抖把win 10装到那个固态上了。另一块机械4个T，我也不知道当初他们买的时候想的是什么。主板上弄俩固态不香么？</p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/6.png" title="Optional title"></p><h2 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h2><p>因为要手动分区，所以操作比较多。</p><p>但是实际上手动分区之后，从bios启动ubuntu，直接选硬盘启动不了。装是装成功了的，重启了之后找不到启动位置这我也没办法。手动分区的过程在下面，是没有问题的。</p><p>以及如果直接选从整个硬盘安装的的话，是没有下面这么多步骤的，直接就进那个设置用户名和密码的页面的。</p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/7.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/8.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/9.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/10.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/11.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/12.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/13.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/14.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/15.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/16.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/17.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/18.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/19.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/20.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/21.png" title="Optional title"></p><p>这部分过程是手机拍的，糊了点儿，凑合看。</p><p>这部分的参考：</p><blockquote><p><a href="https://blog.csdn.net/fanxueya1322/article/details/90205143">https://blog.csdn.net/fanxueya1322/article/details/90205143</a><br><a href="https://rufus.ie/zh/">https://rufus.ie/zh/</a></p></blockquote><p><strong>以及不联网装的ubuntu连make和gcc都没有，血泪教训。</strong>后面还得解决这个问题，不过装系统肯定不叫事儿，随便装就行了。</p><h1 id="使用Mac中的小机器人搞定drop导致的-HEIC格式图片"><a href="#使用Mac中的小机器人搞定drop导致的-HEIC格式图片" class="headerlink" title="使用Mac中的小机器人搞定drop导致的.HEIC格式图片"></a>使用Mac中的小机器人搞定drop导致的.HEIC格式图片</h1><blockquote><p>2021.4.21更新。</p></blockquote><p>从ip上把图片drop到Mac上，默认的格式是.HEIC。去查了一下这个格式，发现是Apple在后面的更新中新加的机制，图片默认的存储格式就是这个，如果不设置的话，drop到其他苹果设备上，就会保持这个格式。这个格式可以在我的Mac上打开和查看，但好像其他设备和低版本的Mac OS就不得行了。</p><p>两个解决方法，一个是从ip上设置传输的时候选择兼容性最佳，就在相机设置里，很简单。但我这么做了之后，drop过来的图片将两种格式掺在一起drop给我了，个别图片还是保持了.HEIC格式，不知道为什么。</p><p>另一个就是直接将.HEIC转化为常见的.jpg或者.png。转化的话，网络上有很多工具，但实际上大家都知道不怎么靠谱，要么是广告，要么让你下载软件，我反正很讨厌这个，没有online的我是绝对不会用的。然后就在查百度的过程中看见了有人说可以用Mac自带的小机器人完成这项工作，所以就探索了下，发现简单又好用。</p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/22.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/23.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/24.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/25.png" title="Optional title"></p><p>最后做了两个，一个是2jpeg，另一个是2png。是因为发现转换成jpeg格式之后文件还是有点大，一张图占个3M往上就有点夸张，不好存储了。</p><p>以及好像这个小机器人很好用，我看还有一堆能用的上的功能，以后如果有机会的话再开发一下。</p><h1 id="卸载并重装mac的python环境"><a href="#卸载并重装mac的python环境" class="headerlink" title="卸载并重装mac的python环境"></a>卸载并重装mac的python环境</h1><blockquote><p>2022.3.4更新。</p></blockquote><p>今天下午搞代码的时候，开pycharm配置服务器，服务器倒是没什么问题，就是mac的python一直弹停止工作的窗口，然后conda什么的命令也都不好使了，这些东西都是18年装的，很老了，期间我想要更新conda，但每次都要3.6.5的python也加入更新列表，尝试更新之后环境直接全部乱掉了，所以就从备份恢复过后再也没弄过。今天这刚好有个理由把这些东西全部更新一遍。</p><h2 id="卸载anaconda、python3-6"><a href="#卸载anaconda、python3-6" class="headerlink" title="卸载anaconda、python3.6"></a>卸载anaconda、python3.6</h2><p>删anaconda。</p><ul><li>Mac 如何彻底删除 Anaconda? <a href="https://www.cnblogs.com/bigband/p/13646296.html">https://www.cnblogs.com/bigband/p/13646296.html</a></li></ul><p>删python3.6，里面讲的3.7，3.6同理。</p><ul><li>Mac 安装及卸载 Python3 详细教程 <a href="https://zhuanlan.zhihu.com/p/142810523">https://zhuanlan.zhihu.com/p/142810523</a></li></ul><p>3.6.5删除干净之后，命令行输入<code>python3</code>，结果又弹出一个3.8.2版本。查一下位置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">print(<span class="string">&quot;sys.executable&quot;</span>)</span><br></pre></td></tr></table></figure><p>结果是<code>/Library/Developer/CommandLineTools/usr/bin/python3</code>，<code>CommandLineTools</code>好像是之前Xcode需要的组件。</p><p>解释：<a href="https://www.zhihu.com/question/420273182/answer/1697779720">https://www.zhihu.com/question/420273182/answer/1697779720</a> 。</p><p>然后在这个bin里再把跟python3有关的都删除了，再输入python3，会提示要使用python3，命令行工具需要安装什么什么的，应该不会影响后面的安装了。</p><h2 id="安装python3-8-10"><a href="#安装python3-8-10" class="headerlink" title="安装python3.8.10"></a>安装python3.8.10</h2><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/26.png" title="Optional title"></p><p>当前官网上最新的安全版本是3.8，mac只提供到小版本号10，下载.pkg文件，按顺序安装。完成之后验证一下：</p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/27.png" title="Optional title"></p><p>conda没必要装了感觉。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;经验内容，欢迎转载。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;一个新坑。这个坑实际上好久之前就想开了（</summary>
      
    
    
    
    
    <category term="闲谈" scheme="http://example.com/tags/%E9%97%B2%E8%B0%88/"/>
    
    <category term="操作系统" scheme="http://example.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：Degradation Model Learning for Real-World Single Image Super-resolution</title>
    <link href="http://example.com/2021/04/03/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ADegradation-Model-Learning-for-Real-World-Single-Image-Super-resolution/"/>
    <id>http://example.com/2021/04/03/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ADegradation-Model-Learning-for-Real-World-Single-Image-Super-resolution/</id>
    <published>2021-04-03T11:19:38.000Z</published>
    <updated>2021-04-14T07:23:44.104Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>2020,ACCV</p></blockquote><blockquote><p>真实世界单幅图像超分辨率退化模型学习</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>在做超分问题的时候，数据集是非常重要的一部分。</p><p><strong>目前绝大多数图像或视频超分辨率重建工作，都假定了LR是由对应HR经过某种固定的已知退化所得（超分问题的一般流程，《基础：图像超分辨率》有提）。然而事实证明，当测试数据的退化设置与训练阶段存在差异时，超分辨率重建的性能会显著下降。</strong></p><blockquote><p>举例解释：现在一组HR图像，首先通过双三次差值（绝大部分文章所用到的数据集都是这个选择）将这组图像下采样，得到对应的LR图像。但实际上真实的HR到LR的退化过程是非常复杂的，是一个非线性问题。文章的意思是这种普遍存在的常规做法会明显的影响SR模型的性能。</p></blockquote><p>为了解决上述问题，本文从真实数据集中学习一个真实的退化模型，并使用所学习的退化模型来合成真实的HR-LR图像对。</p><blockquote><p>本文的重点在从HR图像到LR图像的这个过程。</p></blockquote><blockquote><p>It is well-known that the single image super-resolution (SISR) models trained on those synthetic datasets, where a low-resolution (LR) image is generated by applying a simple degradation operator (e.g., bicubic downsampling) to its high-resolution (HR) counterpart, have limited generalization capability on real-world LR images, whose degradation process is much more complex. Several real-world SISR datasets have been constructed to reduce this gap; however, their scale is relatively small due to laborious and costly data collection process. To remedy this issue, we propose to learn a realistic degradation model from the existing real-world datasets, and use the learned degradation model to synthesize realistic HR-LR image pairs.</p></blockquote><h1 id="超分数据集构建"><a href="#超分数据集构建" class="headerlink" title="超分数据集构建"></a>超分数据集构建</h1><p><strong>上述问题又来源于超分问题的数据集构建问题，简言之就是LR到底是怎么来的。</strong></p><p>文中所提到的，超分数据集的构建和合成问题的大致方法有如下几种：</p><ul><li>通过设置模糊核，直接由HR图像生成LR图像。这是最常见的方法。会产生的问题是模糊核的可解释性差。</li></ul><blockquote><p>Most of the existing CNN based SISR models are trained on synthetic HR-LR image pairs, which are generated by applying a simple degradation model (e.g., bicubic downsampling) to the HR images [14, 15, 18, 19, 21, 23, 24]. However, the authentic HR to LR image degradation process is much more complicated than these simple uniform downsample operators. As a result, the SISR networks trained on such synthetic datasets have low generalization capability to real-world LR images, largely limiting their value in practical applications.</p></blockquote><ul><li>进行真实的pair的数据对的构建，通常是通过相机捕捉相同场景，变量则设置为相机本身的某些参数。这种方法产生的问题是：构建数据集的成本太高，以至于该类数据集的容量都不会太大。以及在进行拍照的时候，物理条件的限制因素太多，包括但不限于天气、光照、场景多样性等。</li></ul><blockquote><p>Very recently, researchers have started to construct real-world datasets by using digital cam- eras to capture images of the same scene under different focal lengths [25–27]……However, constructing such datasets of real-world HR-LR pairs is laborious and costly, and the existing datasets of this kind [25–27] are all limited in number of image pairs, diversity of scenes and illuminating conditions.</p></blockquote><ul><li>从unpair的HR和LR中学习图像的退化过程，并且将学习到的退化过程用到SR的流程之中。一般选取GAN来完成这件事情。会出现的问题是，这种方法的训练过程是非常困难的，最终的结果可能不会收敛，以及使用网络来完成退化过程，忽略了图片的一些先验信息，导致了可解释性差。</li></ul><blockquote><p>While constructing real-world datasets of HR-LR image pairs, researchers have also proposed to learn the image degradation process from unpaired HR and LR images, and use the learned degradation model to generate HR-LR image pairs for SISR model learning [28–31]. All these methods employ the Generative Adversarial Network (GAN) [32] to learn the degradation process by differentiating the distribution between generated LR and real LR images. Unfortunately, training such a GAN with unpaired data is very difficult and may not converge to the desired result. Moreover, using a network to model the degradation from HR to LR images makes it hard to interpret the degradation process, ignoring some prior knowledge on the image formation.</p></blockquote><h1 id="网络模型"><a href="#网络模型" class="headerlink" title="网络模型"></a>网络模型</h1><h2 id="模型叙述"><a href="#模型叙述" class="headerlink" title="模型叙述"></a>模型叙述</h2><p>目前被广泛认可的从HR到LR的泛用退化模型可进行如下表示。其中，“*”是卷积算子，k是退化核，↓d是下采样算子，v是加性随机噪声，L和H分别代表低分和高分图像。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/1.png" title="Optional title"></p><p>上面提到，大多数现有工作对HR使用的是双三次下采样以采集LR图像，即这些工作都假设退化核k在整个图像上是均匀的，即空间不变的。</p><p>而在现实世界的SISR问题中，退化核要复杂得多，与场景的深度和局部内容相关。<strong>因此，退化核是典型的非均匀和空间变异的。</strong>对于图像上的每个像素点(i,j)，应该有相应的模糊核和噪声。</p><p>经过上述改动后，从HR到LR的空间变化的图像退化可以表示如下。其中，H(i,j)表示以(i,j)为中心的局部图像窗口，其大小与核k(i,j)相同，“⊙”是内积算子。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/2.png" title="Optional title"></p><p><strong>从上式可以看出，对真实世界的图像退化过程建模的关键是如何预测像素级退化核k(i,j)。</strong></p><p>一个直观的想法是直接通过真实HR-LR对作为监督去学习退化核参数。但这种做法的问题是需要求解的参数量太大，且解空间非常大，在有限的数据量条件下易出现过拟合的问题。</p><p>文章提出，由于光学系统成像的原理限制，退化过程普遍可以用一个钟形函数（bell-shaped smooth functions）进行描述。所以可以进一步缩小解空间范围，即可以<strong>通过一系列基础退化核的线性组合实现对任意退化核的描述</strong>。综上所述，pixel-wise的退化核可以进行如下表示，其中Φm表示第m个基础退化核，C表示系数矩阵。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/3.png" title="Optional title"></p><p>上式将原本复杂的退化过程的解空间约束在一个较小的子控件中，使得在数据量不大的条件下更容易被学习。</p><h2 id="Degradation-Model-Learning"><a href="#Degradation-Model-Learning" class="headerlink" title="Degradation Model Learning"></a>Degradation Model Learning</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/4.png" title="Optional title"></p><p>退化模型学习（DML）方法如上所示。</p><p>整个网络以H作为输入，学习具有参数Θ的CNN F来预测权重，即C＝F(H|Θ)，其中C是权重向量C(i,j)的集合。</p><p>还学习了基核φm，从而可以根据式（3）预测核k(i,j)。将预测的退化核应用于HR图像H以输出预测的LR图像，用ˆL表示。</p><p>上图中的F为基于编解码结构的权值预测网络。以HR图像作为输入，并在每个位置输出一个权重向量。为了获得大的接收野，使用最大池层进行特征下采样，并使用双线性上采样层来提高特征分辨率和保证像素级的输出。采用3×3滤波器的卷积层，用ReLU作为激活函数。为了输出每像素的权值，在最后一个卷积层之后使用sigmoid函数进行归一化。通过SGD或ADAM优化器可以很容易地对整个网络进行优化。</p><h2 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h2><p>假设有N对HR-LR训练图像，则目标函数为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/5.png" title="Optional title"></p><h2 id="SISR-Model-Learning"><a href="#SISR-Model-Learning" class="headerlink" title="SISR Model Learning"></a>SISR Model Learning</h2><p>为了进一步缩小合成与真实的差距LR图像，根据式（1）中描述的LR图像形成过程向合成的LR图像ˆL添加随机噪声。</p><p>设置为加性高斯白噪声（AWGN），并根据经验将噪声级设置为σ=5。</p><blockquote><p>To address this issue and further diminish the gap between synthetic and real<br>LR images, we add random noise to the synthesized LR image ˆILn according to the LR image formulation process described in Eq. (1). Without additional in- formation on the imaging system (e.g., sensors, lens), we simply assume additive white Gaussian noise (AWGN) and empirically set the noise level as σ = 5.</p></blockquote><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>两部分实验。</p><h2 id="实验设置和数据集"><a href="#实验设置和数据集" class="headerlink" title="实验设置和数据集"></a>实验设置和数据集</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>验证DML在退化过程学习和SISR模型训练中的性能，一共用到了三个数据集。</p><ul><li><p><strong>RealSR（v2、pair）</strong>：包含由两台相机采集的559个场景的对齐HR-LR图像对，具有3个缩放因子：×2、×3和×4。RealSR的划分：训练集/测试集=459/100。<strong>这个数据集的主要用来训练DML退化模型，并使用测试部分来定量评估DML的性能及其在实际SISR中的应用。</strong></p></li><li><p><strong>Flickr2K+互联网图片（unpair）</strong>：<strong>DML学习完成后，这部分数据主要用来监测这个退化学习模型，即通过HR生成HR-LR对。</strong>该HR数据集总共包含3150张图片，其中Flickr2k含有2650张不同场景的高质量图像，分辨率大多为1500×2000。互联网图像是从[39]下载了500张4K分辨率的原始图像。然后将PhotoShop CameraRaw工具应用于这些原始图像，获得4K分辨率的未压缩高质量RGB图像。</p></li><li><p><strong>SR-RGB</strong>：该数据集由真实世界的LR图像和通过DSLR光学变焦获得的未对齐HR图像组成。由于HR和LR图像没有对齐，因此无法计算PSNR、SSIM、LPIPS度量，但HR图像可以用作视觉比较的参考。<strong>这个数据集验证DML对现实世界SISR的有效性。</strong></p></li></ul><blockquote><p>SR-RGB来自《Zoom to learn, learn to zoom》，下篇会提到</p></blockquote><h3 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h3><p>Y通道训练、DA=左右 or 上下翻转、Adam、learning rate=1e−4、epoch=100K or 300K、batch size=16 or 2、进行测试的网络=VDSR + RCAN。</p><blockquote><p>对于所有缩放比例×2、×3和×4，我们将要学习的基核的大小设置为15×15。对基核进行随机初始化，然后归一化为求和1，以便进一步更新。使用Xavier初始值设定项初始化权重预测网络。在DML和SISR网络的训练中，我们将RGB图像转换成YCbCr颜色空间，在Y通道上进行训练或测试。将图像裁剪成192×192块，用于所有模型的训练。左右翻转和上下翻转用于数据扩充。使用带有默认参数设置（β1=0.9，β2=0.999）的Adam优化器作为优化器。我们使用1e−4的固定学习率分别为100K和300K次迭代来训练DML和SISR模型。在DML训练中，批大小设置为16。对于SISR模型，我们采用了两种具有代表性的网络结构：VDSR和RCAN。我们用100个卷积层来实现RCAN。批大小分别设置为16和2，用于训练VDSR和RCAN模型。</p></blockquote><blockquote><p>RCAN来自《Image Super-Resolution Using Very Deep Residual Channel Attention Networks》，残差通道注意力网络</p></blockquote><h2 id="DML"><a href="#DML" class="headerlink" title="DML"></a>DML</h2><h3 id="DML中基核数的选择"><a href="#DML中基核数的选择" class="headerlink" title="DML中基核数的选择"></a>DML中基核数的选择</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/6.png" title="Optional title"></p><p>研究DML中基核的合适数目。</p><p>利用RealSR数据集的训练部分，学习了N=4、8、16基核及其相关的权值预测网络。然后将学习到的模型应用于RealSR数据集测试部分的HR图像，生成LR图像。PSNR、SSIM结果在表1中列出。</p><p><strong>从N=4到N=8，可以获得更好的LR生成性能，N=16基核的性能略差于N=8。</strong></p><blockquote><p>N=8，表一中第三行，红色的数据。</p></blockquote><h3 id="有8个基核的DML在不同缩放倍数下的效果"><a href="#有8个基核的DML在不同缩放倍数下的效果" class="headerlink" title="有8个基核的DML在不同缩放倍数下的效果"></a>有8个基核的DML在不同缩放倍数下的效果</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/7.png" title="Optional title"></p><p>随着缩放因子从2增加到4，核变得更加分散和复杂，这符合我们对图像退化过程的共同认识。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/8.png" title="Optional title"></p><p>用DML方法对基核的预测组合权重进行×2的可视化。最左边的图像是输入的HR图像，右侧8图像可视化对应于每个基核的预测权重。<strong>亮度越高，权重越高。</strong>通过实例分析，我们的权重预测网络可以根据场景内容和局部结构自适应地分配不同的权重。</p><blockquote><p>The brighter intensity denotes larger weight.</p></blockquote><h3 id="与其他两种LR图像生成模型的对比"><a href="#与其他两种LR图像生成模型的对比" class="headerlink" title="与其他两种LR图像生成模型的对比"></a>与其他两种LR图像生成模型的对比</h3><p>一种是学习CNN，直接将HR图像映射到LR图像，表示为DirectNet。另一种是学习内核预测网络[42]，以预测退化内核，表示为DirectKPN。</p><p>加上DML，这三种方法，都在RealSR的训练集上进行训练，并在RealSR测试集上进行测试。</p><p>比较这三种方法在LR图像生成中的性能。<strong>DML在所有三个缩放倍数上都始终优于其他两个，以及DirectKPN的性能略优于DirectNet。这表明，通过考虑图像退化过程，通过学习预测像素核可以获得比直接预测LR图像像素更好的LR生成性能。</strong></p><h3 id="三种LR图像生成模型在SISR任务下的效果"><a href="#三种LR图像生成模型在SISR任务下的效果" class="headerlink" title="三种LR图像生成模型在SISR任务下的效果"></a>三种LR图像生成模型在SISR任务下的效果</h3><p>将上述三种LR图像生成模型应用于采集的HR图像数据集，使用Flickr2K+互联网图片的那个数据集，生成了三组，每组3150对HR-LR图像。在这些HR-LR对中添加了小AWGN（additive white Gaussian noise），并训练了三个VDSR模型。最后，将这三种VDSR模型应用于RealSR数据集测试部分的LR图像，得到超分辨率的HR图像。</p><blockquote><p>这部分的实验过程：<br>（1）LR生成模型+HR图像 = 3组LR-HR图像对<br>（2）LR-HR图像对添加AWGN<br>（3）根据3组LR-HR图像对训练3个VDSR<br>（4）用VDSR测试RealSR的459张测试图片的LR并计算指标</p></blockquote><p>结果表明，在DML方法生成的HR-LR图片对上训练的VDSR网络的性能，比DirectNet或DirectKPN生成的对上训练的VDSR网络好（PSNR约为0.15dB）。验证了DML在提高SISR性能方面优于DirectNet和DirectKPN。</p><h2 id="DML对SISR任务的影响"><a href="#DML对SISR任务的影响" class="headerlink" title="DML对SISR任务的影响"></a>DML对SISR任务的影响</h2><h3 id="测试集"><a href="#测试集" class="headerlink" title="测试集"></a>测试集</h3><p><strong>首先做了5个不同的训练数据集</strong>：only RealSR、only Syn DSGAN、only Syn DML、RealSR+DSGAN、RealSR+Syn DML。</p><blockquote><p>RealSR：就是上面提到的RealSR以及划分。<br>Syn DSGAN：一种基于GAN的HR-LR对合成方法，称为DSGAN，来自《Frequency separation for real-world super- resolution》，2019。<br>Syn DML：DML在Flickr2K+互联网图片上的合成，共3150对。</p></blockquote><h3 id="以RealSR为测试集做定量比较"><a href="#以RealSR为测试集做定量比较" class="headerlink" title="以RealSR为测试集做定量比较"></a>以RealSR为测试集做定量比较</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/9.png" title="Optional title"></p><p><strong>这部分实验是：用VDSR（20层）和RCAN（100层）对5个不同的数据集做了10个模型，计算并对比指标。</strong></p><p>结论：</p><ul><li>LPIPS（Syn-DML上的VDSR/RCAN，任意情况下） &lt; LPIPS（RealSR），<strong>使用这个指标证明DML的效果。</strong></li></ul><blockquote><p>LPIPS是一个感知指数，衡量图像的感知质量（越低越好）。</p></blockquote><ul><li><p>PSNR/SSIM（Syn-DML上的VDSR/RCAN，任意情况下） = PSNR/SSIM（RealSR），因为后者的训练集合和测试集合都是本身，<strong>换句话DML在测试集劣势的情况下得到的结果却没有劣势</strong>。</p></li><li><p>Syn-DML &gt; Syn-DSGAN，证明DML比DSGAN优越。</p></li><li><p>RealSR+合成 &gt; only</p></li></ul><h3 id="以RealSR为测试集做定性比较"><a href="#以RealSR为测试集做定性比较" class="headerlink" title="以RealSR为测试集做定性比较"></a>以RealSR为测试集做定性比较</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/10.png" title="Optional title"></p><p>在Syn-DML和RealSR+Syn-DML上训练的模型比仅使用RealSR数据集训练的模型能有效地恢复更多的图像细节和更令人愉快的感知质量。<strong>特别是RealSR+Syn-DML上训练的模型达到了最佳的视觉质量。</strong></p><h3 id="以SR-RGB为测试集做定性比较"><a href="#以SR-RGB为测试集做定性比较" class="headerlink" title="以SR-RGB为测试集做定性比较"></a>以SR-RGB为测试集做定性比较</h3><p>测试数据集用了两个，一个是RealSR，另一个是SR-RGB。</p><p>SR-RGB由许多LR图像及其未对齐的HR对应图像组成，<strong>做不了指标评价，只能做定性的视觉评价，但原文中提到这部分实际上更看重</strong>。使用该数据集的原因：由于SR-RGB数据集是独立于RealSR数据集，通过使用不同的摄像机和镜头构建的，结果可以更公平地证明SISR模型对真实场景的泛化能力。</p><blockquote><p>Another is the SR-RGB dataset , which consists of many LR images and their unaligned HR counterparts. Qualitative visual comparisons can be made on it for the different SISR models. Wed like to stress that the testing on the second dataset is more important (though qualitative) because it is independent of the RealSR dataset, part of whose samples are used to train the DML and VDSR/RCAN models. The testing results on the SR-RGB dataset can more faithfully reflect the generalization capability of competing SISR models than those on the RealSR dataset.</p></blockquote><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/11.png" title="Optional title"></p><p>在RealSR数据集上训练的模型只能适度地恢复一些细节。在Syn Dsgan上训练的模型会产生严重的伪影。在Syn-DML上训练的SISR模型可以得到更为精细的细节，从而获得令人满意的结果。特别是，<strong>在RealSR+Syn-DML上训练的模型提供了最佳的超分辨率HR图像感知质量。</strong></p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>DML能够有效地提高SISR模型在实际应用中的泛化性能。</p><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>思路挺新，感觉方法论做烂了之后，大家都开始从超分的最基础流程开始下手写东西了，也许会成为以后的小热点。</p><p>这篇的实验做的是又细又好，看着很舒服。尤其是针对训练集和测试集的设置方面，确实比较亮眼。</p><p>但是第一部分实验的细节又没有说的很清楚，就是Table 1的那部分，导致了对不上哪部分是哪部分的数据。</p><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>code：没提，应该是没公开。</p></li><li><p>文章：<a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Xiao_Degradation_Model_Learning_for_Real-World_Single_Image_Super-resolution_ACCV_2020_paper.pdf">https://openaccess.thecvf.com/content/ACCV2020/papers/Xiao_Degradation_Model_Learning_for_Real-World_Single_Image_Super-resolution_ACCV_2020_paper.pdf</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;2020,ACCV&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;真实世界单幅图像超分辨率退化模型学习&lt;/p&gt;
&lt;</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：Evaluation and development of deep neural networks for image super-resolution in optical microscopy</title>
    <link href="http://example.com/2021/01/30/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AEvaluation-and-development-of-deep-neural-networks-for-image-super-resolution-in-optical-microscopy/"/>
    <id>http://example.com/2021/01/30/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AEvaluation-and-development-of-deep-neural-networks-for-image-super-resolution-in-optical-microscopy/</id>
    <published>2021-01-30T11:35:46.000Z</published>
    <updated>2021-04-03T11:34:16.555Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><blockquote><p>2021，加油。</p></blockquote><hr><blockquote><p>2021.1.21 Nature</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>文章做了三项工作：</p><ul><li><p>数据集BioSR：近年来，基于深度学习的SISR模型被应用于提高科学显微镜图像的分辨率。与SISR在增强宏观真实照片纹理方面的应用相比，用于科学分析的超分辨率显微图像对推断出的纳米尺度结构提出了更高的精度和可量化性要求。然而，基于深度学习的超分辨率（DLSR）图像所传递的信息在多大程度上可以用于定量分析，以及在什么条件下DLSR方法优于传统的超分辨率显微镜，仍然是个未知数。在这里，使用自制的多模结构照明显微镜（SIM）系统，该系统集成了全内反射荧光（TIRF-SIM）、掠入射（GI-SIM）和非线性SIM（方法），在输入LR图像的信噪比（SNR）水平的宽范围内获得匹配良好的LR–SR图像对，观察到的生物结构的复杂性和期望的放大因子。这个数据集被命名为BioSR。</p></li><li><p>DFCAN及其衍生的生成对抗训练策略DFGAN：DLSR网络的训练可以看作是一个提取高维特征的过程，这些特征连接了LR和SR图像空间。众所周知，输入LR图像的功率谱被限制在衍射限制频率以下，因此推测，利用傅里叶域中不同特征的频率内容差异，而不是空间域中的结构差异，可能使DLSR网络能够学习高频信息的分层表示更加精确和有效。受深度剩余通道注意网络（RCAN）中空间域通道注意机制的启发，开发了DFCAN及其派生的生成性对抗网络（GAN）训练策略，称为DFGAN。</p></li><li><p>证明DFCAN的Fourier域聚焦能够在低信噪比条件下实现SIM图像的鲁棒重建。在多色活体细胞成像实验中，证明df可以在10倍的时间内获得与SIM相当的图像质量，揭示了线粒体嵴和类核细胞的详细结构以及细胞器和细胞骨架的相互作用动力学。</p></li></ul><blockquote><p>Deep neural networks have enabled astonishing transformations from low-resolution (LR) to super-resolved images. However, whether, and under what imaging conditions, such deep-learning models outperform super-resolution (SR) microscopy is poorly explored. Here, using multimodality structured illumination microscopy (SIM), we first provide an extensive dataset of LR–SR image pairs and evaluate the deep-learning SR models in terms of structural complexity, signal-to-noise ratio and upscaling fac- tor. Second, we devise the deep Fourier channel attention network (DFCAN), which leverages the frequency content difference across distinct features to learn precise hierarchical representations of high-frequency information about diverse biological structures. Third, we show that DFCAN’s Fourier domain focalization enables robust reconstruction of SIM images under low signal-to-noise ratio conditions. We demonstrate that DFCAN achieves comparable image quality to SIM over a tenfold longer duration in multicolor live-cell imaging experiments, which reveal the detailed structures of mitochondrial cristae and nucleoids and the interaction dynamics of organelles and cytoskeleton.</p></blockquote><h1 id="BioSR"><a href="#BioSR" class="headerlink" title="BioSR"></a>BioSR</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>使用multimodality structured illumination microscopy（SIM，多模结构照明显微镜），获取了如下类型的数据集，这些数据集代表了结构复杂性的增加：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/1.jpg" title="Optional title"></p><ul><li><p>clathrin-coated pits（CCPs，网格蛋白包被坑/网格蛋白小窝）</p></li><li><p>endoplasmic reticulum（ER，内质网）</p></li><li><p>microtubules（MTs，微管）</p></li><li><p>F-actin filaments（F-肌动蛋白丝）</p></li></ul><p>对于每种类型的样本，在10个不断上升的激发光强度（excitation light intensity）水平上获得了大约50组原始SIM图像。且在最高激发水平下，确保所有原始图像的SNR（signal-to-noise ratio，信噪比）足够高，以重建高质量的SIM图像。</p><p>每一组原始SIM图像被平均化为WF（diffraction-limited wide-field，衍射限制宽场）图像。</p><p><strong>WF图像用作DLSR网络的输入LR图像，而SIM图像用作评估DLSR方法在特定成像条件下是否优于传统SR显微镜的参考。</strong></p><blockquote><p>WF图像是如何做出来的？</p></blockquote><blockquote><p>SIM的结构和工作原理在Method部分给出，比较专业。</p><blockquote><p>将405 nm（LBX-405-300，Oxxius）、488 nm（Genesis MX SLM，相干）和560 nm（2RU-VFL-P-500-560，MPB通信）的三束激光组合在一起，然后通过声光可调谐滤波器（AOTF，AOTFnC-400.650，AA Quanta Tech）。在特定的成像模式下，AOTF可以灵活地控制所需激光束的曝光顺序和曝光时间。<br>将9幅TIRF-SIM或GI-SIM原始图像和25幅非线性SIM原始图像分别重建为分辨率提高2倍和3倍的SR图像。为了减小中低荧光强度下SIM图像的重建伪影，采用截止频率与重建光学传递函数（OTF）边界匹配的高斯切趾函数（Gaussian apodization function）来抑制高频噪声。</p></blockquote></blockquote><h2 id="采集数据细节"><a href="#采集数据细节" class="headerlink" title="采集数据细节"></a>采集数据细节</h2><p>对于每种类型的标本和每种成像方式，从至少50个不同的ROI（regions-of-interest，感兴趣区域）获取原始数据集，其中35个ROI的数据集用于训练，而其他15个ROI的数据集用于生成评估矩阵。</p><blockquote><p>数据分布。</p></blockquote><p>对于每个感兴趣区，获得了9组N phase × M orientation的原始图像，曝光时间不变，但激发光强度增加，其中N和M分别为TIRF-SIM和GI-SIM的3组和非线性SIM的5组。</p><p>将每组N×M原始图像平均为衍射受限的WF图像，然后对其进行掩模（masked）以计算其每像素的平均光子计数。</p><p>将不同荧光水平的原始SIM图像和WF图像作为DLSR网络的输入LR图像。同时，将每组N×M的原始图像重建成与相应WF图像具有相同荧光水平的SIM图像，作为评价该荧光水平下DLSR图像质量的参考。</p><blockquote><p>DLSR的输入和输出。</p></blockquote><p>此外，在相同的ROI中，最终提高了激发强度和曝光时间（通常为120W 平方cm 10ms）以达到平均光子数&gt;1200的高荧光水平，并独立获得三组N×M原始图像。将得到的三幅超高信噪比的SIM图像平均为GT-SIM图像，保证了图像的高质量。</p><blockquote><p>在最高激发水平下，确保所有原始图像的SNR（signal-to-noise ratio，信噪比）足够高，以重建高质量的SIM图像。</p></blockquote><p>在转染后16-36小时将细胞固定以获得CCPs、MTs和F-肌动蛋白的数据。然而，发现目前的化学固定方法导致的内质网标记蛋白calnexin明显聚集，这显著改变了内质网的形态，因此从活细胞（live cells）中获得了内质网数据。</p><blockquote><p>内质网使用了live cells获取数据。</p></blockquote><p>仅使用肌动蛋白细胞骨架结构评估了3倍放大的DLSR成像性能。</p><blockquote><p>只有F-actin filaments做了3×，其他都是2×。</p></blockquote><h2 id="在BioSR上评估几种典型DLSR方法"><a href="#在BioSR上评估几种典型DLSR方法" class="headerlink" title="在BioSR上评估几种典型DLSR方法"></a>在BioSR上评估几种典型DLSR方法</h2><p>选择了四个具有代表性的DLSR模型，其中包括：<strong>SRCNN、EDSR、Pix2Pix、CM（cross-modality，跨模态）GAN。</strong></p><h3 id="不同荧光程度图像下SR方法的效果"><a href="#不同荧光程度图像下SR方法的效果" class="headerlink" title="不同荧光程度图像下SR方法的效果"></a>不同荧光程度图像下SR方法的效果</h3><p>对于所有的图像，文章以“荧光程度（fluorescence）”作为区分标准，将图像（或者一张图像中的不同部分）分为低荧光、中荧光和高荧光三种。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/2.png" title="Optional title"></p><blockquote><p>图1的a、b，分别是高、低荧光水平下的DLSR结果。</p></blockquote><p>文章计算了SISR图像和GT（SIM）的像素级绝对差（MAE，平均绝对误差）：<strong>中等或相对较高的荧光信号电平（这是活细胞SIM成像的典型值）足以允许常规SIM通常优于当前的DLSR方法。</strong>相比之下，在低荧光的成像条件下，SISR图像显示出比传统SIM图像更少的残余差异，因为数据驱动的DLSR方法通常有利于从生物结构中分离噪声。</p><blockquote><p>这里的意思就是说，在常规的活细胞SIM成像情况下，中等荧光水平以上的成像条件得到的图像已经足够用了，DLSR方法做出的结果并不特别接近GT。</p></blockquote><p>然而，如果期望的上缩放因子增加到3×，则SISR图像（图1a、b的底行）将包含太多错误或伪影，使得人们无法信任所推断的精细结构。</p><blockquote><p>3×的DLSR的结果会更差。</p></blockquote><h3 id="不同SR方法的性能"><a href="#不同SR方法的性能" class="headerlink" title="不同SR方法的性能"></a>不同SR方法的性能</h3><p>为了定量评估不同SR方法的性能，综合了标准化均方根误差（NRMSE）、多尺度结构相似性指数（MS-SSIM）和分辨率三个指标，以测量评估矩阵中每个成像条件下SR图像的质量。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/3.png" title="Optional title"></p><p>由于科学研究通常需要以牺牲荧光光子为代价的高真实性和可量化的SR图像，因此首先在评估矩阵的空间中确定了常规SIM的适用区域（图1c中用黑色虚线勾勒），其中常规SIM图像的三个矩阵与之接近GT。常规SIM的适用范围主要是中、高荧光区，符合活体细胞SIM成像实验的实际情况。</p><p>此外，利用这三个指标来评估四个DLSR模型，根据评估结果，确定了使用DLSR模型的优先区域，其中SISR图像的三种方法与传统SIM图像的方法相当或更好（图1c和补充说明4中以绿色列出）。显然，优先级区域越大，DLSR模型的性能越好。</p><p>然而，<strong>所有四种DLSR模型的优先区域都相对较小，并且集中在低荧光和低结构复杂性的区域，这些区域很少与常规SIM的适用区域重叠（图1c）</strong>。这些数据表明，硬件SR显微镜，例如SIM，比最新的DLSR模型更有效地利用增加的荧光来产生高保真SR信息，这可能阻碍DLSR模型在实际实验中的广泛应用。</p><h1 id="DFCAN和DFGAN"><a href="#DFCAN和DFGAN" class="headerlink" title="DFCAN和DFGAN"></a>DFCAN和DFGAN</h1><p>在学习了上述DLSR模型的特性之后，设计了DFCAN和DFGAN。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/4.png" title="Optional title"></p><h2 id="DFCAN"><a href="#DFCAN" class="headerlink" title="DFCAN"></a>DFCAN</h2><p>从卷积层和GELU开始。GELU的输出后面是四个相同的RG（residual groups，残差组），每个RG由四个FCAB（Fourier channel attention blocks傅立叶通道注意块）和一个跳跃连接组成。RG的操作表示为如下。x表示RG的输入特征映射。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/5.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/6.png" title="Optional title"></p><p>在每个FCAB中，特征图按如下方式按通道重新缩放：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/7.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/8.png" title="Optional title"></p><p>FFT（·）表示快速傅里叶变换，γ用于增强图像质量。</p><p>平均池化层，使得φ（y）的每个分量都可以解释为每个特征图的代表值。</p><p>WD和WU分别是下向和上向权值，这两种权值均由网络中的1×1卷积层实现。</p><p>f（·）和δ（·）分别是sigmoid激活函数和ReLU激活函数。它们共同产生了一种选通机制，可以自适应地计算最终的重缩放因子。</p><p>最后一个RG的输出被送入由GELU激活函数激活的卷积层。然后使用像素混洗层（Pixel Shuffle）、卷积层和sigmoid激活层将图像放大到与GT图像相同的大小（以适应推断的高频信息）。</p><p>输出为单色灰度SR图像。</p><p><strong>该网络的损失函数定义为MSE损耗和SSIM损耗的组合。</strong></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/9.png" title="Optional title"></p><p>其中，Y^定义为DFCAN的输出，Y定义为相应的GT，（w，h）定义为输出图像的像素大小。<strong>λ是用于平衡SSIM和MSE相对贡献的标量权重，文中大多数情况下设置为0.1。</strong></p><p><strong>MSE损失保证了像素级的精度，均衡了预测的动态范围，SSIM损失增强了输出的结构相似性。</strong></p><h2 id="DFGAN"><a href="#DFGAN" class="headerlink" title="DFGAN"></a>DFGAN</h2><p>DFGAN是基于cGAN（条件GAN）框架构建的。</p><p>在DFGAN中，更深层次的DFCAN充当G，G以低分辨率荧光图像作为输入，其输出是放大的SR图像。D基于传统的CNN架构，由12个卷积层组成，每个卷积层的输出由leaky因子α=0.1的LeakyReLU激活函数激活。</p><p>分别定义了G和D的损失函数。G的损失函数L_G/D由两项组成：SR误差，用于惩罚G输出和GT图像之间的差异；判别误差，与D计算的概率有关。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/10.png" title="Optional title"></p><p>其中X是输入的低分辨率图像，Y是SR目标图像。β、γ和λ是用来平衡相应项的标量加权因子，根据经验将其设置为β=0.1，γ=1，λ=0.1。</p><blockquote><p>最终结果对上面三个加权因子不是很敏感。</p></blockquote><p>D的损失函数定义为二元交叉熵：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/11.png" title="Optional title"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="FCA和SCA"><a href="#FCA和SCA" class="headerlink" title="FCA和SCA"></a>FCA和SCA</h3><p>DFCAN和DFGAN的设计利用了<strong>Fourier域中不同特征映射的功率谱特性。</strong></p><blockquote><p>何为“Fourier域中不同特征映射的功率谱特性”？</p></blockquote><p>在每个残差块中，FCA（Fourier channel attention，傅立叶信道注意）机制（图2a）使得网络能够根据其功率谱中包含的所有频率分量的综合贡献自适应地重新缩放每个特征图。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/12.png" title="Optional title"></p><p>相比之下，SCA（spatial channel attention，空间通道注意）机制仅利用特征映射的平均强度（相当于零频率（即直流分量））来计算重缩放因子。</p><blockquote><p>FCA的初衷是什么？凭空想出来的吗？还是有借鉴有曾今的论文做过这类理论？<br>为什么要和SCA进行比较？</p></blockquote><h4 id="FCA和SCA的性能比较"><a href="#FCA和SCA的性能比较" class="headerlink" title="FCA和SCA的性能比较"></a>FCA和SCA的性能比较</h4><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/13.png" title="Optional title"></p><p>为了比较FCA机制与SCA的性能，将DFCAN中的FCA替换为SCA，从而形成了深空间通道注意网络（DSCAN）。此外，通过去除DFCAN中的FCA，构造了一个改进的ResNet进行比较。</p><p><strong>结果显示，DFCAN比其他两个网络实现更快的收敛和更低的验证NRMSE。</strong></p><h4 id="FCA是否可以应用于其他网络结构？"><a href="#FCA是否可以应用于其他网络结构？" class="headerlink" title="FCA是否可以应用于其他网络结构？"></a>FCA是否可以应用于其他网络结构？</h4><p>为了进一步验证FCA机制是否可以普遍应用于其他类型的神经网络结构，在两个广泛使用的网络上实现了FCA：U-net18和DenseNet19。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/14.png" title="Optional title"></p><p>用三个下采样块和三个上采样块构造了U网络，并将两个机制引入到U网络中，生成的网络分别命名为U-net-FCA和U-net-SCA。</p><p>DenseNet由三个dense blocks和两个过渡层交织而成。每个dense blocks由8个密集连接的ReLU-Conv模块组成，每个过渡层由一个ReLU激活层和1×1卷积层组成。在每个dense blocks的末尾实现了FCA和SCA模块，分别生成DenseNet FCA和DenseNet SCA网络。</p><p>所有这些基于U-net或DenseNet的模型都是用管状结构的模拟数据进行训练的。<strong>结果显示，基于FCA的模型比基于SCA的相应模型更精确地推断出交错管状结构的精细结构。</strong></p><p>上述两部分实验的数据结果：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/15.png" title="Optional title"></p><h3 id="基于NRMSE、MS-SSIM和分辨率的DLSR模型性能评估"><a href="#基于NRMSE、MS-SSIM和分辨率的DLSR模型性能评估" class="headerlink" title="基于NRMSE、MS-SSIM和分辨率的DLSR模型性能评估"></a>基于NRMSE、MS-SSIM和分辨率的DLSR模型性能评估</h3><p>为了清楚地说明所有被评估的DLSR模型的性能差异，将它们分为两类：非GAN和GAN基模型。</p><p>对于每种类型的样品，分别绘制了同一类别模型的NRMSE、MS-SSIM和分辨率作为荧光强度的函数。<strong>结果表明，无论是DFCAN还是DFGAN的NRMSE都比同类的DLSR模型小。且在NRMSE和MS-SSIM指标方面，只有达到相对较高的荧光强度，传统的SIM才能超过DFCAN和DFGAN。</strong></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/16.png" title="Optional title"></p><blockquote><p>这里只放了NRMSE，其他指标的图示在补充图内。</p></blockquote><p>因此，相对于其他DLSR模型，DFCAN和DFGAN都提供了扩大的优先级区域。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/17.png" title="Optional title"></p><blockquote><p>这里是与前面的一张评估矩阵进行相比，绿色部分的面积扩大了。</p></blockquote><h3 id="DFCAN-DFGAN在SISR的2倍放大活细胞成像任务表现"><a href="#DFCAN-DFGAN在SISR的2倍放大活细胞成像任务表现" class="headerlink" title="DFCAN/DFGAN在SISR的2倍放大活细胞成像任务表现"></a>DFCAN/DFGAN在SISR的2倍放大活细胞成像任务表现</h3><p><strong>线粒体是高度动态的细胞器，线粒体动力学对于维持线粒体功能和细胞质量控制非常重要。</strong>然而，实现线粒体动力学的长时移SR成像仍然具有挑战性，当前SR成像技术需要高光照强度或长曝光时间来获取多个原始图像，这些原始图像容易引起线粒体的光毒性。</p><p>由于DFCAN采集单个WF图像所需的荧光比传统SR方法少得多，DFCAN-SISR允许长延时SR活细胞成像超过1000帧，与传统SIM或STED显微镜相比，成像持续时间延长了约10倍。</p><blockquote><p>生物相关，这部分实验与线粒体的内部结构有关。实验不放了，看不懂。</p></blockquote><h3 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h3><blockquote><p>直接从原文摘过来的。</p></blockquote><ul><li><p>图像预处理：对于每种类型的标本和每种成像方式，总共获得了约50组WF（512×512像素）和GT-SIM（1024×1024像素）或GT-NLSIM（1536×1536像素）图像。每一组都有9个不断上升的荧光水平。为了生成训练数据集，选取了35组原始数据（补充表5），采用随机裁剪、水平/垂直翻转和旋转变换等方法进一步丰富训练数据集，最终生成了20000对WF（128×128像素）和GT-SIM（256×256像素）图像，即2，每个荧光水平200对。对于每种类型的DLSR网络，用属于同一类型样品的所有荧光水平的数据训练一个专用模型。为了生成测试数据集，将剩余的15组数据扩充为WF（256×256像素）和GT-SIM（512×512像素）数据集，然后根据平均光子计数（即特定细胞的表达水平）将这些配对图像分为25到600的16个荧光水平。每一个荧光水平都保证有100多个图像。</p></li><li><p>训练：在一台计算机工作站上进行的，该工作站配备了3.20 GHz的Xeon（R）Gold 6134 CPU（Intel）和两块RTX 2080Ti图形处理卡（NVIDIA），其中包含python v.3.6、Tensorflow v.1.11.0和Keras v.2.2.4。在训练过程中，根据网络规模使用Adam优化器和2到6个批量大小。以CCPs的训练过程为例，对SRCNN、EDSR和DFCAN的非GAN方法，随机初始化网络，以1×10−4的典型起始学习率训练模型。<strong>最后的SRCNN、EDSR、DFCAN和RCAN模型分别经过约70000、150000、200000和500000次小批量迭代训练，时间分别为10、120、24和120h。对于基于GAN的Pix2Pix、CMGAN和DFGAN方法，还随机初始化了网络，训练了具有典型启动的生成模型和判别模型学习率分别为2×10−5和1×10−4。最后的Pix2Pix、CMGAN和DFGAN模型分别进行了约50000、90000和80000次小批量迭代，每次迭代时间分别为20、120和80h。在不同网络的训练过程期间的验证NRMSE的代表图示于补充图13中。通过迁移学习和混合精度训练可以缩短训练时间。一旦网络被训练，所有这些DLSR模型通常需要不到1s的时间来重建1024×1024像素的SR图像。</strong></p></li></ul><h3 id="统计和复现性"><a href="#统计和复现性" class="headerlink" title="统计和复现性"></a>统计和复现性</h3><blockquote><p>直接从原文摘过来的。</p></blockquote><p><strong>每个DLSR模型用相同的训练数据和超参数独立训练三次，然后采用NRMSE最低的模型进行进一步的评估。</strong></p><p>1a、b、3a、5a–d和扩展数据图2重复了120个测试图像，用于每种类型的样本和放大因子，都获得了相似的结果。</p><p>评估矩阵或曲线中的所有数据均来自100多幅图像的测试。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><ul><li><p>BioSR，它由广泛的LR–SR图像对组成，涵盖了广泛的信噪比水平、结构复杂性和放大因子。</p></li><li><p>非GAN模型更适用于低至中等荧光成像条件，以产生具有良好可量化性的SR图像。然而，如果GAN模型能够提供与传统SIM（例如DFGAN）相当的NRMSE和MS-SSIM，那么它将是首选的DLSR模型，特别是对于高结构复杂性的样本。</p></li><li><p>无论采用哪种DLSR模型，NRMSE和MS-SSIM对荧光强度的评估函数都很快接近渐近稳定，但即使接近无限荧光，也不能达到理想值。相比之下，随着荧光强度接近GT成像水平，传统SIM图像的度量越来越接近deal值。因此，这一局限性表明了用纯计算方法完全取代SR显微镜的巨大挑战。所以，将深度学习算法巧妙地集成到显微镜硬件开发中的整体设计可能是下一代SR显微镜的一种有前途的方法。</p></li></ul><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>文章：<a href="https://www.nature.com/articles/s41592-020-01048-5#data-availability">https://www.nature.com/articles/s41592-020-01048-5#data-availability</a></p></li><li><p>code：<a href="https://github.com/qc17-THU/DL-SR">https://github.com/qc17-THU/DL-SR</a></p></li><li><p>数据集（BioSR）：<a href="https://figshare.com/articles/dataset/BioSR/13264793">https://figshare.com/articles/dataset/BioSR/13264793</a></p></li></ul><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>文章内容太多了，一时半会都不能理解它到底在推什么东西，到底是数据集，还是算法，还是思想，还是硬件。但是非常有借鉴意义，得多读几遍。</p><hr><h1 id="GELU"><a href="#GELU" class="headerlink" title="GELU"></a>GELU</h1><blockquote><p>Gaussian Error Linear Units (GELUs)<br><a href="https://arxiv.org/abs/1606.08415">https://arxiv.org/abs/1606.08415</a></p></blockquote><p>高斯误差线性单元。</p><p>GELU非线性的实现是对神经网络的输入进行随机正则化的变化，为输入匹配一个或0或1的随机值。与ReLU的不同，GELU为其按照输入的magnitude（等级）为inputs加权值的；ReLUs是根据inputs的sign（正负）来gate（加门限）的。</p><p>论文实验证明GELU在多项计算机视觉，自然语言处理，语音任务上效果优于ReLU，ELU。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/18.png" title="Optional title"></p><h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><h2 id="MAE"><a href="#MAE" class="headerlink" title="MAE"></a>MAE</h2><p>MAE（Mean Square Error,平均绝对误差），所有单个观测值与算术平均值的偏差的绝对值的平均。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/19.png" title="Optional title"></p><h2 id="MSE"><a href="#MSE" class="headerlink" title="MSE"></a>MSE</h2><p>见《基础：图像超分辨率》。</p><h2 id="PSNR"><a href="#PSNR" class="headerlink" title="PSNR"></a>PSNR</h2><p>见《基础：图像超分辨率》。</p><h2 id="SSIM"><a href="#SSIM" class="headerlink" title="SSIM"></a>SSIM</h2><p>见《基础：图像超分辨率》。</p><h2 id="RMSE"><a href="#RMSE" class="headerlink" title="RMSE"></a>RMSE</h2><p>RMSE（Root Mean Square Error，均方根误差），观测值与真值偏差的平方和与观测次数m比值的平方根。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/20.png" title="Optional title"></p><blockquote><p>假如有2000次观测，即m=2000，对于某一次（第i次）观测来说，y值是真实值，而h(x)是观测值，对所有m次观测的的偏差取平方后相加，得到的值再除以m，然后再开根号，就得到RMSE了。</p></blockquote><p>RMSE对偏差做了一次平方（相比于MAE），这样，如果误差的离散度高，也就是说，如果最大偏差值大的话，RMSE就放大了。</p><h2 id="NRMSE"><a href="#NRMSE" class="headerlink" title="NRMSE"></a>NRMSE</h2><p>NRMSE（Normalized Root Mean Square Error，归一化均方根误差）</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/21.png" title="Optional title"></p><p>从数值角度，NRMSE就是将RMSE的值变成(0,1)之间。</p><h2 id="MS-SSIM"><a href="#MS-SSIM" class="headerlink" title="MS-SSIM"></a>MS-SSIM</h2><p>单尺度SSIM需要在特定的配置下才能表现良好，而MSSIM对不同分辨率的图像都能保持性能稳定。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/22.png" title="Optional title"></p><p>MSSIM的流程如上图所示。将参考图像和失真图像作为输入，然后分别依次迭代的使用低通滤波器和1/2降采样。假设原始图像为Scale 1，最高尺度为Scale M经过M-1次迭代得到。对于第j个尺度，只计算对比度c(x,y)和结构相似度s(x,y)。仅在Scale M计算亮度相似度l(x,y)。如上图所示。最终的SSIM是将各个尺度的结果连接起来：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/23.png" title="Optional title"></p><p>SSIM不能很好的反应人类真实的视觉感受。</p><p>有人认为，这种方法之所以比SSIM好，是因为人类一般不是使用最清晰的图像，而是用低通分辨率处理之后的图像，所以把不同分辨率的图像纳入到评价参数中更符合人的使用习惯。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;2021，加油。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;2021.1.21 Nature&lt;/p&gt;
&lt;/bl</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Docker：配置、基础、应用栈、私有云</title>
    <link href="http://example.com/2020/12/23/Docker%EF%BC%9A%E9%85%8D%E7%BD%AE%E3%80%81%E5%9F%BA%E7%A1%80%E3%80%81%E5%BA%94%E7%94%A8%E6%A0%88%E3%80%81%E7%A7%81%E6%9C%89%E4%BA%91/"/>
    <id>http://example.com/2020/12/23/Docker%EF%BC%9A%E9%85%8D%E7%BD%AE%E3%80%81%E5%9F%BA%E7%A1%80%E3%80%81%E5%BA%94%E7%94%A8%E6%A0%88%E3%80%81%E7%A7%81%E6%9C%89%E4%BA%91/</id>
    <published>2020-12-23T03:52:20.000Z</published>
    <updated>2020-12-26T09:21:26.163Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h1><ul><li><p>Docker的安装：在Ubuntu、CentOS或者Windows上的安装Docker。</p></li><li><p>容器操作：启动容器、守护态运行、终止容器。</p></li><li><p>搭建一个Docker应用栈：获取镜像、应用栈容器节点互联、应用栈容器节点启动、应用栈容器节点配置。</p></li><li><p>实现私有云：启动Docker、获取镜像、实现sshd，在Base镜像基础上生成一个新镜像、分配容器、搭建自己的私有仓库。</p></li></ul><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>Docker是PaaS提供商dotCloud开源的一个基于LXC（Linux Containers，基于Linux的容器机制）的高级容器引擎，源代码托管在Github上，基于go语言并遵从Apache2.0协议开源的虚拟化技术。</p><p>可以把Docker近似地理解成是一个“轻量级的虚拟机”：<strong>只消耗较少的资源就能实现对进程的隔离保护</strong>。使用Docker可以把应用程序和它相关的各种依赖（如底层库、组件等）“打包”在一起，这就是Docker镜像（Docker image）。Docker镜像可以让应用程序不再顾虑环境的差异，在任意的系统中以容器的形式运行（当然必须要基于Docker环境），极大地增强了应用部署的灵活性和适应性。</p><p>在Docker的网站上提到了Docker的典型场景：</p><ul><li><p>Automating the packaging and deployment of applications（使应用的打包与部署自动化）</p></li><li><p>Creation of lightweight, private PAAS environments（创建轻量、私密的PAAS环境）</p></li><li><p>Automated testing and continuous integration/deployment（实现自动化测试和持续的集成/部署）</p></li><li><p>Deploying and scaling web apps, databases and backend services（部署与扩展webapp、数据库和后台服务）</p></li></ul><h1 id="Docker配置"><a href="#Docker配置" class="headerlink" title="Docker配置"></a>Docker配置</h1><p>在Mac OS上配置Docker。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>和安装Mac普通应用一样，下载.dmg安装包，运行安装包，把应用拖到到应用程序目录即可。</p><p>官方下载地址：<a href="https://download.docker.com/mac/stable/Docker.dmg">https://download.docker.com/mac/stable/Docker.dmg</a></p><p>可以使用国内地址（阿里云）进行下载：<a href="http://mirrors.aliyun.com/docker-toolbox/mac/docker-for-mac/stable/Docker.dmg">http://mirrors.aliyun.com/docker-toolbox/mac/docker-for-mac/stable/Docker.dmg</a></p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/1.png" title="Optional title"></p><p>初次打开会提示输入Mac的密码，输入后点击安装帮助程序即可。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/2.png" title="Optional title"></p><p>安装完之后，打开Docker桌面程序，可以看到如下用户界面。</p><p>教程。帮助建立一个Docker镜像和容器，最后一步是发布到自己的个人账号上。需要sign in。可以跳过。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/3.png" title="Optional title"></p><p>用户界面，左边栏分为Containers和Images，也是Docker的两个最重要的概念。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/4.png" title="Optional title"></p><p>可以在用户界面操作Docker，也可以直接使用terminal进行操作（实际上Docker可以直接在其用户界面使用terminal）。</p><h2 id="配置镜像加速"><a href="#配置镜像加速" class="headerlink" title="配置镜像加速"></a>配置镜像加速</h2><p>如果需要使用别人发布的镜像时，Docker默认从DockerHub拉取镜像。<strong>这个操作有时会遇到网络方面的困难</strong>，此时可以配置镜像加速器。</p><p>Docker官方在中国区提供了镜像地址，而且国内很多云服务商都提供了国内加速器服务，例如网易、阿里云等。</p><p>这里选择阿里云的镜像加速。登入阿里云网站：<a href="https://cr.console.aliyun.com/">https://cr.console.aliyun.com</a> ，左边栏-&gt;容器镜像服务-&gt;镜像中心-&gt;镜像加速器，默认会生成属于个人的一个加速器地址（需要注册阿里云账号）。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/5.png" title="Optional title"></p><p>得到上述加速器之后，打开Docker桌面程序-&gt;Preferences-&gt;Docker Engine，使用json格式进行配置。</p><p>原来的配置：</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/6.png" title="Optional title"></p><p>修改为：</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/7.png" title="Optional title"></p><h2 id="配置结果"><a href="#配置结果" class="headerlink" title="配置结果"></a>配置结果</h2><p>在terminal中，使用<code>docker info</code>可以查看Docker是否安装成功以及配置的结果。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/8.png" title="Optional title"></p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/9.png" title="Optional title"></p><h1 id="容器：基础操作"><a href="#容器：基础操作" class="headerlink" title="容器：基础操作"></a>容器：基础操作</h1><p><strong>创建一个Apache容器并进行一些基础操作</strong>。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/10.png" title="Optional title"></p><p><code>docker image ls</code>：用于列出所有镜像的属性。</p><p><code>docker run -p 80:80 httpd</code>：创建一个Apache容器。其中，<code>-p</code>用来指定端口映射关系：第一个为本地端口，第二个为容器端口；<code>httpd</code>是Apache超文本传输协议（HTTP）服务器的主程序，被设计为一个独立运行的后台进程，它会建立一个处理请求的子进程或线程的池。主要的命令是<code>docker run</code>，该命令会运行本地的镜像，如果该镜像不存在的话，会先从镜像仓库拉取该镜像。因此，这条命令的输出包含了两部分结果：第一部分是<code>docker pull</code>，用来从镜像仓库中拉取或者更新指定镜像，第二部分是运行该容器并输出log内容。</p><p>使用浏览器及<code>本地ip</code>访问80端口，会出现如下页面，说明Apache服务成功运行：</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/11.png" title="Optional title"></p><p>对运行的容器做如下操作：</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/12.png" title="Optional title"></p><p><code>docker ps -a</code>：列出所有的容器（不管是运行中的还是未运行的），可以看到刚刚建立的容器的一些状态。其中，<code>CONTAINER ID</code>给出了容器的ID，可以用这个ID指定该容器进行一些操作。<code>STATUS</code>中<code>Up</code>表示该容器正在运行。</p><p><code>docker logs 540cb924672b</code>：指定容器查看其log内容。可以看到这里的输出内容和上一图中的内容是一致的，也证明了服务成功运行。</p><p><code>docker stop 540cb924672b</code>：停止该容器。停止之后再使用ps命令查看状态，可以发现该容器的状态变成了<code>Exited (0)</code>。</p><p><code>docker run -d -p 80:80 httpd</code>：与上面一条run命令相比，加入了<code>-d</code>参数，<strong>表示容器运行于前台还是后台（即守护态）</strong>，默认为false。接着再使用ps命令查看状态，可以发现从该镜像新建了一个容器（不同的ID）。</p><p>在Docker桌面程序中也可以看到刚刚建立的两个容器。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/13.png" title="Optional title"></p><h1 id="应用栈"><a href="#应用栈" class="headerlink" title="应用栈"></a>应用栈</h1><p>Mac上做Docker应用栈，配置容器的时候会遇到一个极其让人难受的问题：</p><p><a href="https://forums.docker.com/t/var-lib-docker-does-not-exist-on-host/18314">https://forums.docker.com/t/var-lib-docker-does-not-exist-on-host/18314</a></p><p>本来是想用网上那个大家已经做烂了的例子去做一下的：</p><p><a href="https://blog.csdn.net/u012066426/article/details/52597991">https://blog.csdn.net/u012066426/article/details/52597991</a></p><blockquote><p>参考《Docker容器和容器云》 2.3.2章节应用栈搭建过程</p></blockquote><p><strong>这个问题本质上还是机器架构导致的软件环境的问题。应用栈搭建本身是比较简单的工作。</strong></p><p>因为这个问题，决定放弃了，因为真的很浪费时间，4天了没有找到什么解决方案。</p><h1 id="私有云"><a href="#私有云" class="headerlink" title="私有云"></a>私有云</h1><p>hub.docker.com（官方的Docker hub）上可以保存镜像，是一个用于管理公共镜像的地方，我们可以在上面找到我们想要的镜像，也可以把我们自己的镜像推送上去，但是网速相对较慢。修改了镜像地址可以加快网速，但是某些情况下，开发需要更加定制化和私人化的要求时，再或者个人的服务器无法访问互联网时，在内部环境中搭建一个私有的公共仓库是个更好的方案，这就是Docker私有云（私库）的意义。</p><p>过程如下：</p><h2 id="私有云工具：registry"><a href="#私有云工具：registry" class="headerlink" title="私有云工具：registry"></a>私有云工具：registry</h2><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/14.png" title="Optional title"></p><p><code>docker pull docker.io/registry</code>：利用官方提供的工具<strong>registry</strong>来配置私库。需要注意的是，这个工具是个本身就是个镜像，直接下载并使用该镜像启动容器就可以完成私库的搭建。</p><h2 id="搭建"><a href="#搭建" class="headerlink" title="搭建"></a>搭建</h2><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/15.png" title="Optional title"></p><p><code>docker run -d -p 5000:5000 --name registry_jfy --restart always -v /Users/qiwu/docker/registry:/var/lib/registry registry</code>：建立私库。其中，<code>-d</code>设置该私库为后台守护态运行；<code>-p</code>给出端口及其映射；<code>--name</code>设置该私库的名称为<code>registry_jfy</code>；<code>--restart</code>表示该容器总随着docker服务的开启而启动；<code>-v</code>把registry的镜像路径映射到本地。</p><p>使用<code>ps</code>命令查看容器，可看到刚刚新建立的私库的情况。使用 <a href="http://127.0.0.1:5000/v2/_catalog">http://127.0.0.1:5000/v2/_catalog</a> 测试刚刚搭建的私库的内容。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/16.png" title="Optional title"></p><blockquote><p>这里应该是空的repositores，busybox是之前用来做尝试而上传上去的镜像，此处无视之。</p></blockquote><p><code>docker tag ubuntu:latest 127.0.0.1:5000/ubuntu</code>：希望将本地已有的ubuntu镜像（本来是用来做应用栈的）作为私库的测试文件。标记本地镜像并指向目标仓库。<strong>这一步是必须的，如果直接将未tag的镜像上传，会提示此镜像不存在。</strong></p><p>tag操作之后，使用<code>ps</code>命令查看容器，发现多了一个刚刚标记过的镜像，大小和原本的ubuntu是一样的。</p><p>还是使用 <a href="http://127.0.0.1:5000/v2/_catalog">http://127.0.0.1:5000/v2/_catalog</a> 查看私库的内容，可以发现增添了刚才push的ubuntu。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/17.png" title="Optional title"></p><p><code>docker push 127.0.0.1:5000/ubuntu</code>：为了测试私库的镜像，先将本地已有的两个ubuntu镜像删除。</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/18.png" title="Optional title"></p><p><code>docker pull 127.0.0.1:5000/ubuntu</code>：从本地仓库拉取到docker容器。</p><p>使用<code>ps</code>命令查看pull前后的镜像，发现多了名为<code>127.0.0.1:5000/ubuntu</code>的ubuntu镜像，说明成功的从本地仓库pull到了刚刚push上去的ubuntu。</p><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p><strong>实验本身比较简单</strong>。Docker的使用是比较友好的，尤其是在Mac上，还有桌面程序，即使对敲terminal命令不怎么熟悉的人，也有内置的一些教程帮助初步的使用。但实际上大部分的操作还是需要熟悉terminal命令。</p><p><strong>但是也有不怎么友好的地方，就是在搭应用栈的时候，因为操作系统导致的问题，上面已经说过，不再赘述，这里给差评。</strong></p><p>做完了这个实验之后，突然感觉虚拟化的东西真的已经不是一个纸面上的概念了，大家都在用这类应用做东西。尤其是在查找应用栈和私有云的资料的时候，会有很多的商业化广告，内容都是帮助公司等搭建Docker等的私有云。也非常惊诧居然有Docker这种轻量级的东西来做虚拟化，我个人的理解就是将“阉割”做到了标准化和规范化，以减少成本。这个概念以后也许不会再用到，但这种思路对于个人的学习是非常有帮助的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;目标&quot;&gt;&lt;a href=&quot;#目标&quot; class=&quot;headerlink&quot; title=&quot;目标&quot;&gt;&lt;/a&gt;目标&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Docke</summary>
      
    
    
    
    
    <category term="云计算与分布式系统" scheme="http://example.com/tags/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="Docker" scheme="http://example.com/tags/Docker/"/>
    
  </entry>
  
</feed>
