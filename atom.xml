<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>MoyangSensei</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2023-07-05T09:50:35.652Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Fy J</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>博士申请指南：2023+bjtu+计院+cs（二）</title>
    <link href="http://example.com/2023/07/04/%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97%EF%BC%9A2023-bjtu-%E8%AE%A1%E9%99%A2-cs%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://example.com/2023/07/04/%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97%EF%BC%9A2023-bjtu-%E8%AE%A1%E9%99%A2-cs%EF%BC%88%E4%BA%8C%EF%BC%89/</id>
    <published>2023-07-04T08:40:58.000Z</published>
    <updated>2023-07-05T09:50:35.652Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>个人经历，转载也没啥意义，看看就得了。</p></blockquote><blockquote><p>“关于本博客中内容的重要说明”请看（一）。</p></blockquote><hr><h1 id="复试：2022-12-2023-01"><a href="#复试：2022-12-2023-01" class="headerlink" title="复试：2022.12-2023.01"></a>复试：2022.12-2023.01</h1><h2 id="复试安排"><a href="#复试安排" class="headerlink" title="复试安排"></a>复试安排</h2><p>相关通知：</p><ul><li><p>221226：<code>2023年申请考核博士考生复试安排</code>。面试时间230105，从发通知到面试中间也就10天时间。</p></li><li><p>221226：<code>2023年申请考核博士复试流程说明</code>。安排双机位，跟22年的通知几乎一样。</p></li><li><p>221226：<code>2023年申请考核博士研究生报考材料审核通过名单（非定向就业）</code>。公布了<code>69个进面</code>的人。</p></li><li><p>221228：<code>申请考核复试秘书老师微信查询方式已在我校博士招生系统信息动态栏中发布，请同学们及时添加</code>。内容如题。</p></li></ul><p>首先根据复试安排，回复邮件确认参加面试：</p><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/16.png" title="23复试确认邮件"></p><p>综合素质在线测评从邮件里参加：</p><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/17.png" title="23综合素质在线测评邮件"></p><p><code>复试序号</code>、加<code>面试秘书老师微信</code>的通知是在博士招生系统上给的，加上微信了之后听指挥走流程就行。</p><p>首先是面试测试：</p><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/18.png" title="面试测试与秘书聊天记录"></p><p>检查面试环境是在<code>腾讯会议</code>，举着手机给老师看看你那个面试的独立房间，个人感觉比研究生复试的时候检查的松很多，只要保证你摆机器的桌面上干净就行，屋子内杂七杂八的东西基本不会要求清理。甚至我桌子上摆了一个台式机，机箱在下，显示屏跟我的笔记本都放在桌子上一前一后，我特意问了需不需要把台式机挪到屋子外，毕竟也是电子产品，得到的回答是不用，面试的时候别开台式机就行。双机位一般都是用手机放在座位侧后方，左右都行，需要提前买一个手机支架，可伸缩的三角架加上上面有一个可以夹手机的头。</p><h2 id="复试当天的准备工作"><a href="#复试当天的准备工作" class="headerlink" title="复试当天的准备工作"></a>复试当天的准备工作</h2><p>我当天的面试顺序应该是倒数第二个，所以从下午1点钟开始等待，一直等到晚上8点半左右才轮到我，非常的煎熬。</p><p>面试当天上午：</p><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/19.png" title="面试当天与秘书聊天记录1"></p><p>然后就一直干等，期间秘书老师还大致跟我讲了时间，我也能根据自己的序号判断出来自己应该很靠后：</p><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/20.png" title="面试当天与秘书聊天记录2"></p><p>得到了会议号之后，双机位都进入这个会议等待。最开始会议里只有3个账号，是秘书和我的两个号。在里面等了大概10来分钟之后，突然进来了一个账号，一个男老师的声音直接跟我说考生可以开始了，我问了一句“老师，我的声音和屏幕共享正常吗”，得到答复后我就开始念叨我的ppt了。据我导消息，面试是老师们在线下的一个教室里一起听的，因此只有一个账号进了会议。问我的那个男老师应该是主持人，后续问了我几个问题也是这个老师进行询问的。</p><h2 id="面试现场"><a href="#面试现场" class="headerlink" title="面试现场"></a>面试现场</h2><p>面试有两个主要内容，一个是ppt和个人讲解，另一个是问答环节。</p><p>ppt的结构和内容主要还是多找导师把关，导师们在这方面比较懂，他们知道老师们爱听什么不爱听什么。ppt的结构是尤其重要的，每个人仅有<code>8分钟</code>时间讲ppt，科研内容中非常细节的啰里八嗦的东西是没时间说的。这里给出我ppt的目录页和我个人讲解内容的提炼：</p><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/21.png" title="面试ppt目录页"></p><p>个人讲解没什么好说的，我的做法是：在ppt的内容固定了之后，就定好自己要讲什么，每一页的哪一个部分要固定的说哪写话（话本身不用固定，话传达的内容固定），每天对着ppt掐时间练，基本上<code>练到7分半</code>就是最完美的状态。我练到最后固定在7分20秒-7分23秒之间。千万别超时，这种场合被打断了那可是相当搞心态的。</p><p>我讲完之后被询问到：</p><p><strong><em>Q1：（英语问答）你为什么选择重新回到bjtu？</em></strong><br>A1：我在三年前的研究生考试中失败了，从那以后我就very very的想回来，我想在这里继续学习。</p><p><strong><em>Q2：你博士为什么选择做你说的这个方向？</em></strong><br>A2：是导师的强势方向，实验室有积累，且跟我之前做的CV方向相关。</p><p><strong><em>Q3：GAN为什么是无监督的？（被叫停后）你的毕业设计不是用了GAN吗，你解释一下？</em></strong><br>A3：解释了一通GAN的运行原理。（被叫停后）我还没有做，只是设想。</p><blockquote><p>这里尴尬的一，虽然我知道GAN从头到尾的运作原理，但是我被问懵了。我甚至提到了初始的数据是random的，训练数据是无标签的，当场脑子短路也没解释清楚为啥是无监督网络……</p></blockquote><p><strong><em>Q4：你博士研究设想中为什么对“小目标目标检测”提了这些问题？</em></strong><br>A4：从持续学习和灾难性遗忘方面进行了解释。</p><p>我已经忘记现场是问了我4个问题还是5个问题，但大体就是上面那些。实际上问出这上面这些问题来是完全出乎我的意料了。<strong>首先英文问答，2022年是没有英语问答的，我是在等待面试的过程中被我导告知今年有英文问答。然后我问我导“英文问题问的是专业相关问题还是像研究生复试那样天南海北的问”，我导回“专业相关的问题问得多”。</strong>其次，也许是我论文做的太差，我想着怎么着也得针对我已经做出来的工作挑挑刺吧，结果也是一个都不问。所以我猜测老师们更喜欢对未来的规划，对于过去的事情老师们基本是看不上眼的。尤其是Q2和Q4，在ppt讲解中第三部分都是略过的，前后加起来1分钟都不到，问题提出部分我都是直接照着ppt念的，结果人家老师抓住这里问。得亏我导在面试前给我补了课，行业内的大方向大问题相关概念等的关键词都给我讲了，不然这俩问题我都得烂掉。回答的时候我导给我讲到的我几乎都用过到了，并且我还多提了几次“我导该方向做的强”之类的话。<strong>这里实际上凸出了自己对所选择导师的情况确实是有所了解的，博士研究设想也不是画饼，虽然大家都知道这是画饼，进来之后做什么要看情况变化和导师安排。</strong>面下来总共用时15分钟左右。</p><h1 id="等待成绩：2023-1-2023-5"><a href="#等待成绩：2023-1-2023-5" class="headerlink" title="等待成绩：2023.1-2023.5"></a>等待成绩：2023.1-2023.5</h1><p>成绩出来之前的这段日子中没有任何事情，也没有任何通知。这段日子长到让人怀疑人生（反正我深刻怀疑了我的人生），因为这中间考研成绩出来了，要搞硕士研究生复试。等硕士的事情都搞的差不多了，学院才会想起这一帮博士，近两年都是如此。可以根据硕士研究生的通知来大致判断出成绩的日子，基本就是五一前后。23年公布成绩算是相当晚的了，比22年晚了20多天。</p><p>左等右等等不到，导师也天天在学校询问消息。终于5月16号发了第一波拟录取名单：</p><ul><li>230516：<code>2023年博士研究生招生拟录取名单公示</code>。如题。</li></ul><p>然后听我导说有申请考核放弃拟录取的，随后出了补录：</p><ul><li>230612：<code>2023年拟录取博士研究生名单(补报名批次)</code>。补录了2个申请考核和3个硕博连读。</li></ul><p>公示是研究生院官网为全体拟录取博士统一发的：</p><ul><li>230612：<code>北京交通大学2023年博士研究生拟录取名单公示</code>。公示日期为10个工作日，2023年6月12日-2023年6月26日。</li></ul><blockquote><p>公示中的说明：<br>根据教育部、北京市相关文件要求，经各学院研究生招生工作小组审核，校研究生招生工作领导小组审批，在各招生学院公布拟录取考生名单和成绩的基础上，现将我校2023年博士研究生拟录取名单进行统一公示，公示期10个工作日（2023年6月12日-2023年6月26日）。公示期间，对拟录取名单有异议者，请通过电子邮件向我办实名咨询和反映。……<br>特别说明：此名单为拟录取名单，需要上报教育部审核通过后才能成为正式录取名单。未通过审核、未按时送交所需材料、弄虚作假一经查实的考生，将取消其录取资格。</p></blockquote><p>随后又出了一次补录：</p><ul><li>230621：<code>2023年拟录取博士研究生名单(补报名批次)</code>。补录了2个硕博连读。</li></ul><p>补录名单的两个人单另发了一个公示：</p><ul><li>230621：<code>北京交通大学2023年博士研究生补录名单公示</code>。也是10个工作日。</li></ul><p>到这里复试过程算完整结束了。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>面试这种事情真的是千变万化，每一年的情况都会有变化。这些变化看着简单，可一旦影响到了自己，那就跟一座山压在头上了一样。上述经历在24年说不定会直接过时，因为这是疫情时代中我个人的沉浮。因此再次强调，个人经历仅供参考。能帮助到各位读者自然是好，没帮到的话就看看图一乐。有价值的信息我会随时更新进来。本来想接着写关于拟录取之后调档、转关系、工位、宿舍、录取通知书的事情，但转念一想，只要能录取，这些琐碎事情都无大所谓（事情太细碎了，主要还是懒得写）。</p><p>有道是<strong>莫愁前路无知己，天下谁人不识君</strong>，我祝各位同好早日达成心中心愿。录取之后记得来我工位找我打牌喝茶。</p><hr><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="对于“官网公告”的说明"><a href="#对于“官网公告”的说明" class="headerlink" title="对于“官网公告”的说明"></a>对于“官网公告”的说明</h2><p>申博过程中大部分信息都是来自学院官网、研究生院、博士招生系统的公告。本博客不放重要官网公告链接、名单附件的理由是：</p><p>（1）出于隐私考虑。学院官网、研究生网的内容虽然都是全网公开的，但是我认为我个人也只能对这些内容进行简单描述。我也不是不能全搬过来，只是我确实怕搞出一些不必要的事情来。具体的内容各位想看想查，还是亲力亲为吧。</p><p>（2）我按时间序列记录的官网公告，它们其中的一些内容是会在事后被删除的，包括但不限于名单公示、博士招生系统的内容等。至于什么时候删除，也是未知的，就像学院官网现在还挂着22年的拟录取名单等公告，但是再往前几年的内容全删了。研究生院把22年的公示公告都删了，以前的那就更没有了，不过其他有关博士招生的基本都保留了。博士招生系统的公告都是些琐碎的小事情，比如交材料、改信息等，也会随时删除。</p><h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>记录在申博之路上为我提供了直接或间接帮助的人，排名无先后。</p><p>致谢统一写在这里，后面两篇就不写了。</p><ul><li><p>znc：读研时我导介绍来带我的导师。2016年就带我做比赛了，是我进行硕士课题中的最强依靠，能够在极限时间内拿下论文，全靠该老师和我导。</p></li><li><p>gw：读研时同门。他曾经也想读博，报考了bjtu软件学院23年的博士申请考核。他与我共享了他几乎所有的报考材料。</p></li><li><p>不知名的复试秘书老师：秘书老师很负责，面试时能及时的告知我等待时常，缓解了我当天的焦虑。</p></li><li><p>zys：现组内师兄。22级申请考核进来的博，为我提供大量技术支持、情报支持和心理支持。</p></li><li><p>hsl：读研时学院大领导。为我提供专家推荐信，且很认真的帮我改了信内容。</p></li><li><p>cyj：我导。本科时就带我搞各种比赛和科研，博士期间是我导。<strong>在此强推我导，学术素养高，对待学生耐心，愿意为学生解决各种困难，脾气好。</strong></p></li><li><p>lt：读研时我导。为我提供专家推荐信及大量信息。</p></li><li><p>xkh：读研时同门。多次帮我解决材料问题。</p></li><li><p>jl：读研时同门。加上hxk，我们三个人同为宿舍好兄弟，这俩时常安慰我，请我吃个火锅啥的。</p></li><li><p>cxw：读研时同门。研一下晋升为女朋友（目前仍是），在我文章翻译、投稿工作中提供有力帮助。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;个人经历，转载也没啥意义，看看就得了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“关于本博客中内容的重要说明”请看（一）。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;复试：2022-12-2023-0</summary>
      
    
    
    
    
    <category term="闲谈" scheme="http://example.com/tags/%E9%97%B2%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title>博士申请指南：2023+bjtu+计院+cs（一）</title>
    <link href="http://example.com/2023/07/02/%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97%EF%BC%9A2023-bjtu-%E8%AE%A1%E9%99%A2-cs%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://example.com/2023/07/02/%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97%EF%BC%9A2023-bjtu-%E8%AE%A1%E9%99%A2-cs%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2023-07-02T06:40:18.000Z</published>
    <updated>2023-07-05T09:50:37.319Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>个人经历，转载也没啥意义，看看就得了。</p></blockquote><hr><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这一篇博客离上一篇有接近8个月了，这期间忙活的都是硕士毕业和博士申请的事情。自2020年研究生入学的时候，就有考虑读博，这个考虑基于三年疫情导致的工作难找，也基于个人的特殊情况。</p><p>时至今日，这件事情也算是圆满完成。在这期间有很多人为我提供了帮助，以及自身的努力加上相当不错的运气，才能有这样的结果。此时坐在新工位上，我想把这段经历做下分享，在记录生活的同时也为后来人指引一下方向，能积一份德就积一份德。</p><h1 id="关于本博客中内容的重要说明"><a href="#关于本博客中内容的重要说明" class="headerlink" title="关于本博客中内容的重要说明"></a>关于本博客中内容的重要说明</h1><blockquote><p>用来免责的，毕竟我在网络上分享这些，我等于也在网上透明了，怕出事。</p></blockquote><ul><li><p>本博客的目的是本博客中的所有内容均为个人经历，不可复制，仅供参考；</p></li><li><p>本博客中包含大量互联网内容，均为相关单位在互联网中公开发表的内容，使用相关单位之外的ip也可访问到这些内容；</p></li><li><p>本博客中包含涉及他人隐私的内容，包括聊天记录截图、邮件等，已将所有个人信息做屏蔽处理，仅提供与博士申请相关的内容；</p></li><li><p>如果本博客中的内容有任何侵权或者其他违法行为，敬请邮件告知，本人将立即删除。</p></li></ul><h1 id="个人情况"><a href="#个人情况" class="headerlink" title="个人情况"></a>个人情况</h1><p>为什么申请bjtu呢？因为实际上我本科就是bjtu计算机与信息技术学院2016级的一个小透明，专业同样是cs，2016-2020呆了4年。2020年初赶上疫情，也没回校参加毕业典礼和收拾行李，黯然离场了属于是。19年大三，绩点稍微差些，保研没报上，记得当时保到3.6几的绩点，我加上保研加分也就只有3.58。然后考研惨遭滑铁卢，就考了294，数一58差点没过国家线。我数学实在是烂，虽然没挂过科，但几大门数学课的绩点拖了很多后腿。2020年bjtu的专硕线在前后一年内都是最低的，只有315，复试几乎没压力，导师也找好了，可以说是进面就随便上。但就这都没考上，真的属于是好牌打得稀烂。然后调剂呗，当时考研还没卷到所有学校所有位置考爆满的地步，因此通过导师的人脉和神通，调剂到了btbu计院的学硕。但由于考的实在是有点烂，没调剂到cs，专业成了轻工技术与工程。这专业是btbu的优势学科与计算机交叉的结果，就跟bjtu的铁信一个性质。当时还有一条路就是bjtu再参加非全的面试，这俩一权衡，那还是选择全日制得了。期间种种的心酸和不甘在此不表，反正这就算是有学上了。</p><p>btbu在读期间，上的课搞的内容都是计算机相关的，唯独专业歪了。因此22年秋招到23年春招，找工作真的就突出一个难字。简历关都过不了几个，能有机会面了几个大厂也是因为第一学历对口和熟人内推，但也没啥用，都挂了。最后一合计：申请博士！于是在这条路上猛冲，毕竟实在是没路子了，总不能回家躺着啃老。接下来就有了以下的经历。</p><h1 id="2022申请考核博士的重要通知时间线"><a href="#2022申请考核博士的重要通知时间线" class="headerlink" title="2022申请考核博士的重要通知时间线"></a>2022申请考核博士的重要通知时间线</h1><blockquote><p>所有通知的标题中，含有“北京交通大学计算机与信息技术学院”、“计算机与信息技术学院”的都省略了，后同。</p></blockquote><ul><li><p>211025：<code>2022年学术型博士研究生申请考核制招生实施办法</code>。就是招博士的总章，主要是申请条件（外国语水平）和申请流程（提交材料列表），内容在近几年应该不会有太大变化，以当年为准。</p></li><li><p>211025：<code>2022年工程博士研究生申请考核制招生实施办法</code>。同上。</p></li></ul><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/1.png" title="招生办法"></p><ul><li>211230：<code>2022年申请考核博士考生复试安排</code>。邮件确认复试和参加综合素质测评（主要就是一些心理测试，正常选就行），然后面试是远程，需要测试面试环境。</li></ul><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/2.png" title="复试安排1"></p><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/3.png" title="复试安排2"></p><ul><li><p>211230：<code>2022年申请考核博士复试考核流程说明</code>。主要说双机位的事情，最需要注意的两点：不可以使用任何类型耳机、不要迟到。</p></li><li><p>211230：<code>2022年申请考核博士研究生报考材料审核通过名单</code>。公布所有通过材料审核进入面试的人，部分人因为外国语水平不达标提示“须参加1月19日上午英语水平考核”。</p></li><li><p>220420：<code>2022年博士招生考核成绩</code>。这里仅仅只是考核成绩，不是拟录取名单，但在这之前通过导师的消息渠道基本上就知道自己能否被录取。给了附件，看就完事了。</p></li><li><p>220424：<code>2022年博士研究生招生录取工作办法</code>。与之前公布的招生实施办法的内容有部分耦合，最重要的是公布了招生计划、成绩怎么算。</p></li></ul><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/4.png" title="招生计划"></p><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/5.png" title="成绩算法"></p><ul><li><p>220428：<code>2022年博士研究生招生拟录取名单</code>。给了附件，看就完事了。</p></li><li><p>220506：<code>2022年博士招生拟录取信息变动公示</code>。有1个cs的人放弃，往上替补了1个人。</p></li><li><p>220520：<code>2022年博士招生拟录取信息变动公示</code>。补录了1个cs的 科研博士专项。</p></li><li><p>220612：<code>2022级博士同学建钉钉群通知</code>。全日制的同学使用手机号加钉钉。</p></li><li><p>220628：<code>关于2022年拟录取博士研究生调档函及定向培养告知书开通下载的通知</code>。调档，这个通知是研究生院对所有学院统一发的。</p></li></ul><h1 id="前期准备：2022-9-2022-10"><a href="#前期准备：2022-9-2022-10" class="headerlink" title="前期准备：2022.9-2022.10"></a>前期准备：2022.9-2022.10</h1><p>前期准备工作最主要的就是收集前一年的博士申请考核信息，并且找个有经验的人带一带。</p><ul><li>220909：经导师推送加了22级申请考核进来的师兄的微信，拿到了他所有的资料，包括提交系统的材料和面试时的ppt。</li></ul><h1 id="材料提交：2022-11-2022-12"><a href="#材料提交：2022-11-2022-12" class="headerlink" title="材料提交：2022.11-2022.12"></a>材料提交：2022.11-2022.12</h1><p>根据下面两个通知，博士招生系统材料的上传时间和纸质材料的邮寄时间为<code>221112-221216</code>。</p><ul><li><p>221112：<code>2023年学术型博士研究生申请考核制招生实施办法</code>。</p></li><li><p>221112：<code>2023年工程博士研究生申请考核制招生实施办法</code>。</p></li></ul><h2 id="博士招生系统注册"><a href="#博士招生系统注册" class="headerlink" title="博士招生系统注册"></a>博士招生系统注册</h2><p>需要先在博士招生系统注册账号，需要一个<code>常用邮箱</code>作为登陆号。注册成功后会给一个<code>报考号</code>，这是报考期间的唯一身份ID，比如我的就是2023000xxxx，xxxx就是你在系统上注册的序号，我是第1000多位在系统上注册的。</p><p>然后需要在系统上填一系列的材料，我记忆中首先是选择导师和方向，然后填自己的教育信息，反正乱七八糟一堆，不确定的内容具体询问导师或者自己师兄。填完之后确认所有自己填的信息，点完确定之后就不能改了，只能看信息确认页面，如下：</p><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/10.png" title="信息确认页面1"></p><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/11.png" title="信息确认页面2"></p><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/12.png" title="信息确认页面3"></p><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/13.png" title="信息确认页面4"></p><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/14.png" title="信息确认页面5"></p><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/15.png" title="信息确认页面6"></p><p>最上面的4个按钮下载生成的文件，都是pdf格式。其中<code>政审表</code>和<code>诚信承诺书</code>是需要打印签字并邮寄的，其他两个到目前为止也没啥地方用得到。</p><p>最下面的8个按钮可以下载在系统中上传的8项内容。</p><h2 id="博士招生系统上传材料"><a href="#博士招生系统上传材料" class="headerlink" title="博士招生系统上传材料"></a>博士招生系统上传材料</h2><p>这里一共8项内容，第1项是证件照，这个是录取以后学生卡上的照片，需要近期蓝底照片，大家可以去精修证件照然后上传。</p><p>接下来7项在系统里称为“其他文件”，并且在系统的对应页面上会提供详细的<code>其他文件上传要求</code>，今年的上传要求原文为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1、“有效居民身份证”上传要求格式为PDF文件，正反面均需在文件中。</span><br><span class="line">2、“最后学历、学位证书及其认证报告”上传要求格式为PDF文件；直博生不用上传，硕博连读生和应届硕士毕业生上传本科学历、学位证书及其对应的认证报告；其他考生应上传本人的最后学历、学位证书及其对应认证报告；学历认证报考获取网址为中国高等教育学生信息网（https:&#x2F;&#x2F;www.chsi.com.cn&#x2F;xlcx&#x2F;），学位认证报告获取网址为学位网（https:&#x2F;&#x2F;www.chinadegrees.cn&#x2F;cqva&#x2F;gateway.html）。</span><br><span class="line">3、“课程成绩单”上传格式为PDF文件，直博生上传本科阶段的成绩单，硕博连读生、应届硕士毕业生和已获得硕士学位人员上传硕士课程成绩单，同等学力人员上传进修相同或相近专业的硕士研究生课程成绩单。</span><br><span class="line">4、“学生证及其认证报告”上传格式为PDF文件，直博生、硕博连读生及应届硕士毕业生需要上传学生信息页及学籍认证报告，其他考生不需要上传。学籍认证报告获取网址中国高等教育学生信息网（https:&#x2F;&#x2F;www.chsi.com.cn&#x2F;xlcx&#x2F;）。</span><br><span class="line">5、“英语水平材料”上传格式为PDF文件，符合学校及学院规定的英语水平证明材料。</span><br><span class="line">6、“基础水平材料”上传将以下（1）—（8）要求的材料按照顺序扫描、编辑、汇总为一个带有目录的PDF文件。</span><br><span class="line">（1）有效居民身份证，将正反面均放置。</span><br><span class="line">（2）至少两名所报考学科专业领域内的教授（或相当专业技术职称的专家）专家的推荐信（由专家填写，如果打印，专家须在每一页纸上签字）。</span><br><span class="line">（3）往届生提供硕士学位、学历证书，应届硕士毕业生及同等学力考生提供本科学历、学位证书。</span><br><span class="line">（4）应届硕士毕业生提供硕士研究生学生证或学校研究生管理部门开具的在校证明。</span><br><span class="line">（5）硕士课程学习成绩单（或硕士成绩单复印件加盖档案部门公章），同等学力考生提供进修相同或相近专业的硕士研究生课程成绩单原件（加盖研究生培养部门的公章）。</span><br><span class="line">（6）往届生提供硕士学位论文，应届硕士毕业生提供论文目录、详细摘要和主要成果。</span><br><span class="line">（7）获奖证书、公开发表的学术论文、所获专利及其他原创性研究成果的证明材料。</span><br><span class="line">（8）外国语水平材料。</span><br><span class="line">（9）报考学院要求的其他材料。</span><br><span class="line">7、“学科综述与研究设想” 上传格式为PDF文件，按照报考学院的要求撰写完成后，提供一份攻读博士学位期间的学科综述与研究设想</span><br></pre></td></tr></table></figure><p>对于上述7项内容的建议：</p><ul><li><p>第1项直接拿身份证去正反面<code>扫描</code>，因为要用很多次，别搞手机拍照最好。</p></li><li><p>第2项按照他给的网址下载就行，实际上就是学信网，但这里要注意需要<code>手动调整有效期</code>，默认的有效期是1个月，调整到最长，否则学校的人验证的时候扫的那个二维码会失效。pdf的安排顺序可以为：毕业证、学位证、学历备案、学位在线验证。搞的时候我还没拿到硕士毕业，因此放的是本科的双证。学历备案、学位验证要和双证的阶段对应。</p></li><li><p>第3项直接问学校要，一定记得是带<code>公章</code>的那种，搞那种自己弄的成绩表格要去学校加公章然后扫描。</p></li><li><p>第4项学生证从头到尾<code>扫描</code>，学籍在线验证一般是硕士阶段的，同样记得<code>手动调整有效期</code>。 </p></li><li><p>第5项上<code>CET官网</code>查自己能用的6级成绩然后<code>下载电子版</code>，这里需要注意的就是<code>考试成绩时效</code>的问题，你要是特别特别早就考过了6级，比如大二就过了，等你研三申请这个博士，按照时间规定这个成绩是不能用的。学校那边对于这种情况是如何看待和处理我不清楚。</p></li><li><p>第6项中的（2），自己拟完之后去找大佬们签字，注意推荐信模板的<code>封面虽然没有提供签名的地方，但也需要签名</code>。推荐信的内容可以打印，但是<code>必须手写签名</code>。搞完之后扫描完整内容，后面要多次用。</p></li><li><p>第6项中的（4），在校证明可以自己拟好之后找学院盖章，自己学校有模板的直接按照按照模板搞。他这里两项材料写的是“或”，因为学生证前面就用过一次，能全部提供最好全部提供。</p></li><li><p>第6项中的（7），所有能证明自己水平的东西都往上弄，各类证书能扫描最好扫描，能用电子版最好用电子版，所有内容按照重要程度排排序（中了的论文往最前面放），这里的排版能够彰显自己的态度和水平，老师们肯定会比较看重这里。</p></li><li><p>第6项中的（9）实际上指的就是第7项，<code>这里不需要</code>。材料从（1）写到（8）即可。</p></li><li><p>第6项所有内容在整体组织的时候最好正式一些，使用封面+目录，并为所有内容起好题目，材料内容这么多，没点条理的话这绝对会成为材料审核中的减分项。我自己的搞了30页，目录如下。</p></li></ul><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/6.png" title="基础水平材料-目录页"></p><ul><li>第7项是非常重要的材料，体现自己的各类水平，按照模板填，注意排版。最后会给一个<code>二维码</code>，记得自己扫了然后填编号，并加上自己的<code>电子签</code>。我自己的搞了22页。</li></ul><h2 id="纸质材料邮寄"><a href="#纸质材料邮寄" class="headerlink" title="纸质材料邮寄"></a>纸质材料邮寄</h2><p>招生实施办法里面会提供<code>计算机与信息技术学院申请考核博士报考材料整理目录</code>，按照目录整理，并且<code>按时</code>邮寄即可。</p><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/7.png" title="23报考材料整理目录1"></p><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/8.png" title="23报考材料整理目录2"></p><p><img src="/images/%E9%97%B2%E8%B0%88/bjtu%E5%8D%9A%E5%A3%AB%E7%94%B3%E8%AF%B7%E6%8C%87%E5%8D%97/9.png" title="23报考材料整理目录3"></p><p>对于上述14项内容的建议（不是在职博士不用管第15项）：</p><ul><li><p>纸质材料内容与系统上传的内容基本耦合，看清楚整理目录的要求，<code>基本上所有盖章的、签名的都需要提交原件</code>。</p></li><li><p>第3项，<code>政审表的模板是pdf格式</code>，不方便编辑。并且<code>政审表模板在系统提交完所有材料并确认提交之后才能下载</code>。建议自己拟好政审内容找笔迹不同的人手写填入，签名自己签，然后去盖章。当然了有学校的专门负责人直接帮你搞定政审的所有内容更好。</p></li><li><p>第10项，英语水平材料同样注意<code>有效期</code>问题。</p></li><li><p>第11项，<code>诚信承诺书同样是在系统提交完所有材料并确认提交之后才能下载</code>。</p></li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>至此，面试前的工作就做完了。总的来说，前期工作要的就是<code>情报收集能力</code>和一个<code>认真态度</code>，没啥难度。能够找到上一届师兄师姐或者一起报考的同好，一起交流一下是最好的。有些事情，尤其是这个事情，有人帮你参谋和你自己搞，那完全就是两个难度。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;个人经历，转载也没啥意义，看看就得了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;这一篇博客离上一篇有接</summary>
      
    
    
    
    
    <category term="闲谈" scheme="http://example.com/tags/%E9%97%B2%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：DDPM+SR3</title>
    <link href="http://example.com/2022/10/25/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ADDPM-SR3/"/>
    <id>http://example.com/2022/10/25/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ADDPM-SR3/</id>
    <published>2022-10-25T08:16:58.000Z</published>
    <updated>2023-07-02T06:40:30.359Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>关于image SR方向的最后一次论文精读和组会分享，懒得写字了，直接上ppt。</p></blockquote><h1 id="DDPM"><a href="#DDPM" class="headerlink" title="DDPM"></a>DDPM</h1><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/1.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/2.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/3.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/4.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/5.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/6.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/7.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/8.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/9.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/10.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/11.png" title="Optional title"></p><h1 id="SR3"><a href="#SR3" class="headerlink" title="SR3"></a>SR3</h1><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/12.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/13.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/14.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/15.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/16.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/17.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/18.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/19.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/20.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/21.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/22.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/23.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/24.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DDPM+SR3/25.png" title="Optional title"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;关于image SR方向的最后一次论文精读和组会分享，懒得写字了，直接上ppt。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;D</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：TTSR</title>
    <link href="http://example.com/2022/06/15/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ATTSR/"/>
    <id>http://example.com/2022/06/15/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ATTSR/</id>
    <published>2022-06-15T08:53:15.000Z</published>
    <updated>2022-08-02T10:51:12.150Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Learning Texture Transformer Network for Image Super-Resolution</p></blockquote><blockquote><p>CVPR 2020</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>文章指出，现有的超分辨率方法忽略了使用注意力机制从Ref图像传输高分辨率的纹理。</p><blockquote><p>使用高分辨率图像作为参考，把这个称为Ref，同SISR是两种不同的范式，abstract和introduction里提到。</p><blockquote><p>We study on image super-resolution (SR), which aims to recover realistic textures from a low-resolution (LR) image. Recent progress has been made by taking high-resolution images as references (Ref), so that relevant textures can be transferred to LR images.<br>The research on image SR is usually conducted on two paradigms, including single image super-resolution (SISR), and reference-based image super-resolution (RefSR).</p></blockquote></blockquote><blockquote><p>However, existing SR approaches neglect to use attention mechanisms to transfer high-resolution (HR) textures from Ref images, which limits these approaches in challenging cases.</p></blockquote><p>解决思路：TTSR，其中有4个模块。<strong>最大的贡献：最早将transformer体系结构引入图像生成任务。</strong></p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="Texture-Transformer"><a href="#Texture-Transformer" class="headerlink" title="Texture Transformer"></a>Texture Transformer</h2><p>用于纹理变换的transformer，也是本文的核心。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/1.png" title="Optional title"></p><h3 id="Learnable-Texture-Extractor-LTE"><a href="#Learnable-Texture-Extractor-LTE" class="headerlink" title="Learnable Texture Extractor (LTE)"></a>Learnable Texture Extractor (LTE)</h3><p><strong>过去通常使用预训练好的VGG网络提取特征</strong>，LTE是被设计用来进行特征提取的提取器，这里可以自设训练，在超分过程中训练从而更好地提取跨LR和Ref的联合特征。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/2.png" title="Optional title"></p><p>其中LR↑是对原始LR图像经过bicubic上采样得到的，Ref↓↑是通过同样的核先下采样再上采样得到，使和LR↑是domain-consistent。</p><h3 id="Relevance-Embedding-RE"><a href="#Relevance-Embedding-RE" class="headerlink" title="Relevance Embedding (RE)"></a>Relevance Embedding (RE)</h3><p>RE的目的是通过估计Q和K之间的相似度来估计LR和REF图像之间的相关性。</p><p>LTE提取的Q和K被分成多个patch，其中每个patch可以分别表示为:</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/3.png" title="Optional title"></p><p>计算q和k的归一化内积，作为相似度：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/4.png" title="Optional title"></p><blockquote><p>R是没有出现在图上的，R是RE的结果，H和S是通过它得到的。</p></blockquote><h3 id="Hard-Attention-HA"><a href="#Hard-Attention-HA" class="headerlink" title="Hard-Attention (HA)"></a>Hard-Attention (HA)</h3><p>传统的注意力机制会直接计算qi与V的加权和，作者认为这样会造成模糊。具体原因是<strong>由于Ref图像和LR图像之间的domain gap比较大导致直接这样模型会缺乏传递HR纹理特征的能力</strong>。HA则用来传递Ref图像中纹理特征以解决这个问题。</p><p>计算HA map：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/5.png" title="Optional title"></p><p>hi是一个索引值，表示Ref图像中和LR图像中第i个patch最相关的patch的索引。根据这个值去V中索引得到最相关的纹理特征T。</p><h3 id="Soft-Attention-SA"><a href="#Soft-Attention-SA" class="headerlink" title="Soft-Attention (SA)"></a>Soft-Attention (SA)</h3><p>HA是负责找出Ref特征中和LR图像特征相关较强的部分从而建立对应关系，SA是负责将两个特征通过加权的方式更好地融合。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/6.png" title="Optional title"></p><blockquote><p>跳过的公式7就是从HA map中找到最大的（Max）那个。</p></blockquote><h2 id="Cross-Scale-Feature-Integration-CSFI"><a href="#Cross-Scale-Feature-Integration-CSFI" class="headerlink" title="Cross-Scale Feature Integration (CSFI)"></a>Cross-Scale Feature Integration (CSFI)</h2><p>通过堆叠Texture Transformer可以实现不同倍数的放大，作者提出了一个跨尺度特征集成模块（CSFI）在不同尺度的特征之间交换信息。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/7.png" title="Optional title"></p><h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>线性组合Loss，三部分组成：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/8.png" title="Optional title"></p><p>Reconstruction loss：<strong>就是L1。大家都认为这个比L2强。</strong></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/9.png" title="Optional title"></p><p>Adversarial loss：参照WGAN-GP。提出用梯度范数的惩罚来代替权重裁剪加粗样式，使得训练更加稳定，性能更好。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/10.png" title="Optional title"></p><p>Perceptual loss：SRGAN里已经详细提到过了。感知损失的关键思想是增强预测图像与目标图像在特征空间上的相似性，这里包含两部分，第一部分是传统的基于VGG19的特征。第二部分是转移感知损失（transferal perceptual loss），对应LTE提取的特征，<strong>这种传递感知损失限制了预测的SR图像与传递的纹理特征T具有相似的纹理特征，这使得作者的方法更有效地传递Ref纹理。</strong></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/11.png" title="Optional title"></p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><ul><li><p>可学习纹理提取器包含5个卷积层和2个池层，它们以三种不同的尺度输出纹理特征。为了减少时间和GPU内存的消耗，相关性嵌入只应用于最小尺度，并进一步传播到其他尺度；</p></li><li><p>鉴别器G：我们采用了SRNTT中使用的相同网络，并删除了所有BN层；</p></li><li><p>数据增广：随机水平和垂直翻转，然后随机旋转90，180和270；每个minibatch包含9块大小为40×40的LR patch，以及9块大小为160×160的HR和Ref patch；</p></li><li><p>组合Loss的权重：Lrec、Ladv和Lper的权重系数分别为1、1e-3和1e-2；</p></li><li><p>优化器：使用β1=0.9、β2=0.999和ǫ=1e-8的Adam opti-mizer；</p></li><li><p>学习率：1e-4；</p></li><li><p>训练策略：前2个epoch只用Lrec，之后将所有的Loss都用上后训练50个epoch；</p></li></ul><blockquote><p>这部分写在原文的method的最后，也就是3.4，没放在4.1去写。</p></blockquote><ul><li><p>数据集：CUFED5【41】，训练集包含11871对，每对由输入图像和Ref图像组成；测试集共有126幅测试图像，每幅图像由4幅相似程度不同的Ref图像组成；</p></li><li><p>额外数据集1：Sun80【26】包含80幅自然图像，每幅图像都有多幅Ref图像；</p></li><li><p>额外数据集2：Urban100【11】，该数据集没有Ref图像，将其LR图像视为Ref图像。由于Urban100都是具有强自相似性的建筑图像，因此这种设计可以实现显式的自相似搜索和传输过程；</p></li><li><p>额外数据集3：Manga109【20】，同样缺少Ref图像，随机抽取该数据集中的HR图像作为Ref图像；</p></li><li><p>评价指标：在YCbCr空间的Y通道上，对SR结果进行了PSNR和SSIM评估。</p></li></ul><h2 id="定量实验"><a href="#定量实验" class="headerlink" title="定量实验"></a>定量实验</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/12.png" title="Optional title"></p><h2 id="定性实验"><a href="#定性实验" class="headerlink" title="定性实验"></a>定性实验</h2><p>可视化：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/13.png" title="Optional title"></p><p>用户调研：对RCAN【39】、RSR-GAN【38】、CrossNet【43】和SRNTT【41】，10名受试者，在CUFED5测试集上收集了2520张选票。对于每个比较过程，我们为用户提供两个图像，其中包括一个TTSR图像。要求用户选择视觉质量更高的。其中Y轴上的值表示倾向于TTSR而非其他方法的用户的年龄百分比。超过90%的用户投票支持的TTSR。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/14.png" title="Optional title"></p><blockquote><p>用户调研这里很像LPIPS中的用户调研，都是同时给你两张图让你选更清楚的。</p></blockquote><h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><h3 id="Texture-Transformer中模块的影响"><a href="#Texture-Transformer中模块的影响" class="headerlink" title="Texture Transformer中模块的影响"></a>Texture Transformer中模块的影响</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/15.png" title="Optional title"></p><h3 id="CSFI的影响"><a href="#CSFI的影响" class="headerlink" title="CSFI的影响"></a>CSFI的影响</h3><p>第一行显示了仅使用TT时我们模型的性能，而第二行证明了CSFI的有效性，它使PSNR指标增加了0.17。</p><p>为了验证性能的改善不是由参数大小的增加带来的，将“Base+TT”模型的信道数增加到80和96。“Base+TT（C80）”几乎没有增长，其参数数几乎与“Base+TT+CSFI”相同。“Base+TT（C96）”将参数数量增加到9.10M也没有增长。<strong>CSFI可以以相对较小的参数大小有效地利用参考纹理信息。</strong></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/16.png" title="Optional title"></p><h3 id="Adv-loss和Transferal-per-loss的影响"><a href="#Adv-loss和Transferal-per-loss的影响" class="headerlink" title="Adv loss和Transferal per loss的影响"></a>Adv loss和Transferal per loss的影响</h3><p>两幅图，不重要。</p><h3 id="不同的Ref图像相关性的影响"><a href="#不同的Ref图像相关性的影响" class="headerlink" title="不同的Ref图像相关性的影响"></a>不同的Ref图像相关性的影响</h3><p>为了研究LR和Ref图像之间的相关性如何影响TTSR的结果，在CUFED5测试集上进行了实验。其中，“L1”至“L4”表示CUFED5测试集提供的参考图像，其中L1是最相关的级别，而L4是最不相关的级别。“LR”是指使用输入图像本身作为参考图像。使用L1作为参考图像的TTSR实现了最佳性能。当使用LR作为参考图像时，TTSR的性能仍然优于以前最先进的RefSR方法。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/17.png" title="Optional title"></p><blockquote><p>一张图像的LR难道不应该是最相关的？还是说这个数据集的组成并不是依靠两个客观指标的计算来定义参考图像是否相关？？？</p></blockquote><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>文章：<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Learning_Texture_Transformer_Network_for_Image_Super-Resolution_CVPR_2020_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Learning_Texture_Transformer_Network_for_Image_Super-Resolution_CVPR_2020_paper.pdf</a></p></li><li><p>code：<a href="https://github.com/researchmm/TTSR">https://github.com/researchmm/TTSR</a></p></li></ul><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>Transformer能够用来做很多事情，在看这篇文章之前，以为是一transformer的领域迁移为核心的创新是最大亮点，确实是，但占比重没有绝对的高，文章还设计了自己的方法流程用来解决超分辨率问题，就是CSFI。</p><p>对比实验做的非常的细，该论证到的内容都论证到了。没有提供太高倍数的模型估计是因为计算量的问题，毕竟CSFI是级联的结构，想一想就知道高倍数需要的参数量估计很大。</p><hr><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>懒得写字，直接放PPT。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/18.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/19.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/20.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/21.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/22.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/23.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/24.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/25.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/26.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/27.png" title="Optional title"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Learning Texture Transformer Network for Image Super-Resolution&lt;/p&gt;
</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>随记。</title>
    <link href="http://example.com/2022/06/08/%E9%9A%8F%E8%AE%B0%E3%80%82/"/>
    <id>http://example.com/2022/06/08/%E9%9A%8F%E8%AE%B0%E3%80%82/</id>
    <published>2022-06-08T02:41:06.000Z</published>
    <updated>2022-06-08T03:18:40.984Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，禁止转载。</p></blockquote><hr><p>我坐在工位上，想起了6年前的时光。</p><p>2016年的夏天我几乎忘干净了，只是仅剩的一点点画面还未远去。被分配到离学校有点远的校园考试，提前和妈妈去考场边上租了酒店。考前的一天晚上，告诉自己不要失眠不要失眠，却还是迷迷糊糊的没睡着。没有熬夜没有焦虑的夜晚，从早晨5点起劳累了一整天的身体，在晚上的什么时候都能入眠。时至今日我依旧羡慕当时的睡眠能力。考试的时候特意选了一件衬衣，和考前爸妈陪着去买的一件短袖，坐在考场上的时候还觉得穿件衬衣是真的很热。最后一场考英语，中午睡午觉还差点睡过了头。考完了英语，考场外碰见了英语老师，笑着问我考的怎么样。回到出租屋里，收拾行李，看着成堆的书本和卷子，舍不得扔掉但也不愿意带走。写的很好看的物理笔记和生物笔记，郑重的送给了高二年级的一个学弟。一家人开着车从市里学校回县城，我懒洋洋的躺在后座上，窗外的树荫从车窗上掠过，那一瞬间我觉得我化在了夕阳的温暖中。没有狂欢，没有激动，没有交流答案，没有估算成绩，没有立马去找寻自己将要上什么大学，没有去参加晚上的聚会，没有跟任何同龄人交流什么，但那是一种前所未有的轻松。我实在忘记了那一晚是怎么入睡的，也许是兴奋到大半夜，也许是同往日一样到头就睡。</p><p>再往前追溯，那时好像并不知道什么叫做焦虑。考前的多次模拟考试，成绩时好时坏，最后一次模拟竟然考到了50多名，要知道班里大概有70人左右。我当时是真的不以为意，好像有种天生的自信，一直捧着我，告诉我那不过是众多意外中最不起眼的那么一个罢了，心里思念的女孩子都比那模拟考试的成绩来得重要。考完之后的一段日子，大家在各种地方热烈的讨论答案，我居然也能非常心安理得的逃避，仿佛这场考试从那天起就跟自己一点关系都没有。那时妈妈在医院做完了手术，当时的我并不知道病的严重性，也是，所有人都瞒着我，生怕影响了我的考试，直到妈妈进了手术室，我才知道有这么一回事。</p><p>高中的日子是比现在苦千倍百倍的，现在想来，却也只是单纯的苦。是一种努力向上的痛苦，是成长的阵痛。彼时的单纯心思，充满幼稚但又十分干净，没有什么无奈，没有什么不甘。我并不特别怀念那段日子，但我也明白，我再也不能感受到这种纯粹的痛苦所带来的纯粹的欢愉。</p><p>我愿再一次感受那个夏天，一如那飞逝过时光的温暖夕阳。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，禁止转载。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;我坐在工位上，想起了6年前的时光。&lt;/p&gt;
&lt;p&gt;2016年的夏天我几乎忘干净了，只是仅剩的一点点画面还未远去。被分配到离学校有点远的校园考试，提前和妈妈去考场边上租了酒店。</summary>
      
    
    
    
    
    <category term="闲谈" scheme="http://example.com/tags/%E9%97%B2%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title>leetcode刷题记录（2）</title>
    <link href="http://example.com/2022/02/25/leetcode%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95%EF%BC%882%EF%BC%89/"/>
    <id>http://example.com/2022/02/25/leetcode%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95%EF%BC%882%EF%BC%89/</id>
    <published>2022-02-25T13:40:32.000Z</published>
    <updated>2022-02-26T12:48:35.615Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="统计"><a href="#统计" class="headerlink" title="统计"></a>统计</h1><ul><li><p>hard：</p></li><li><p>medium：537、</p></li><li><p>easy：35、</p></li><li><p>重难点：35、</p></li></ul><hr><h1 id="35-搜索插入位置-array-二分查找"><a href="#35-搜索插入位置-array-二分查找" class="headerlink" title="35. 搜索插入位置 [array/二分查找]"></a>35. 搜索插入位置 [array/二分查找]</h1><p>在有序数组中查找所给的target，找不到就找到其插入位置。要求是log(N)的复杂度。</p><p><img src="/images/leetcode/35.png" title="Optional title"> </p><p>关键词“有序数组”，显然用二分查找。</p><h1 id="537-复数乘法-技巧题"><a href="#537-复数乘法-技巧题" class="headerlink" title="537. 复数乘法 [技巧题]"></a>537. 复数乘法 [技巧题]</h1><p>给定两个复数，用str表示，求其乘。</p><p><img src="/images/leetcode/537.png" title="Optional title"> </p><p>用str的函数分别取出实部和虚部，按照法则运算。注意符号。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;统计&quot;&gt;&lt;a href=&quot;#统计&quot; class=&quot;headerlink&quot; title=&quot;统计&quot;&gt;&lt;/a&gt;统计&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;hard：</summary>
      
    
    
    
    
    <category term="leetcode" scheme="http://example.com/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>MODIS部分数据说明及解析</title>
    <link href="http://example.com/2022/01/11/MODIS%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E8%AF%B4%E6%98%8E%E5%8F%8A%E8%A7%A3%E6%9E%90/"/>
    <id>http://example.com/2022/01/11/MODIS%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E8%AF%B4%E6%98%8E%E5%8F%8A%E8%A7%A3%E6%9E%90/</id>
    <published>2022-01-11T07:33:38.000Z</published>
    <updated>2022-03-01T08:14:17.422Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>项目需要。记录资料。</p></blockquote><h1 id="官网"><a href="#官网" class="headerlink" title="官网"></a>官网</h1><p>官网：<code>https://modis.gsfc.nasa.gov/</code>。</p><p>Google搜关键词MODIS，第一个就是。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>About和Data页面的一些简介：</p><p><img src="/images/MODIS/1.png" title="Optional title"></p><p><img src="/images/MODIS/2.png" title="Optional title"></p><h1 id="数据下载"><a href="#数据下载" class="headerlink" title="数据下载"></a>数据下载</h1><p>数据下载和官方产品都在Data介绍页给的第一个域名：<code>http://ladsweb.nascom.nasa.gov/</code>。</p><p>里面有一个做的很详细的文件系统，可以查、下数据（主要是用最前面那个编码查）和已有产品所使用到的数据。比如MYD03/2013/001的267条数据：<code>https://ladsweb.modaps.eosdis.nasa.gov/archive/allData/61/MYD03/2013/001/</code>：</p><p><img src="/images/MODIS/3.png" title="Optional title"></p><p>下载要求注册用户和登陆。</p><h1 id="文件命名规则"><a href="#文件命名规则" class="headerlink" title="文件命名规则"></a>文件命名规则</h1><p>来自<code>https://lpdaac.usgs.gov/data/get-started-data/collection-overview/missions/modis-overview/#modis-naming-conventions</code>，内有MODIS文件命名规则（3类）、产品长名称命名规则、时空分辨率、平铺系统（Tiling Systems）、数据处理、光谱带（1-36）、元数据（Metadata）等信息。</p><p>取<code>MODIS/MYD03/2013/001/MYD03.A2013001.0000.061.2018043200701.hdf</code>：</p><table><thead><tr><th align="center">命名字段</th><th align="center">解释</th></tr></thead><tbody><tr><td align="center">MYD03</td><td align="center">产品短名，<code>MOD</code>是Terra星（上午星）数据，<code>MYD</code>是Aqua星（下午星）数据</td></tr><tr><td align="center">A2013001</td><td align="center">数据采样时间，<code>A-YYYYDDD</code>，DDD表示YYYY这一年中的第几天，与上一级文件夹命名对应，意义：该数据2013年的第一天被传感器所采集</td></tr><tr><td align="center">0000</td><td align="center">数据采样时间，<code>HHMM</code>，HH取值为[00,23]，MM取值为[00,59]，意义：该数据与00时00分（晚12时整）采集</td></tr><tr><td align="center">061</td><td align="center">数据采集版本号</td></tr><tr><td align="center">2018043200701</td><td align="center">数据生产时间，<code>YYYYDDDHHMMSS</code>，Y、D、H、M同上，SS为秒，意义：2018年第43天的20时07分01秒生产了该数据</td></tr><tr><td align="center">.hdf</td><td align="center">数据格式<code>HDF-EOS</code></td></tr></tbody></table><h1 id="tool"><a href="#tool" class="headerlink" title="tool"></a>tool</h1><h2 id="HDFView"><a href="#HDFView" class="headerlink" title="HDFView"></a>HDFView</h2><p>最新的是3.1.3版本：<code>https://portal.hdfgroup.org/display/support/HDFView+3.1.3</code>，多系统支持，mac系统的是110M，不翻墙下的很慢。</p><h2 id="HDF-Explore"><a href="#HDF-Explore" class="headerlink" title="HDF Explore"></a>HDF Explore</h2><p>1.4版本：<code>https://www.space-research.org/hdf_explorer/explorer_download.htm</code>，只支持windows，有可视化功能，比上一个好用点，需要在网页上填写一些信息才能下载，下载还需要购买使用许可，直接从百度上找破解版。</p><h1 id="数据说明"><a href="#数据说明" class="headerlink" title="数据说明"></a>数据说明</h1><h2 id="官网数据介绍"><a href="#官网数据介绍" class="headerlink" title="官网数据介绍"></a>官网数据介绍</h2><p>MYD0211KM（Level 1B Calibrated Radiances）：<code>https://ladsweb.modaps.eosdis.nasa.gov/missions-and-measurements/products/MYD021KM</code></p><p>MYD03：（Geolocation）<code>https://ladsweb.modaps.eosdis.nasa.gov/missions-and-measurements/products/MYD03</code></p><p>MYD35（Cloud Mask）：<code>https://ladsweb.modaps.eosdis.nasa.gov/missions-and-measurements/science-domain/cloud-mask/</code> </p><table><thead><tr><th align="center">数据产品名称</th><th align="center">解释</th></tr></thead><tbody><tr><td align="center">MYD021KM</td><td align="center">MODIS <code>1B</code></td></tr><tr><td align="center">MYD03</td><td align="center">MODIS数据<code>地理定位</code>文件</td></tr><tr><td align="center">MYD35</td><td align="center">大气2,3级，<code>云掩膜</code>，250m/1㎞</td></tr></tbody></table><h2 id="文件目录"><a href="#文件目录" class="headerlink" title="文件目录"></a>文件目录</h2><p>前三级目录：</p><p><img src="/images/MODIS/4.png" title="Optional title"></p><p>第四级目录，取<code>MODIS/MYD03/2013/001</code>：</p><p><img src="/images/MODIS/5.png" title="Optional title"></p><p><img src="/images/MODIS/6.png" title="Optional title"></p><p>数量：MYD03和MYD021KM都选的是2013年的相同天数，除了两个001内是267个文件，剩下的每个文件夹内都是288（60/5=12，24*12=288）个文件。</p><p>这里给的35是21年的02和03都是13年的，对不上。</p><h2 id="重点字段信息"><a href="#重点字段信息" class="headerlink" title="重点字段信息"></a>重点字段信息</h2><p>三部分产品需要联合起来用的。</p><h3 id="MYD35"><a href="#MYD35" class="headerlink" title="MYD35"></a>MYD35</h3><ul><li>Cloud_Mask：作为GT，为<code>6*2030*1354</code>矩阵。</li></ul><p><img src="/images/MODIS/7.png" title="Optional title"></p><h3 id="MYD02"><a href="#MYD02" class="headerlink" title="MYD02"></a>MYD02</h3><p>对地观测数据文件包括250米、500米和1000米分辨率的定标后的资料。</p><p>给定波段的一组探测器在沿轨道方向对齐在四个焦平面上。图象中的一条扫描线是扫描带中一个探测器的扫描观测资料。<code>1km波段包含10个探测器</code>，500m波段包含20个探测器，250m波段包含40个探测器。探测器之间的扫描间距大致分别为1km、500m和250m，在扫描方向，<code>每帧（frame）大致为1km大小</code>，MODIS仪器<code>在1km波段每帧的取样率为1</code>，500m波段为2，250m波段为4。</p><ul><li><p>EV_1KM_Emissive：热辐射波段，用来计算亮温。</p></li><li><p>EV_1KM_RefSB：太阳光反射波段，计算反射率。</p></li></ul><p><img src="/images/MODIS/8.png" title="Optional title"></p><p><img src="/images/MODIS/9.png" title="Optional title"></p><h3 id="MYD03"><a href="#MYD03" class="headerlink" title="MYD03"></a>MYD03</h3><p>暂且不用，因为不考虑特定区域划分的话，使用02和35的文件名即可对准数据。</p><h1 id="云检测实验"><a href="#云检测实验" class="headerlink" title="云检测实验"></a>云检测实验</h1><h2 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h2><ul><li><p>数据：使用2013年001天的0000、0005和0010进行实验，只取用02和35产品，不使用03产品；3个hdf文件中共有3<em>2030</em>1354=<code>8245860</code>个点</p></li><li><p>真值：使用35产品中，0通道数值的第1、2比特位</p></li><li><p>input：使用02产品中，<code>EV_1KM_Emissive的4个通道</code>和<code>EV_1KM_RefSB的4个通道</code></p></li><li><p>数据预处理：对于真值，取第一通道数据，取00状态作为有云，其他三个状态统一处理为无云，不考虑背景的冰雪、耀斑等；对于input数据，对于不同通道的处理为<code>(data+offset)*scale</code></p></li><li><p>数据划分：训练:测试=8:2，划分后的数据量为<code>6596688:1649172</code></p></li><li><p>评估指标：同真值计算<code>ACC</code></p></li></ul><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><table><thead><tr><th align="center">方法</th><th align="center">参数设置</th><th align="center">结果</th></tr></thead><tbody><tr><td align="center">PCA+逻辑回归</td><td align="center">lr的max_iter=3000，PCA缩小围度范围为8-2</td><td align="center">0.941/0.761/0.794/0.788/0.832/0.854/0.864</td></tr><tr><td align="center">SVM</td><td align="center">sklearn默认</td><td align="center">0.955（100w）</td></tr><tr><td align="center">GDBT</td><td align="center">sklearn默认</td><td align="center">0.958</td></tr><tr><td align="center">GaussianNB</td><td align="center">sklearn默认</td><td align="center">0.867</td></tr><tr><td align="center">BP神经网络</td><td align="center">三个隐藏层(50,15,5)，solver=’adam’，max_iter=1000</td><td align="center">0.969</td></tr><tr><td align="center">LSTM</td><td align="center">LSTM(100)+LSTM(50)+Dense(1)，epochs=100，batch_size=128</td><td align="center">0.919</td></tr><tr><td align="center">随机森林</td><td align="center">n_estimators=1000</td><td align="center">0.948</td></tr><tr><td align="center">RS-Net(U-Net)</td><td align="center"><a href="https://github.com/JacobJeppesen/RS-Net">https://github.com/JacobJeppesen/RS-Net</a></td><td align="center">0.935</td></tr></tbody></table><h2 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h2><ul><li><p>方法流程：有两种：一是直接将HDF文件看做图像，数据处理也当作图像那样整体处理，一般是转存为TIF或者是HDF5，用图像的方法进行云检测，是部分深度学习方法的思路；另一种是把这些离散数据取出，将多通道的数据所对应的检测结果当成一个个像素点，用分类的方法进行拟合，偏向于传统机器学习的思路。<code>不知道到底哪种思路适合这个工程</code>。现状：两种方法都有应用；写论文的话，业内人士会尽量避免深度学习的方法，因为卫星数据的物理因素比较多的情况下进行数据处理会引入很大的误差；模型速度上深度方法更适合大量数据，看做图像的话要使用较多的显卡。</p></li><li><p>训练数据不同种类下垫面划分：做了的话肯定有助于提高性能，属于是标数据的一部分。但对于国产数据来说，没有固定的标准，需要制定划分标准（这里可以使用Modis的），试验前做划分，且如果做了划分，就肯定要训练多个模型。</p></li><li><p>算法性能评估指标：简单的使用了ACC，是否可信未知。</p></li><li><p>算法性能比较：推荐的是<code>随机森林</code>、<code>朴素贝叶斯</code>和<code>U-Net</code>。实际进行下来，决策树类算法、BP的性能是第一档；U-Net性能稍差一点但是也能看，估计是因为数据量的原因，上面的数据量对于其他算法来说够了，但是对于U-Net来说肯定是少；基于高斯分布的朴素贝叶斯网络性能就一般；<code>LSTM</code>的性能也算做中档，但实际上LSTM并不适合这个场景，因为数据并没有呈现序列性；SVM就不考虑了，数据量一大的话，效率太低。</p></li><li><p>国产数据与MODIS数据的对齐问题：raw data经纬度对不齐。</p></li><li><p>国产数据的评估问题：真值、指标。</p></li></ul><h2 id="接下来需要做的"><a href="#接下来需要做的" class="headerlink" title="接下来需要做的"></a>接下来需要做的</h2><ul><li><p>可以补一下Deep CNN系的方法，简单粗暴，上面说的两种思路都可以做，但主要的可能还是数值拟合。</p></li><li><p>使用MODIS的数训练FY4的，主要需要解决数据对齐、数据选择（主要是通道选择，推荐通道还需要再缩小一下提高性能）、评估，尽量上深度方法。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;项目需要。记录资料。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;官网&quot;&gt;&lt;a href=&quot;#官网&quot; class=&quot;headerlink&quot; title=&quot;官网&quot;&gt;&lt;/a&gt;官网&lt;/h1&gt;&lt;p&gt;官网：&lt;code&gt;https://modis.gs</summary>
      
    
    
    
    
    <category term="MODIS" scheme="http://example.com/tags/MODIS/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：LPIPS</title>
    <link href="http://example.com/2022/01/07/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ALPIPS/"/>
    <id>http://example.com/2022/01/07/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ALPIPS/</id>
    <published>2022-01-07T12:51:01.000Z</published>
    <updated>2022-06-15T13:29:58.289Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</p></blockquote><blockquote><p>CVPR 2018</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>在计算机科学的许多领域中，比较数据都不会构成太大的困难：比如汉明距离、欧几里得距离等，但是视觉相似性（visual similarity）的概念往往是很主观的，旨在模仿人类的视觉感知。</p><p><strong>期望得到一个真正的“感知距离”（也就是能明确证明这个感知指标是接近人的主观视觉感知的）</strong>，已有一些依据感知的距离度量，比如SSIM[58]、MSSIM[60]、FSIM[62]和HDR-VDP[34]。</p><blockquote><p>[58] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 2004. 1, 2, 8, 12, 14<br>[60] Z. Wang, E. P. Simoncelli, and A. C. Bovik. Multiscale struc- tural similarity for image quality assessment. In Signals, Systems and Computers. IEEE, 2004. 1<br>[62] L. Zhang, L. Zhang, X. Mou, and D. Zhang. Fsim: A feature similarity index for image quality assessment. TIP, 2011. 1, 2, 12, 14<br>[34] R.Mantiuk,K.J.Kim,A.G.Rempel,andW.Heidrich.Hdr-<br>vdp-2: A calibrated visual metric for visibility and quality predictions in all luminance conditions. In ACM Transac- tions on Graphics (TOG), 2011. 1, 12</p></blockquote><p>根据以上背景，文章提出问题：</p><ul><li><p>如何得到一个”感知距离(Perceptual Distance)“，以符合人类判断的方式衡量两个图像的相似程度？</p></li><li><p>这些”感知损失“与人类的视觉到底有多对应？</p></li><li><p>它们与网络架构是否有关？</p></li></ul><blockquote><p>怎样以“感知”的形式衡量图像的相似程度？怎么证明或者评估这种衡量指标的性能？这种衡量指标可否在不同网络架构下通用？</p></blockquote><h1 id="Berkeley-Adobe-Perceptual-Patch-Similarity-BAPPS-Dataset"><a href="#Berkeley-Adobe-Perceptual-Patch-Similarity-BAPPS-Dataset" class="headerlink" title="Berkeley-Adobe Perceptual Patch Similarity (BAPPS) Dataset"></a>Berkeley-Adobe Perceptual Patch Similarity (BAPPS) Dataset</h1><p>为了评估不同感知方法的性能，首先对已有方法造成的图像扭曲进行分类和收集，然后使用两种方法收集了一个大规模的高度不同的认知判断数据集，称为BAPPS。</p><h2 id="Distortions"><a href="#Distortions" class="headerlink" title="Distortions"></a>Distortions</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LPIPS/1.png" title="Optional title"></p><h3 id="Traditional-distortions"><a href="#Traditional-distortions" class="headerlink" title="Traditional distortions"></a>Traditional distortions</h3><p>使用了亮度失真(photometric distortions)，随机噪声(random noise)，模糊(blurring)，空间移位(spatial shifts)，损坏(corruptions)，压缩伪影(compression artifacts)等20种传统方法进行顺序组合成308种顺序的失真。</p><p><strong>这里的“扭曲”，指的是人为添加的失真</strong>，也就是直接用图像处理算法对每张图像进行处理。可以看作是数据增广的概念和操作。</p><blockquote><p>Our traditional distortions (left) are performed by basic low-level image editing operations.</p></blockquote><h3 id="CNN-based-distortions"><a href="#CNN-based-distortions" class="headerlink" title="CNN-based distortions"></a>CNN-based distortions</h3><p><strong>通过探索各种图像任务、网络结构和损失函数来模拟可能的算法输出</strong>，比如编码(autoencoding)、去噪(Denoising)、着色(Colorization)、超分(Super Resolution)、去模糊(Video Deblurring)、帧插值(Frame Interpolation)等。总成生成了96个去噪自编码器(denoising autoencoders)，每个网络的目标不是本身解决任务，而是探索困扰基于深度学习方法输出的常见工件。</p><blockquote><p>Our CNN-based distortions (right) are formed by randomly varying parameters such as task, network architecture, and learning parameters. The goal of the distortions is to mimic plausible distortions seen in real algorithm outputs.</p></blockquote><h3 id="超分部分的工作内容"><a href="#超分部分的工作内容" class="headerlink" title="超分部分的工作内容"></a>超分部分的工作内容</h3><p>传统扭曲方法：评估了NTIRE 2017 workshop的结果，使用×2、×3、×4上采样率，使用“未知”下采样创建输入图像，每个上采样倍数都有大约20个算法提交。</p><p>基于CNN的扭曲方法：包括双三次上采样和四种性能最好的深度超分辨率方法[24、59、31、48]。结果比较：从Div2K数据集中随机位置的图像中随机抽取64×64 patch进行比较。</p><blockquote><p>原文中没做划分，但应该就是按照之前的两个区分标准进行的扭曲方法。给的四个“效果最好”的模型只看过SRGAN，其他三个都见过。<br>[24] J.Kim,J.KwonLee,andK.MuLee.Accurateimagesuper- resolution using very deep convolutional networks. In CVPR,<br>pages 1646–1654, 2016. 4<br>[59] Z. Wang, D. Liu, J. Yang, W. Han, and T. Huang. Deep networks for image super-resolution with sparse prior. In ICCV, 2015. 4<br>[31] SRGAN<br>[48] M. S. Sajjadi, B. Scho ̈lkopf, and M. Hirsch. Enhancenet: Single image super-resolution through automated texture synthesis. ICCV, 2017. 4</p></blockquote><h2 id="Psychophysical-Similarity-Measurements"><a href="#Psychophysical-Similarity-Measurements" class="headerlink" title="Psychophysical Similarity Measurements"></a>Psychophysical Similarity Measurements</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LPIPS/2.png" title="Optional title"></p><h3 id="2AFC-similarity-judgments"><a href="#2AFC-similarity-judgments" class="headerlink" title="2AFC similarity judgments"></a>2AFC similarity judgments</h3><p>随机选取一个图片x，使用两种失真（变换）方法生成两张图片x0和x1，让人来判断哪一个更接近原图，记录结果h，最终这一组实验数据表示为<code>T(x,x0,x1,h)</code>。</p><p>把每一个测试样本称为一个双向强迫选择（2AFC），人平均每次判断花费3秒钟。收集了训练样本151.4K个。</p><h3 id="Just-noticeable-differences-JND"><a href="#Just-noticeable-differences-JND" class="headerlink" title="Just noticeable differences (JND)"></a>Just noticeable differences (JND)</h3><p><strong>2AFC的一个缺点是它是”cognitively penetrable(可认知渗透的)“，参与者可以有意识地选择他们在完成任务时选择关注的相似性方面，这就向判断中引入了主观性。</strong></p><p>为了反应客观、有意义的内容，还需要收集JND的判断：只给出两张图像，先给出一个参考图像(reference image，2AFC中的x0)，然后再给出一张随机失真的图像，询问人图像是相同的还是不同的。</p><p>两张图像分别显示1s，间隔为250ms。收集了9.6K个样本。</p><h1 id="Deep-Feature-Spaces"><a href="#Deep-Feature-Spaces" class="headerlink" title="Deep Feature Spaces"></a>Deep Feature Spaces</h1><p>在不同网络的深度特征空间中评估特征的距离。</p><h2 id="Network-architectures"><a href="#Network-architectures" class="headerlink" title="Network architectures"></a>Network architectures</h2><ul><li><p>评估了Squeezenet、AlexNet、VGG网络。使用了VGG网络的5个conv层，还使用了更接近人类视觉皮层结构的较浅的AlexNet以及具有与AlexNet相似性能的轻量SqueezeNet架构。</p></li><li><p>评估了自监督的方法，包括解谜(puzzle-solving)、跨通道预测(cross-channel prediction)、视频学习(learning from video)和生成建模(generative modeling)。</p></li></ul><h2 id="Network-activations-to-distance"><a href="#Network-activations-to-distance" class="headerlink" title="Network activations to distance"></a>Network activations to distance</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LPIPS/3.png" title="Optional title"></p><p>上图左半部分说明了如何用网络计算<code>原图x</code>和<code>失真图像x0</code>之间的距离：<strong>从L层中提取特征堆栈，并在通道维度中进行单元归一化(unit-normalize in the channel dimension)。</strong></p><p>对于<code>l层</code>，将得到的结果记为<code>y^l,y^l0(Hl*wl*cl)</code>。利用向量<code>wl</code>缩放激活通道并计算<code>l2距离</code>，最后在空间上求平均值，在信道上求和。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LPIPS/4.png" title="Optional title"></p><blockquote><p>Note that using <code>wl = 1∀l</code> is equivalent to computing <code>cosine distance</code>.</p></blockquote><p>上图的右半部分用于映射<code>得分h</code>的小网络G使用了 2个32通道的FC-ReLU层、单通道FC层和sigmoid层。</p><p>相似性度量的损失函数为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LPIPS/5.png" title="Optional title"> </p><blockquote><p>来自Appendix部分的B. Model Training Details，正文中没有提到。</p></blockquote><h2 id="Training-on-our-data"><a href="#Training-on-our-data" class="headerlink" title="Training on our data"></a>Training on our data</h2><p><strong>考虑一些不同的训练方式来进行感知判断：lin，tune，scratch。</strong>三个变种统称为Learned Perceptual Image Patch Similarity (LPIPS) metric。</p><ul><li><p>lin：保持预训练网络权重不变，在顶部学习线性权重w，这构成了已有特征空间中一些参数的”感知校准(perceptual calibration)；</p></li><li><p>tune：从预先训练的分类模型初始化，并允许对网络的所有权重进行微调；</p></li><li><p>scratch：从随机高斯分布进行权重初始化，并完全根据前文判断对其进行训练。</p></li></ul><blockquote><p>这里的训练策略，lin没有查到；tune相关的词是fine-tune；scratch是有的，但具体所指意义不明。对于这三个训练策略，原文中也没有引用什么论文加以说明。</p></blockquote><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>部分实验结果：图4、5、6（11）；表5。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LPIPS/6.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LPIPS/7.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LPIPS/8.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LPIPS/9.png" title="Optional title"></p><p>以问答形式解释了实验结果，部分重点如下：</p><ol><li><p>low-level metrics和Classification networks的性能如何？<br>答：<code>图4a，红色同黑色的比较和其他颜色同黑色的比较；表5（Appendix中A. Quantitative Results）</code>。Classification networks的表现要明显优于low-level metrics。</p></li><li><p>网络是否必须是Classification的任务？<br>答：<code>图4a，绿色右1、2和蓝色左1</code>。网络不一定要在Classification任务上训练。BiGAN，Puzzle，Splitbrain等无监督和自监督模型特征和监督模型差距不大。</p></li><li><p>不同的感知任务之间是否存在相关性？<br>答：<code>图5</code>。没有说明相关的程度，只给了结果。</p></li><li><p>我们可以针对传统的和基于CNN的失真训练一个指标吗？（这个问题的问法和回答有点对不上，因该是问题叙述没叙述清楚，应该是<code>是否可以用第二部分收集到的数据训练出一个可以用来评估的感知指标，该指标的效果如何？</code>）<br>答：<code>图4a，右1-9，紫、粉、棕黄色</code>。通过实验证实了更高容量的网络VGG比低容量的SqueezeNet和AlexNet架构表现更好，也就证实了网络确实可以从感知判断中学习。</p></li><li><p>关于传统和基于CNN的扭曲的训练是否转移到了real-world场景中？（<code>迁移到真实算法中是否能提高性能？</code>）<br>答：<code>图4b，右1-9，紫、粉、棕黄色</code>。学习线性分类器（紫）可以提高所有网络的性能。从零开始训练一个网络（粉），AlexNet的性能略低，而VGG的性能略高于线性校准。<strong>总结：使用数据“校准”预先存在的表征的激活是实现性能小幅提升的安全方法（分别为1.1%、0.3%和1.5%）。</strong></p></li><li><p>deep metrics和low-level metrics在哪里不一致?（<code>扭曲的定性比较？</code>）<br>答：<code>图6；图11（Appendix中B. Model Training Details，图6的拓展，和图6性质一样）</code>。使用BiGAN进行对比实验，认为相关噪声模式比SSIM失真更小。</p></li></ol><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>文章：<a href="https://arxiv.org/abs/1801.03924">https://arxiv.org/abs/1801.03924</a></p><p>code：<a href="https://www.github.com/richzhang/PerceptualSimilarity">https://www.github.com/richzhang/PerceptualSimilarity</a></p><p>如何使用：<a href="https://blog.csdn.net/Magic_o/article/details/106770317">https://blog.csdn.net/Magic_o/article/details/106770317</a></p><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>LPIPS这个方法本身，从数据收集到网络训练，难度并不高，但非常流畅。对实验结果做了很详细的解释，这一点是非常好的，因为如果只是将提高的性能作为结果摆上去，说服力也就那样，但针对不同位置进行的解释，就给了这个方法极大的可用性和拓展性。</p><p>估计是受限于篇幅，并没有对每个算法之间的指标进行逻辑上的比较，算是这篇文章没有完成的内容，可以作为自己写论文过程中指标对比的一个论点去写。</p><p>使用过程中发现LPIPS计算图像距离，速度并不是特别快，想要将这个当作新的SR指标，还是需要考虑时间成本的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;The Unreasonable Effectiveness of Deep Features as a Perceptual Metr</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>SR Baseline Model code detail</title>
    <link href="http://example.com/2021/12/22/SR-Baseline-Model-code-detail/"/>
    <id>http://example.com/2021/12/22/SR-Baseline-Model-code-detail/</id>
    <published>2021-12-22T09:49:20.000Z</published>
    <updated>2022-03-11T15:09:54.445Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据集WSI"><a href="#数据集WSI" class="headerlink" title="数据集WSI"></a>数据集WSI</h1><table><thead><tr><th>info</th><th align="left">command</th></tr></thead><tbody><tr><td>home</td><td align="left">/home/jfy/project/data/WSI</td></tr><tr><td>10训2验2测_训</td><td align="left">/home/jfy/project/data/WSI/lv0_small_10_2_2/train</td></tr><tr><td>10训2验2测_验</td><td align="left">/home/jfy/project/data/WSI/lv0_small_10_2_2/val</td></tr><tr><td>10训2验2测_测</td><td align="left">/home/jfy/project/data/WSI/lv0_small_10_2_2/test</td></tr><tr><td>1000训100验100测_训</td><td align="left">/home/jfy/project/data/WSI/lv0_medium_1000_100_100/train</td></tr><tr><td>1000训100验100测_验</td><td align="left">/home/jfy/project/data/WSI/lv0_medium_1000_100_100/val</td></tr><tr><td>1000训100验100测_测</td><td align="left">/home/jfy/project/data/WSI/lv0_medium_1000_100_100/test</td></tr></tbody></table><h1 id="SRCNN"><a href="#SRCNN" class="headerlink" title="SRCNN"></a>SRCNN</h1><p>code：<a href="https://github.com/yjn870/SRCNN-pytorch">https://github.com/yjn870/SRCNN-pytorch</a></p><table><thead><tr><th>info</th><th align="left">command</th></tr></thead><tbody><tr><td>home</td><td align="left">/home/jfy/project/SRCNN-pytorch-master</td></tr><tr><td>prepare</td><td align="left">python prepare.py --images-dir <code>xxx</code> --output-path result/2x/lv0_0_9.h5 --patch-size 32 --stride 14 --scale 2</td></tr><tr><td>train</td><td align="left">python train.py --train-file result/2x/lv0_0_9.h5 --eval-file result/2x/lv0_11_12.h5 --outputs-dir result/2x/little --num-epochs 2 --batch-size 16 --seed 11</td></tr></tbody></table><p><code>更改成个人数据集</code>：用prepare.py分别生成测试集和训练集的h5文件，train的时候输入两个h5文件即可。</p><p>仅有PSNR指标，Y通道计算，utils.py中<code>def calc_psnr(img1, img2)</code>传的是tensor。</p><p><code>添加SSIM指标：utils.py添加函数def calc_ssim(img1, img2)</code>，代码来自IGNN，需要将tensor先转成array。</p><p><code>添加LPIPS指标：utils.py添加函数def calc_lpips(img1, img2)</code>，需要pip install lpips并在最上面import，调用需要统一将tensor转到device=cpu，并将Y通道重复三次。</p><p>上两周跑起来没问题，这周开始报h5的错误了，暂且放下。</p><h1 id="SRGAN"><a href="#SRGAN" class="headerlink" title="SRGAN"></a>SRGAN</h1><p>code：<a href="https://github.com/leftthomas/SRGAN">https://github.com/leftthomas/SRGAN</a></p><table><thead><tr><th>info</th><th align="left">command</th></tr></thead><tbody><tr><td>home</td><td align="left">/home/jfy/project/SRGAN-master</td></tr><tr><td>train</td><td align="left">python train.py –train_set_path ../data/WSI/lv0_medium_1000_100_100/train –val_set_path ../data/WSI/lv0_medium_1000_100_100/test –upscale_factor 2 –num_epochs 100 –train_bs 128</td></tr></tbody></table><h1 id="EDSR"><a href="#EDSR" class="headerlink" title="EDSR"></a>EDSR</h1><p>code：<a href="https://github.com/sanghyun-son/EDSR-PyTorch">https://github.com/sanghyun-son/EDSR-PyTorch</a></p><table><thead><tr><th>info</th><th align="left">command</th></tr></thead><tbody><tr><td>home</td><td align="left">/home/jfy/project/local/EDSR-PyTorch-master/src</td></tr><tr><td>train</td><td align="left">sh wsi3.sh</td></tr></tbody></table><p><code>更改成个人数据集</code>：仿照DIV2K，步骤：按照DIV2K的文件目录制作数据集的文件目录（尤其是图片命名）、修改option中参数（dir_data、data_train、data_test，其他的写到sh文件内）、仿照./EDSR-PyTorch-master/src/data/div2k.py写一个wsi.py。三个步骤缺一不可。</p><p>使用medium的测试结果：</p><table><thead><tr><th>参数设置</th><th align="left">结果</th></tr></thead><tbody><tr><td>lr=1e-3,bs=32</td><td align="left">loss曲线不收敛，psnr从38开始到40多</td></tr><tr><td>lr=1e-4,bs=16,n_resblocks=8,n_feats=32</td><td align="left">收敛，但300epoch的loss2.2左右</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;数据集WSI&quot;&gt;&lt;a href=&quot;#数据集WSI&quot; class=&quot;headerlink&quot; title=&quot;数据集WSI&quot;&gt;&lt;/a&gt;数据集WSI&lt;/h1&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;info&lt;/th&gt;
&lt;th align=&quot;left&quot;&gt;comma</summary>
      
    
    
    
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>leetcode刷题记录</title>
    <link href="http://example.com/2021/11/20/leetcode%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    <id>http://example.com/2021/11/20/leetcode%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/</id>
    <published>2021-11-20T06:11:06.000Z</published>
    <updated>2021-12-09T11:20:09.933Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="统计"><a href="#统计" class="headerlink" title="统计"></a>统计</h1><ul><li><p>hard：4、458</p></li><li><p>medium：2、3、5、6、11、12、15、17、56、148、165、189、215</p></li><li><p>easy：*1、7、9、20、21、26、27、53、66、70、88、*118/119、141、202、206、*231、283、*461、*2073</p></li><li><p>重难点：2、3、^4、5、15、20、21、53、70、141、148、206、215</p></li></ul><hr><h1 id="2-两数相加-LinkList"><a href="#2-两数相加-LinkList" class="headerlink" title="2. 两数相加 [LinkList]"></a>2. 两数相加 [LinkList]</h1><p><img src="/images/leetcode/2_1.png" title="Optional title"> </p><p><img src="/images/leetcode/2_2.png" title="Optional title"> </p><p>按顺序加，主要就是保持一个进位flag，以及如果最后有进位，需要再新设一个node，值为1。</p><h1 id="3-无重复字符的最长子串-array"><a href="#3-无重复字符的最长子串-array" class="headerlink" title="3. 无重复字符的最长子串 [array]"></a>3. 无重复字符的最长子串 [array]</h1><p><img src="/images/leetcode/3.png" title="Optional title"></p><p>设两个指针head和tail，分别从0和1开始，tail每次向下走一个，head走到在[head（包）,tail（不包）]区间内跟当前tail重复的位置，显然这个重复位置如果有的话那么就只有一个，如果无重复，那么head不动即可。每次tail走动一下后，将其区间长度的最大值保留即可。tail从头到尾走一遍就可以得到答案。</p><p>题解有提示到查看是否重复和以用到一些容器，比如python中的set和cpp中的map。上面解法时间太慢也是因为判断是否重复我没有用这类容器，但实际上也省下了空间，时间换空间属于是。</p><h1 id="4-寻找两个正序数组的中位数-array-num-技巧题"><a href="#4-寻找两个正序数组的中位数-array-num-技巧题" class="headerlink" title="4. 寻找两个正序数组的中位数 [array/num/技巧题]"></a>4. 寻找两个正序数组的中位数 [array/num/技巧题]</h1><p><img src="/images/leetcode/4.png" title="Optional title"> </p><p>把两个数组放到一个里面，然后返回中位数即可。放的时候都从头循环，谁小就放谁。其中一个放空之后，把另一个剩下的所有跟上前面，就能保证新的依旧是有序的。</p><p><code>这题作为hard是因为要求的时间为O(log(m+n))</code>，这样的话就需要二分查找来解决，说实话给的解析确实没看懂。</p><h1 id="5-最长回文子串-array-DP-技巧题"><a href="#5-最长回文子串-array-DP-技巧题" class="headerlink" title="5. 最长回文子串 [array/DP/技巧题]"></a>5. 最长回文子串 [array/DP/技巧题]</h1><p><img src="/images/leetcode/5_1.png" title="Optional title"></p><p>首先是好理解的方法，就是将这个<code>倒置串</code>，寻找本体和其倒置的 <code>最长公共</code> <code>回文</code> 子串。该做法的时间和空间都是0(n*n)。对于这两个操作分两部分走：</p><ul><li><p>最长公共子串就是设置一个len*len的数组arr，初始全部置0，从头循环，如果两个指针i、j指的字符相同，那么其状态就是他们的前面的最大子串长度+1，也就是arr[i][j]=arr[i-1][j-1]+1。这里需要注意的是当i和j是0的时候要单独判断。设置2个容器记录这个长度和末尾位置，每当当前的长度（arr[i][j]）大于记录的长度，那么就以当前的长度和i替换这两个记录容器。</p></li><li><p>在上述进行两个容器的记录之前需要判断这一段是否是回文串，具体做法有两种，一种就是把这一段整体取下来整体判断，另一种就是判断最末尾一位的下标。</p></li></ul><p>在这里，对最长公共子串的求法是DP，而对这个题目本身有DP的做法，这两者的状态转移是完全不一样的，要区分好。</p><p><img src="/images/leetcode/5_2.png" title="Optional title"></p><p><img src="/images/leetcode/5_3.png" title="Optional title"></p><p>其次，还是要了解本题的<code>扩展中心法</code>和<code>Manacher算法</code></p><h1 id="6-Z字形变换-array-num-技巧题"><a href="#6-Z字形变换-array-num-技巧题" class="headerlink" title="6. Z字形变换 [array/num/技巧题]"></a>6. Z字形变换 [array/num/技巧题]</h1><p><img src="/images/leetcode/6.png" title="Optional title"> </p><p>没啥好说的，几行就设置几个str，按顺序往里存，最后从上到下排成一行。也可以找规律，每一行都是从某一位开始，间隔多少个数取（这个数肯定是不一样的）。</p><h1 id="7-整数反转-array-num"><a href="#7-整数反转-array-num" class="headerlink" title="7. 整数反转 [array/num]"></a>7. 整数反转 [array/num]</h1><p>反转有符号整数，超过[−2<strong>31,  2</strong>31 − 1]就返回0。</p><p><img src="/images/leetcode/7.png" title="Optional title"> </p><p>常规方法设置一个n，每次对x取10的余数，再加上10倍的n（相当于前一位的进位）。</p><p>使用python的切片可以反转str，将int保留符号，转成str，然后再添上符号。</p><p>一定要注意反转后的数字有可能超过范围。</p><h1 id="9-回文数-array"><a href="#9-回文数-array" class="headerlink" title="9. 回文数 [array]"></a>9. 回文数 [array]</h1><p>判断一个数从前向后读和从后向前读是不是一样的，是就称为回文数。</p><p>常规思路就是和<code>7. 整数反转</code>，一样的操作，把这个整数反转然后看他们是否相等即可。偷鸡思路还是使用python切片反转str。</p><p>注意，负数一定不会是回文数，因为多个符号。</p><h1 id="11-Container-With-Most-Water-array"><a href="#11-Container-With-Most-Water-array" class="headerlink" title="11. Container With Most Water [array]"></a>11. Container With Most Water [array]</h1><p><img src="/images/leetcode/11.png" title="Optional title"></p><p>直接两层循环会TLE，评论做法是<code>从头和尾开始相对着走</code>，实际上就是考虑了坐标轴长度，<code>保留最大的h</code>然后逐渐缩小坐标轴长度，时间O(1)。</p><h1 id="12-整数转罗马数字-num-技巧题"><a href="#12-整数转罗马数字-num-技巧题" class="headerlink" title="12. 整数转罗马数字 [num/技巧题]"></a>12. 整数转罗马数字 [num/技巧题]</h1><p><img src="/images/leetcode/12.png" title="Optional title"></p><p>写好个位数，然后替换即可，替换规则都是相通的。</p><p>题解里有提到用贪心，就是写好1000、<code>900</code>、500、<code>400</code>、100、…，以9和4的开头是需要用减法构造的，单独写出来，剩下的所有内容都可以用加法构造，所以就每次用num减去最大的，说是贪心，但这是贪心的意义么，后面写到这类题再说。</p><h1 id="15-三数之和-num-技巧题"><a href="#15-三数之和-num-技巧题" class="headerlink" title="15. 三数之和 [num/技巧题]"></a>15. 三数之和 [num/技巧题]</h1><p><img src="/images/leetcode/15.png" title="Optional title"></p><p>面试重点。</p><p>首先，思路方面，<code>三重循环肯定是TLE</code>，那么就力争变成二重循环。最外面的一层循环肯定不能省略，那么就要对里面的两层动心思。做法就是先排序，然后对内层的[i+1,n]区间的循环，设置<code>双指针</code>，j从i+1向后走，k从n向前走。能这么的核心就是因为排过序，前后操作是对应的。判断的时候，如果是0，那么记录结果，如果不是，就看是比0大还是比0小，结果比0大证明后面加的两个数过大了，那么k就往小变；同理，结果小了证明所加的数不够大，那么j就往大变。</p><p>其次，操作方面，比较重要的就是<code>去重</code>，对于ijk共同来说，就是要跳过相同的数，因为相同的数判断过会出现相同的结果，这里同时做到了去重和优化循环；对i来说，单独有一个优化，就是排了序之后，大于0的i，j和k都会大于，三个大于0的数相加必大于0。</p><p>最后整体做下来，排序用的是O(logN)，循环遍历用的是<code>O(N*N)</code>。</p><p>以及并不能为了去重而对nums上来就做去重，写题的时候就一直疑惑，写总结的时候突然发现确实不能这么写，不解释，懂得都懂。</p><h1 id="17-电话号码的字母组合-DP"><a href="#17-电话号码的字母组合-DP" class="headerlink" title="17. 电话号码的字母组合 [DP]"></a>17. 电话号码的字母组合 [DP]</h1><p><img src="/images/leetcode/17.png" title="Optional title"></p><p>并不算难，实际上是最简单的那种DP：<code>把之前的内容，每一个都重新加上新加的这个就行</code>。就是要记得对每个老内容，添加3或4个新内容之后，把老内容删了。</p><p>上述写法模拟了一个队列的操作，每次都操作队头的那个就行，count则控制每新添加一个数之前一共有多少老内容。</p><h1 id="20-有效的括号-技巧题"><a href="#20-有效的括号-技巧题" class="headerlink" title="20. 有效的括号 [技巧题]"></a>20. 有效的括号 [技巧题]</h1><p><img src="/images/leetcode/20.png" title="Optional title"> </p><p>模拟<code>栈（先进先出）</code>的存储方式匹配括号序列。</p><h1 id="21-合并两个有序链表-LinkList"><a href="#21-合并两个有序链表-LinkList" class="headerlink" title="21. 合并两个有序链表 [LinkList]"></a>21. 合并两个有序链表 [LinkList]</h1><p><img src="/images/leetcode/21.png" title="Optional title"> </p><p>新设一个头和操作这个头的操作指针，两个链表从头遍历有两个操作指针，当前两个指针哪个指的值小，就把这个node接上，该node后面断掉，指针向前走一个。最后一定记得把两个表中剩下的部分接上。</p><h1 id="26-Remove-Duplicates-from-Sorted-Array-array"><a href="#26-Remove-Duplicates-from-Sorted-Array-array" class="headerlink" title="26. Remove Duplicates from Sorted Array [array]"></a>26. Remove Duplicates from Sorted Array [array]</h1><p>给定按非降序排序的整数数组nums，请删除重复项，以便每个唯一元素只显示一次。元素的相对顺序应保持不变。</p><p><img src="/images/leetcode/26.png" title="Optional title"></p><p>这题返回值只有k，但检测结果还是操作num变成前k个不重复的值。</p><h1 id="27-Remove-Element-array"><a href="#27-Remove-Element-array" class="headerlink" title="27. Remove Element [array]"></a>27. Remove Element [array]</h1><p>将所有非val的数挪到list最前面，不论顺序，返回非val的个数。</p><p><img src="/images/leetcode/27.png" title="Optional title"> </p><p>与<code>283. Move Zeroes</code>同一个原理，就是把0换成是val然后做一个反向问题，代码都是一个逻辑，也是需要注意list长度是0或者list内没有要操作的数。</p><p>细细看了下，与<code>26. Remove Duplicates from Sorted Array</code>应该也是同理，在26的<code>去重</code>中，无非就是把k=0，k=val换成了动态的k，就是当前的重复的那个值。当然，能这么想只存在于<code>26是有序</code>的，无序的话就不是这个道理了。</p><p>时间O(1)，循环指针每次变动到新的不重复的值上，然后交换到当前k的位置，两件事情分开做就行。</p><h1 id="53-最大子数组和-num-DP"><a href="#53-最大子数组和-num-DP" class="headerlink" title="53. 最大子数组和 [num/DP]"></a>53. 最大子数组和 [num/DP]</h1><p>求在一个整数数组内，和最大的子序列的这个和。</p><p><img src="/images/leetcode/53_1.png" title="Optional title"></p><p>屏蔽掉的是暴力求解，肯定TLE（亏我还想了半天，用矩阵存储了一部分结果，把每个子列求和的N省略了，以为从O(N^3)变成O(N^2)，TLE之后再一寻思，nmd用切片和sum函数求和好像也是线性，人家本来就是O(N^2)，我优化了个寂寞，我是个逆天）。</p><p>上面代码确实没体现出DP，好像就是用了常规的规律，但确实是DP的思想。</p><p><img src="/images/leetcode/53_2.png" title="Optional title"></p><p>以及，这题是easy就多少有点说不过去了。</p><h1 id="56-Merge-Intervals-array"><a href="#56-Merge-Intervals-array" class="headerlink" title="56. Merge Intervals [array]"></a>56. Merge Intervals [array]</h1><p>融合所给区间，区间相交取交集。</p><p>自己想的方法：先求从第几个位置到第几个位置需要融合，然后融合了放进新的result，做得很复杂，要考虑开头结尾长度啥的，搞了半天还写的不对。</p><p><img src="/images/leetcode/56_1.png" title="Optional title"> </p><p>看评论区，实际上这题的思路非常简单，就是从头遍历，设置一个指针i，从0开始，看i和i+1，左区间是i[0]，右区间是max(i[1],i+1[1])。<code>如果有融合，那么指针不动，没有做融合操作，指针再向前走</code>。</p><p><img src="/images/leetcode/56_2.png" title="Optional title"> </p><h1 id="66-Plus-One-array"><a href="#66-Plus-One-array" class="headerlink" title="66. Plus One [array]"></a>66. Plus One [array]</h1><p><img src="/images/leetcode/66.png" title="Optional title"></p><p>设置一个进位flag。时间O(1)循环。</p><p>注意最后一位如果进位的话，要在第一个位置多加一个1.</p><h1 id="70-Climbing-Stairs-DP-array"><a href="#70-Climbing-Stairs-DP-array" class="headerlink" title="70. Climbing Stairs [DP/array]"></a>70. Climbing Stairs [DP/array]</h1><p><img src="/images/leetcode/70_1.png" title="Optional title"></p><p>dp问题，本质上是fib，重点。</p><p>最高赞解释：</p><p><img src="/images/leetcode/70_2.png" title="Optional title"></p><p><code>递归fib肯定会TLE</code>，两种做法，一种是设置n个空间，一种是就用两个空间做连续数组存放。</p><p>这题也能看作是DFS，左子树是走一步，右子树是走两步，结果就是这棵树的子节点数量。</p><h1 id="88-Merge-Sorted-Array-array"><a href="#88-Merge-Sorted-Array-array" class="headerlink" title="88. Merge Sorted Array [array]"></a>88. Merge Sorted Array [array]</h1><p>两个升序数组融合，要求在nums1上修改。</p><p><img src="/images/leetcode/88_1.png" title="Optional title"></p><p><img src="/images/leetcode/88_2.png" title="Optional title"></p><p><img src="/images/leetcode/88_3.png" title="Optional title"></p><p>这题有几个坑，首先如果用python的话，直接使用<code>nums1=</code>这种赋值方法，会<code>改变内存空间</code>，导致最后的结果系统不认；其次，需要考虑m和n是0的情况（图2），在考虑这个情况的时候同样要考虑前者。</p><p>python可以有两个做法，偷懒就是用切片把两个list合起来，然后sort()（图1）；其次就是常规做法，在时间O(m+n)下，<code>倒着遍历两个list</code>，谁比较大就放到nums1的最后，收尾工作就是把最小的那些挨着放到nums1的最前面即可（图3）。</p><p>这题逆大天，一定一定要注意python的赋值，这个以前从来没考虑过。</p><h1 id="141-环形链表-LinkList"><a href="#141-环形链表-LinkList" class="headerlink" title="141. 环形链表 [LinkList]"></a>141. 环形链表 [LinkList]</h1><p>检查是不是环形链表。</p><p><img src="/images/leetcode/141.png" title="Optional title"></p><p>必须要遵从链表思想的话是做法是<code>快慢指针</code>：两个指针，一个每次前进1步，一个每次前进2步，如果有环的话它们必定相遇。此时内存用的是O(1)。</p><p>常规做法就是每过一个内存地址，就存储，如果得到一个重复的，证明有环，指针遇到末尾了证明无环。此时用的内存是O(N)。检查重复自然需要用到一些数据结构比如hash和set等，实际上还是第一个方法简单。</p><h1 id="148-Sort-List-Sort-LinkList"><a href="#148-Sort-List-Sort-LinkList" class="headerlink" title="148. Sort List [Sort/LinkList]"></a>148. Sort List [Sort/LinkList]</h1><p>链表排序。</p><p><img src="/images/leetcode/148_1.png" title="Optional title"> </p><p><img src="/images/leetcode/148_2.png" title="Optional title"> </p><p><img src="/images/leetcode/148_3.png" title="Optional title"> </p><p>三种方法：</p><p>第一种是只对每个node的value进行冒泡排序，不动next，这样做显然会TLE。</p><p>第二种是偷鸡做法，就是把所有value取到一个list里，直接对list进行排序，这时的排序就是线性表的排序而不是链表的排序，最后把排好序的内容从链表头开始依次填进去，这样做面试估计不给过。</p><p>第三种是正常做法，就是<code>归并排序</code>。</p><p>对于链表而言最好的排序方法就是归并。要牢牢记住第三种方法的流程：<code>共三个函数</code>，<code>主函数</code>返回head的归并排序调用；<code>归并排序函数</code>就是将当前表头一分为二，并且递归的归并排左和右，最后返回的左右两部分的merge函数调用；<code>merge函数</code>就是设置一个新的暂时链表头和两个指针，把两个子链表，用操作指针从头遍历融合，返回的是暂时链表表头的指针next。</p><p><strong>三种O(logn)的排序算法：快排、堆排序、归并排序。</strong></p><h1 id="165-比较版本号"><a href="#165-比较版本号" class="headerlink" title="165. 比较版本号"></a>165. 比较版本号</h1><p>题目比较拗口，但是不难，中等就这？</p><p><img src="/images/leetcode/165.png" title="Optional title"></p><p>主要用到python的<code>split</code>和<code>int强制类型转换时会忽略前导0</code>。</p><h1 id="189-Rotate-Array-array"><a href="#189-Rotate-Array-array" class="headerlink" title="189. Rotate Array [array]"></a>189. Rotate Array [array]</h1><p>根据题意，就是将list中的后k个挪到前面来，就是尾出头进k次，每次一个人，之后的队列顺序。</p><p><img src="/images/leetcode/189.png" title="Optional title"></p><p>使用<code>切片</code>做即可。注意循环次数超过了队列长度，意味着要回归原位一次，那么<code>先用k对list长度取余数</code>即可。</p><h1 id="206-反转链表-LinkList"><a href="#206-反转链表-LinkList" class="headerlink" title="206. 反转链表 [LinkList]"></a>206. 反转链表 [LinkList]</h1><p><img src="/images/leetcode/206.png" title="Optional title"></p><p>在我看来还是递归比较舒服一点：从头开始循环，两个指针分别是头p和头的下一个p_next，如果p_next的下一个，也就是p_next-&gt;next不为空，就递归传入p_next和p_next-&gt;next，p_next-&gt;next为空就证明循环到了最后俩，此时p_next-&gt;next指向p，也就是反转操作，p的下一个也就是p-&gt;next指空，也就是改递归边界。</p><p>不递归纯循环的话就从头设立两个指针p和p_next，初始化为head和head-&gt;next，然后翻转，反转之后根据p_next的定位，挪p，然后p_next向下指。</p><p>两种方法从<code>操作顺序</code>上来说，<code>递归是从尾部开始反转，迭代就是从头部开始循环</code>。</p><p>非常规方法就是设其他空间去记录一部分信息，比如将值信息按顺序取下来，反着再装到这个表里；再或者用栈记录从头到尾的位置信息，然后每次反转栈头的两个。</p><h1 id="215-数组中的第K个最大元素-quick-sort"><a href="#215-数组中的第K个最大元素-quick-sort" class="headerlink" title="215. 数组中的第K个最大元素 [quick sort]"></a>215. 数组中的第K个最大元素 [quick sort]</h1><p><img src="/images/leetcode/215.png" title="Optional title"></p><p>主要是<code>快排</code>的代码，背下来。</p><h1 id="283-Move-Zeroes-array"><a href="#283-Move-Zeroes-array" class="headerlink" title="283. Move Zeroes [array]"></a>283. Move Zeroes [array]</h1><p>将数组内的所有0挪到最后。</p><p><img src="/images/leetcode/283.png" title="Optional title"></p><p>从头开始遍历，设置一个flag=0，非0的就放到flag位置，flag++，也统计了非0的个数，最后把从第flag位置到最后设置成0即可，注意list长度是0或者list内没有要操作的数这两个问题。</p><h1 id="458-可怜的小猪-技巧题-num"><a href="#458-可怜的小猪-技巧题-num" class="headerlink" title="458. 可怜的小猪 [技巧题/num]"></a>458. 可怜的小猪 [技巧题/num]</h1><p><img src="/images/leetcode/458.png" title="Optional title"> </p><p>比传统的那个用2进制做的题多了一个测试次数。使用测试次数可以扩展维度，即从<code>2进制（测试次数=1，2=1+1）变为测试次数+1进制</code>。</p><p>可以以一个小时时长为例，每次15min，那么对于每个猪，就能测试4次。</p><p>当只有猪1时，测试4次就意味着<code>猪1能断言一共5桶水的情况</code>：如果前4次没死，有毒的就是第5桶，如果前4次死了的话那就是死了那次的那一桶。</p><p>两只猪的话，<code>把25个桶排列成5*5，猪1每次喝列5桶，猪2每次喝行5桶，交叉死的就是有毒的</code>；</p><p>三只猪的话，<code>把125个桶排列成5*5*5</code>，猪1每次喝x面的25桶，猪2每次喝y面的25桶，猪3每次喝z面的25桶；</p><p>四只猪的话，<code>排5组的5*5*5</code>，<code>猪1每次喝这5组的x面共5*25=125桶</code>，猪2、3同理，<code>猪4每次喝一组的5*5*5共125桶</code>；</p><p>五只猪的话，<code>排列5*5=25组的5*5*5</code>，猪4每次喝一列的5<em>5</em>5，猪5每次喝1行的5<em>5</em>5…</p><p>注意上述过程中的乘法，有助于理解，因为我们只有3维，猪喝水喝到4维之后就是3维又3维的叠加。</p><hr><h1 id="代码技巧"><a href="#代码技巧" class="headerlink" title="代码技巧"></a>代码技巧</h1><blockquote><p>都是python和cpp</p></blockquote><ul><li>python可以函数里面写函数；</li><li>python <code>max和min、count</code>能用，list的许多操作可用，主要是append和extend；</li><li>cpp中vector常用的函数：size()，可按数组循环；insert(locat,num,value)指定位置插入指定个数的指定元素；</li><li>python的list切片，记住[:a]和[a:]的区别；</li><li>一些要求在原list上修改，不返回值的题，python操作数据赋值一定要用切片，不然会改变内存位置，导致过不去；</li><li>【56】<code>python如果想写变循环上限的循环，一定不要用for，不管用</code>：就是说用i当循环变量写for，如果在for内部修改了这个i，对于循环来说i的值是没有变动的，这一点非常重要，和c语言的循环是不一样的，如果想写，可采用外设循环i，然后用while当循环，在while内部修改i的值，这样才行；</li><li>python <code>str用切片反转：a -&gt; a[::-1]</code>（两个冒号）；</li><li><code>负数取余数和正数不一样</code>；</li><li>cpp链表，新设节点最好用<code>ListNode *temp = new ListNode(val,next)</code>，两个参数建议为-1和nullptr，用的时候<code>直接用 -&gt; 取元素</code>即可；</li><li><code>python str replace</code>的话，一定记得<code>用变量去接一下结果</code>，直接调用不接的话，原本的str是不会变的，会导致误判，觉得replace没生效或者写错了啥的；</li><li>python 用0初始化m<em>n二维数组：<code>arr=[[0]*m for _ in range(n)]</code>；不能用arr=[[0]\</em>m]*n，这样做修改某一行的某个位置会导致所有行的该位置做同样修改，意思就是把第一行的内存存了n遍；</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;统计&quot;&gt;&lt;a href=&quot;#统计&quot; class=&quot;headerlink&quot; title=&quot;统计&quot;&gt;&lt;/a&gt;统计&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;hard：</summary>
      
    
    
    
    
    <category term="leetcode" scheme="http://example.com/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：RealVSR</title>
    <link href="http://example.com/2021/11/02/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ARealVSR/"/>
    <id>http://example.com/2021/11/02/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ARealVSR/</id>
    <published>2021-11-02T07:59:53.000Z</published>
    <updated>2021-12-21T08:37:15.232Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Real-world Video Super-resolution: A Benchmark Dataset and A Decomposition based Learning Scheme</p></blockquote><blockquote><p>ICCV 2021</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>针对的问题是视频超分在<strong>下采样、退化</strong>时候产生的误差问题，认为使用简单的退化方法所得到的模型并不能很好的在实际应用中发挥作用。以及所使用的VSR数据集大多都是合成的（与SISR是一样的痛点）</p><p>为了解决上述问题，本文首先使用iPhone 11 Pro Max的多摄像头系统捕获成对的LR-HR视频序列，构建了一个真实世界的视频超分辨率（RealVSR）数据集。</p><p>接下来分析指出，由于LR-HR视频对由两个单独的摄像头捕获，因此它们之间不可避免地存在一定的错位和亮度/颜色差异。为了更稳健地训练VSR模型并从LR输入中恢复更多细节，将LR-HR视频转换为YCbCr空间，并将亮度通道分解为拉普拉斯金字塔，然后对不同的分量应用不同的损失函数。</p><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><blockquote><p>因为做的是比较有意义的新数据集，所以这部分有必要介绍一下。</p></blockquote><ul><li><p>常用的VSR数据集：Vimeo-90k[31]、REDS[23]、一些私有数据集[25]。</p></li><li><p>真实世界的SISR数据集：[6][33][4][30]。</p></li><li><p>在这篇文章之前，没有公开的真实世界VSR数据集。</p></li></ul><blockquote><p>[31] Video enhancement with task-oriented flow<br>[23] Ntire 2019 challenge on video deblurring and super-resolution: Dataset and study<br>[25] Detail-revealing deep video super-resolution</p></blockquote><blockquote><p>[6] Camera lens super-resolution<br>[33] Zoom to learn, learn to zoom<br>[4] Toward real-world single image super-resolution: A new benchmark and a new mode<br>[30] Component divide-and-conquer for real-world image super-resolution<br>上述文章中，33是之前读过的一篇；30是本文章的SISR的工作，ICCV 2019，后面可能会读。</p></blockquote><h1 id="The-Real-world-VSR-Dataset"><a href="#The-Real-world-VSR-Dataset" class="headerlink" title="The Real-world VSR Dataset"></a>The Real-world VSR Dataset</h1><p>使用<code>iPhoen 11 Pro Max + DoubleTake</code>制作该数据集。其中，ip11pm有3个摄像头，分别是13mm超宽摄像头、26mm宽摄像头和52mm长焦摄像头，每个摄像头都可以拍1200万像素的照片。DoubleTake可以通过具有不同焦距的两个摄像机以不同的比例捕获两个近似同步的视频。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-RealVSR/1.png" title="Optional title"></p><p>所构建数据集的一些主要信息：</p><ul><li><p>考虑到13mm镜头（也就是广角）有较为严重的失真，<strong>采用26mm镜头和52mm镜头构建数据集</strong>；</p></li><li><p><strong>使用52mm镜头拍摄的视频作为GT，也就是HR；使用26mm镜头拍摄的视频作为LR，从而生成×2的数据集，文中提到2倍就是VSR的主要需求</strong>；</p></li><li><p>前期制作总共<code>做了700个</code>视频对，每对是两个<code>帧速率30fps</code>和<code>分辨率1080P</code>的<code>近似同步</code>视频成，场景包括室内外、白天黑夜、静止场景和运动场景（包括摄像机运动和对象运动）等，文中提到具有丰富纹理的场景是首选场景；</p></li><li><p>数据收集后进行后处理，首先筛掉约200个质量较差的视频，例如严重模糊、噪声、过度曝光或曝光不足、严重对齐错误等，最终<code>保留500个</code>序列对。</p></li><li><p>后处理第二步是对齐，对齐算法来自[4]，考虑到相邻帧之间可能存在一些小的配准漂移，扩展了[4]中的配准算法，使用五个相邻帧作为输入来计算中心帧的配准矩阵。</p></li><li><p>后处理第三步是切割，对齐后在<code>1024×512</code>大小的中心区域裁剪对齐每一对序列，并将所有序列切割为<code>50帧</code>长度。</p></li></ul><p>综上，最终的数据集结果为<code>多场景/500个序列对/每个序列对有HR和LR2个序列/每个序列50帧/帧大小1024×512</code>。</p><blockquote><p>[4] Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. Toward real-world single image super-resolution: A new benchmark and a new model. In Proceedings of the IEEE International Conference on Computer Vision, pages 3086–3095, 2019. 2, 3, 4</p></blockquote><p>App Store内的DoubleTake软件页面：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-RealVSR/2.png" title="Optional title"></p><h1 id="VSR-Model-Learning"><a href="#VSR-Model-Learning" class="headerlink" title="VSR Model Learning"></a>VSR Model Learning</h1><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>文章：<a href="https://www4.comp.polyu.edu.hk/~cslzhang/paper/ICCV21_RealVSR.pdf">https://www4.comp.polyu.edu.hk/~cslzhang/paper/ICCV21_RealVSR.pdf</a></p></li><li><p>code：<a href="https://github.com/IanYeung/RealVSR">https://github.com/IanYeung/RealVSR</a>.</p></li></ul><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Real-world Video Super-resolution: A Benchmark Dataset and A Decompo</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：LIIF</title>
    <link href="http://example.com/2021/09/01/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ALIIF/"/>
    <id>http://example.com/2021/09/01/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ALIIF/</id>
    <published>2021-09-01T11:20:43.000Z</published>
    <updated>2021-09-10T13:53:04.727Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Learning Continuous Image Representation with Local Implicit Image Function</p></blockquote><blockquote><p>cvpr 2021</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>现实世界的图像是连续的，但计算机和采样设备得到的图像是非连续的。将现实世界的图像存储到计算机中，所使用的方法是基于像素（pixel）的2D矩阵。</p><p>文章受到了<strong>隐式神经表征（implicit neural representation）</strong>的启发，希望将图像表示为连续形式，提出了局部隐式图像函数（LIIF），用于以连续方式表示自然图像和复杂图像。</p><blockquote><p>Inspired by the recent progress in 3D reconstruc- tion with implicit neural representation, we propose Local Implicit Image Function (LIIF), which takes an image co- ordinate and the 2D deep features around the coordinate as inputs, predicts the RGB value at a given coordinate as an output. Since the coordinates are continuous, LIIF can be presented in arbitrary resolution. To generate the contin- uous representation for images, we train an encoder with LIIF representation via a self-supervised task with super- resolution. The learned continuous representation can be presented in arbitrary resolution even extrapolate to ×30 higher resolution, where the training tasks are not provided.</p></blockquote><h1 id="Implicit-Neural-Representation"><a href="#Implicit-Neural-Representation" class="headerlink" title="Implicit Neural Representation"></a>Implicit Neural Representation</h1><h2 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h2><p>来自论文SIREN。</p><blockquote><p>Implicit Neural Representations with Periodic Activation Functions<br><a href="https://github.com/vsitzmann/siren">https://github.com/vsitzmann/siren</a><br><a href="https://vsitzmann.github.io/siren/">https://vsitzmann.github.io/siren/</a></p></blockquote><p>作者本人对该方法的论文做了综述。</p><blockquote><p><a href="https://github.com/vsitzmann/awesome-implicit-representations">https://github.com/vsitzmann/awesome-implicit-representations</a></p></blockquote><p>比较著名的有NeRF等。</p><h2 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h2><p>以图像为例，其最常见的表示方式为二维空间上的离散像素点。</p><p>但是，在真实世界中，我们观察到的世界可以认为是连续的，或者近似连续。于是，<strong>可以考虑使用一个连续函数来表示图像的真实状态（本篇的思路）</strong>。</p><p>然而我们无从得知这个连续函数的准确形式，因此有人提出<strong>用神经网络来逼近这个连续函数</strong>，这种表示方法被称为“隐式神经表示“ （Implicit Neural Representation，INR）。</p><p>对于图像，INR函数将二维坐标映射到rgb值。对于视频，INR函数将时刻t以及图像二维坐标xy映射到rgb值。对于一个三维形状，INR函数将三维坐标xyz映射到0或1，表示空间中的某一位置处于物体内部还是外部。当然还有其他形式，如NERF将xyz映射到rgb和sigma。<strong>总而言之，这个函数就是将坐标映射到目标值。一旦该函数确定，那么一个图像/视频/体素就确定了。</strong></p><p><strong>INR是一个连续的函数，函数（网络）的复杂程度和信号的复杂程度成正比，但是和信号的分辨率无关。比如一个16<em>16的图像，和一个32</em>32的图像，如果内容一样，那么INR就会一样。</strong></p><blockquote><p>Implicit Neural Representation 隐式神经表示 - 知乎专栏<br><a href="https://zhuanlan.zhihu.com/p/372338398">https://zhuanlan.zhihu.com/p/372338398</a></p></blockquote><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="Local-Implicit-Image-Function"><a href="#Local-Implicit-Image-Function" class="headerlink" title="Local Implicit Image Function"></a>Local Implicit Image Function</h2><blockquote><p>局部隐式图像函数</p></blockquote><p>在LIIF表示中，设每个连续图像<code>Ii</code>表示为2D特征映射<code>M(I) ∈ R_H×W×D</code>。解码函数<code>fθ</code>（以θ为参数）由所有图像共享，其形式如下：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/1.png" title="Optional title"></p><p>其中，<code>z</code>是一个向量，<code>x</code>是连续图像域中的被查询的二维坐标，<code>s</code>是预测信号，即RGB值。</p><p>假设<code>M</code>（latent codes，潜在代码）均匀分布在连续图像域（图2中的蓝色圆圈）的2D空间中，根据<code>f</code>，对任意位置<code>xq</code>，其重构的RGB值为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/2.png" title="Optional title"></p><p>其中，<code>z*</code>为离<code>xq</code>最近（欧几里得距离）的特征向量（<code>M</code>中的一部分），<code>v*</code>为<code>z*</code>对应的坐标。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/3.png" title="Optional title"></p><blockquote><p><code>z*</code>表示特征向量，它可以看作代表了一整块的像素；<code>角标00、01、10、11</code>分别是指离<code>xq</code>最近的左上、右上、左下、右下四个<code>z*</code>。所有图像共享上述解码函数<code>fθ</code>。<br>以图2为例，<code>z*11</code>是距离<code>xq</code>最近的（欧氏距离）潜码（xq在z*11的块内），<code>v*</code>就是潜码<code>z*11</code>的二维坐标。</p></blockquote><h2 id="Feature-unfolding"><a href="#Feature-unfolding" class="headerlink" title="Feature unfolding"></a>Feature unfolding</h2><blockquote><p>特征展开</p></blockquote><p>为了丰富潜码<code>M</code>，对<code>M</code>进行特征展开，称为得到<code>M^</code>：<code>M^</code>中的潜码是<code>M</code>中3×3个相邻潜码的<code>级联（Concat）</code>，形式为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/4.png" title="Optional title"></p><p>其中，<code>级联</code>指的是一组向量的连接，<code>M</code>在其边界外用零向量填充。</p><p>后面为了简洁，直接使用<code>M</code>表示这一步操作。</p><h2 id="Local-ensemble"><a href="#Local-ensemble" class="headerlink" title="Local ensemble"></a>Local ensemble</h2><blockquote><p>局部集合</p></blockquote><p><strong>式(2)是不连续预测</strong>：<code>xq</code>处的信号预测是通过查询最近的潜码<code>z*</code>来完成的，在<code>M</code>中，当<code>xq</code>在2D域中移动时，不同<code>z*</code>的边界是不连续的，会发生突变。<strong>这会导致坐标上无限接近的点，所选择的<code>z*</code>可能非常不同。</strong></p><p>例如，<code>xq</code>穿过图2中的边界虚线（或者是极其接近边界位置），在这些坐标周围选择<code>z*</code>，两个无限接近坐标的信号将由不同的<code>z*</code>进行预测。只要学习到的函数<code>fθ</code>不是完美的，这些边界就会出现不连续的图案。</p><p>为了解决这个问题，扩展式(2)为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/5.png" title="Optional title"></p><p>其中，<code>St</code>是指<code>xq</code>与<code>z*u</code>所围成的矩形的面积，<code>S</code>为四块矩形面积之和。</p><blockquote><p>记u为t的对角特征，即t=0-&gt;u=1。</p></blockquote><blockquote><p>这里采用面积之比作为权重，是为了维持在四个特征向量之间任何一点的总权重是相等的，如果采用距离之比则不能达到这一点。</p></blockquote><h2 id="Cell-decoding"><a href="#Cell-decoding" class="headerlink" title="Cell decoding"></a>Cell decoding</h2><blockquote><p>单元解码</p></blockquote><p>根据上述步骤，则可以在任意分辨率下使用LIIF对图像进行表示。</p><p>对于给定的分辨率，最直接的方式就是根据像素点中心坐标求得对应的RGB值。但这样的方式是独立于size的，也就是说像素点包围的位置中的其他信息都丢失了。</p><p>为此，采用Cell decoding的策略，表示为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/6.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/7.png" title="Optional title"></p><p>其中,<code>c=[ch，cw]</code>包含两个指定查询像素高度和宽度的值，<code>[x，c]</code>表示x和c的串联(concatenation)。</p><p><code>fcell（z，[x，c]）</code>的含义可以解释为：如果用<code>形状c</code>渲染以坐标x为中心的像素，RGB值应该是什么。在给定分辨率下呈现连续表示时，拥有额外的输入c是有益的。</p><blockquote><p>逻辑上，当c -&gt; 0时，<code>f(z，x) = fcell(z，[x，c])</code>，即：连续图像可以被视为具有无限小像素的图像。</p><blockquote><p>The meaning of fcell(z, [x, c]) can be interpreted as: what the RGB value should be, if we ren- der a pixel centered at coordinate x with shape c. As we will show in the experiments, having an extra input c can be beneficial when presenting the continuous representation in a given resolution.</p></blockquote></blockquote><h2 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/8.png" title="Optional title"></p><p>对于给定的一张训练图片，以随机的scale下采样作为input。对应的ground-truth表示为<code>xhr</code>、<code>shr</code>，其中，<code>xhr</code>是中心坐标，<code>shr</code>是对应的RGB值。</p><p><code>Eϕ</code>将input映射为二维特征图作为LIIF表示，使用<code>xhr</code>进行query，<code>fθ</code>会预测对应RGB值<code>Spred</code>，与<code>shr</code>计算loss。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><ul><li><p><code>Eϕ</code>是EDSR或RDN去掉upsampler部分。</p></li><li><p>选用的loss是L1 loss。</p></li><li><p>训练：训练集：DIV2K，1000幅中的800幅；下采样：比例为×2、×3、×4的低分辨率对应图像，由Matlab中的imresize函数生成，默认设置为双三次插值；训练细节：初始学习率为1*1^-4的Adam优化器（每200个阶段衰减0.5倍）、训练时间为1000个epoch、bs=16、MetaSR的实验设置与LIIF相同，只是将LIIF表示替换为其元解码器。</p></li><li><p>测试：DIV2K、Set5、Set14、B100、Urban100。</p></li><li><p>平台：Pytorch。</p></li></ul><h2 id="定量实验"><a href="#定量实验" class="headerlink" title="定量实验"></a>定量实验</h2><p>训练时在1倍到4倍之间均匀采样，在测试时对6倍到30倍都进行了验证。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/9.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/10.png" title="Optional title"></p><blockquote><p>由于针对如此大倍数的SR方法很少，这里实际上的SOTA就只有MetaSR。</p></blockquote><h2 id="定量实验-1"><a href="#定量实验-1" class="headerlink" title="定量实验"></a>定量实验</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-LIIF/11.png" title="Optional title"></p><h2 id="对比实验"><a href="#对比实验" class="headerlink" title="对比实验"></a>对比实验</h2><ul><li><p>cell decoding似乎会影响out-of-distribution high resolution时的PSNR值。实验结果为：可以看到针对30倍的任务，cell-1/30明显好于其他设定。如果decoding cell大于实际的像素大小，这就类似于用一个比较大的平均核对图像进行处理。结论是，使用cell decoding有助于in-distribution scales，当scale过大时可能会影响PSNR但是仍然可以提升视觉质量。（图7）</p></li><li><p>实验在训练时使用固定的scale，虽然可以提升该scale的结果，但是对于其他scale效果不好。（表4）</p></li><li><p>针对size-varied ground-truth问题进行了实验。</p></li></ul><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>文章：<a href="https://arxiv.org/pdf/2012.09161.pdf">https://arxiv.org/pdf/2012.09161.pdf</a></p><p>code/home：<a href="https://yinboc.github.io/liif/">https://yinboc.github.io/liif/</a></p><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>新的点的思路之一！就是用连续进行表示！如果真的出来的话还是非常nice的！</p><hr><blockquote><p>截出来的图片糊的一，以后再也不在副屏上截图了，或者等有钱换个好一点的副屏……</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Learning Continuous Image Representation with Local Implicit Image F</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：IGNN</title>
    <link href="http://example.com/2021/06/21/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AIGNN/"/>
    <id>http://example.com/2021/06/21/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AIGNN/</id>
    <published>2021-06-21T08:40:41.000Z</published>
    <updated>2021-06-29T08:45:47.839Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Cross-Scale Internal Graph Neural Network for Image Super-Resolution</p></blockquote><blockquote><p>Nips 2020</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>和CS-NL那篇相似，都是利用一张图片中的相似patch做超分。</p><p>解决了两个问题：</p><ul><li><p>怎么准确地找到这些相似的patch？</p></li><li><p>怎么合理地融合这些patch？</p></li></ul><h1 id="Non-local"><a href="#Non-local" class="headerlink" title="Non-local"></a>Non-local</h1><h2 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h2><p>Non-local在图像重建中用的很多。许多经典的方法如非局部均值和BM3D，将相似的patch聚集起来进行图像去噪。</p><p>相似patch聚集的过程可表述为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-IGNN/1.png" title="Optional title"></p><p>其中，<code>Xi</code>和<code>Yi</code>是第i个位置（聚集中心）的输入和输出patch；<code>Xj</code>是第i个位置的相邻特征patch集<code>Si</code>中包含的第j个邻居；<code>Q(·)</code>将输入X变换到另一个特征空间；<code>C(·,·)</code>根据Xj和Xi的相关性得到的权重，越相关权重越大；<code>δi(X)</code>给前面的和项做归一化。</p><p><strong>将patch和权重看作图结构的顶点和边，那么上面的过程可以看作是一个GNN。</strong></p><blockquote><p>The above aggregation can be treated as a GNN if we treat the feature patches and weighted connections as vertices and edges respectively. The non-local neural networks actually model a fully-connected self-similarity graph. They estimate the aggregation weights between the query item Xi and all the spatially nearby patches Xj in a d × d window (or within the whole features). To reduce the memory and computational costs introduced by the above dense connection, some k-nearest neighbor based networks, e.g., GCDN and N3Net, only consider k (k ≪ d2) most similar feature patches for aggregation and treat them as the neighbors in Si for every query Xi. For all the above mentioned non-local methods, the aggregated neighboring patches are all in the same scale of the query and no HR information is incorporated, thus leading to a limited performance improvement for SISR.</p></blockquote><h2 id="一般化图像是否泛用？"><a href="#一般化图像是否泛用？" class="headerlink" title="一般化图像是否泛用？"></a>一般化图像是否泛用？</h2><p>论文中给出的图片是比较特殊的一类。实际拍摄图像中往往不会特别多重复的图像块（一栋大楼上都是一模一样的窗户，但拍大楼的图片只是极狭窄的题材）。</p><p><strong>真实算法中考虑的图像块非常小，往往只是很小的重复纹理repetitive texture、边edge或角corner，所以Non-local patch复现的性质对于任何一般图片都是成立的。</strong></p><blockquote><p>Zontak and Irani, Internal statistics of a single natural image<br>CVPR 2011</p></blockquote><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-IGNN/2.png" title="Optional title"></p><h2 id="Graph-Construction"><a href="#Graph-Construction" class="headerlink" title="Graph Construction"></a>Graph Construction</h2><p>这部分的过程如下：</p><ul><li><p>对LR下采样（指向左边的黑色箭头）：先使用双三次将输入的LR图像<code>IL</code>下采样，倍率为<code>s</code>，表示为<code>IL↓s</code>。s的取值所需的上采样的倍率。且文章指出对于上采样，s=2比s=4好。</p></li><li><p>特征提取：用VGG19对<code>IL</code>和<code>IL↓s</code>进行特征提取得到对应特征图。</p></li><li><p>查找相似patch：对于<code>EL</code>中要搜索的一个patch，<strong>在下采样特征图<code>EL↓s</code>中以MSE作为metric，搜索到k个相似patch，然后按位置对应回EL（图中红色虚线Vertex Mapping）</strong>，这样对于一个搜索patch就得到了k个EL中的相似patch。</p></li></ul><p>要搜索的patch和得到的k个相似patch是图的节点，搜索patch和相似patch的差是图的边，逐个patch进行搜索完成，图就构建完毕了。</p><p>根据对图片的统计验证，图像的相似patch很多。<strong>也就是说Graph Construction这部分如果按照最朴素的想法来做的话，计算量会很大。</strong></p><p><strong>设定k和s就是为了保证模型速度。</strong></p><ul><li><p>s：选定在IL下采样s倍的特征图上做搜索，而不是IL本身对应的特征图中去搜索；</p></li><li><p>k：只选k个相似的patch，减少后面聚合的计算量。</p></li></ul><h2 id="Patch-Aggregation"><a href="#Patch-Aggregation" class="headerlink" title="Patch Aggregation"></a>Patch Aggregation</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-IGNN/3.png" title="Optional title"></p><p>这部分是对前面获得的相似patch信息的应用。本质和一开始列的那个non-local聚合公式是一样的，都是加权，只不过形式复杂一点：这里是对于要搜索patch的对应k个相似patch进行加权，权重就是前面说到的图结构的边，将边值送至网络的输出取对数作为权。</p><h2 id="跳转连接"><a href="#跳转连接" class="headerlink" title="跳转连接"></a>跳转连接</h2><p>通过跨不同规模的跳转连接，聚合HR功能中的丰富HR信息↑s在网络中直接从中间位置传递到后面位置。这种机制允许HR信息帮助网络生成更详细的输出。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><ul><li><p>骨干网络：为了证明GraphAgg模块的有效性，选择了<code>EDSR</code>作为骨干网络，它包含32个ResBlock。所提出的GraphAgg模块在<code>IGNN中只使用一次</code>，并且<code>插入在第16个ResBlock之后</code>。</p></li><li><p>图构造：使用VGG19的前三层和固定的预训练参数来嵌入图像IL和IL↓s到EL和EL↓s。</p></li><li><p>训练集：800 high-quality (2K resolution) images from DIV2K dataset</p></li><li><p>测试集：Set5, Set14, BSD100, Urban100 and Manga109 in three upscaling factors: ×2, ×3 and ×4. </p></li></ul><h2 id="定量实验"><a href="#定量实验" class="headerlink" title="定量实验"></a>定量实验</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-IGNN/4.png" title="Optional title"></p><h2 id="定性试验"><a href="#定性试验" class="headerlink" title="定性试验"></a>定性试验</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-IGNN/5.png" title="Optional title"></p><h2 id="window-size-d-和-neighbor-k-的取值"><a href="#window-size-d-和-neighbor-k-的取值" class="headerlink" title="window size d 和 neighbor k 的取值"></a>window size d 和 neighbor k 的取值</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-IGNN/6.png" title="Optional title"></p><blockquote><p>windos size 实验中固定为60 × 60。</p></blockquote><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>文章：<a href="https://arxiv.org/pdf/2006.16673.pdf">https://arxiv.org/pdf/2006.16673.pdf</a></p></li><li><p>code：<a href="https://github.com/sczhou/IGNN">https://github.com/sczhou/IGNN</a></p></li></ul><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>看这篇是因为跟CS-NL那篇的思想相似，想做一对儿文章来讲。但实际上看的顺序反了，应该先看这篇。</p><p>IGNN这篇对于我来说，最大的好处就是<strong>解释清楚了Non-local的前世今生</strong>。这一点CS-NL没太解释也没太重视。</p><p>思路方面的评价是一样的，Non-local这个思路的有效性之前就说过。<strong>这类通过对LR-HR exemplars的挖掘，可以在一定程度上缓解超分任务中ill-posed的问题。</strong></p><p>以及还是比较在意效率问题，这点文章没提，估计也是效率方面差，扬长避短了。虽然在Graph Construction那边做了限制，但以图结构的复杂度，还是平方级别的计算量增长。这一点好像避不开，那么后面的优化就需要多做一做了。</p><p>结构方面，这一篇做的就不如CS-NL的解释性好了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Cross-Scale Internal Graph Neural Network for Image Super-Resolution</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：CSNLA</title>
    <link href="http://example.com/2021/06/13/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ACSNLA/"/>
    <id>http://example.com/2021/06/13/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ACSNLA/</id>
    <published>2021-06-13T07:46:31.000Z</published>
    <updated>2021-06-21T08:38:36.976Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining</p></blockquote><blockquote><p>CVPR 2020</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>文章指出，传统SISR方法着眼于LR图像，主要集中在局部先验和非局部先验的匹配和重建上。特别是基于局部先验（双线性、双三次插值）的方法，仅仅通过相邻像素的加权和来重建像素。</p><p><strong>自然图像中广泛存在跨尺度的块相似性。</strong></p><p>基于上述两点，<strong>为了突破局部限制，考虑基于非局部均值滤波的方法：即整个LR图像上全局搜索相似的块。</strong>也就是说，除了非局部像素对像素的匹配外，像素还可以与较大的图像块进行匹配。</p><p><strong>对于SISR任务来说，由于自然的跨尺度特征对应关系，可以直接从LR图像中搜索高频细节。</strong></p><p>论文主要有以下三点贡献：</p><ul><li><p>提出了用于SISR任务的第一个跨尺度非局部（CS-NL）注意力模块，计算图像内部的像素到块以及块到块的相似性。</p></li><li><p>提出了一个强大的SEM单元，在单元内部，通过结合<code>局部</code>、<code>尺度内非局部</code>和<code>跨尺度非局部</code>特征相关性，尽可能挖掘更多的的先验信息。</p></li><li><p>该网络在多个图像基准数据集上达到了最佳性能。</p></li></ul><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/1.png" title="Optional title"></p><h2 id="In-Scale-Non-Local-IS-NL-Attention"><a href="#In-Scale-Non-Local-IS-NL-Attention" class="headerlink" title="In-Scale Non-Local (IS-NL) Attention"></a>In-Scale Non-Local (IS-NL) Attention</h2><p>非局部注意可以通过从整体图像中总结相关特征来探索自我样本。形式上，给定图像特征映射X，非局部注意定义为</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/2.png" title="Optional title"></p><p>其中，<code>(i，j)、(g，h)和(u，v)</code>是X的坐标对，<code>ψ(·)</code>是特征变换函数，<code>φ(·,·)</code> 是用来衡量相似性的相关函数，定义为</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/3.png" title="Optional title"></p><p>其中，<code>θ(·)、δ(·)</code>是特征变换。</p><h2 id="Cross-Scale-Non-Local-CS-NL-Attention"><a href="#Cross-Scale-Non-Local-CS-NL-Attention" class="headerlink" title="Cross-Scale Non-Local (CS-NL) Attention"></a>Cross-Scale Non-Local (CS-NL) Attention</h2><p><strong>CS-NL注意是建立在IS-NL注意的基础上的，主要做法是：在根据比例<code>s</code>下采样过的图像中寻找特征Y=X中的候选特征。下采样操作是双线性插值。</strong></p><p>这样做的原因是，由于空间维度的差异，使用公共相似性度量直接将像素与patch匹配是不可行的。因此，只需对特征进行降采样，将patch表示为像素，并测量其距离（affinity）。</p><blockquote><p>The reason to do so is because directly matching pixels with patches using common similarity measurement is infeasible due to spatial dimension difference. So we simply downsample the features to represent the patch as pixel and measure the affinity.</p></blockquote><p>IS-NL公式扩展到CS-NL版本：设缩放比例为<code>s</code>为了计算像素和patch的相似性，首先需要从<code>X(w×h)</code>到<code>Y(w/s×h/s)</code>进行下采样，找到X和Y的pixel-wise相似性，最后使用相应的<code>s×s patch</code>重建X，输出是<code>Z(sh×sh)</code>。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/4.png" title="Optional title"></p><p>CS-NL attention module的流程：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/5.png" title="Optional title"></p><h2 id="Patch-Based-Cross-Scale-Non-Local-Attention"><a href="#Patch-Based-Cross-Scale-Non-Local-Attention" class="headerlink" title="Patch-Based Cross-Scale Non-Local Attention"></a>Patch-Based Cross-Scale Non-Local Attention</h2><p>CS-NL attention 的距离（affinity）度量可能存在问题。</p><blockquote><p>First, high-level features are robust to transformations and distortions, that is rotated/distorted low-level patches may yield same high-level features.<br>Besides, adjacent target regions are generated in a non-overlapping fashion, possibly creating discontinuous region boundaries artifacts.</p></blockquote><p>给出新的公式：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/6.png" title="Optional title"></p><p>p×p给出了非1的patch大小，当p=1的时候就回归了CS-NL。</p><blockquote><p>解释原理是上一个公式，代码里用的是这一个公式。</p></blockquote><h2 id="Self-Exemplars-Mining-SEM-Cell"><a href="#Self-Exemplars-Mining-SEM-Cell" class="headerlink" title="Self-Exemplars Mining (SEM) Cell"></a>Self-Exemplars Mining (SEM) Cell</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/7.png" title="Optional title"></p><h3 id="Multi-Branch-Exemplars"><a href="#Multi-Branch-Exemplars" class="headerlink" title="Multi-Branch Exemplars"></a>Multi-Branch Exemplars</h3><p>Local Branch不对Li-1做任何操作，直接搬过来；Cross-Scale NL Attention前面提过；In-Scale NL Attention计算的是特征图内像素间的非局部相似性。</p><h3 id="Mutual-Projected-Fusion"><a href="#Mutual-Projected-Fusion" class="headerlink" title="Mutual-Projected Fusion"></a>Mutual-Projected Fusion</h3><p>将三部分的特征进行融合：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/8.png" title="Optional title"></p><blockquote><p>式(5)-(8)</p></blockquote><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><ul><li><p>训练集：DIV2K（800 images）</p></li><li><p>测试集：Set5, Set14, B100, Urban100, and Manga109</p></li><li><p>SEM：12</p></li><li><p>p = 3</p></li></ul><h2 id="定量试验"><a href="#定量试验" class="headerlink" title="定量试验"></a>定量试验</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/9.png" title="Optional title"></p><h2 id="定性试验"><a href="#定性试验" class="headerlink" title="定性试验"></a>定性试验</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/10.png" title="Optional title"></p><h2 id="branch效果对比"><a href="#branch效果对比" class="headerlink" title="branch效果对比"></a>branch效果对比</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/11.png" title="Optional title"></p><h2 id="branch融合方式对比"><a href="#branch融合方式对比" class="headerlink" title="branch融合方式对比"></a>branch融合方式对比</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-CSNLA/12.png" title="Optional title"></p><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>文章：<a href="https://arxiv.org/pdf/2006.01424.pdf">https://arxiv.org/pdf/2006.01424.pdf</a></p></li><li><p>code：<a href="https://github.com/SHI-Labs/Cross-ScaleNon-Local-Attention">https://github.com/SHI-Labs/Cross-ScaleNon-Local-Attention</a></p></li></ul><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>从文章的结果来看，用相似块来做SR的这个思路是非常成功的，也符合自然的想法，合理是非常合理的总之。</p><p>实验做的很细，但感觉少了一个非常重要的内容，就是SEM的数量问题。p的大小是做了实验的，上面没放。</p><p>最终的对比结果里，用了注意力机制的都是效果不错的，还是要多关注Attention。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Image Super-Resolution with Cross-Scale Non-Local Attention and Exha</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：AdderNet/AdderSR</title>
    <link href="http://example.com/2021/05/28/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AAdderNet:AdderSR/"/>
    <id>http://example.com/2021/05/28/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AAdderNet:AdderSR/</id>
    <published>2021-05-28T06:47:01.000Z</published>
    <updated>2021-06-02T12:48:12.361Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="AdderNet"><a href="#AdderNet" class="headerlink" title="AdderNet"></a>AdderNet</h1><blockquote><p>AdderNet: Do We Really Need Multiplications in Deep Learning?</p></blockquote><blockquote><p>CVPR 2019</p></blockquote><h2 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h2><p>在深度学习算法中，卷积操作被广泛的用于度量输入特征和卷积滤波器之间的相似性。</p><p>GPU+深度卷积神经网络，大大加快了计算机视觉领域的发展。但GPU有着显而易见的缺陷，主要是物理因素所导致的。第一个是功耗问题，再一个是体积问题。这两个问题导致了现有的多数深度学习方法只能运行在装有大型GPU的机器上。人们希望这些算法能在小巧的移动设备上实现。因此，目的就是降低算法的运行成本，使得在硬件配置不够的情况下也能高效的运行这些深度算法。</p><p><strong>在正向推理过程中，深层神经网络的计算大多是浮值权和浮值激活的乘法运算。显然，乘法运算比加法运算要慢。</strong></p><p><strong>本文提出的AdderNet，就是在放弃卷积运算的同时，利用l1距离，最大限度地描述和利用加法，逼近卷积操作的精度。为了保证模板的充分更新和网络的收敛性，设计了一种改进的梯度正则化反向传播算法。</strong></p><h2 id="数学描述"><a href="#数学描述" class="headerlink" title="数学描述"></a>数学描述</h2><h3 id="Adder-Network"><a href="#Adder-Network" class="headerlink" title="Adder Network"></a>Adder Network</h3><p>对于CNN中的卷积运算，假定输入特征<code>X</code>，filter表示为<code>F</code>，卷积后输出的是二者的相似性度量，表述如下面公式：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/1.png" title="Optional title"></p><p>其中，<code>d</code>为kernel size，<code>c_in</code>和<code>c_out</code>分别为输入、输出通道数；卷积核<code>F</code>的大小则为<code>d × d × c_in × c_out</code>；H和W为输入特征的长和宽，输入特征<code>X</code>的大小为<code>H × W × c_in</code>。</p><p>以及，<code>S(·,·)</code>是相似性度量，也就是距离度量函数。如果将互相关（cross-correlation）作为距离的度量，即<code>S(x,y)= x × y</code>，这时式（1）成为卷积运算。式（1）还可以表示当d=1时计算完全连接的层。</p><p>还有许多其他度量来测量滤波器和输入特征之间的距离，但这些度量大多涉及乘法。</p><p>于是，希望在式（1）中用加法来代替乘法。<strong>L1距离仅涉及到两个向量差的绝对值</strong>，不包含乘法。因此，通过计算滤波器和输入特征之间的L1距离，式（1）可以重新表示为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/2.png" title="Optional title"></p><p>不论是使用互相关，还是L1距离，都可以完成相似性度量，但二者的输出结果还是有一些差别的：通过卷积核完成输入特征图谱的加权和计算，结果可正可负；<strong>但adder filter输出的结果恒为负，为此作者引入了batch normalization将结果归一化到一定范围区间内，从而保证传统CNN使用的激活函数在此依旧可以正常使用。</strong></p><blockquote><p>用式（3）计算的特征都是负数，因此在特征计算后，采用BN层来归一化，之后再用激活函数提高特征非线性。</p></blockquote><h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><p>神经网络利用<strong>反向传播</strong>来计算滤波器的梯度和<strong>随机梯度下降</strong>来更新参数。</p><p>在CNN中，输出特征Y相对于滤波器F的偏导数被计算为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/3.png" title="Optional title"></p><p>在AdderNets中，Y相对于F的偏导数是：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/4.png" title="Optional title"></p><blockquote><p>sgn(·)是阶跃函数，取-1、0、1。</p></blockquote><p>式（4）中等号右边的是<code>signSGD</code>，<strong>signSGD被证明的缺陷是几乎不会选择到最陡的方向，而且随着维度增加效果会更差。</strong></p><p>因此AdderNet使用如下公式进行梯度更新：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/5.png" title="Optional title"></p><p>此外，如果使用全精度梯度的更新方法，由于涉及到前层的梯度值，很容易导致梯度爆炸。这里使用<code>HardTanh</code>将输出限定在[-1,1]范围内：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/6.png" title="Optional title"></p><blockquote><p>梯度计算也好了，接下来的问题是：加法层的方差会很大。</p></blockquote><h3 id="Adaptive-Learning-Rate-Scaling"><a href="#Adaptive-Learning-Rate-Scaling" class="headerlink" title="Adaptive Learning Rate Scaling"></a>Adaptive Learning Rate Scaling</h3><p>在传统的CNN中，假设权值和输入特征是独立的，服从正态分布，输出的方差大致可以估计为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/7.png" title="Optional title"></p><p>对于AdderNet，输出的方差可以近似为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/8.png" title="Optional title"></p><p><strong>实际上，权重var[F]的方差通常非常小（10^−3～10^-4）。因此，式（9）中的加法运算会比式（8）带来更大的输出方差。</strong></p><blockquote><p>式（8）:var[X] × var[F]<br>式（9）:var[X] + var[F]</p></blockquote><p>AdderNet的输出具有较大方差，在更新时根据常规的链式法则会导致梯度比常规CNN更小，从而导致参数更新过慢。</p><p>这里，最容易想到的解决方法就是：使用大的learning rate。这样虽然可以提高学习速度，但是文中又指出，<strong>不同层其权重梯度变化很大</strong>：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/9.png" title="Optional title"></p><p><strong>这表明每一层的参数需要不同。</strong>因此作者给<strong>每一个adder layer</strong>设计了不同学习率：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/10.png" title="Optional title"></p><p>其中，<code>γ</code>是整个神经网络的全局学习速率，<code>∆L(Fl)</code>是层L中过滤器的梯度，<code>αl</code>是其相应的局部学习率。</p><blockquote><p>输入是经过BN的，也就是说在AdderNet中，filters的值也是经过了归一化。</p></blockquote><p>局部学习率<code>αl</code>定义为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/11.png" title="Optional title"></p><p>其中，<code>η</code>是一个控制adder filters学习率的超参数，<code>k</code>是Fl中参数的个数。</p><h3 id="Adder-Network的前馈和反传流程"><a href="#Adder-Network的前馈和反传流程" class="headerlink" title="Adder Network的前馈和反传流程"></a>Adder Network的前馈和反传流程</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/12.png" title="Optional title"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h3><ul><li><p>NVIDIA Tesla V100 GPU：2017年中发布、16/32GB（文中没提用的是什么显存的版本）</p></li><li><p>PyTorch</p></li></ul><h3 id="MINST-LeNet-5-BN-AddNN-CNN"><a href="#MINST-LeNet-5-BN-AddNN-CNN" class="headerlink" title="MINST + LeNet-5-BN + AddNN/CNN"></a>MINST + LeNet-5-BN + AddNN/CNN</h3><p>结果：</p><ul><li><p>CNN的准确率为99.4%∼435K乘法。</p></li><li><p><strong>通过将卷积运算中的乘法替换为加法运算，该加法网的准确率达到99.4%∼870K加法，几乎没有乘法。</strong></p></li></ul><p>训练设置：</p><ul><li>图像大小：调整为32×32；</li><li>优化：Nesterov加速梯度法（NAG）；</li><li>weight decay/momentum：5×10^−4/0.9；</li><li>初始lr为0.1、cosine learning rate decay（余弦学习率衰减）；</li><li><strong>用加法器滤波器代替LeNet-5-BN中的卷积滤波器、将全连接层中的乘法替换为减法；</strong></li><li>50个epochs的训练。</li></ul><h3 id="CIFAR-10-100-VGG-ResNet-BNN-AddNN-CNN"><a href="#CIFAR-10-100-VGG-ResNet-BNN-AddNN-CNN" class="headerlink" title="CIFAR-10/100 + VGG/ResNet + BNN/AddNN/CNN"></a>CIFAR-10/100 + VGG/ResNet + BNN/AddNN/CNN</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/13.png" title="Optional title"></p><p>结果：</p><ul><li><p>对于VGG，AddNN与CNNs（CIFAR-10为93.80%，CIFAR-100为72.73%）的结果基本相同（CIFAR-10为93.72%，CIFAR-100为72.64%）；BNN的模型尺寸比AdderNet和CNN小得多，精度也低得多（CIFAR-10为89.80%，CIFAR-100为65.41%）；</p></li><li><p>对于ResNet-20，CNNs的精确度最高（CIFAR10为92.25%，CIFAR-100为68.14%），但乘法次数较多（41.17M），AddNN少了0.5个点左右（CIFAR-10和CIFAR-100中的无乘法运算精度分别为91.84%和67.60%）。</p></li></ul><blockquote><p>由于二进制神经网络BNN可以使用XNOR运算来代替乘法，所以还比较了BNN的结果。</p><blockquote><p>XNOR：同或。（1 XNOR 1 = 0 XNOR 0 = 1，1 XNOR 0 = 0 XNOR 1 = 0）</p></blockquote></blockquote><p>训练设置：</p><ul><li>图像大小：32×32；</li><li>与何凯明大佬2016年残差那篇相同的DA和处理方法；</li><li>初始lr为0.1、后续遵循多项式学习率时间表；</li><li>bs=256；</li><li>BNN中，将第一层和最后一层设置为全精度卷积层；</li><li>400个epochs的训练。</li></ul><h3 id="ImageNet-ResNet-BNN-AddNN-CNN"><a href="#ImageNet-ResNet-BNN-AddNN-CNN" class="headerlink" title="ImageNet + ResNet + BNN/AddNN/CNN"></a>ImageNet + ResNet + BNN/AddNN/CNN</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/14.png" title="Optional title"></p><h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><ul><li><p>文章：<a href="https://arxiv.org/pdf/1912.13200.pdf">https://arxiv.org/pdf/1912.13200.pdf</a></p></li><li><p>code：<a href="https://github.com/huawei-noah/AdderNet">https://github.com/huawei-noah/AdderNet</a></p></li></ul><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>首先，文章思路很不错。相似的思路之前也提过，这一篇的内容做的挺多的，不管是数学方面的论证还是实验部分做的都还不错，比较清晰。</p><p>以及看了知乎上这篇文章的团队内成员的答复和一些网友的评论，整体的评价也还不错，去年6月份的时候，大部分都还在求硬件加速之类的，官方也说会给出预训练模型，接下来的一篇文章也是继承了这个工作，说明这个工作持续的在做。这是比较nice的。</p><p>缺陷也有，最主要的一点就是<strong>为什么最后的实验不去对比GPU下的速度？</strong>显得有点文不对题，故意隐瞒，不是特别的convincing。以及实验主要做的是图像分类的问题，但也没提修改了卷积操作之后，在别的领域的应用，也没特别说明只是针对图像分类问题做的。</p><p>最后，有一点没看懂，就是做的是简化网络，从GPU转移到CPU，但是最终部署的时候用的还是GPU，且没给CPU的主频信息等，包括文章中没有特别提到的硬件方面的东西，包括cuda编程等。这些都是没说清楚的点，也许不重要？但是这要不说清楚，代码都不怎么看得懂，哪有心思跑它。</p><hr><h1 id="AdderSR"><a href="#AdderSR" class="headerlink" title="AdderSR"></a>AdderSR</h1><blockquote><p>AdderSR: Towards Energy Efficient Image Super-Resolution</p></blockquote><blockquote><p>CVPR 2021</p></blockquote><h2 id="立意-1"><a href="#立意-1" class="headerlink" title="立意"></a>立意</h2><p>AdderNet用于SISR问题中会出现两个问题：</p><ul><li><p>加法器不能很容易的学习身份映射（identity mapping）；</p></li><li><p>加法器不能实现高通滤波。</p></li></ul><blockquote><p>Specifically, the adder operation cannot easily learn the identity mapping, which is essential for image processing tasks. In addition, the functionality of high-pass filters cannot be ensured by AdderNets.</p></blockquote><p>换言之，在SISR中，AdderNet需要保证两个重要特性：<strong>各卷积层输入输出特征的相似性和高频信息的细节增强</strong>。</p><blockquote><p>In SISR, there are two important properties that should be ensured by adder neural networks: the similarity between the input and output features of each convolutional layer, and the enhancement of details w.r.t. high frequency information.</p></blockquote><p>VDSR中输入图像“蝴蝶”的不同层的输出特征映射。<strong>任何两个相邻层之间的差异在纹理和颜色信息上都非常相似。高频信息的细节随着深度的增加而增强。</strong>这两个重要性质需要加法神经网络来保证。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/15.png" title="Optional title"></p><p>这篇文章分别设计了模块去解决了上述问题，最终使AdderNet的SISR模型逼近CNN性能。</p><blockquote><p>To maximally excavate the potential for exploiting AdderNets to establish SISR models, we first analyze the theoretical difficulties for applying the additions into SISR tasks. Specifically, input and output features in any two neighbor layers in SISR models are very close with the similar global texture and color information as shown in Figure 2. However, the identify mapping cannot be learned by a one-layer adder network. Thus, we suggest to insert self-shortcuts and formulate new adder models for the SISR task. Moreover, we find that the high-pass filter is also hard to approximate by adder units. We then develop a learnable power activation. By exploiting these two techniques, we replace the conventional convolution filters in modern SISR networks by adder filters and establish AdderSR models accordingly. </p></blockquote><h2 id="数学描述-1"><a href="#数学描述-1" class="headerlink" title="数学描述"></a>数学描述</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>一般的SISR任务的目标函数可以表示为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/16.png" title="Optional title"></p><p>其中，其中<code>Iy</code>是观测数据，即低分，<code>Ix</code>是期望的高分辨率图像，<code>R(·)</code>表示所使用的先验信息，例如平滑和加性噪声，<code>λ</code>是权重。</p><p>式（2）又提了一遍加法器模型，就是上一篇的式（2）。</p><blockquote><p>本篇的式（2）跟上一篇的式（2）有一点点不一样，就是求和号的角标都是从1开始的，上一篇是从0；以及换了两个字母表示。不过应该不是什么问题。</p></blockquote><h3 id="Learning-Identity-Mapping-using-AdderNet"><a href="#Learning-Identity-Mapping-using-AdderNet" class="headerlink" title="Learning Identity Mapping using AdderNet"></a>Learning Identity Mapping using AdderNet</h3><blockquote><p>一上来，这个恒等映射的作用就没太看懂，查只能查到何大佬那篇文章，但也没有引，公式是一模一样的，不知道说的是不是一个事情。</p></blockquote><p>图2表明，在使用深度学习方法的SISR任务中，恒等映射<code>Iy=F(Iy)</code>是非常必要的。</p><p><strong>对于传统的卷积网络，当权值为单位矩阵时，恒等映射很容易学习。但AdderNet使用的是l1范数，虽然l1距离可以很好地完成图像分类任务，但是不能逼近恒等映射。</strong></p><p>使用多个Adder Layer叠加可以逼近，但是会显著的增加参数量。</p><h4 id="Adder-Layer为什么无法逼近恒等映射？"><a href="#Adder-Layer为什么无法逼近恒等映射？" class="headerlink" title="Adder Layer为什么无法逼近恒等映射？"></a>Adder Layer为什么无法逼近恒等映射？</h4><blockquote><p>文中的Theorem 1。</p></blockquote><p>描述低分图像<code>Iy</code>和带权重的Adder Layer<code>W</code>的恒等映射：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/17.png" title="Optional title"></p><p><strong>这个等式是<span style="border-bottom:2px dashed red;">不成立</span>的</strong>，是因为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/18.png" title="Optional title"></p><h4 id="如何实现恒等映射？"><a href="#如何实现恒等映射？" class="headerlink" title="如何实现恒等映射？"></a>如何实现恒等映射？</h4><p>改进Adder Layer：为每个Adder Layer增加<code>自快捷操作（self-shortcut operation）</code>：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/19.png" title="Optional title"></p><p>其中<code>Wl</code>是第l层中加法器滤波器的权重，<code>Xl</code>是输入，<code>Yl</code>是输出。</p><p><strong>根据式（7），<code>Yl</code>包含<code>Xl</code>本身，则可以通过减小<code>Wl</code>，使等号右边近似等号左边。</strong></p><p>与图像识别任务不同的是，大多数SISR问题的特征应该保持一个固定的大小，即Xl和Yl的宽度和高度完全相同。因此，式（7）可嵌入大多数常规SISR模型中。</p><h3 id="Learnable-Power-Activation"><a href="#Learnable-Power-Activation" class="headerlink" title="Learnable Power Activation"></a>Learnable Power Activation</h3><p>自然图像通常由不同的频率信息组成。例如，背景和大面积的草都是低频信息，其中大部分相邻像素非常接近。与此相反，物体和某些建筑物的边缘对于给定的整个图像来说，都是精确的高频信息。</p><p>理想的高通滤波器<code>Φ(·)</code>可定义为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/20.png" title="Optional title"></p><p>对于卷积操作，高通滤波的实现也很简单，比如：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/21.png" title="Optional title"></p><p>但是Adder Layer无法实现高通滤波。</p><h4 id="Adder-Layer为什么无法实现高通滤波？"><a href="#Adder-Layer为什么无法实现高通滤波？" class="headerlink" title="Adder Layer为什么无法实现高通滤波？"></a>Adder Layer为什么无法实现高通滤波？</h4><blockquote><p>文中的Theorem 2。</p></blockquote><p>令<code>E ∈ R_d×d</code> 是给定SR模型的输入，其中每个元素等于1。<code>W</code>表示任意Adder Layer的权重。<code>s ∈ R</code>。则<span style="border-bottom:2px dashed red;">不存在</span><code>W</code>和常数<code>a ∈ R</code>满足以下等式：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/22.png" title="Optional title"></p><p>是因为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/23.png" title="Optional title"></p><h4 id="如何实现高通滤波？"><a href="#如何实现高通滤波？" class="headerlink" title="如何实现高通滤波？"></a>如何实现高通滤波？</h4><p>提出了<code>Learnable Power Activation</code>函数来解决Adder Layer的高通滤波问题和细化输出图像：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/24.png" title="Optional title"></p><p>其中，<code>Y</code>为输出特征，<code>sgn(·)</code>为符号函数。</p><p><strong><code>α &gt; 0</code>是一个可学习的参数，用于调整信息和分布。当<code>α &gt; 1</code>，式（11）可以增强输出图像的对比度，强调高频信息；当<code>0 &lt; α &lt; 1</code>，式（11）可以平滑输出图像中的所有信号并去除伪影和噪声。</strong></p><p>式（11）可以很容易地嵌入到任何SISR模型的常规ReLU中。</p><blockquote><p>式（11）的背景：Box-Cox变换应用于图像去噪任务中、Box-Cox变换在图像超分辨率中的功能。没有过多解释和证明。</p><blockquote><p>Fortunately, Sharabati and Xi [29] applied the Box-Cox transformation [28] in the image denoising task and found that this transformation can achieve the similar functionality to that of the high-pass filters without adding massive parameters and calculations. Oliveira et al. [26] further discussed the functionality of Box-Cox transformation in image super- resolution. In addition, a sign-preserving power law point transformation is also explored for emphasizing the areas with abundant details in the input image [27]. </p></blockquote></blockquote><h2 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h2><p>实验具体细节不放了，直接放数据：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/25.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/26.png" title="Optional title"></p><p>做出来的3倍的效果：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/27.png" title="Optional title"></p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul><li><p>文章：<a href="https://arxiv.org/pdf/2009.08891.pdf">https://arxiv.org/pdf/2009.08891.pdf</a></p></li><li><p>code：上一个工作的拓展，所以和上一篇的code放在一起了。</p></li></ul><h2 id="感想-1"><a href="#感想-1" class="headerlink" title="感想"></a>感想</h2><p>最大的问题还是实验部分没有提速度，这本来是一个加速的工作，不提速度只说工作量，确实有点问题。</p><p>评价指标结果相当相当接近CNN了，这方面真的很厉害。</p><hr><h1 id="其他-1"><a href="#其他-1" class="headerlink" title="其他"></a>其他</h1><h2 id="卷积、互相关、自相关"><a href="#卷积、互相关、自相关" class="headerlink" title="卷积、互相关、自相关"></a>卷积、互相关、自相关</h2><p>卷积的数学表示：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/28.png" title="Optional title"></p><p>互相关的数学表示：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/29.png" title="Optional title"></p><p>互相关的公式中，变量和积分对象不再是t，而是<code>tao</code>。也就是说互相关研究室的是两个对象产生一段时间差<code>tao</code>之后的关系，也就是相不相关，如果平移之后两个函数正交，那么他们就没有相互关系，互相关为0。</p><p>如果<code>tao</code>是偶函数的话，互相关与卷积的结果是一样的。</p><blockquote><p>卷积、互相关与自相关 - 知乎专栏<br><a href="https://zhuanlan.zhihu.com/p/62292503">https://zhuanlan.zhihu.com/p/62292503</a></p></blockquote><h2 id="L1距离-L1-norm"><a href="#L1距离-L1-norm" class="headerlink" title="L1距离/L1-norm"></a>L1距离/L1-norm</h2><p><code>L1-norm</code>又叫做<code>taxicab-norm</code>或者 <code>Manhattan-norm</code>，可<code>用在曼哈顿区坐出租车来做比喻</code>。图中绿线是两个黑点的L2距离，而其他几根是L1距离。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/30.png" title="Optional title"></p><blockquote><p>理解L1，L2 范数在机器学习中应用 - 简书<br><a href="https://www.jianshu.com/p/6cf5d60db634">https://www.jianshu.com/p/6cf5d60db634</a></p></blockquote><h2 id="signSGD"><a href="#signSGD" class="headerlink" title="signSGD"></a>signSGD</h2><blockquote><p>SIGNSGD: compressed optimisation for non-convex problems</p></blockquote><p>文章核心是：SGD里面，梯度真正有用的是方向而不是大小。所以，即使只保留梯度的符号来对模型进行更新，也能得到收敛的效果。甚至有些情况下，这么做能减少梯度的噪声，使得收敛速度更快。</p><p>根据上面的结论，进而衍生出了三种算法。<strong>SignSGD是第一种：直接把gradient求sign：</strong></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/31.png" title="Optional title"></p><blockquote><p>SignSGD 及其 MXNet 实现解读 - 知乎专栏<br><a href="https://zhuanlan.zhihu.com/p/112346480">https://zhuanlan.zhihu.com/p/112346480</a></p></blockquote><h2 id="HardTanh"><a href="#HardTanh" class="headerlink" title="HardTanh"></a>HardTanh</h2><p>HardTanh比Tanh计算开销小，缺点是和dying relu类似，部分参数可能不更新。</p><p>文章中的公式：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/32.png" title="Optional title"></p><p>图像：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/AdderSR/33.png" title="Optional title"></p><h2 id="Identity-Mapping"><a href="#Identity-Mapping" class="headerlink" title="Identity Mapping"></a>Identity Mapping</h2><blockquote><p>Identity Mappings in Deep Residual Networks</p></blockquote><blockquote><p>2016 ECCV</p></blockquote><p>这个关键词何凯明大佬为了残差网络而提到的。这篇文章的核心是：分析了残差模块的传播方式，能过解释为什么使用identity mapping作为跳跃连接和加和的激活项，能使得前向和反向的信号能直接在模块之间传播。</p><h2 id="Box-Cox变换"><a href="#Box-Cox变换" class="headerlink" title="Box-Cox变换"></a>Box-Cox变换</h2><p>是Box和Cox在1964年提出的一种广义幂变换方法，是统计建模中常用的一种数据变换，用于连续的响应变量不满足正态分布的情况。</p><p>Box-Cox变换之后，可以一定程度上减小不可观测的误差和预测变量的相关性。</p><blockquote><p>box-cox变换 - 百度百科<br><a href="https://baike.baidu.com/item/box-cox%E5%8F%98%E6%8D%A2/10278422?fr=aladdin">https://baike.baidu.com/item/box-cox变换/10278422?fr=aladdin</a></p></blockquote><blockquote><p><a href="https://onlinestatbook.com/2/transformations/box-cox.html">https://onlinestatbook.com/2/transformations/box-cox.html</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;AdderNet&quot;&gt;&lt;a href=&quot;#AdderNet&quot; class=&quot;headerlink&quot; title=&quot;AdderNet&quot;&gt;&lt;/a&gt;AdderN</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04虚拟机+Hadoop：伪分布式</title>
    <link href="http://example.com/2021/05/28/Ubuntu16-04%E8%99%9A%E6%8B%9F%E6%9C%BA-Hadoop%EF%BC%9A%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    <id>http://example.com/2021/05/28/Ubuntu16-04%E8%99%9A%E6%8B%9F%E6%9C%BA-Hadoop%EF%BC%9A%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F/</id>
    <published>2021-05-28T06:17:20.000Z</published>
    <updated>2021-05-28T06:42:30.529Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>Hadoop是一个由Apache基金会所开发的分布式系统基础架构。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力进行高速运算和存储。</p><p>Hadoop实现了一个分布式文件系统（Distributed File System），其中一个组件是HDFS（Hadoop Distributed File System）。HDFS有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。HDFS放宽了（relax）POSIX的要求，可以以流的形式访问（streaming access）文件系统中的数据。</p><p>Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算。</p><p>Hadoop一共有三种部署方式，分别是<strong>本地部署、伪分布部署、集群部署</strong>。Hadoop默认模式为非分布式模式（本地模式），无需进行其他配置即可运行。非分布式即单Java进程，方便进行调试。而Hadoop伪分布模式的工作原理和集群模式的工作原理一样。</p><h1 id="Hadoop伪分布式"><a href="#Hadoop伪分布式" class="headerlink" title="Hadoop伪分布式"></a>Hadoop伪分布式</h1><p><strong>使用64位的Ubuntu 16.04虚拟机以及Hadoop 2.7.2 (stable)版本进行Hadoop伪分布式配置和测试。</strong></p><h2 id="创建hadoop用户"><a href="#创建hadoop用户" class="headerlink" title="创建hadoop用户"></a>创建hadoop用户</h2><p>需要使用名为hadoop的用户来进行接下来的步骤。</p><p>在原用户的terminal中进行如下操作：</p><p><img src="/images/Hadoop/1.png" title="Optional title"></p><h2 id="SSH"><a href="#SSH" class="headerlink" title="SSH"></a>SSH</h2><p>切换到新用户hadoop，准备工作就是做一下apt-get的更新以及需要用到的文本编辑器的安装。</p><p>集群、单节点模式都需要用到SSH登陆，需要安装SSH server：</p><p><img src="/images/Hadoop/2.png" title="Optional title"></p><p><img src="/images/Hadoop/3.png" title="Optional title"></p><p>SSH server安装完成后，使用ssh localhost即可登录本机。但这样登陆需要每次输入密码。再将SSH配置成无密码登陆：</p><p><img src="/images/Hadoop/4.png" title="Optional title"></p><p>进行上图命令之前，需要先登陆一次本地SSH，exit后再操作。</p><h2 id="Java环境"><a href="#Java环境" class="headerlink" title="Java环境"></a>Java环境</h2><p>Hadoop需要Java环境。使用如下命令安装JDK：</p><p><img src="/images/Hadoop/5.png" title="Optional title"></p><p><img src="/images/Hadoop/6.png" title="Optional title"></p><p>JDK安装结束以后，需要配置JAVA_HOME环境变量：</p><p><img src="/images/Hadoop/7.png" title="Optional title"></p><p>更新环境变量之后，检验Java环境：</p><p><img src="/images/Hadoop/8.png" title="Optional title"></p><p>上图中信息证明JDK已成功安装。</p><h2 id="Hadoop安装及非分布式实例测试"><a href="#Hadoop安装及非分布式实例测试" class="headerlink" title="Hadoop安装及非分布式实例测试"></a>Hadoop安装及非分布式实例测试</h2><p>在官网中选择需要的版本进行Hadoop下载，为压缩文件格式：</p><p><img src="/images/Hadoop/9.png" title="Optional title"></p><p><img src="/images/Hadoop/10.png" title="Optional title"></p><p>Hadoop解压后即可使用。检查Hadoop环境：</p><p><img src="/images/Hadoop/11.png" title="Optional title"></p><p>上图中成功输出了Hadoop版本信息，说明Hadoop安装成功。</p><p>Hadoop附带了丰富的例子，见命令<code>./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar</code>。</p><p>选择运行<code>grep</code>例子来测试Hadoop非分布式，步骤如下：</p><ul><li><p>给出input目录；</p></li><li><p>复制hadoop配置文件至input目录，用作grep例子的输入；</p></li><li><p>将input中的所有文件作为输入，筛选当中符合正则表达式 dfs[a-z.]+ 的单词并统计出现的次数，最后输出结果到output文件夹中；</p></li></ul><p><img src="/images/Hadoop/12.png" title="Optional title"></p><ul><li>执行成功后，查看结果：</li></ul><p><img src="/images/Hadoop/13.png" title="Optional title"></p><p>可见，执行成功后，输出的结果是符合正则的单词dfsadmin，该词出现了1次。</p><h2 id="伪分布式配置及实例测试"><a href="#伪分布式配置及实例测试" class="headerlink" title="伪分布式配置及实例测试"></a>伪分布式配置及实例测试</h2><p>Hadoop的配置文件是xml格式，每个配置以声明property的name和value的方式来实现，位于<code>/usr/local/hadoop/etc/hadoop/</code>中。</p><p>伪分布式需要修改2个配置文件<code>core-site.xml</code>和<code>hdfs-site.xml</code>：</p><p><img src="/images/Hadoop/14.png" title="Optional title"></p><p><img src="/images/Hadoop/15.png" title="Optional title"></p><p>修改完配置文件之后，进行NameNode格式化：</p><p><img src="/images/Hadoop/16.png" title="Optional title"></p><p><img src="/images/Hadoop/17.png" title="Optional title"></p><p><code>successfully formatted</code>、<code>Exitting with status 0</code>证明NameNode格式化操作成功。</p><p>然后开启NameNode和DataNode守护进程。可以使用<code>jps</code>命令和Web界面<code>http://localhost:50070</code>检查是否成功：</p><p><img src="/images/Hadoop/18.png" title="Optional title"></p><p>按照如下步骤进行Hadoop伪分布式实例测试：</p><ul><li><p>单机模式的grep例子读取的是本地数据，伪分布式读取的则是HDFS上的数据，使用HDFS需要在HDFS中创建用户目录；</p></li><li><p>给出input目录；</p></li><li><p>将./etc/hadoop中的xml文件作为输入文件复制到分布式文件系统中，即将/usr/local/hadoop/etc/hadoop复制到分布式文件系统中的/user/hadoop/input中；</p></li><li><p>同样将input中的所有文件作为输入，筛选当中符合正则表达式dfs[a-z.]+的单词并统计出现的次数，最后输出结果到output文件夹中（伪分布式运行 MapReduce 作业的方式跟单机模式相同，区别在于伪分布式读取的是HDFS中的文件）；</p></li></ul><p><img src="/images/Hadoop/19.png" title="Optional title"></p><ul><li>查看位于HDFS中的输出结果：</li></ul><p><img src="/images/Hadoop/20.png" title="Optional title"></p><p>输出结果有4条内容。因为修改了配置文件，所以这里的运行结果与本地模式下的运行结果不同。</p><hr><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><blockquote><p>Hadoop安装教程_单机/伪分布式配置_Hadoop2.6.0(2.7.1)/Ubuntu14.04(16.04)<br><a href="http://dblab.xmu.edu.cn/blog/install-hadoop/">http://dblab.xmu.edu.cn/blog/install-hadoop/</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;Hadoop是一个由Apac</summary>
      
    
    
    
    
    <category term="Hadoop" scheme="http://example.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>社会媒体计算：知乎问答信息挖掘</title>
    <link href="http://example.com/2021/04/14/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97%EF%BC%9A%E7%9F%A5%E4%B9%8E%E9%97%AE%E7%AD%94%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98/"/>
    <id>http://example.com/2021/04/14/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97%EF%BC%9A%E7%9F%A5%E4%B9%8E%E9%97%AE%E7%AD%94%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98/</id>
    <published>2021-04-14T07:33:25.000Z</published>
    <updated>2021-04-21T01:28:30.299Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>刚好在摸鱼的时候比较喜欢刷知乎，又碰到了这个课程，所以就尝试了做这个内容。以前从来没有接触过文本分析和爬虫这类的技术，就当学新技术了。以及这些在去年12月就写完了，现在闲了才想起搬到这里来。</p><p>依稀记得那是为数不多的有心思敲代码的时间，会珍惜的。</p><hr><h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><h2 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h2><p>从互联网上采集数据，如从 Tewitter、Facebook、新浪微博、B站等采集数据，包括用户基本信息、互相浏览、互相关注等信息，以及对应某一段时间发布的文本内容信息。</p><p>对上述数据进行预处理，要求用程序进行预处理。</p><p>对上述处理数据进行社团挖掘，包括基本统计信息、社团发现等，对预处理的文本进行情感分析、主题挖掘、分类或聚类等研究。要求用到社会网络计算、文本挖掘等技术。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>知乎是一个网络问答社区，用户彼此分享知识、经验和见解，围绕着某一感兴趣的话题进行相关的讨论，同时也可以关注兴趣一致的人。</p><p>知乎的基本模式是：用户提问，每一个问题都会有一个独有的id。其他对此问题感兴趣的用户在此问题下进行回答，每一个此问题下的回答也会有一个id。用户可对回答进行点赞、点踩、评论等操作。</p><p>选择从知乎采集要用到的数据。所选取的问题是：你打算在 12 月 31 日发什么朋友圈跨年文案？</p><p>链接：<a href="https://www.zhihu.com/question/360940960">https://www.zhihu.com/question/360940960</a></p><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/1.png" title="Optional title"></p><h2 id="编程环境"><a href="#编程环境" class="headerlink" title="编程环境"></a>编程环境</h2><p>Mac OS 10.14.6 + Jupyter notebook + Python 3.6.5</p><h1 id="知乎数据采集"><a href="#知乎数据采集" class="headerlink" title="知乎数据采集"></a>知乎数据采集</h1><h2 id="数据爬虫"><a href="#数据爬虫" class="headerlink" title="数据爬虫"></a>数据爬虫</h2><p>这部分爬虫功能的主要作用就是从知乎的网页上拿数据。</p><p><strong>这部分的原理参考最下面唯一的一条引用，包括从网络请求找到数据存放位置、分析请求头的格式和构造代码中所需要的新的请求头。</strong></p><p>def crawler(question_num)是该部分功能的组织函数，其他4个函数的具体功能见注释。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span>(<span class="params">url</span>):</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：访问 url 的网页，获取网页内容并返回</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         url ：目标网页的 url</span></span><br><span class="line"><span class="comment">#     返回：目标网页的 html 内容</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 设置多个用户代理</span></span><br><span class="line">    agent=[<span class="string">&#x27;Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36&#x27;</span>,  </span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.80 Safari/537.36&#x27;</span>,  </span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:30.0) Gecko/20100101 Firefox/30.0&#x27;</span>,  </span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/537.75.14&#x27;</span>,  </span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Win64; x64; Trident/6.0)&#x27;</span> ]</span><br><span class="line">    <span class="comment"># 每次随机抽取一个，防止被封ip</span></span><br><span class="line">    randdom_agent=random.choice(agent)</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;accept&#x27;</span>: <span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;user-agent&#x27;</span>: randdom_agent</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 爬取数据</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url, headers=headers)</span><br><span class="line">        r.encoding = <span class="string">&#x27;UTF8&#x27;</span></span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span> requests.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">        print(<span class="string">&quot;HTTPError&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> requests.RequestException <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">&quot;Unknown Error !&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_data</span>(<span class="params">html</span>):</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：提取 html 页面信息中的关键信息，并整合一个数组并返回</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         html：根据 url 获取到的网页内容</span></span><br><span class="line"><span class="comment">#     返回：存储有 html 中提取出的关键信息的数组</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    json_data = json.loads(html)[<span class="string">&#x27;data&#x27;</span>]</span><br><span class="line">    comments = []</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> json_data:</span><br><span class="line">            comment = []</span><br><span class="line">            comment.append(item[<span class="string">&#x27;author&#x27;</span>][<span class="string">&#x27;name&#x27;</span>]) <span class="comment"># 姓名</span></span><br><span class="line">            comment.append(item[<span class="string">&#x27;author&#x27;</span>][<span class="string">&#x27;gender&#x27;</span>]) <span class="comment"># 性别</span></span><br><span class="line">            comment.append(item[<span class="string">&#x27;voteup_count&#x27;</span>]) <span class="comment"># 点赞数</span></span><br><span class="line">            comment.append(item[<span class="string">&#x27;comment_count&#x27;</span>])  <span class="comment"># 评论数</span></span><br><span class="line">            comment.append(item[<span class="string">&#x27;url&#x27;</span>])  <span class="comment"># 回答链接</span></span><br><span class="line">            comment.append(item[<span class="string">&#x27;created_time&#x27;</span>])  <span class="comment"># 回答时间</span></span><br><span class="line">            comment.append(item[<span class="string">&#x27;content&#x27;</span>]) <span class="comment"># 回答时间</span></span><br><span class="line">            comments.append(comment)   </span><br><span class="line">        <span class="keyword">return</span> comments    </span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(comment)</span><br><span class="line">        print(e)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_file</span>(<span class="params">question_num</span>):</span></span><br><span class="line"><span class="comment">#      &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：根据知乎的问题id构造文件路径和文件名，csv、png都会用到</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         question_num：知乎提问的问题id</span></span><br><span class="line"><span class="comment">#     返回：csv文件路径、csv文件名</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    current_path = os.getcwd()</span><br><span class="line">    file_name=str(question_num)+<span class="string">&#x27;_&#x27;</span>+str(datetime.datetime.now())+<span class="string">&#x27;.csv&#x27;</span></span><br><span class="line">    path = current_path+<span class="string">&#x27;/&#x27;</span>+file_name</span><br><span class="line">    print(<span class="string">&quot;文件：&quot;</span>+path)</span><br><span class="line">    <span class="keyword">return</span> path,file_name</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_data</span>(<span class="params">comments,header,path</span>):</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：将comments中的信息输出到文件/数据库中</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         comments：将要保存的数据  </span></span><br><span class="line"><span class="comment">#     返回：无</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    dataframe = pd.DataFrame(comments)</span><br><span class="line">    <span class="keyword">if</span> header:</span><br><span class="line">        dataframe.to_csv(path, mode=<span class="string">&#x27;a&#x27;</span>, index=<span class="literal">False</span>, sep=<span class="string">&#x27;,&#x27;</span>, header=[<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;gender&#x27;</span>,<span class="string">&#x27;voteup&#x27;</span>,<span class="string">&#x27;cmt_count&#x27;</span>,<span class="string">&#x27;ans_url&#x27;</span>,<span class="string">&#x27;ans_time&#x27;</span>,<span class="string">&#x27;ans_content&#x27;</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dataframe.to_csv(path, mode=<span class="string">&#x27;a&#x27;</span>, index=<span class="literal">False</span>, sep=<span class="string">&#x27;,&#x27;</span>, header=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crawler</span>(<span class="params">question_num</span>):</span></span><br><span class="line"><span class="comment">#      &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：爬虫功能的组织函数，爬取数据并存储到csv文件中</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         question_num：知乎提问的问题id</span></span><br><span class="line"><span class="comment">#     返回：文件名</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    url_start = <span class="string">&#x27;https://www.zhihu.com/api/v4/questions/&#x27;</span>+str(question_num)+<span class="string">&#x27;/answers?include=data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cis_labeled%3Bdata%5B%2A%5D.mark_infos%5B%2A%5D.url%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%2A%5D.topics&amp;limit=5&amp;offset=&#x27;</span></span><br><span class="line">    url_end=<span class="string">&#x27;&amp;platform=desktop&amp;sort_by=default&#x27;</span></span><br><span class="line">    html=get_data(url_start+str(<span class="number">5</span>)+url_end)</span><br><span class="line">    totals=json.loads(html)[<span class="string">&#x27;paging&#x27;</span>][<span class="string">&#x27;totals&#x27;</span>]</span><br><span class="line">    path,file_name=get_file(question_num)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">&quot;总回答数：&quot;</span>+str(totals))</span><br><span class="line">    page = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span>(page &lt; totals):</span><br><span class="line">        url = url_start+str(page) +url_end</span><br><span class="line"> </span><br><span class="line">        html = get_data(url)</span><br><span class="line">        comments = parse_data(html)</span><br><span class="line">        <span class="keyword">if</span> page==<span class="number">0</span>:</span><br><span class="line">            save_data(comments,<span class="literal">True</span>,path)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            save_data(comments,<span class="literal">False</span>,path)</span><br><span class="line">        page += <span class="number">5</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> file_name</span><br></pre></td></tr></table></figure><h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><p>def save_data(comments,header,path)将爬到的数据存到了.CSV中，数据读取函数则将CSV中的内容读到内存里供。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_csv</span>(<span class="params">file_name</span>):</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：读取爬虫函数得到的csv文件</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         file_name：文件名  </span></span><br><span class="line"><span class="comment">#     返回：csv文件的list</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> open(file_name, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        reader = csv.reader(f)</span><br><span class="line">        result = list(reader)</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>爬取下来的数据需要预处理。<strong>主要是因为所关注的“ans_content”内，除了中文之外，还有一些奇奇怪怪的东西</strong>：</p><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/2.png" title="Optional title"></p><p>可以看到，<strong>有emoji、有非中文的特殊字符、有h5的标签</strong>（由&lt;&gt;括起的部分，主要由于回答中的图片的网页链接需要用这种方式在网页源码内引用）。<strong>使用正则表达式去掉这些对中文文本分析没有用的内容。</strong></p><p>其他的一些处理还包括<strong>秒级时间戳的转换和知乎用户系统中的“匿名用户”进行编号等</strong>。这些无关紧要，只是为了看着舒服。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">date_pre_treatment</span>(<span class="params">csv_data</span>):</span></span><br><span class="line"><span class="comment">#      &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：数据预处理</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         csv_data：已经读取好的list  </span></span><br><span class="line"><span class="comment">#     返回：经过了预处理的list</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    niming_num=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(csv_data[<span class="number">0</span>])):</span><br><span class="line">        <span class="keyword">if</span> csv_data[<span class="number">0</span>][j]==<span class="string">&#x27;name&#x27;</span>:</span><br><span class="line">            <span class="keyword">for</span> i1 <span class="keyword">in</span> range(len(csv_data)):</span><br><span class="line">                <span class="comment"># 将知乎用户、匿名用户的名称统一，加上编号做区分</span></span><br><span class="line">                <span class="keyword">if</span> csv_data[i1][j]==<span class="string">&#x27;知乎用户&#x27;</span> <span class="keyword">or</span> csv_data[i1][j]==<span class="string">&#x27;匿名用户&#x27;</span>:</span><br><span class="line">                    csv_data[i1][j]=<span class="string">&#x27;匿名用户&#x27;</span>+str(++niming_num)</span><br><span class="line">        <span class="keyword">if</span> csv_data[<span class="number">0</span>][j]==<span class="string">&#x27;ans_time&#x27;</span>:</span><br><span class="line">            <span class="keyword">for</span> i2 <span class="keyword">in</span> range(len(csv_data)<span class="number">-1</span>):</span><br><span class="line">                ltime = time.localtime(int(csv_data[i2+<span class="number">1</span>][j]))</span><br><span class="line">                <span class="comment"># 将秒级时间戳转化为&quot;年月日时分秒&quot;格式</span></span><br><span class="line">                csv_data[i2+<span class="number">1</span>][j]=time.strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>,ltime)</span><br><span class="line">        <span class="keyword">if</span> csv_data[<span class="number">0</span>][j]==<span class="string">&#x27;ans_content&#x27;</span>:</span><br><span class="line">            <span class="keyword">for</span> i3 <span class="keyword">in</span> range(len(csv_data)):</span><br><span class="line">                <span class="comment"># 去除文本信息中的h5标签</span></span><br><span class="line">                csv_data[i3][j] = re.sub(<span class="string">u&quot;\\&lt;.*?\\&gt;&quot;</span>, <span class="string">&quot; &quot;</span>, csv_data[i3][j])</span><br><span class="line">                <span class="comment"># 去除文本信息除中文、英文、ascll字符、常用标点以外的所有内容</span></span><br><span class="line">                csv_data[i3][j] = re.sub(<span class="string">&quot;[^\u4e00-\u9fa5 ^a-z ^A-Z ^0-9 ^~!@#$%&amp;*()_+-=:;,.^～！，。？、《》]&quot;</span>,<span class="string">&#x27;&#x27;</span>, csv_data[i3][j])</span><br><span class="line">        <span class="keyword">if</span> csv_data[<span class="number">0</span>][j]==<span class="string">&#x27;voteup&#x27;</span>:</span><br><span class="line">            <span class="keyword">for</span> i4 <span class="keyword">in</span> range(len(csv_data)<span class="number">-1</span>):</span><br><span class="line">                <span class="comment"># 将投票数转化为int（读取的时候会存储为str）</span></span><br><span class="line">                csv_data[i4+<span class="number">1</span>][j] = int(float(csv_data[i4+<span class="number">1</span>][j]))</span><br><span class="line">    <span class="keyword">return</span> csv_data</span><br></pre></td></tr></table></figure><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/3.png" title="Optional title"></p><h1 id="词云"><a href="#词云" class="headerlink" title="词云"></a>词云</h1><p>使用wordcloud包进行所有回答文本的词云的绘制，并存储为png图片文件。用到的功能为WordCloud，参数和说明见代码注释。</p><p><strong>词云的图如果想做的有意义且好看，最主要的参数是stopwords、max_words。</strong></p><p>其中，stopwords指的是词云里什么词不能出现，默认为空。毕竟不管是中文还是英文，文本里都有许多没有意义的词，比如语气词“嗯”、“啊”、“呢”这种类似的。所以如果不加限制的话，词云里最大的那个词一定是没什么意义的词（知乎作为一个网络社区，大家的回答肯定是偏向口头语的，那么无意义的词就会多起来，毕竟没人在知乎上写论文是吧）。这里的stopwords是通过txt导入的。stopwords网上能找到许多，我是在GitHub上找到了一份中文停用词，直接拿过来用了。后期又在里面填了一些自己不想看见的词。</p><p>max_words就是图片上最多出现多少个词，个人感觉还是多一些好看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud</span><br><span class="line">now_time= datetime.datetime.now()</span><br><span class="line">print(<span class="string">&quot;2.词云&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置数据，将所有文本放到一个list里方便处理</span></span><br><span class="line">text = <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(csv_data)<span class="number">-1</span>):</span><br><span class="line">    text+=csv_data[i+<span class="number">1</span>][<span class="number">6</span>]+<span class="string">&#x27; &#x27;</span></span><br><span class="line"><span class="comment"># 结巴中文分词，生成字符串，默认精确模式，如果不通过分词，无法直接生成正确的中文词云</span></span><br><span class="line">cut_text = jieba.cut(text)</span><br><span class="line"><span class="comment"># 必须给个符号分隔开分词结果来形成字符串,否则不能绘制词云</span></span><br><span class="line">result = <span class="string">&quot; &quot;</span>.join(cut_text)</span><br><span class="line"><span class="comment"># 从文件中获取停用词</span></span><br><span class="line">stopwords=get_stopwords()</span><br><span class="line"><span class="comment"># 词云设置</span></span><br><span class="line">wc = WordCloud(     </span><br><span class="line">        font_path=<span class="string">&quot;仿宋_GB2312.ttf&quot;</span>,<span class="comment"># 设置字体（不指定就会出现乱码）</span></span><br><span class="line">        background_color=<span class="string">&#x27;white&#x27;</span>,<span class="comment"># 设置背景色</span></span><br><span class="line">        width=<span class="number">1500</span>, height=<span class="number">900</span>,<span class="comment"># 设置背景宽、高</span></span><br><span class="line">        max_font_size=<span class="number">400</span>, min_font_size=<span class="number">20</span>,<span class="comment"># 设置字体大小上下限</span></span><br><span class="line">        stopwords = stopwords,<span class="comment"># 停用词</span></span><br><span class="line">        max_words=<span class="number">150</span> <span class="comment"># 图片中显示词的最大数量</span></span><br><span class="line">        )</span><br><span class="line"><span class="comment"># 产生词云</span></span><br><span class="line">wc.generate(result)</span><br><span class="line"><span class="comment"># 保存图片</span></span><br><span class="line">pic_name=re.sub(<span class="string">&#x27;.csv&#x27;</span>,<span class="string">&#x27;&#x27;</span>,file_name)+<span class="string">&#x27;_wordcloud.png&#x27;</span></span><br><span class="line">wc.to_file(pic_name) </span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">10</span>))</span><br><span class="line">plt.imshow(wc)</span><br><span class="line">plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Runtime:%d s&#x27;</span>%(datetime.datetime.now()-now_time).seconds)</span><br></pre></td></tr></table></figure><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/4.png" title="Optional title"></p><h1 id="keywords"><a href="#keywords" class="headerlink" title="keywords"></a>keywords</h1><p>使用jieba包进行关键词提取。</p><p>Jieba提供了两种关键词的提取算法，分别是：</p><ul><li><p>基于TF-IDF（term frequency–inverse document frequency）算法的关键词抽取。函数参数如下：sentence：待提取的文本；topK：返回topK个 TF/IDF 权重最大的关键词；withWeight：是否一并返回关键词权重值，默认值为False；allowPOS：仅包括指定词性的词，默认值为空，即不筛选。</p></li><li><p>基于TextRank算法的关键词抽取。函数接口同TF-IDF相同。不同的是allowPOS默认指定了一些词性的词。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba.analyse</span><br><span class="line">now_time= datetime.datetime.now()</span><br><span class="line">print(<span class="string">&quot;3.keywords&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;3.1. TF-IDF&quot;</span>)</span><br><span class="line">keywords1=jieba.analyse.extract_tags(text, topK=<span class="number">20</span>, withWeight=<span class="literal">True</span>, allowPOS=(<span class="string">&#x27;ns&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;vn&#x27;</span>, <span class="string">&#x27;v&#x27;</span>))</span><br><span class="line">print(keywords1)</span><br><span class="line">print(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;3.2. textrank&quot;</span>)</span><br><span class="line">keywords2=jieba.analyse.textrank(text, topK=<span class="number">20</span>, withWeight=<span class="literal">True</span>, allowPOS=(<span class="string">&#x27;ns&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;vn&#x27;</span>, <span class="string">&#x27;v&#x27;</span>)) </span><br><span class="line">print(keywords2)</span><br><span class="line">print(<span class="string">&#x27;Runtime:%d s&#x27;</span>%(datetime.datetime.now()-now_time).seconds)</span><br></pre></td></tr></table></figure><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/5.png" title="Optional title"></p><h1 id="情感分析"><a href="#情感分析" class="headerlink" title="情感分析"></a>情感分析</h1><p>使用snownlp进行情感分析。</p><p>snownlp是一个python类库，可以方便的处理中文文本内容，是受到了TextBlob的启发而写的，并且和TextBlob不同的是，这里没有用NLTK，所有的算法都是重新实现的，并且自带了一些训练好的字典。</p><blockquote><p>snownlp: <a href="https://github.com/isnowfy/snownlp">https://github.com/isnowfy/snownlp</a><br>TextBlob: <a href="https://github.com/sloria/TextBlob">https://github.com/sloria/TextBlob</a></p></blockquote><p><strong>snownlp给出了基于贝叶斯分类的情感分析函数SnowNLP。对于每一条文本，该函数会给出一个0-1之间的评分，越接近1代表积极情绪占比越高。</strong></p><p>使用SnowNLP对每一条回答文本进行情感分析，并以评分达到<strong>0.6、0.5认定为是积极文本</strong>，分别给出积极回答文本在所有文本中的比重。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> snownlp <span class="keyword">import</span> SnowNLP</span><br><span class="line">now_time= datetime.datetime.now()</span><br><span class="line">print(<span class="string">&quot;4.情感分析&quot;</span>)</span><br><span class="line">s=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(csv_data)<span class="number">-1</span>):</span><br><span class="line">    a=SnowNLP(csv_data[i+<span class="number">1</span>][<span class="number">6</span>])</span><br><span class="line">    s.append(round(a.sentiments,<span class="number">3</span>))</span><br><span class="line">s.sort(reverse=<span class="literal">True</span>)</span><br><span class="line">flag06=<span class="number">-1</span></span><br><span class="line">flag05=<span class="number">-1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(s)):</span><br><span class="line">    <span class="keyword">if</span> s[i]&lt;<span class="number">0.5</span> :</span><br><span class="line">        flag05=i</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(s)):</span><br><span class="line">    <span class="keyword">if</span> s[i]&lt;<span class="number">0.6</span> :</span><br><span class="line">        flag06=i</span><br><span class="line">        <span class="keyword">break</span>    </span><br><span class="line">print(<span class="string">&quot;积极（60%）百分比：&quot;</span>,round(flag06/(len(s)),<span class="number">2</span>)) </span><br><span class="line">print(<span class="string">&quot;积极（50%）百分比：&quot;</span>,round(flag05/(len(s)),<span class="number">2</span>))         </span><br><span class="line">print(<span class="string">&#x27;Runtime:%d s&#x27;</span>%(datetime.datetime.now()-now_time).seconds)</span><br></pre></td></tr></table></figure><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/6.png" title="Optional title"></p><p><strong>两个评分阈值下的大部分的回答都被认为是积极的，这也符合这个问题的背景：新年。</strong></p><h1 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h1><p>对点赞数最多的100条数据进行聚类。</p><p>进行涉及文本的向量化表示。sklearn提供了传统的词袋模型。使用sklearn中的TfidfVectorizer计算tf-idf矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line">print(<span class="string">&#x27;5. 文本聚类&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;5.1. 准备点赞数最多的100条数据&#x27;</span>)</span><br><span class="line">user_id=[]</span><br><span class="line">content=[]</span><br><span class="line">csv_data_sorted=csv_data</span><br><span class="line"><span class="keyword">del</span>(csv_data_sorted[<span class="number">0</span>])</span><br><span class="line">csv_data_sorted=sorted(csv_data,key=itemgetter(<span class="number">2</span>))</span><br><span class="line"><span class="comment"># print(csv_data[:10])</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    user_id.append(csv_data_sorted[i][<span class="number">0</span>])</span><br><span class="line">    content.append(csv_data_sorted[i][<span class="number">6</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line">now_time= datetime.datetime.now()</span><br><span class="line">print(<span class="string">&#x27;5.2. tfidf matrix&#x27;</span>)</span><br><span class="line"><span class="comment">#max_df: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.</span></span><br><span class="line"><span class="comment">#min_df: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.</span></span><br><span class="line">tfidf_vectorizer = TfidfVectorizer(max_df=<span class="number">0.9</span>, max_features=<span class="number">200000</span>,min_df=<span class="number">0.1</span>, stop_words=<span class="string">&#x27;english&#x27;</span>,use_idf=<span class="literal">True</span>, tokenizer=segment)</span><br><span class="line">tfidf_matrix = tfidf_vectorizer.fit_transform(content) <span class="comment">#fit the vectorizer to synopses</span></span><br><span class="line">print(tfidf_matrix.shape)</span><br><span class="line">print(<span class="string">&#x27;Runtime:%d s&#x27;</span>%(datetime.datetime.now()-now_time).seconds)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scipy.cluster.hierarchy <span class="keyword">import</span> ward, dendrogram, linkage</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line">now_time= datetime.datetime.now()</span><br><span class="line">print(<span class="string">&#x27;5.3. linkage matrix&#x27;</span>)</span><br><span class="line">dist = <span class="number">1</span> - cosine_similarity(tfidf_matrix)</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>]=[<span class="string">&#x27;Microsoft YaHei&#x27;</span>] <span class="comment">#用来正常显示中文标签</span></span><br><span class="line">linkage_matrix = linkage(dist, method=<span class="string">&#x27;ward&#x27;</span>, metric=<span class="string">&#x27;euclidean&#x27;</span>, optimal_ordering=<span class="literal">False</span>)</span><br><span class="line">print(<span class="string">&#x27;Runtime:%d s&#x27;</span>%(datetime.datetime.now()-now_time).seconds)</span><br><span class="line">print(linkage_matrix)</span><br></pre></td></tr></table></figure><p>由于选取了100条回答，tf-idf矩阵的第一维为100。</p><p>然后根据tf-idf矩阵进行层次聚类，给出linkage矩阵。使用函数：scipy.cluster.hierarchy.linkage(y, method=’single’, metric=’euclidean’, optimal_ordering=False)。其中，计算新形成的聚类簇u和v之间距离的方法是用的是single，即最近邻点算法。</p><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/7.png" title="Optional title"></p><p>对上述矩阵进行可视化，并将结果保存为png文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">now_time= datetime.datetime.now()</span><br><span class="line">print(<span class="string">&#x27;5.4. linkage matrix可视化&#x27;</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">40</span>, <span class="number">15</span>))</span><br><span class="line">plt.title(<span class="string">&#x27;层次聚类树状图&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;知乎用户名称&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;距离（越低表示文本越类似）&#x27;</span>)</span><br><span class="line">dendrogram(</span><br><span class="line">    linkage_matrix,</span><br><span class="line">    labels=user_id, </span><br><span class="line">    leaf_rotation=<span class="number">-70</span>,  <span class="comment"># rotates the x axis labels</span></span><br><span class="line">    leaf_font_size=<span class="number">12</span>  <span class="comment"># font size for the x axis labels</span></span><br><span class="line">)</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>]=[<span class="string">&#x27;SimHei&#x27;</span>] <span class="comment">#用来正常显示中文标签</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span> <span class="comment">#用来正常显示负号</span></span><br><span class="line">plt.savefig(re.sub(<span class="string">&#x27;.csv&#x27;</span>,<span class="string">&#x27;&#x27;</span>,file_name)+<span class="string">&#x27;_linkage.png&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;Runtime:%d s&#x27;</span>%(datetime.datetime.now()-now_time).seconds)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/8.png" title="Optional title"></p><p>其中，横坐标为知乎用户名，每种的线连起来的用户名代表这些用户的回答可被分为相似的一类。</p><p>（看到这里明白为什么只选100个了吧，因为选多了图就画不下了）</p><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>在代码中更改需要爬取的知乎问题的问题id，就可以实现对任意知乎问题下的所有回答内容的上述操作。</p><p>代码每次运行都会按照以“问题id+时间”为文件名进行各项文件的保存，不会覆盖之前运行所保留的文件。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><blockquote><p>Python网络爬虫实战：爬取知乎话题下 18934 条回答数据-csdn<br><a href="https://blog.csdn.net/wenxuhonghe/article/details/86515558">https://blog.csdn.net/wenxuhonghe/article/details/86515558</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;刚好在摸鱼的时候比较喜欢刷知</summary>
      
    
    
    
    
    <category term="社会媒体计算" scheme="http://example.com/tags/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>想分享给别人看的一些影像</title>
    <link href="http://example.com/2021/04/14/%E6%83%B3%E5%88%86%E4%BA%AB%E7%BB%99%E5%88%AB%E4%BA%BA%E7%9C%8B%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BD%B1%E5%83%8F/"/>
    <id>http://example.com/2021/04/14/%E6%83%B3%E5%88%86%E4%BA%AB%E7%BB%99%E5%88%AB%E4%BA%BA%E7%9C%8B%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BD%B1%E5%83%8F/</id>
    <published>2021-04-14T07:23:04.000Z</published>
    <updated>2021-11-15T10:33:35.208Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创内容。如果喜欢，告知后自取即可。</p></blockquote><hr><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>仪式感是很重要的东西。喜欢把生活中的内容留个纪念，这里就用来留一些想给别人看的图片。</p><p>不定期更新。</p><hr><blockquote><p>2021.4.14更新。</p></blockquote><p><img src="/images/%E7%9B%B8%E5%86%8C/1.jpg" title="打羽毛球"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/2.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/3.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/4.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/5.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/6.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/7.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/8.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/9.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/10.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/11.jpg" title="打羽毛球"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/12.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/13.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/14.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/15.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/16.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/17.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/18.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/19.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/20.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/21.jpg" title="打羽毛球"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/22.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/23.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/24.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/25.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/26.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/27.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/28.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/29.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/30.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/31.jpg" title="打羽毛球"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/32.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/33.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/34.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/35.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/36.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/37.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/38.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/39.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/40.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/41.jpg" title="打羽毛球"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/42.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/43.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/44.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/45.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/46.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/47.jpg" title="Optional title"></p><!-- ![](/images/相册/48.jpg "Optional title") --><p><img src="/images/%E7%9B%B8%E5%86%8C/49.jpg" title="Optional title"></p><p><img src="/images/%E7%9B%B8%E5%86%8C/50.JPG" title="Optional title"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创内容。如果喜欢，告知后自取即可。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;仪式感是很重要的东西。喜</summary>
      
    
    
    
    
    <category term="闲谈" scheme="http://example.com/tags/%E9%97%B2%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：Zoom to learn, learn to zoom</title>
    <link href="http://example.com/2021/04/12/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AZoom-to-learn-learn-to-zoom/"/>
    <id>http://example.com/2021/04/12/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AZoom-to-learn-learn-to-zoom/</id>
    <published>2021-04-12T02:29:43.000Z</published>
    <updated>2021-04-14T07:23:41.397Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>变焦学习，学习变焦</p></blockquote><blockquote><p>CVPR 2019</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>变焦功能是当今手机和相机的必备功能。</p><p>光学变焦是图像变焦的最佳选择，可以保持较高的图像质量。但变焦镜头价格昂贵且其物理组件比较笨重。数码变焦方法是一种降低成本的选择，但数码变焦只是简单地向上采样相机传感器输入的裁剪区域，产生模糊的输出。</p><p>文章提出，常规的SISR方法有以下两个限制：</p><ul><li><p>现有的大多数方法使用的是合成方法来逼近真实变焦，即其中输入图像是高分辨率图像的下采样版本。这种方法间接降低了输入中的噪声水平，但实际上，由于在曝光时间内进入光圈的光子更少，遥远物体的区域往往包含更多的噪声。</p></li><li><p>其次，现有的大多数方法都是从8位RGB图像开始的，该图像由相机图像信号处理器（ISP）处理，ISP将高位原始传感器数据中的高频信号用于其他目标（例如降噪）。</p></li></ul><blockquote><p>第二点的解读：本文训练的数据均是Raw Data，这是专业单反拍摄的格式，而RGB图片是Raw Data经过图像处理器（Image SIgnal Processer, ISP）制作的，在某种程度上来说，RGB图片也是有损的。</p></blockquote><p>基于上述限制，文章做了以下内容：</p><ul><li><p>使用真实的高位传感器数据进行计算变焦的实用性，而不是处理8位RGB图像或合成传感器模型。</p></li><li><p>新的数据集SR-RAW，它是第一个具有光学ground truth的超分辨率原始数据集。SR-RAW是用变焦镜头拍摄的。对于焦距较短的图像，长焦距图像作为光学ground truth。</p></li><li><p>提出了一种新的上下文双边损失（CoBi）处理稍微失调的图像对。</p></li></ul><h1 id="SR-RAW"><a href="#SR-RAW" class="headerlink" title="SR-RAW"></a>SR-RAW</h1><h2 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h2><p>使用24-240毫米变焦镜头来收集不同光学变焦水平的原始图像对。</p><p>采集过程中，在每个场景的7个光学变焦设置下采集了7幅图像。来自7幅图像序列的每一对图像形成一个数据对。总共500个室内外场景，ISO从100到400。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/1.png" title="Optional title"></p><p><strong>在训练模型中，以短焦距原始传感器图像作为输入，以长焦距RGB图像作为超分辨率的基础。</strong></p><blockquote><p>例如，使用70mm焦距拍摄的RGB图像作为使用35mm焦距拍摄的原始传感器数据的2X缩放地面真相。</p></blockquote><p>相机有特殊设置，来自原文：</p><ul><li><p>景深（DOF）随着焦距的变化而变化，调整孔径大小使每个焦距的DOF相同是不现实的。选择一个小的光圈尺寸（至少f/20）来最小化DOF差异（在图2的B2中仍然可以看到），使用一个三脚架来捕捉长时间曝光的室内场景。</p></li><li><p>其次，使用相同的曝光时间的所有图像在一个序列，使噪音水平不受焦距变化的影响。但是仍然观察到由于快门和物理光瞳是机械的并且涉及到动作变化而引起的明显的光照变化。这种颜色的变化是避免使用像素对像素的损失进行训练的另一个动机。</p></li><li><p>第三，虽然透视不随焦距的变化而变化，但当镜头放大或缩小时，在投影中心存在微小的变化（镜头的长度），在不同深度的物体之间产生明显的透视变化（图2 B1）。使用的Sony FE 24-240mm镜头，需要与被摄对象至少56.4米的距离，才能在相距5米的物体之间产生小于1像素的透视位移。因此，避免捕获非常接近的对象，但允许在数据集中进行这样的透视图转换。</p></li></ul><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>对于任意一对训练图像，用RGB-L表示低分，用RAW-L表示高分，也就是相机的传感器的数据，GT。</p><p>高分GT，使用RGB-H和RAW-H。首先匹配RAW-H和RGB-H之间的视图(FOV)。然后计算RGB-L和RGB-H之间的对齐（手动缩放相机以调整焦距所引起的相机轻微移动）。</p><p>使用一个欧几里德运动模型，通过增强相关系数最小化来实现图像的旋转和平移。</p><p>训练时，将匹配FOV的RAW-L作为输入，它的GT是RGB-H，与RAW-L对齐并具有相同的FOV。如果光学变焦与目标变焦比不完全匹配，则对图像应用比例偏移。</p><blockquote><p>例如，如果使用（35mm，150mm）训练一个4X变焦模型，那么目标图像的偏移量为1.07。</p></blockquote><h2 id="非对齐分析"><a href="#非对齐分析" class="headerlink" title="非对齐分析"></a>非对齐分析</h2><p>预处理步骤很难消除偏差。由于捕捉的数据焦距不同，视角的变化会导致视角的不对齐。此外，当对不同分辨率的图像进行对齐时，高分辨率图像中的锐边不能与低分辨率图像中的模糊边精确对齐(图2 B3)。</p><blockquote><p>SR-RAW中描述的失调通常会导致800万像素图像对中的40-80像素偏移。</p></blockquote><blockquote><p>该数据集中的HR和LR是不是对齐的，这也是后面给出的算法所解决的问题之一，非常重要。</p></blockquote><h1 id="Contextual-Bilateral-Loss"><a href="#Contextual-Bilateral-Loss" class="headerlink" title="Contextual Bilateral Loss"></a>Contextual Bilateral Loss</h1><h2 id="Contextual-Loss"><a href="#Contextual-Loss" class="headerlink" title="Contextual Loss"></a>Contextual Loss</h2><p>CoBi Loss来自Contextual Loss。Contextual Loss的原文中的叙述为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/2.png" title="Optional title"></p><blockquote><p>Contextual Loss来自《The Contextual Loss for Image Transformation with Non-Aligned Data》，ECCV 2018。<br><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Roey_Mechrez_The_Contextual_Loss_ECCV_2018_paper.pdf">https://openaccess.thecvf.com/content_ECCV_2018/papers/Roey_Mechrez_The_Contextual_Loss_ECCV_2018_paper.pdf</a><br><a href="https://www.github.com/roimehrez/contextualLoss">https://www.github.com/roimehrez/contextualLoss</a></p></blockquote><blockquote><p>Contextual Loss的原文中是跟Perceptual Loss进行对比的。<br>Perceptual Loss来自《Perceptual Losses for Real-Time Style Transfer and Super-Resolution》，ECCV 2016。<br>SRGAN的Loss就是从这篇文章来的。</p></blockquote><p>本篇的Contextual Loss的公式：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/3.png" title="Optional title"></p><h2 id="CoBi-Loss"><a href="#CoBi-Loss" class="headerlink" title="CoBi Loss"></a>CoBi Loss</h2><p>作者用这个Contextual Loss去训练模型发现会出现很多artifacts。作者认为这是由于CX损失函数中不准确的特征匹配造成的。<br>受到保边滤波器的启发，作者将空间区域也加入到损失函数中：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/4.png" title="Optional title"></p><p>又借鉴了Perceptual Loss，引入VGG loss。本文最终的Loss为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/5.png" title="Optional title"></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><h2 id="实验步骤"><a href="#实验步骤" class="headerlink" title="实验步骤"></a>实验步骤</h2><p>使用来自SR-RAW的图像来训练一个4X模型和一个8X模型。<strong>采用了一个16层的ResNet架构，然后是log_2( N + 1) 上卷积层，其中N是缩放因子。</strong></p><blockquote><p>文章所使用的模型没有图和其他说明，只有这一句文字叙述。</p></blockquote><p>将SR-RAW中的500个序列按8:1:1的比例分割为训练、验证和测试集。</p><p>对于4X变焦模型，每个序列有3对数据对用于训练。对于8X变焦模型，每个序列有1对数据。</p><p>每对包含一个全分辨率（800万像素）Bayer mosaic图像及其相应的全分辨率光学放大RGB图像。随机从一个全分辨率的Bayer mosaic中裁剪64个patch作为训练的输入。</p><blockquote><p><a href="https://petapixel.com/2017/03/03/x-trans-vs-bayer-sensors-fantastic-claims-test/">https://petapixel.com/2017/03/03/x-trans-vs-bayer-sensors-fantastic-claims-test/</a></p></blockquote><p>选择了几种具有代表性的超分辨方法进行比较：SRGAN、SRResnet、LapSRN、Johnson等人提出的Perceptual Loss的模型以及ESRGAN。</p><p>使用公共的预训练模型。首先采用原文献中的标准设置尝试在SR-RAW上微调模型，对比模型的下采样方式是双三次。但与未经微调的预训练模型相比，平均性能差异不大(SSIM &lt; 0.04, PSNR &lt; 0.05, lpip &lt; 0.025)，所以原文直接采用了没有微调的原模型。对于没有预先训练模型的方法，在SR-RAW上从零开始训练它们的模型。</p><h2 id="定量分析"><a href="#定量分析" class="headerlink" title="定量分析"></a>定量分析</h2><h3 id="不同模型使用SR-RAW进行训练"><a href="#不同模型使用SR-RAW进行训练" class="headerlink" title="不同模型使用SR-RAW进行训练"></a>不同模型使用SR-RAW进行训练</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/6.png" title="Optional title"></p><p>结果很明显，现有的超分模型在这个数据集上的表现比较差。</p><h3 id="不同训练策略"><a href="#不同训练策略" class="headerlink" title="不同训练策略"></a>不同训练策略</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/7.png" title="Optional title"></p><p>依旧证明了本文提出的模型和数据是比较契合的。</p><blockquote><p>Ours-png：为了进行比较，使用经过8位处理的RGB图像来训练模型的副本(our-png)，以评估拥有真实原始传感器数据的好处。与5.1节中描述的合成设置不同，没有使用向下采样的RGB图像作为输入，而是使用较短焦距拍摄的RGB图像作为输入。用较长焦距拍摄的RGB图像作为地面真实值。</p></blockquote><blockquote><p>Ours-syn-raw：测试合成的原始数据是否可以代替训练的感知真实数据。采用Gharbi等人描述的标准传感器合成模型[9]代替真实的传感器数据进行训练，从8位RGB图像中生成合成的Bayer马赛克。简而言之，我们根据白平衡、gammacorrected sRGB图像的Bayer镶嵌模式，每个像素保留一个颜色通道，并引入随机方差高斯噪声。在这些合成传感器数据上训练我们的模型的一个副本，并在经过白平衡和伽玛校正的真实传感器数据上进行测试。</p></blockquote><h2 id="定性分析"><a href="#定性分析" class="headerlink" title="定性分析"></a>定性分析</h2><p>视觉效果：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/8.png" title="Optional title"></p><p>可以看到，给出的图示在色块交界区域的效果确实不错。</p><p>以及在Amazon Mechanical Turk上进行感知实验来评估生成图像的感知质量。有50人参加了这个测试：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/9.png" title="Optional title"></p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><ul><li><p>使用RAW进行超分，相比经过ISP处理的JPG，RAW有丰富的信息。</p></li><li><p>提出CoBi Loss解决不对齐的问题。</p></li></ul><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>文章：<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8954193&amp;tag=1">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8954193&amp;tag=1</a></p></li><li><p>补充材料：<a href="https://ceciliavision.github.io/files/Zoom_Learn_Supplementary_Material_CVPR2019.pdf">https://ceciliavision.github.io/files/Zoom_Learn_Supplementary_Material_CVPR2019.pdf</a></p></li></ul><blockquote><p>补充材料还挺长，11页pdf。</p></blockquote><ul><li>code：<a href="https://github.com/ceciliavision/zoom-learn-zoom">https://github.com/ceciliavision/zoom-learn-zoom</a></li></ul><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>看上一篇的时候，看到第三部分的那个数据集，顺手去看了一下那个reference，然后顺手找到了这篇文章。看完之后觉得还是很有意义的。</p><p>说句实话这个方法初看下来也算那种大神级别的了。感觉以后用不对齐做超分好像也不是难事情？也许还是我们思路不够开阔。还是要敢想啊。</p><p>但是再一细看，局限性感觉也是很大。</p><p>首先一点就是第一部分针对不同模型的对比实验，感觉不是很公平。毕竟是从Loss方面做的改变，直接就比PSNR和SSIM，感觉有点流氓，意义不大。</p><p>第二个就是文章中也没特别解释他们用的网络模型和原因，读着有点难受。</p><p>第三个就是感觉不对齐这个问题还是太广了，这个好像只是水平偏移，如果偏移的更多样的话，不知道会有怎么样的解决思路。感觉这一点能用到光学显微镜成像的那里。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;变焦学习，学习变焦&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;CVPR 2019&lt;/p&gt;
&lt;/blockquo</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>能够提高个人体验的计算机软硬件问题合集</title>
    <link href="http://example.com/2021/04/03/%E8%83%BD%E5%A4%9F%E6%8F%90%E9%AB%98%E4%B8%AA%E4%BA%BA%E4%BD%93%E9%AA%8C%E7%9A%84%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/"/>
    <id>http://example.com/2021/04/03/%E8%83%BD%E5%A4%9F%E6%8F%90%E9%AB%98%E4%B8%AA%E4%BA%BA%E4%BD%93%E9%AA%8C%E7%9A%84%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/</id>
    <published>2021-04-03T11:20:56.000Z</published>
    <updated>2022-03-04T10:45:21.028Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>经验内容，欢迎转载。</p></blockquote><hr><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>一个新坑。这个坑实际上好久之前就想开了（没错，就是在根目录上给777权限的那次）。</p><p>自诩是一个电脑维修铺子的老板，软硬件什么的也是天天都在摸索和学习。科班只教了如何搞软件，现在大家也都在一股脑的搞软件。但是碰到了硬件问题，好像大家都没那么感兴趣？只是作为一个喜欢搞机和经常胡乱搞机的人，常常会碰到这类问题。从硬件购买安装到操作系统层面的软件配置问题，每一次遇到了，每一次去解决，都是新的尝试和学习。恰好这段时间从zkyzdhs那边搞到了一台主机，有条件搞机了，因此借这个机会开新坑。</p><p>这里将记录在搞机过程中遇到的一切非专业但有意义的问题。随时更新，标题间几乎无关联。</p><p>实际上，遇到问题之后，基本的解决方法就是百度，然后选择靠谱的方法自己进行测试。唯一不同的是，我将会在这里更加详细的记录细节，尤其是硬件的型号或者时间或者是具体的软硬件版本这类的内容，毕竟自己也是一个怕出错的人，我要是查百度，对于具体的细节查不到，是会非常恼火的。</p><hr><h1 id="windosw-10基础上安装Ubuntu-18-04"><a href="#windosw-10基础上安装Ubuntu-18-04" class="headerlink" title="windosw 10基础上安装Ubuntu 18.04"></a>windosw 10基础上安装Ubuntu 18.04</h1><blockquote><p>2021.4.3更新。</p></blockquote><blockquote><p>新搞来的主机，原本是带着Ubuntu 18的。结果我在装win 10双系统的时候，没看清硬盘名字，直接用win 10给覆盖上了。Ubuntu肯定是得有，所以得重新装一下，此为问题背景。以及学校的校园网，有线连接不支持Ubuntu，机箱内也无无线网卡，需要解决这个问题。</p></blockquote><p>直接上图：</p><h2 id="win10上的操作"><a href="#win10上的操作" class="headerlink" title="win10上的操作"></a>win10上的操作</h2><p>主要就是使用Rufus做系统盘。下载好iso，u盘准备好，按照提示做。</p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/1.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/2.png" title="Optional title"></p><p>出现这个是因为用的那个u盘之前被我分了两个盘，在做系统盘的时候是要合并成一个的。无脑点确定就完事了。</p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/3.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/4.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/5.png" title="Optional title"></p><p>以及看一下这个机箱的硬盘情况。就是手抖把win 10装到那个固态上了。另一块机械4个T，我也不知道当初他们买的时候想的是什么。主板上弄俩固态不香么？</p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/6.png" title="Optional title"></p><h2 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h2><p>因为要手动分区，所以操作比较多。</p><p>但是实际上手动分区之后，从bios启动ubuntu，直接选硬盘启动不了。装是装成功了的，重启了之后找不到启动位置这我也没办法。手动分区的过程在下面，是没有问题的。</p><p>以及如果直接选从整个硬盘安装的的话，是没有下面这么多步骤的，直接就进那个设置用户名和密码的页面的。</p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/7.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/8.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/9.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/10.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/11.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/12.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/13.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/14.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/15.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/16.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/17.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/18.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/19.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/20.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/21.png" title="Optional title"></p><p>这部分过程是手机拍的，糊了点儿，凑合看。</p><p>这部分的参考：</p><blockquote><p><a href="https://blog.csdn.net/fanxueya1322/article/details/90205143">https://blog.csdn.net/fanxueya1322/article/details/90205143</a><br><a href="https://rufus.ie/zh/">https://rufus.ie/zh/</a></p></blockquote><p><strong>以及不联网装的ubuntu连make和gcc都没有，血泪教训。</strong>后面还得解决这个问题，不过装系统肯定不叫事儿，随便装就行了。</p><h1 id="使用Mac中的小机器人搞定drop导致的-HEIC格式图片"><a href="#使用Mac中的小机器人搞定drop导致的-HEIC格式图片" class="headerlink" title="使用Mac中的小机器人搞定drop导致的.HEIC格式图片"></a>使用Mac中的小机器人搞定drop导致的.HEIC格式图片</h1><blockquote><p>2021.4.21更新。</p></blockquote><p>从ip上把图片drop到Mac上，默认的格式是.HEIC。去查了一下这个格式，发现是Apple在后面的更新中新加的机制，图片默认的存储格式就是这个，如果不设置的话，drop到其他苹果设备上，就会保持这个格式。这个格式可以在我的Mac上打开和查看，但好像其他设备和低版本的Mac OS就不得行了。</p><p>两个解决方法，一个是从ip上设置传输的时候选择兼容性最佳，就在相机设置里，很简单。但我这么做了之后，drop过来的图片将两种格式掺在一起drop给我了，个别图片还是保持了.HEIC格式，不知道为什么。</p><p>另一个就是直接将.HEIC转化为常见的.jpg或者.png。转化的话，网络上有很多工具，但实际上大家都知道不怎么靠谱，要么是广告，要么让你下载软件，我反正很讨厌这个，没有online的我是绝对不会用的。然后就在查百度的过程中看见了有人说可以用Mac自带的小机器人完成这项工作，所以就探索了下，发现简单又好用。</p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/22.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/23.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/24.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/25.png" title="Optional title"></p><p>最后做了两个，一个是2jpeg，另一个是2png。是因为发现转换成jpeg格式之后文件还是有点大，一张图占个3M往上就有点夸张，不好存储了。</p><p>以及好像这个小机器人很好用，我看还有一堆能用的上的功能，以后如果有机会的话再开发一下。</p><h1 id="卸载并重装mac的python环境"><a href="#卸载并重装mac的python环境" class="headerlink" title="卸载并重装mac的python环境"></a>卸载并重装mac的python环境</h1><blockquote><p>2022.3.4更新。</p></blockquote><p>今天下午搞代码的时候，开pycharm配置服务器，服务器倒是没什么问题，就是mac的python一直弹停止工作的窗口，然后conda什么的命令也都不好使了，这些东西都是18年装的，很老了，期间我想要更新conda，但每次都要3.6.5的python也加入更新列表，尝试更新之后环境直接全部乱掉了，所以就从备份恢复过后再也没弄过。今天这刚好有个理由把这些东西全部更新一遍。</p><h2 id="卸载anaconda、python3-6"><a href="#卸载anaconda、python3-6" class="headerlink" title="卸载anaconda、python3.6"></a>卸载anaconda、python3.6</h2><p>删anaconda。</p><ul><li>Mac 如何彻底删除 Anaconda? <a href="https://www.cnblogs.com/bigband/p/13646296.html">https://www.cnblogs.com/bigband/p/13646296.html</a></li></ul><p>删python3.6，里面讲的3.7，3.6同理。</p><ul><li>Mac 安装及卸载 Python3 详细教程 <a href="https://zhuanlan.zhihu.com/p/142810523">https://zhuanlan.zhihu.com/p/142810523</a></li></ul><p>3.6.5删除干净之后，命令行输入<code>python3</code>，结果又弹出一个3.8.2版本。查一下位置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">print(<span class="string">&quot;sys.executable&quot;</span>)</span><br></pre></td></tr></table></figure><p>结果是<code>/Library/Developer/CommandLineTools/usr/bin/python3</code>，<code>CommandLineTools</code>好像是之前Xcode需要的组件。</p><p>解释：<a href="https://www.zhihu.com/question/420273182/answer/1697779720">https://www.zhihu.com/question/420273182/answer/1697779720</a> 。</p><p>然后在这个bin里再把跟python3有关的都删除了，再输入python3，会提示要使用python3，命令行工具需要安装什么什么的，应该不会影响后面的安装了。</p><h2 id="安装python3-8-10"><a href="#安装python3-8-10" class="headerlink" title="安装python3.8.10"></a>安装python3.8.10</h2><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/26.png" title="Optional title"></p><p>当前官网上最新的安全版本是3.8，mac只提供到小版本号10，下载.pkg文件，按顺序安装。完成之后验证一下：</p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/27.png" title="Optional title"></p><p>conda没必要装了感觉。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;经验内容，欢迎转载。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;一个新坑。这个坑实际上好久之前就想开了（</summary>
      
    
    
    
    
    <category term="闲谈" scheme="http://example.com/tags/%E9%97%B2%E8%B0%88/"/>
    
    <category term="操作系统" scheme="http://example.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
</feed>
