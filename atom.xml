<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>MoyangSensei</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-04-14T08:55:19.708Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Fy J</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>社会媒体计算：知乎问答信息挖掘</title>
    <link href="http://example.com/2021/04/14/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97%EF%BC%9A%E7%9F%A5%E4%B9%8E%E9%97%AE%E7%AD%94%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98/"/>
    <id>http://example.com/2021/04/14/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97%EF%BC%9A%E7%9F%A5%E4%B9%8E%E9%97%AE%E7%AD%94%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98/</id>
    <published>2021-04-14T07:33:25.000Z</published>
    <updated>2021-04-14T08:55:19.708Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>刚好在摸鱼的时候比较喜欢刷知乎，又碰到了这个课程，所以就尝试了做这个内容。以前从来没有接触过文本分析和爬虫这类的技术，就当学新技术了。以及这些在去年12月就写完了，现在闲了才想起搬到这里来。</p><p>依稀记得那是为数不多的有心思敲代码的时间，会珍惜的。</p><hr><h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><h2 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h2><p>从互联网上采集数据，如从 Tewitter、Facebook、新浪微博、B站等采集数据，包括用户基本信息、互相浏览、互相关注等信息，以及对应某一段时间发布的文本内容信息。</p><p>对上述数据进行预处理，要求用程序进行预处理。</p><p>对上述处理数据进行社团挖掘，包括基本统计信息、社团发现等，对预处理的文本进行情感分析、主题挖掘、分类或聚类等研究。要求用到社会网络计算、文本挖掘等技术。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>知乎是一个网络问答社区，用户彼此分享知识、经验和见解，围绕着某一感兴趣的话题进行相关的讨论，同时也可以关注兴趣一致的人。</p><p>知乎的基本模式是：用户提问，每一个问题都会有一个独有的id。其他对此问题感兴趣的用户在此问题下进行回答，每一个此问题下的回答也会有一个id。用户可对回答进行点赞、点踩、评论等操作。</p><p>选择从知乎采集要用到的数据。所选取的问题是：你打算在 12 月 31 日发什么朋友圈跨年文案？</p><p>链接：<a href="https://www.zhihu.com/question/360940960">https://www.zhihu.com/question/360940960</a></p><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/1.png" title="Optional title"></p><h2 id="编程环境"><a href="#编程环境" class="headerlink" title="编程环境"></a>编程环境</h2><p>Mac OS 10.14.6 + Jupyter notebook + Python 3.6.5</p><h1 id="知乎数据采集"><a href="#知乎数据采集" class="headerlink" title="知乎数据采集"></a>知乎数据采集</h1><h2 id="数据爬虫"><a href="#数据爬虫" class="headerlink" title="数据爬虫"></a>数据爬虫</h2><p>这部分爬虫功能的主要作用就是从知乎的网页上拿数据。</p><p><strong>这部分的原理参考最下面唯一的一条引用，包括从网络请求找到数据存放位置、分析请求头的格式和构造代码中所需要的新的请求头。</strong></p><p>def crawler(question_num)是该部分功能的组织函数，其他4个函数的具体功能见注释。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span>(<span class="params">url</span>):</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：访问 url 的网页，获取网页内容并返回</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         url ：目标网页的 url</span></span><br><span class="line"><span class="comment">#     返回：目标网页的 html 内容</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 设置多个用户代理</span></span><br><span class="line">    agent=[<span class="string">&#x27;Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36&#x27;</span>,  </span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.80 Safari/537.36&#x27;</span>,  </span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:30.0) Gecko/20100101 Firefox/30.0&#x27;</span>,  </span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/537.75.14&#x27;</span>,  </span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Win64; x64; Trident/6.0)&#x27;</span> ]</span><br><span class="line">    <span class="comment"># 每次随机抽取一个，防止被封ip</span></span><br><span class="line">    randdom_agent=random.choice(agent)</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;accept&#x27;</span>: <span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;user-agent&#x27;</span>: randdom_agent</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 爬取数据</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url, headers=headers)</span><br><span class="line">        r.encoding = <span class="string">&#x27;UTF8&#x27;</span></span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span> requests.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">        print(<span class="string">&quot;HTTPError&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> requests.RequestException <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">&quot;Unknown Error !&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_data</span>(<span class="params">html</span>):</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：提取 html 页面信息中的关键信息，并整合一个数组并返回</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         html：根据 url 获取到的网页内容</span></span><br><span class="line"><span class="comment">#     返回：存储有 html 中提取出的关键信息的数组</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    json_data = json.loads(html)[<span class="string">&#x27;data&#x27;</span>]</span><br><span class="line">    comments = []</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> json_data:</span><br><span class="line">            comment = []</span><br><span class="line">            comment.append(item[<span class="string">&#x27;author&#x27;</span>][<span class="string">&#x27;name&#x27;</span>]) <span class="comment"># 姓名</span></span><br><span class="line">            comment.append(item[<span class="string">&#x27;author&#x27;</span>][<span class="string">&#x27;gender&#x27;</span>]) <span class="comment"># 性别</span></span><br><span class="line">            comment.append(item[<span class="string">&#x27;voteup_count&#x27;</span>]) <span class="comment"># 点赞数</span></span><br><span class="line">            comment.append(item[<span class="string">&#x27;comment_count&#x27;</span>])  <span class="comment"># 评论数</span></span><br><span class="line">            comment.append(item[<span class="string">&#x27;url&#x27;</span>])  <span class="comment"># 回答链接</span></span><br><span class="line">            comment.append(item[<span class="string">&#x27;created_time&#x27;</span>])  <span class="comment"># 回答时间</span></span><br><span class="line">            comment.append(item[<span class="string">&#x27;content&#x27;</span>]) <span class="comment"># 回答时间</span></span><br><span class="line">            comments.append(comment)   </span><br><span class="line">        <span class="keyword">return</span> comments    </span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(comment)</span><br><span class="line">        print(e)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_file</span>(<span class="params">question_num</span>):</span></span><br><span class="line"><span class="comment">#      &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：根据知乎的问题id构造文件路径和文件名，csv、png都会用到</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         question_num：知乎提问的问题id</span></span><br><span class="line"><span class="comment">#     返回：csv文件路径、csv文件名</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    current_path = os.getcwd()</span><br><span class="line">    file_name=str(question_num)+<span class="string">&#x27;_&#x27;</span>+str(datetime.datetime.now())+<span class="string">&#x27;.csv&#x27;</span></span><br><span class="line">    path = current_path+<span class="string">&#x27;/&#x27;</span>+file_name</span><br><span class="line">    print(<span class="string">&quot;文件：&quot;</span>+path)</span><br><span class="line">    <span class="keyword">return</span> path,file_name</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_data</span>(<span class="params">comments,header,path</span>):</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：将comments中的信息输出到文件/数据库中</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         comments：将要保存的数据  </span></span><br><span class="line"><span class="comment">#     返回：无</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    dataframe = pd.DataFrame(comments)</span><br><span class="line">    <span class="keyword">if</span> header:</span><br><span class="line">        dataframe.to_csv(path, mode=<span class="string">&#x27;a&#x27;</span>, index=<span class="literal">False</span>, sep=<span class="string">&#x27;,&#x27;</span>, header=[<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;gender&#x27;</span>,<span class="string">&#x27;voteup&#x27;</span>,<span class="string">&#x27;cmt_count&#x27;</span>,<span class="string">&#x27;ans_url&#x27;</span>,<span class="string">&#x27;ans_time&#x27;</span>,<span class="string">&#x27;ans_content&#x27;</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dataframe.to_csv(path, mode=<span class="string">&#x27;a&#x27;</span>, index=<span class="literal">False</span>, sep=<span class="string">&#x27;,&#x27;</span>, header=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crawler</span>(<span class="params">question_num</span>):</span></span><br><span class="line"><span class="comment">#      &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：爬虫功能的组织函数，爬取数据并存储到csv文件中</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         question_num：知乎提问的问题id</span></span><br><span class="line"><span class="comment">#     返回：文件名</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    url_start = <span class="string">&#x27;https://www.zhihu.com/api/v4/questions/&#x27;</span>+str(question_num)+<span class="string">&#x27;/answers?include=data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cis_labeled%3Bdata%5B%2A%5D.mark_infos%5B%2A%5D.url%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%2A%5D.topics&amp;limit=5&amp;offset=&#x27;</span></span><br><span class="line">    url_end=<span class="string">&#x27;&amp;platform=desktop&amp;sort_by=default&#x27;</span></span><br><span class="line">    html=get_data(url_start+str(<span class="number">5</span>)+url_end)</span><br><span class="line">    totals=json.loads(html)[<span class="string">&#x27;paging&#x27;</span>][<span class="string">&#x27;totals&#x27;</span>]</span><br><span class="line">    path,file_name=get_file(question_num)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">&quot;总回答数：&quot;</span>+str(totals))</span><br><span class="line">    page = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span>(page &lt; totals):</span><br><span class="line">        url = url_start+str(page) +url_end</span><br><span class="line"> </span><br><span class="line">        html = get_data(url)</span><br><span class="line">        comments = parse_data(html)</span><br><span class="line">        <span class="keyword">if</span> page==<span class="number">0</span>:</span><br><span class="line">            save_data(comments,<span class="literal">True</span>,path)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            save_data(comments,<span class="literal">False</span>,path)</span><br><span class="line">        page += <span class="number">5</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> file_name</span><br></pre></td></tr></table></figure><h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><p>def save_data(comments,header,path)将爬到的数据存到了.CSV中，数据读取函数则将CSV中的内容读到内存里供。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_csv</span>(<span class="params">file_name</span>):</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：读取爬虫函数得到的csv文件</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         file_name：文件名  </span></span><br><span class="line"><span class="comment">#     返回：csv文件的list</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> open(file_name, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        reader = csv.reader(f)</span><br><span class="line">        result = list(reader)</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>爬取下来的数据需要预处理。<strong>主要是因为所关注的“ans_content”内，除了中文之外，还有一些奇奇怪怪的东西</strong>：</p><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/2.png" title="Optional title"></p><p>可以看到，<strong>有emoji、有非中文的特殊字符、有h5的标签</strong>（由&lt;&gt;括起的部分，主要由于回答中的图片的网页链接需要用这种方式在网页源码内引用）。<strong>使用正则表达式去掉这些对中文文本分析没有用的内容。</strong></p><p>其他的一些处理还包括<strong>秒级时间戳的转换和知乎用户系统中的“匿名用户”进行编号等</strong>。这些无关紧要，只是为了看着舒服。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">date_pre_treatment</span>(<span class="params">csv_data</span>):</span></span><br><span class="line"><span class="comment">#      &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#     功能：数据预处理</span></span><br><span class="line"><span class="comment">#     参数：</span></span><br><span class="line"><span class="comment">#         csv_data：已经读取好的list  </span></span><br><span class="line"><span class="comment">#     返回：经过了预处理的list</span></span><br><span class="line"><span class="comment">#     &#x27;&#x27;&#x27;</span></span><br><span class="line">    niming_num=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(csv_data[<span class="number">0</span>])):</span><br><span class="line">        <span class="keyword">if</span> csv_data[<span class="number">0</span>][j]==<span class="string">&#x27;name&#x27;</span>:</span><br><span class="line">            <span class="keyword">for</span> i1 <span class="keyword">in</span> range(len(csv_data)):</span><br><span class="line">                <span class="comment"># 将知乎用户、匿名用户的名称统一，加上编号做区分</span></span><br><span class="line">                <span class="keyword">if</span> csv_data[i1][j]==<span class="string">&#x27;知乎用户&#x27;</span> <span class="keyword">or</span> csv_data[i1][j]==<span class="string">&#x27;匿名用户&#x27;</span>:</span><br><span class="line">                    csv_data[i1][j]=<span class="string">&#x27;匿名用户&#x27;</span>+str(++niming_num)</span><br><span class="line">        <span class="keyword">if</span> csv_data[<span class="number">0</span>][j]==<span class="string">&#x27;ans_time&#x27;</span>:</span><br><span class="line">            <span class="keyword">for</span> i2 <span class="keyword">in</span> range(len(csv_data)<span class="number">-1</span>):</span><br><span class="line">                ltime = time.localtime(int(csv_data[i2+<span class="number">1</span>][j]))</span><br><span class="line">                <span class="comment"># 将秒级时间戳转化为&quot;年月日时分秒&quot;格式</span></span><br><span class="line">                csv_data[i2+<span class="number">1</span>][j]=time.strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>,ltime)</span><br><span class="line">        <span class="keyword">if</span> csv_data[<span class="number">0</span>][j]==<span class="string">&#x27;ans_content&#x27;</span>:</span><br><span class="line">            <span class="keyword">for</span> i3 <span class="keyword">in</span> range(len(csv_data)):</span><br><span class="line">                <span class="comment"># 去除文本信息中的h5标签</span></span><br><span class="line">                csv_data[i3][j] = re.sub(<span class="string">u&quot;\\&lt;.*?\\&gt;&quot;</span>, <span class="string">&quot; &quot;</span>, csv_data[i3][j])</span><br><span class="line">                <span class="comment"># 去除文本信息除中文、英文、ascll字符、常用标点以外的所有内容</span></span><br><span class="line">                csv_data[i3][j] = re.sub(<span class="string">&quot;[^\u4e00-\u9fa5 ^a-z ^A-Z ^0-9 ^~!@#$%&amp;*()_+-=:;,.^～！，。？、《》]&quot;</span>,<span class="string">&#x27;&#x27;</span>, csv_data[i3][j])</span><br><span class="line">        <span class="keyword">if</span> csv_data[<span class="number">0</span>][j]==<span class="string">&#x27;voteup&#x27;</span>:</span><br><span class="line">            <span class="keyword">for</span> i4 <span class="keyword">in</span> range(len(csv_data)<span class="number">-1</span>):</span><br><span class="line">                <span class="comment"># 将投票数转化为int（读取的时候会存储为str）</span></span><br><span class="line">                csv_data[i4+<span class="number">1</span>][j] = int(float(csv_data[i4+<span class="number">1</span>][j]))</span><br><span class="line">    <span class="keyword">return</span> csv_data</span><br></pre></td></tr></table></figure><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/3.png" title="Optional title"></p><h1 id="词云"><a href="#词云" class="headerlink" title="词云"></a>词云</h1><p>使用wordcloud包进行所有回答文本的词云的绘制，并存储为png图片文件。用到的功能为WordCloud，参数和说明见代码注释。</p><p><strong>词云的图如果想做的有意义且好看，最主要的参数是stopwords、max_words。</strong></p><p>其中，stopwords指的是词云里什么词不能出现，默认为空。毕竟不管是中文还是英文，文本里都有许多没有意义的词，比如语气词“嗯”、“啊”、“呢”这种类似的。所以如果不加限制的话，词云里最大的那个词一定是没什么意义的词（知乎作为一个网络社区，大家的回答肯定是偏向口头语的，那么无意义的词就会多起来，毕竟没人在知乎上写论文是吧）。这里的stopwords是通过txt导入的。stopwords网上能找到许多，我实在GitHub上找到了一份中文停用词，直接拿过来用了。后期又在里面填了一些自己不想看见的词。</p><p>max_words就是图片上最多出现多少个词，个人感觉还是多一些好看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud</span><br><span class="line">now_time= datetime.datetime.now()</span><br><span class="line">print(<span class="string">&quot;2.词云&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置数据，将所有文本放到一个list里方便处理</span></span><br><span class="line">text = <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(csv_data)<span class="number">-1</span>):</span><br><span class="line">    text+=csv_data[i+<span class="number">1</span>][<span class="number">6</span>]+<span class="string">&#x27; &#x27;</span></span><br><span class="line"><span class="comment"># 结巴中文分词，生成字符串，默认精确模式，如果不通过分词，无法直接生成正确的中文词云</span></span><br><span class="line">cut_text = jieba.cut(text)</span><br><span class="line"><span class="comment"># 必须给个符号分隔开分词结果来形成字符串,否则不能绘制词云</span></span><br><span class="line">result = <span class="string">&quot; &quot;</span>.join(cut_text)</span><br><span class="line"><span class="comment"># 从文件中获取停用词</span></span><br><span class="line">stopwords=get_stopwords()</span><br><span class="line"><span class="comment"># 词云设置</span></span><br><span class="line">wc = WordCloud(     </span><br><span class="line">        font_path=<span class="string">&quot;仿宋_GB2312.ttf&quot;</span>,<span class="comment"># 设置字体（不指定就会出现乱码）</span></span><br><span class="line">        background_color=<span class="string">&#x27;white&#x27;</span>,<span class="comment"># 设置背景色</span></span><br><span class="line">        width=<span class="number">1500</span>, height=<span class="number">900</span>,<span class="comment"># 设置背景宽、高</span></span><br><span class="line">        max_font_size=<span class="number">400</span>, min_font_size=<span class="number">20</span>,<span class="comment"># 设置字体大小上下限</span></span><br><span class="line">        stopwords = stopwords,<span class="comment"># 停用词</span></span><br><span class="line">        max_words=<span class="number">150</span> <span class="comment"># 图片中显示词的最大数量</span></span><br><span class="line">        )</span><br><span class="line"><span class="comment"># 产生词云</span></span><br><span class="line">wc.generate(result)</span><br><span class="line"><span class="comment"># 保存图片</span></span><br><span class="line">pic_name=re.sub(<span class="string">&#x27;.csv&#x27;</span>,<span class="string">&#x27;&#x27;</span>,file_name)+<span class="string">&#x27;_wordcloud.png&#x27;</span></span><br><span class="line">wc.to_file(pic_name) </span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">10</span>))</span><br><span class="line">plt.imshow(wc)</span><br><span class="line">plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Runtime:%d s&#x27;</span>%(datetime.datetime.now()-now_time).seconds)</span><br></pre></td></tr></table></figure><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/4.png" title="Optional title"></p><h1 id="keywords"><a href="#keywords" class="headerlink" title="keywords"></a>keywords</h1><p>使用jieba包进行关键词提取。</p><p>Jieba提供了两种关键词的提取算法，分别是：</p><ul><li><p>基于TF-IDF（term frequency–inverse document frequency）算法的关键词抽取。函数参数如下：sentence：待提取的文本；topK：返回topK个 TF/IDF 权重最大的关键词；withWeight：是否一并返回关键词权重值，默认值为False；allowPOS：仅包括指定词性的词，默认值为空，即不筛选。</p></li><li><p>基于TextRank算法的关键词抽取。函数接口同TF-IDF相同。不同的是allowPOS默认指定了一些词性的词。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba.analyse</span><br><span class="line">now_time= datetime.datetime.now()</span><br><span class="line">print(<span class="string">&quot;3.keywords&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;3.1. TF-IDF&quot;</span>)</span><br><span class="line">keywords1=jieba.analyse.extract_tags(text, topK=<span class="number">20</span>, withWeight=<span class="literal">True</span>, allowPOS=(<span class="string">&#x27;ns&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;vn&#x27;</span>, <span class="string">&#x27;v&#x27;</span>))</span><br><span class="line">print(keywords1)</span><br><span class="line">print(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;3.2. textrank&quot;</span>)</span><br><span class="line">keywords2=jieba.analyse.textrank(text, topK=<span class="number">20</span>, withWeight=<span class="literal">True</span>, allowPOS=(<span class="string">&#x27;ns&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;vn&#x27;</span>, <span class="string">&#x27;v&#x27;</span>)) </span><br><span class="line">print(keywords2)</span><br><span class="line">print(<span class="string">&#x27;Runtime:%d s&#x27;</span>%(datetime.datetime.now()-now_time).seconds)</span><br></pre></td></tr></table></figure><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/5.png" title="Optional title"></p><h1 id="情感分析"><a href="#情感分析" class="headerlink" title="情感分析"></a>情感分析</h1><p>使用snownlp进行情感分析。</p><p>snownlp是一个python类库，可以方便的处理中文文本内容，是受到了TextBlob的启发而写的，并且和TextBlob不同的是，这里没有用NLTK，所有的算法都是重新实现的，并且自带了一些训练好的字典。</p><blockquote><p>snownlp: <a href="https://github.com/isnowfy/snownlp">https://github.com/isnowfy/snownlp</a><br>TextBlob: <a href="https://github.com/sloria/TextBlob">https://github.com/sloria/TextBlob</a></p></blockquote><p><strong>snownlp给出了基于贝叶斯分类的情感分析函数SnowNLP。对于每一条文本，该函数会给出一个0-1之间的评分，越接近1代表积极情绪占比越高。</strong></p><p>使用SnowNLP对每一条回答文本进行情感分析，并以评分达到<strong>0.6、0.5认定为是积极文本</strong>，分别给出积极回答文本在所有文本中的比重。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> snownlp <span class="keyword">import</span> SnowNLP</span><br><span class="line">now_time= datetime.datetime.now()</span><br><span class="line">print(<span class="string">&quot;4.情感分析&quot;</span>)</span><br><span class="line">s=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(csv_data)<span class="number">-1</span>):</span><br><span class="line">    a=SnowNLP(csv_data[i+<span class="number">1</span>][<span class="number">6</span>])</span><br><span class="line">    s.append(round(a.sentiments,<span class="number">3</span>))</span><br><span class="line">s.sort(reverse=<span class="literal">True</span>)</span><br><span class="line">flag06=<span class="number">-1</span></span><br><span class="line">flag05=<span class="number">-1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(s)):</span><br><span class="line">    <span class="keyword">if</span> s[i]&lt;<span class="number">0.5</span> :</span><br><span class="line">        flag05=i</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(s)):</span><br><span class="line">    <span class="keyword">if</span> s[i]&lt;<span class="number">0.6</span> :</span><br><span class="line">        flag06=i</span><br><span class="line">        <span class="keyword">break</span>    </span><br><span class="line">print(<span class="string">&quot;积极（60%）百分比：&quot;</span>,round(flag06/(len(s)),<span class="number">2</span>)) </span><br><span class="line">print(<span class="string">&quot;积极（50%）百分比：&quot;</span>,round(flag05/(len(s)),<span class="number">2</span>))         </span><br><span class="line">print(<span class="string">&#x27;Runtime:%d s&#x27;</span>%(datetime.datetime.now()-now_time).seconds)</span><br></pre></td></tr></table></figure><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/6.png" title="Optional title"></p><p><strong>两个评分阈值下的大部分的回答都被认为是积极的，这也符合这个问题的背景：新年。</strong></p><h1 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h1><p>对点赞数最多的100条数据进行聚类。</p><p>进行涉及文本的向量化表示。sklearn提供了传统的词袋模型。使用sklearn中的TfidfVectorizer计算tf-idf矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line">print(<span class="string">&#x27;5. 文本聚类&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;5.1. 准备点赞数最多的100条数据&#x27;</span>)</span><br><span class="line">user_id=[]</span><br><span class="line">content=[]</span><br><span class="line">csv_data_sorted=csv_data</span><br><span class="line"><span class="keyword">del</span>(csv_data_sorted[<span class="number">0</span>])</span><br><span class="line">csv_data_sorted=sorted(csv_data,key=itemgetter(<span class="number">2</span>))</span><br><span class="line"><span class="comment"># print(csv_data[:10])</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    user_id.append(csv_data_sorted[i][<span class="number">0</span>])</span><br><span class="line">    content.append(csv_data_sorted[i][<span class="number">6</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line">now_time= datetime.datetime.now()</span><br><span class="line">print(<span class="string">&#x27;5.2. tfidf matrix&#x27;</span>)</span><br><span class="line"><span class="comment">#max_df: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.</span></span><br><span class="line"><span class="comment">#min_df: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.</span></span><br><span class="line">tfidf_vectorizer = TfidfVectorizer(max_df=<span class="number">0.9</span>, max_features=<span class="number">200000</span>,min_df=<span class="number">0.1</span>, stop_words=<span class="string">&#x27;english&#x27;</span>,use_idf=<span class="literal">True</span>, tokenizer=segment)</span><br><span class="line">tfidf_matrix = tfidf_vectorizer.fit_transform(content) <span class="comment">#fit the vectorizer to synopses</span></span><br><span class="line">print(tfidf_matrix.shape)</span><br><span class="line">print(<span class="string">&#x27;Runtime:%d s&#x27;</span>%(datetime.datetime.now()-now_time).seconds)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scipy.cluster.hierarchy <span class="keyword">import</span> ward, dendrogram, linkage</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line">now_time= datetime.datetime.now()</span><br><span class="line">print(<span class="string">&#x27;5.3. linkage matrix&#x27;</span>)</span><br><span class="line">dist = <span class="number">1</span> - cosine_similarity(tfidf_matrix)</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>]=[<span class="string">&#x27;Microsoft YaHei&#x27;</span>] <span class="comment">#用来正常显示中文标签</span></span><br><span class="line">linkage_matrix = linkage(dist, method=<span class="string">&#x27;ward&#x27;</span>, metric=<span class="string">&#x27;euclidean&#x27;</span>, optimal_ordering=<span class="literal">False</span>)</span><br><span class="line">print(<span class="string">&#x27;Runtime:%d s&#x27;</span>%(datetime.datetime.now()-now_time).seconds)</span><br><span class="line">print(linkage_matrix)</span><br></pre></td></tr></table></figure><p>由于选取了100条回答，tf-idf矩阵的第一维为100。</p><p>然后根据tf-idf矩阵进行层次聚类，给出linkage矩阵。使用函数：scipy.cluster.hierarchy.linkage(y, method=’single’, metric=’euclidean’, optimal_ordering=False)。其中，计算新形成的聚类簇u和v之间距离的方法是用的是single，即最近邻点算法。</p><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/7.png" title="Optional title"></p><p>对上述矩阵进行可视化，并将结果保存为png文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">now_time= datetime.datetime.now()</span><br><span class="line">print(<span class="string">&#x27;5.4. linkage matrix可视化&#x27;</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">40</span>, <span class="number">15</span>))</span><br><span class="line">plt.title(<span class="string">&#x27;层次聚类树状图&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;知乎用户名称&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;距离（越低表示文本越类似）&#x27;</span>)</span><br><span class="line">dendrogram(</span><br><span class="line">    linkage_matrix,</span><br><span class="line">    labels=user_id, </span><br><span class="line">    leaf_rotation=<span class="number">-70</span>,  <span class="comment"># rotates the x axis labels</span></span><br><span class="line">    leaf_font_size=<span class="number">12</span>  <span class="comment"># font size for the x axis labels</span></span><br><span class="line">)</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>]=[<span class="string">&#x27;SimHei&#x27;</span>] <span class="comment">#用来正常显示中文标签</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span> <span class="comment">#用来正常显示负号</span></span><br><span class="line">plt.savefig(re.sub(<span class="string">&#x27;.csv&#x27;</span>,<span class="string">&#x27;&#x27;</span>,file_name)+<span class="string">&#x27;_linkage.png&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;Runtime:%d s&#x27;</span>%(datetime.datetime.now()-now_time).seconds)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/8.png" title="Optional title"></p><p>其中，横坐标为知乎用户名，每种的线连起来的用户名代表这些用户的回答可被分为相似的一类。</p><p>（看到这里明白为什么只选100个了吧，因为选多了图就画不下了）</p><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>在代码中更改需要爬取的知乎问题的问题id，就可以实现对任意知乎问题下的所有回答内容的上述操作。</p><p>代码每次运行都会按照以“问题id+时间”为文件名进行各项文件的保存，不会覆盖之前运行所保留的文件。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><blockquote><p>Python网络爬虫实战：爬取知乎话题下 18934 条回答数据-csdn<br><a href="https://blog.csdn.net/wenxuhonghe/article/details/86515558">https://blog.csdn.net/wenxuhonghe/article/details/86515558</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;刚好在摸鱼的时候比较喜欢刷知</summary>
      
    
    
    
    
    <category term="社会媒体计算" scheme="http://example.com/tags/%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>想分享给别人看的一些影像</title>
    <link href="http://example.com/2021/04/14/%E6%83%B3%E5%88%86%E4%BA%AB%E7%BB%99%E5%88%AB%E4%BA%BA%E7%9C%8B%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BD%B1%E5%83%8F/"/>
    <id>http://example.com/2021/04/14/%E6%83%B3%E5%88%86%E4%BA%AB%E7%BB%99%E5%88%AB%E4%BA%BA%E7%9C%8B%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BD%B1%E5%83%8F/</id>
    <published>2021-04-14T07:23:04.000Z</published>
    <updated>2021-04-14T09:41:02.721Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创内容。如果喜欢，告知后自取即可。</p></blockquote><hr><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>仪式感是很重要的东西。喜欢把生活中的内容留个纪念，这里就用来留一些想给别人看的图片。</p><p>不定期更新。</p><hr><blockquote><p>2021.4.14更新。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创内容。如果喜欢，告知后自取即可。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;仪式感是很重要的东西。喜</summary>
      
    
    
    
    
    <category term="闲谈" scheme="http://example.com/tags/%E9%97%B2%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：Zoom to learn, learn to zoom</title>
    <link href="http://example.com/2021/04/12/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AZoom-to-learn-learn-to-zoom/"/>
    <id>http://example.com/2021/04/12/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AZoom-to-learn-learn-to-zoom/</id>
    <published>2021-04-12T02:29:43.000Z</published>
    <updated>2021-04-14T07:23:41.397Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>变焦学习，学习变焦</p></blockquote><blockquote><p>CVPR 2019</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>变焦功能是当今手机和相机的必备功能。</p><p>光学变焦是图像变焦的最佳选择，可以保持较高的图像质量。但变焦镜头价格昂贵且其物理组件比较笨重。数码变焦方法是一种降低成本的选择，但数码变焦只是简单地向上采样相机传感器输入的裁剪区域，产生模糊的输出。</p><p>文章提出，常规的SISR方法有以下两个限制：</p><ul><li><p>现有的大多数方法使用的是合成方法来逼近真实变焦，即其中输入图像是高分辨率图像的下采样版本。这种方法间接降低了输入中的噪声水平，但实际上，由于在曝光时间内进入光圈的光子更少，遥远物体的区域往往包含更多的噪声。</p></li><li><p>其次，现有的大多数方法都是从8位RGB图像开始的，该图像由相机图像信号处理器（ISP）处理，ISP将高位原始传感器数据中的高频信号用于其他目标（例如降噪）。</p></li></ul><blockquote><p>第二点的解读：本文训练的数据均是Raw Data，这是专业单反拍摄的格式，而RGB图片是Raw Data经过图像处理器（Image SIgnal Processer, ISP）制作的，在某种程度上来说，RGB图片也是有损的。</p></blockquote><p>基于上述限制，文章做了以下内容：</p><ul><li><p>使用真实的高位传感器数据进行计算变焦的实用性，而不是处理8位RGB图像或合成传感器模型。</p></li><li><p>新的数据集SR-RAW，它是第一个具有光学ground truth的超分辨率原始数据集。SR-RAW是用变焦镜头拍摄的。对于焦距较短的图像，长焦距图像作为光学ground truth。</p></li><li><p>提出了一种新的上下文双边损失（CoBi）处理稍微失调的图像对。</p></li></ul><h1 id="SR-RAW"><a href="#SR-RAW" class="headerlink" title="SR-RAW"></a>SR-RAW</h1><h2 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h2><p>使用24-240毫米变焦镜头来收集不同光学变焦水平的原始图像对。</p><p>采集过程中，在每个场景的7个光学变焦设置下采集了7幅图像。来自7幅图像序列的每一对图像形成一个数据对。总共500个室内外场景，ISO从100到400。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/1.png" title="Optional title"></p><p><strong>在训练模型中，以短焦距原始传感器图像作为输入，以长焦距RGB图像作为超分辨率的基础。</strong></p><blockquote><p>例如，使用70mm焦距拍摄的RGB图像作为使用35mm焦距拍摄的原始传感器数据的2X缩放地面真相。</p></blockquote><p>相机有特殊设置，来自原文：</p><ul><li><p>景深（DOF）随着焦距的变化而变化，调整孔径大小使每个焦距的DOF相同是不现实的。选择一个小的光圈尺寸（至少f/20）来最小化DOF差异（在图2的B2中仍然可以看到），使用一个三脚架来捕捉长时间曝光的室内场景。</p></li><li><p>其次，使用相同的曝光时间的所有图像在一个序列，使噪音水平不受焦距变化的影响。但是仍然观察到由于快门和物理光瞳是机械的并且涉及到动作变化而引起的明显的光照变化。这种颜色的变化是避免使用像素对像素的损失进行训练的另一个动机。</p></li><li><p>第三，虽然透视不随焦距的变化而变化，但当镜头放大或缩小时，在投影中心存在微小的变化（镜头的长度），在不同深度的物体之间产生明显的透视变化（图2 B1）。使用的Sony FE 24-240mm镜头，需要与被摄对象至少56.4米的距离，才能在相距5米的物体之间产生小于1像素的透视位移。因此，避免捕获非常接近的对象，但允许在数据集中进行这样的透视图转换。</p></li></ul><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>对于任意一对训练图像，用RGB-L表示低分，用RAW-L表示高分，也就是相机的传感器的数据，GT。</p><p>高分GT，使用RGB-H和RAW-H。首先匹配RAW-H和RGB-H之间的视图(FOV)。然后计算RGB-L和RGB-H之间的对齐（手动缩放相机以调整焦距所引起的相机轻微移动）。</p><p>使用一个欧几里德运动模型，通过增强相关系数最小化来实现图像的旋转和平移。</p><p>训练时，将匹配FOV的RAW-L作为输入，它的GT是RGB-H，与RAW-L对齐并具有相同的FOV。如果光学变焦与目标变焦比不完全匹配，则对图像应用比例偏移。</p><blockquote><p>例如，如果使用（35mm，150mm）训练一个4X变焦模型，那么目标图像的偏移量为1.07。</p></blockquote><h2 id="非对齐分析"><a href="#非对齐分析" class="headerlink" title="非对齐分析"></a>非对齐分析</h2><p>预处理步骤很难消除偏差。由于捕捉的数据焦距不同，视角的变化会导致视角的不对齐。此外，当对不同分辨率的图像进行对齐时，高分辨率图像中的锐边不能与低分辨率图像中的模糊边精确对齐(图2 B3)。</p><blockquote><p>SR-RAW中描述的失调通常会导致800万像素图像对中的40-80像素偏移。</p></blockquote><blockquote><p>该数据集中的HR和LR是不是对齐的，这也是后面给出的算法所解决的问题之一，非常重要。</p></blockquote><h1 id="Contextual-Bilateral-Loss"><a href="#Contextual-Bilateral-Loss" class="headerlink" title="Contextual Bilateral Loss"></a>Contextual Bilateral Loss</h1><h2 id="Contextual-Loss"><a href="#Contextual-Loss" class="headerlink" title="Contextual Loss"></a>Contextual Loss</h2><p>CoBi Loss来自Contextual Loss。Contextual Loss的原文中的叙述为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/2.png" title="Optional title"></p><blockquote><p>Contextual Loss来自《The Contextual Loss for Image Transformation with Non-Aligned Data》，ECCV 2018。<br><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Roey_Mechrez_The_Contextual_Loss_ECCV_2018_paper.pdf">https://openaccess.thecvf.com/content_ECCV_2018/papers/Roey_Mechrez_The_Contextual_Loss_ECCV_2018_paper.pdf</a><br><a href="https://www.github.com/roimehrez/contextualLoss">https://www.github.com/roimehrez/contextualLoss</a></p></blockquote><blockquote><p>Contextual Loss的原文中是跟Perceptual Loss进行对比的。<br>Perceptual Loss来自《Perceptual Losses for Real-Time Style Transfer and Super-Resolution》，ECCV 2016。<br>SRGAN的Loss就是从这篇文章来的。</p></blockquote><p>本篇的Contextual Loss的公式：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/3.png" title="Optional title"></p><h2 id="CoBi-Loss"><a href="#CoBi-Loss" class="headerlink" title="CoBi Loss"></a>CoBi Loss</h2><p>作者用这个Contextual Loss去训练模型发现会出现很多artifacts。作者认为这是由于CX损失函数中不准确的特征匹配造成的。<br>受到保边滤波器的启发，作者将空间区域也加入到损失函数中：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/4.png" title="Optional title"></p><p>又借鉴了Perceptual Loss，引入VGG loss。本文最终的Loss为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/5.png" title="Optional title"></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><h2 id="实验步骤"><a href="#实验步骤" class="headerlink" title="实验步骤"></a>实验步骤</h2><p>使用来自SR-RAW的图像来训练一个4X模型和一个8X模型。<strong>采用了一个16层的ResNet架构，然后是log_2( N + 1) 上卷积层，其中N是缩放因子。</strong></p><blockquote><p>文章所使用的模型没有图和其他说明，只有这一句文字叙述。</p></blockquote><p>将SR-RAW中的500个序列按8:1:1的比例分割为训练、验证和测试集。</p><p>对于4X变焦模型，每个序列有3对数据对用于训练。对于8X变焦模型，每个序列有1对数据。</p><p>每对包含一个全分辨率（800万像素）Bayer mosaic图像及其相应的全分辨率光学放大RGB图像。随机从一个全分辨率的Bayer mosaic中裁剪64个patch作为训练的输入。</p><blockquote><p><a href="https://petapixel.com/2017/03/03/x-trans-vs-bayer-sensors-fantastic-claims-test/">https://petapixel.com/2017/03/03/x-trans-vs-bayer-sensors-fantastic-claims-test/</a></p></blockquote><p>选择了几种具有代表性的超分辨方法进行比较：SRGAN、SRResnet、LapSRN、Johnson等人提出的Perceptual Loss的模型以及ESRGAN。</p><p>使用公共的预训练模型。首先采用原文献中的标准设置尝试在SR-RAW上微调模型，对比模型的下采样方式是双三次。但与未经微调的预训练模型相比，平均性能差异不大(SSIM &lt; 0.04, PSNR &lt; 0.05, lpip &lt; 0.025)，所以原文直接采用了没有微调的原模型。对于没有预先训练模型的方法，在SR-RAW上从零开始训练它们的模型。</p><h2 id="定量分析"><a href="#定量分析" class="headerlink" title="定量分析"></a>定量分析</h2><h3 id="不同模型使用SR-RAW进行训练"><a href="#不同模型使用SR-RAW进行训练" class="headerlink" title="不同模型使用SR-RAW进行训练"></a>不同模型使用SR-RAW进行训练</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/6.png" title="Optional title"></p><p>结果很明显，现有的超分模型在这个数据集上的表现比较差。</p><h3 id="不同训练策略"><a href="#不同训练策略" class="headerlink" title="不同训练策略"></a>不同训练策略</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/7.png" title="Optional title"></p><p>依旧证明了本文提出的模型和数据是比较契合的。</p><blockquote><p>Ours-png：为了进行比较，使用经过8位处理的RGB图像来训练模型的副本(our-png)，以评估拥有真实原始传感器数据的好处。与5.1节中描述的合成设置不同，没有使用向下采样的RGB图像作为输入，而是使用较短焦距拍摄的RGB图像作为输入。用较长焦距拍摄的RGB图像作为地面真实值。</p></blockquote><blockquote><p>Ours-syn-raw：测试合成的原始数据是否可以代替训练的感知真实数据。采用Gharbi等人描述的标准传感器合成模型[9]代替真实的传感器数据进行训练，从8位RGB图像中生成合成的Bayer马赛克。简而言之，我们根据白平衡、gammacorrected sRGB图像的Bayer镶嵌模式，每个像素保留一个颜色通道，并引入随机方差高斯噪声。在这些合成传感器数据上训练我们的模型的一个副本，并在经过白平衡和伽玛校正的真实传感器数据上进行测试。</p></blockquote><h2 id="定性分析"><a href="#定性分析" class="headerlink" title="定性分析"></a>定性分析</h2><p>视觉效果：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/8.png" title="Optional title"></p><p>可以看到，给出的图示在色块交界区域的效果确实不错。</p><p>以及在Amazon Mechanical Turk上进行感知实验来评估生成图像的感知质量。有50人参加了这个测试：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-ZoomToLearn/9.png" title="Optional title"></p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><ul><li><p>使用RAW进行超分，相比经过ISP处理的JPG，RAW有丰富的信息。</p></li><li><p>提出CoBi Loss解决不对齐的问题。</p></li></ul><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>文章：<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8954193&amp;tag=1">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8954193&amp;tag=1</a></p></li><li><p>补充材料：<a href="https://ceciliavision.github.io/files/Zoom_Learn_Supplementary_Material_CVPR2019.pdf">https://ceciliavision.github.io/files/Zoom_Learn_Supplementary_Material_CVPR2019.pdf</a></p></li></ul><blockquote><p>补充材料还挺长，11页pdf。</p></blockquote><ul><li>code：<a href="https://github.com/ceciliavision/zoom-learn-zoom">https://github.com/ceciliavision/zoom-learn-zoom</a></li></ul><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>看上一篇的时候，看到第三部分的那个数据集，顺手去看了一下那个reference，然后顺手找到了这篇文章。看完之后觉得还是很有意义的。</p><p>说句实话这个方法初看下来也算那种大神级别的了。感觉以后用不对齐做超分好像也不是难事情？也许还是我们思路不够开阔。还是要敢想啊。</p><p>但是再一细看，局限性感觉也是很大。</p><p>首先一点就是第一部分针对不同模型的对比实验，感觉不是很公平。毕竟是从Loss方面做的改变，直接就比PSNR和SSIM，感觉有点流氓，意义不大。</p><p>第二个就是文章中也没特别解释他们用的网络模型和原因，读着有点难受。</p><p>第三个就是感觉不对齐这个问题还是太广了，这个好像只是水平偏移，如果偏移的更多样的话，不知道会有怎么样的解决思路。感觉这一点能用到光学显微镜成像的那里。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;变焦学习，学习变焦&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;CVPR 2019&lt;/p&gt;
&lt;/blockquo</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>能够提高个人体验的计算机软硬件问题合集</title>
    <link href="http://example.com/2021/04/03/%E8%83%BD%E5%A4%9F%E6%8F%90%E9%AB%98%E4%B8%AA%E4%BA%BA%E4%BD%93%E9%AA%8C%E7%9A%84%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/"/>
    <id>http://example.com/2021/04/03/%E8%83%BD%E5%A4%9F%E6%8F%90%E9%AB%98%E4%B8%AA%E4%BA%BA%E4%BD%93%E9%AA%8C%E7%9A%84%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/</id>
    <published>2021-04-03T11:20:56.000Z</published>
    <updated>2021-04-14T09:42:24.361Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>经验内容，欢迎转载。</p></blockquote><hr><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>一个新坑。这个坑实际上好久之前就想开了（没错，就是在根目录上给777权限的那次）。</p><p>自诩是一个电脑维修铺子的老板，软硬件什么的也是天天都在摸索和学习。科班只教了如何搞软件，现在大家也都在一股脑的搞软件。但是碰到了硬件问题，好像大家都没那么感兴趣？只是作为一个喜欢搞机和经常胡乱搞机的人，常常会碰到这类问题。从硬件购买安装到操作系统层面的软件配置问题，每一次遇到了，每一次去解决，都是新的尝试和学习。恰好这段时间从zkyzdhs那边搞到了一台主机，有条件搞机了，因此借这个机会开新坑。</p><p>这里将记录在搞机过程中遇到的一切非专业但有意义的问题。随时更新，标题间几乎无关联。</p><p>实际上，遇到问题之后，基本的解决方法就是百度，然后选择靠谱的方法自己进行测试。唯一不同的是，我将会在这里更加详细的记录细节，尤其是硬件的型号或者时间或者是具体的软硬件版本这类的内容，毕竟自己也是一个怕出错的人，我要是查百度，对于具体的细节查不到，是会非常恼火的。</p><hr><h1 id="windosw-10基础上安装Ubuntu-18-04"><a href="#windosw-10基础上安装Ubuntu-18-04" class="headerlink" title="windosw 10基础上安装Ubuntu 18.04"></a>windosw 10基础上安装Ubuntu 18.04</h1><blockquote><p>2021.4.3更新。</p></blockquote><blockquote><p>新搞来的主机，原本是带着Ubuntu 18的。结果我在装win 10双系统的时候，没看清硬盘名字，直接用win 10给覆盖上了。Ubuntu肯定是得有，所以得重新装一下，此为问题背景。以及学校的校园网，有线连接不支持Ubuntu，机箱内也无无线网卡，需要解决这个问题。</p></blockquote><p>直接上图：</p><h2 id="win10上的操作"><a href="#win10上的操作" class="headerlink" title="win10上的操作"></a>win10上的操作</h2><p>主要就是使用Rufus做系统盘。下载好iso，u盘准备好，按照提示做。</p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/1.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/2.png" title="Optional title"></p><p>出现这个是因为用的那个u盘之前被我分了两个盘，在做系统盘的时候是要合并成一个的。无脑点确定就完事了。</p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/3.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/4.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/5.png" title="Optional title"></p><p>以及看一下这个机箱的硬盘情况。就是手抖把win 10装到那个固态上了。另一块机械4个T，我也不知道当初他们买的时候想的是什么。主板上弄俩固态不香么？</p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/6.png" title="Optional title"></p><h2 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h2><p>因为要手动分区，所以操作比较多。</p><p>但是实际上手动分区之后，从bios启动ubuntu，直接选硬盘启动不了。装是装成功了的，重启了之后找不到启动位置这我也没办法。手动分区的过程在下面，是没有问题的。</p><p>以及如果直接选从整个硬盘安装的的话，是没有下面这么多步骤的，直接就进那个设置用户名和密码的页面的。</p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/7.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/8.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/9.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/10.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/11.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/12.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/13.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/14.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/15.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/16.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/17.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/18.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/19.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/20.png" title="Optional title"></p><p><img src="/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E7%A1%AC%E4%BB%B6%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/21.png" title="Optional title"></p><p>这部分过程是手机拍的，糊了点儿，凑合看。</p><p>这部分的参考：</p><blockquote><p><a href="https://blog.csdn.net/fanxueya1322/article/details/90205143">https://blog.csdn.net/fanxueya1322/article/details/90205143</a><br><a href="https://rufus.ie/zh/">https://rufus.ie/zh/</a></p></blockquote><p><strong>以及不联网装的ubuntu连make和gcc都没有，血泪教训。</strong>后面还得解决这个问题，不过装系统肯定不叫事儿，随便装就行了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;经验内容，欢迎转载。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;一个新坑。这个坑实际上好久之前就想开了（</summary>
      
    
    
    
    
    <category term="闲谈" scheme="http://example.com/tags/%E9%97%B2%E8%B0%88/"/>
    
    <category term="操作系统" scheme="http://example.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：Degradation Model Learning for Real-World Single Image Super-resolution</title>
    <link href="http://example.com/2021/04/03/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ADegradation-Model-Learning-for-Real-World-Single-Image-Super-resolution/"/>
    <id>http://example.com/2021/04/03/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ADegradation-Model-Learning-for-Real-World-Single-Image-Super-resolution/</id>
    <published>2021-04-03T11:19:38.000Z</published>
    <updated>2021-04-14T07:23:44.104Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>2020,ACCV</p></blockquote><blockquote><p>真实世界单幅图像超分辨率退化模型学习</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>在做超分问题的时候，数据集是非常重要的一部分。</p><p><strong>目前绝大多数图像或视频超分辨率重建工作，都假定了LR是由对应HR经过某种固定的已知退化所得（超分问题的一般流程，《基础：图像超分辨率》有提）。然而事实证明，当测试数据的退化设置与训练阶段存在差异时，超分辨率重建的性能会显著下降。</strong></p><blockquote><p>举例解释：现在一组HR图像，首先通过双三次差值（绝大部分文章所用到的数据集都是这个选择）将这组图像下采样，得到对应的LR图像。但实际上真实的HR到LR的退化过程是非常复杂的，是一个非线性问题。文章的意思是这种普遍存在的常规做法会明显的影响SR模型的性能。</p></blockquote><p>为了解决上述问题，本文从真实数据集中学习一个真实的退化模型，并使用所学习的退化模型来合成真实的HR-LR图像对。</p><blockquote><p>本文的重点在从HR图像到LR图像的这个过程。</p></blockquote><blockquote><p>It is well-known that the single image super-resolution (SISR) models trained on those synthetic datasets, where a low-resolution (LR) image is generated by applying a simple degradation operator (e.g., bicubic downsampling) to its high-resolution (HR) counterpart, have limited generalization capability on real-world LR images, whose degradation process is much more complex. Several real-world SISR datasets have been constructed to reduce this gap; however, their scale is relatively small due to laborious and costly data collection process. To remedy this issue, we propose to learn a realistic degradation model from the existing real-world datasets, and use the learned degradation model to synthesize realistic HR-LR image pairs.</p></blockquote><h1 id="超分数据集构建"><a href="#超分数据集构建" class="headerlink" title="超分数据集构建"></a>超分数据集构建</h1><p><strong>上述问题又来源于超分问题的数据集构建问题，简言之就是LR到底是怎么来的。</strong></p><p>文中所提到的，超分数据集的构建和合成问题的大致方法有如下几种：</p><ul><li>通过设置模糊核，直接由HR图像生成LR图像。这是最常见的方法。会产生的问题是模糊核的可解释性差。</li></ul><blockquote><p>Most of the existing CNN based SISR models are trained on synthetic HR-LR image pairs, which are generated by applying a simple degradation model (e.g., bicubic downsampling) to the HR images [14, 15, 18, 19, 21, 23, 24]. However, the authentic HR to LR image degradation process is much more complicated than these simple uniform downsample operators. As a result, the SISR networks trained on such synthetic datasets have low generalization capability to real-world LR images, largely limiting their value in practical applications.</p></blockquote><ul><li>进行真实的pair的数据对的构建，通常是通过相机捕捉相同场景，变量则设置为相机本身的某些参数。这种方法产生的问题是：构建数据集的成本太高，以至于该类数据集的容量都不会太大。以及在进行拍照的时候，物理条件的限制因素太多，包括但不限于天气、光照、场景多样性等。</li></ul><blockquote><p>Very recently, researchers have started to construct real-world datasets by using digital cam- eras to capture images of the same scene under different focal lengths [25–27]……However, constructing such datasets of real-world HR-LR pairs is laborious and costly, and the existing datasets of this kind [25–27] are all limited in number of image pairs, diversity of scenes and illuminating conditions.</p></blockquote><ul><li>从unpair的HR和LR中学习图像的退化过程，并且将学习到的退化过程用到SR的流程之中。一般选取GAN来完成这件事情。会出现的问题是，这种方法的训练过程是非常困难的，最终的结果可能不会收敛，以及使用网络来完成退化过程，忽略了图片的一些先验信息，导致了可解释性差。</li></ul><blockquote><p>While constructing real-world datasets of HR-LR image pairs, researchers have also proposed to learn the image degradation process from unpaired HR and LR images, and use the learned degradation model to generate HR-LR image pairs for SISR model learning [28–31]. All these methods employ the Generative Adversarial Network (GAN) [32] to learn the degradation process by differentiating the distribution between generated LR and real LR images. Unfortunately, training such a GAN with unpaired data is very difficult and may not converge to the desired result. Moreover, using a network to model the degradation from HR to LR images makes it hard to interpret the degradation process, ignoring some prior knowledge on the image formation.</p></blockquote><h1 id="网络模型"><a href="#网络模型" class="headerlink" title="网络模型"></a>网络模型</h1><h2 id="模型叙述"><a href="#模型叙述" class="headerlink" title="模型叙述"></a>模型叙述</h2><p>目前被广泛认可的从HR到LR的泛用退化模型可进行如下表示。其中，“*”是卷积算子，k是退化核，↓d是下采样算子，v是加性随机噪声，L和H分别代表低分和高分图像。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/1.png" title="Optional title"></p><p>上面提到，大多数现有工作对HR使用的是双三次下采样以采集LR图像，即这些工作都假设退化核k在整个图像上是均匀的，即空间不变的。</p><p>而在现实世界的SISR问题中，退化核要复杂得多，与场景的深度和局部内容相关。<strong>因此，退化核是典型的非均匀和空间变异的。</strong>对于图像上的每个像素点(i,j)，应该有相应的模糊核和噪声。</p><p>经过上述改动后，从HR到LR的空间变化的图像退化可以表示如下。其中，H(i,j)表示以(i,j)为中心的局部图像窗口，其大小与核k(i,j)相同，“⊙”是内积算子。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/2.png" title="Optional title"></p><p><strong>从上式可以看出，对真实世界的图像退化过程建模的关键是如何预测像素级退化核k(i,j)。</strong></p><p>一个直观的想法是直接通过真实HR-LR对作为监督去学习退化核参数。但这种做法的问题是需要求解的参数量太大，且解空间非常大，在有限的数据量条件下易出现过拟合的问题。</p><p>文章提出，由于光学系统成像的原理限制，退化过程普遍可以用一个钟形函数（bell-shaped smooth functions）进行描述。所以可以进一步缩小解空间范围，即可以<strong>通过一系列基础退化核的线性组合实现对任意退化核的描述</strong>。综上所述，pixel-wise的退化核可以进行如下表示，其中Φm表示第m个基础退化核，C表示系数矩阵。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/3.png" title="Optional title"></p><p>上式将原本复杂的退化过程的解空间约束在一个较小的子控件中，使得在数据量不大的条件下更容易被学习。</p><h2 id="Degradation-Model-Learning"><a href="#Degradation-Model-Learning" class="headerlink" title="Degradation Model Learning"></a>Degradation Model Learning</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/4.png" title="Optional title"></p><p>退化模型学习（DML）方法如上所示。</p><p>整个网络以H作为输入，学习具有参数Θ的CNN F来预测权重，即C＝F(H|Θ)，其中C是权重向量C(i,j)的集合。</p><p>还学习了基核φm，从而可以根据式（3）预测核k(i,j)。将预测的退化核应用于HR图像H以输出预测的LR图像，用ˆL表示。</p><p>上图中的F为基于编解码结构的权值预测网络。以HR图像作为输入，并在每个位置输出一个权重向量。为了获得大的接收野，使用最大池层进行特征下采样，并使用双线性上采样层来提高特征分辨率和保证像素级的输出。采用3×3滤波器的卷积层，用ReLU作为激活函数。为了输出每像素的权值，在最后一个卷积层之后使用sigmoid函数进行归一化。通过SGD或ADAM优化器可以很容易地对整个网络进行优化。</p><h2 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h2><p>假设有N对HR-LR训练图像，则目标函数为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/5.png" title="Optional title"></p><h2 id="SISR-Model-Learning"><a href="#SISR-Model-Learning" class="headerlink" title="SISR Model Learning"></a>SISR Model Learning</h2><p>为了进一步缩小合成与真实的差距LR图像，根据式（1）中描述的LR图像形成过程向合成的LR图像ˆL添加随机噪声。</p><p>设置为加性高斯白噪声（AWGN），并根据经验将噪声级设置为σ=5。</p><blockquote><p>To address this issue and further diminish the gap between synthetic and real<br>LR images, we add random noise to the synthesized LR image ˆILn according to the LR image formulation process described in Eq. (1). Without additional in- formation on the imaging system (e.g., sensors, lens), we simply assume additive white Gaussian noise (AWGN) and empirically set the noise level as σ = 5.</p></blockquote><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>两部分实验。</p><h2 id="实验设置和数据集"><a href="#实验设置和数据集" class="headerlink" title="实验设置和数据集"></a>实验设置和数据集</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>验证DML在退化过程学习和SISR模型训练中的性能，一共用到了三个数据集。</p><ul><li><p><strong>RealSR（v2、pair）</strong>：包含由两台相机采集的559个场景的对齐HR-LR图像对，具有3个缩放因子：×2、×3和×4。RealSR的划分：训练集/测试集=459/100。<strong>这个数据集的主要用来训练DML退化模型，并使用测试部分来定量评估DML的性能及其在实际SISR中的应用。</strong></p></li><li><p><strong>Flickr2K+互联网图片（unpair）</strong>：<strong>DML学习完成后，这部分数据主要用来监测这个退化学习模型，即通过HR生成HR-LR对。</strong>该HR数据集总共包含3150张图片，其中Flickr2k含有2650张不同场景的高质量图像，分辨率大多为1500×2000。互联网图像是从[39]下载了500张4K分辨率的原始图像。然后将PhotoShop CameraRaw工具应用于这些原始图像，获得4K分辨率的未压缩高质量RGB图像。</p></li><li><p><strong>SR-RGB</strong>：该数据集由真实世界的LR图像和通过DSLR光学变焦获得的未对齐HR图像组成。由于HR和LR图像没有对齐，因此无法计算PSNR、SSIM、LPIPS度量，但HR图像可以用作视觉比较的参考。<strong>这个数据集验证DML对现实世界SISR的有效性。</strong></p></li></ul><blockquote><p>SR-RGB来自《Zoom to learn, learn to zoom》，下篇会提到</p></blockquote><h3 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h3><p>Y通道训练、DA=左右 or 上下翻转、Adam、learning rate=1e−4、epoch=100K or 300K、batch size=16 or 2、进行测试的网络=VDSR + RCAN。</p><blockquote><p>对于所有缩放比例×2、×3和×4，我们将要学习的基核的大小设置为15×15。对基核进行随机初始化，然后归一化为求和1，以便进一步更新。使用Xavier初始值设定项初始化权重预测网络。在DML和SISR网络的训练中，我们将RGB图像转换成YCbCr颜色空间，在Y通道上进行训练或测试。将图像裁剪成192×192块，用于所有模型的训练。左右翻转和上下翻转用于数据扩充。使用带有默认参数设置（β1=0.9，β2=0.999）的Adam优化器作为优化器。我们使用1e−4的固定学习率分别为100K和300K次迭代来训练DML和SISR模型。在DML训练中，批大小设置为16。对于SISR模型，我们采用了两种具有代表性的网络结构：VDSR和RCAN。我们用100个卷积层来实现RCAN。批大小分别设置为16和2，用于训练VDSR和RCAN模型。</p></blockquote><blockquote><p>RCAN来自《Image Super-Resolution Using Very Deep Residual Channel Attention Networks》，残差通道注意力网络</p></blockquote><h2 id="DML"><a href="#DML" class="headerlink" title="DML"></a>DML</h2><h3 id="DML中基核数的选择"><a href="#DML中基核数的选择" class="headerlink" title="DML中基核数的选择"></a>DML中基核数的选择</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/6.png" title="Optional title"></p><p>研究DML中基核的合适数目。</p><p>利用RealSR数据集的训练部分，学习了N=4、8、16基核及其相关的权值预测网络。然后将学习到的模型应用于RealSR数据集测试部分的HR图像，生成LR图像。PSNR、SSIM结果在表1中列出。</p><p><strong>从N=4到N=8，可以获得更好的LR生成性能，N=16基核的性能略差于N=8。</strong></p><blockquote><p>N=8，表一中第三行，红色的数据。</p></blockquote><h3 id="有8个基核的DML在不同缩放倍数下的效果"><a href="#有8个基核的DML在不同缩放倍数下的效果" class="headerlink" title="有8个基核的DML在不同缩放倍数下的效果"></a>有8个基核的DML在不同缩放倍数下的效果</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/7.png" title="Optional title"></p><p>随着缩放因子从2增加到4，核变得更加分散和复杂，这符合我们对图像退化过程的共同认识。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/8.png" title="Optional title"></p><p>用DML方法对基核的预测组合权重进行×2的可视化。最左边的图像是输入的HR图像，右侧8图像可视化对应于每个基核的预测权重。<strong>亮度越高，权重越高。</strong>通过实例分析，我们的权重预测网络可以根据场景内容和局部结构自适应地分配不同的权重。</p><blockquote><p>The brighter intensity denotes larger weight.</p></blockquote><h3 id="与其他两种LR图像生成模型的对比"><a href="#与其他两种LR图像生成模型的对比" class="headerlink" title="与其他两种LR图像生成模型的对比"></a>与其他两种LR图像生成模型的对比</h3><p>一种是学习CNN，直接将HR图像映射到LR图像，表示为DirectNet。另一种是学习内核预测网络[42]，以预测退化内核，表示为DirectKPN。</p><p>加上DML，这三种方法，都在RealSR的训练集上进行训练，并在RealSR测试集上进行测试。</p><p>比较这三种方法在LR图像生成中的性能。<strong>DML在所有三个缩放倍数上都始终优于其他两个，以及DirectKPN的性能略优于DirectNet。这表明，通过考虑图像退化过程，通过学习预测像素核可以获得比直接预测LR图像像素更好的LR生成性能。</strong></p><h3 id="三种LR图像生成模型在SISR任务下的效果"><a href="#三种LR图像生成模型在SISR任务下的效果" class="headerlink" title="三种LR图像生成模型在SISR任务下的效果"></a>三种LR图像生成模型在SISR任务下的效果</h3><p>将上述三种LR图像生成模型应用于采集的HR图像数据集，使用Flickr2K+互联网图片的那个数据集，生成了三组，每组3150对HR-LR图像。在这些HR-LR对中添加了小AWGN（additive white Gaussian noise），并训练了三个VDSR模型。最后，将这三种VDSR模型应用于RealSR数据集测试部分的LR图像，得到超分辨率的HR图像。</p><blockquote><p>这部分的实验过程：<br>（1）LR生成模型+HR图像 = 3组LR-HR图像对<br>（2）LR-HR图像对添加AWGN<br>（3）根据3组LR-HR图像对训练3个VDSR<br>（4）用VDSR测试RealSR的459张测试图片的LR并计算指标</p></blockquote><p>结果表明，在DML方法生成的HR-LR图片对上训练的VDSR网络的性能，比DirectNet或DirectKPN生成的对上训练的VDSR网络好（PSNR约为0.15dB）。验证了DML在提高SISR性能方面优于DirectNet和DirectKPN。</p><h2 id="DML对SISR任务的影响"><a href="#DML对SISR任务的影响" class="headerlink" title="DML对SISR任务的影响"></a>DML对SISR任务的影响</h2><h3 id="测试集"><a href="#测试集" class="headerlink" title="测试集"></a>测试集</h3><p><strong>首先做了5个不同的训练数据集</strong>：only RealSR、only Syn DSGAN、only Syn DML、RealSR+DSGAN、RealSR+Syn DML。</p><blockquote><p>RealSR：就是上面提到的RealSR以及划分。<br>Syn DSGAN：一种基于GAN的HR-LR对合成方法，称为DSGAN，来自《Frequency separation for real-world super- resolution》，2019。<br>Syn DML：DML在Flickr2K+互联网图片上的合成，共3150对。</p></blockquote><h3 id="以RealSR为测试集做定量比较"><a href="#以RealSR为测试集做定量比较" class="headerlink" title="以RealSR为测试集做定量比较"></a>以RealSR为测试集做定量比较</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/9.png" title="Optional title"></p><p><strong>这部分实验是：用VDSR（20层）和RCAN（100层）对5个不同的数据集做了10个模型，计算并对比指标。</strong></p><p>结论：</p><ul><li>LPIPS（Syn-DML上的VDSR/RCAN，任意情况下） &lt; LPIPS（RealSR），<strong>使用这个指标证明DML的效果。</strong></li></ul><blockquote><p>LPIPS是一个感知指数，衡量图像的感知质量（越低越好）。</p></blockquote><ul><li><p>PSNR/SSIM（Syn-DML上的VDSR/RCAN，任意情况下） = PSNR/SSIM（RealSR），因为后者的训练集合和测试集合都是本身，<strong>换句话DML在测试集劣势的情况下得到的结果却没有劣势</strong>。</p></li><li><p>Syn-DML &gt; Syn-DSGAN，证明DML比DSGAN优越。</p></li><li><p>RealSR+合成 &gt; only</p></li></ul><h3 id="以RealSR为测试集做定性比较"><a href="#以RealSR为测试集做定性比较" class="headerlink" title="以RealSR为测试集做定性比较"></a>以RealSR为测试集做定性比较</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/10.png" title="Optional title"></p><p>在Syn-DML和RealSR+Syn-DML上训练的模型比仅使用RealSR数据集训练的模型能有效地恢复更多的图像细节和更令人愉快的感知质量。<strong>特别是RealSR+Syn-DML上训练的模型达到了最佳的视觉质量。</strong></p><h3 id="以SR-RGB为测试集做定性比较"><a href="#以SR-RGB为测试集做定性比较" class="headerlink" title="以SR-RGB为测试集做定性比较"></a>以SR-RGB为测试集做定性比较</h3><p>测试数据集用了两个，一个是RealSR，另一个是SR-RGB。</p><p>SR-RGB由许多LR图像及其未对齐的HR对应图像组成，<strong>做不了指标评价，只能做定性的视觉评价，但原文中提到这部分实际上更看重</strong>。使用该数据集的原因：由于SR-RGB数据集是独立于RealSR数据集，通过使用不同的摄像机和镜头构建的，结果可以更公平地证明SISR模型对真实场景的泛化能力。</p><blockquote><p>Another is the SR-RGB dataset , which consists of many LR images and their unaligned HR counterparts. Qualitative visual comparisons can be made on it for the different SISR models. Wed like to stress that the testing on the second dataset is more important (though qualitative) because it is independent of the RealSR dataset, part of whose samples are used to train the DML and VDSR/RCAN models. The testing results on the SR-RGB dataset can more faithfully reflect the generalization capability of competing SISR models than those on the RealSR dataset.</p></blockquote><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-Degradation/11.png" title="Optional title"></p><p>在RealSR数据集上训练的模型只能适度地恢复一些细节。在Syn Dsgan上训练的模型会产生严重的伪影。在Syn-DML上训练的SISR模型可以得到更为精细的细节，从而获得令人满意的结果。特别是，<strong>在RealSR+Syn-DML上训练的模型提供了最佳的超分辨率HR图像感知质量。</strong></p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>DML能够有效地提高SISR模型在实际应用中的泛化性能。</p><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>思路挺新，感觉方法论做烂了之后，大家都开始从超分的最基础流程开始下手写东西了，也许会成为以后的小热点。</p><p>这篇的实验做的是又细又好，看着很舒服。尤其是针对训练集和测试集的设置方面，确实比较亮眼。</p><p>但是第一部分实验的细节又没有说的很清楚，就是Table 1的那部分，导致了对不上哪部分是哪部分的数据。</p><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>code：没提，应该是没公开。</p></li><li><p>文章：<a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Xiao_Degradation_Model_Learning_for_Real-World_Single_Image_Super-resolution_ACCV_2020_paper.pdf">https://openaccess.thecvf.com/content/ACCV2020/papers/Xiao_Degradation_Model_Learning_for_Real-World_Single_Image_Super-resolution_ACCV_2020_paper.pdf</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;2020,ACCV&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;真实世界单幅图像超分辨率退化模型学习&lt;/p&gt;
&lt;</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：Evaluation and development of deep neural networks for image super-resolution in optical microscopy</title>
    <link href="http://example.com/2021/01/30/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AEvaluation-and-development-of-deep-neural-networks-for-image-super-resolution-in-optical-microscopy/"/>
    <id>http://example.com/2021/01/30/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AEvaluation-and-development-of-deep-neural-networks-for-image-super-resolution-in-optical-microscopy/</id>
    <published>2021-01-30T11:35:46.000Z</published>
    <updated>2021-04-03T11:34:16.555Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><blockquote><p>2021，加油。</p></blockquote><hr><blockquote><p>2021.1.21 Nature</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>文章做了三项工作：</p><ul><li><p>数据集BioSR：近年来，基于深度学习的SISR模型被应用于提高科学显微镜图像的分辨率。与SISR在增强宏观真实照片纹理方面的应用相比，用于科学分析的超分辨率显微图像对推断出的纳米尺度结构提出了更高的精度和可量化性要求。然而，基于深度学习的超分辨率（DLSR）图像所传递的信息在多大程度上可以用于定量分析，以及在什么条件下DLSR方法优于传统的超分辨率显微镜，仍然是个未知数。在这里，使用自制的多模结构照明显微镜（SIM）系统，该系统集成了全内反射荧光（TIRF-SIM）、掠入射（GI-SIM）和非线性SIM（方法），在输入LR图像的信噪比（SNR）水平的宽范围内获得匹配良好的LR–SR图像对，观察到的生物结构的复杂性和期望的放大因子。这个数据集被命名为BioSR。</p></li><li><p>DFCAN及其衍生的生成对抗训练策略DFGAN：DLSR网络的训练可以看作是一个提取高维特征的过程，这些特征连接了LR和SR图像空间。众所周知，输入LR图像的功率谱被限制在衍射限制频率以下，因此推测，利用傅里叶域中不同特征的频率内容差异，而不是空间域中的结构差异，可能使DLSR网络能够学习高频信息的分层表示更加精确和有效。受深度剩余通道注意网络（RCAN）中空间域通道注意机制的启发，开发了DFCAN及其派生的生成性对抗网络（GAN）训练策略，称为DFGAN。</p></li><li><p>证明DFCAN的Fourier域聚焦能够在低信噪比条件下实现SIM图像的鲁棒重建。在多色活体细胞成像实验中，证明df可以在10倍的时间内获得与SIM相当的图像质量，揭示了线粒体嵴和类核细胞的详细结构以及细胞器和细胞骨架的相互作用动力学。</p></li></ul><blockquote><p>Deep neural networks have enabled astonishing transformations from low-resolution (LR) to super-resolved images. However, whether, and under what imaging conditions, such deep-learning models outperform super-resolution (SR) microscopy is poorly explored. Here, using multimodality structured illumination microscopy (SIM), we first provide an extensive dataset of LR–SR image pairs and evaluate the deep-learning SR models in terms of structural complexity, signal-to-noise ratio and upscaling fac- tor. Second, we devise the deep Fourier channel attention network (DFCAN), which leverages the frequency content difference across distinct features to learn precise hierarchical representations of high-frequency information about diverse biological structures. Third, we show that DFCAN’s Fourier domain focalization enables robust reconstruction of SIM images under low signal-to-noise ratio conditions. We demonstrate that DFCAN achieves comparable image quality to SIM over a tenfold longer duration in multicolor live-cell imaging experiments, which reveal the detailed structures of mitochondrial cristae and nucleoids and the interaction dynamics of organelles and cytoskeleton.</p></blockquote><h1 id="BioSR"><a href="#BioSR" class="headerlink" title="BioSR"></a>BioSR</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>使用multimodality structured illumination microscopy（SIM，多模结构照明显微镜），获取了如下类型的数据集，这些数据集代表了结构复杂性的增加：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/1.jpg" title="Optional title"></p><ul><li><p>clathrin-coated pits（CCPs，网格蛋白包被坑/网格蛋白小窝）</p></li><li><p>endoplasmic reticulum（ER，内质网）</p></li><li><p>microtubules（MTs，微管）</p></li><li><p>F-actin filaments（F-肌动蛋白丝）</p></li></ul><p>对于每种类型的样本，在10个不断上升的激发光强度（excitation light intensity）水平上获得了大约50组原始SIM图像。且在最高激发水平下，确保所有原始图像的SNR（signal-to-noise ratio，信噪比）足够高，以重建高质量的SIM图像。</p><p>每一组原始SIM图像被平均化为WF（diffraction-limited wide-field，衍射限制宽场）图像。</p><p><strong>WF图像用作DLSR网络的输入LR图像，而SIM图像用作评估DLSR方法在特定成像条件下是否优于传统SR显微镜的参考。</strong></p><blockquote><p>WF图像是如何做出来的？</p></blockquote><blockquote><p>SIM的结构和工作原理在Method部分给出，比较专业。</p><blockquote><p>将405 nm（LBX-405-300，Oxxius）、488 nm（Genesis MX SLM，相干）和560 nm（2RU-VFL-P-500-560，MPB通信）的三束激光组合在一起，然后通过声光可调谐滤波器（AOTF，AOTFnC-400.650，AA Quanta Tech）。在特定的成像模式下，AOTF可以灵活地控制所需激光束的曝光顺序和曝光时间。<br>将9幅TIRF-SIM或GI-SIM原始图像和25幅非线性SIM原始图像分别重建为分辨率提高2倍和3倍的SR图像。为了减小中低荧光强度下SIM图像的重建伪影，采用截止频率与重建光学传递函数（OTF）边界匹配的高斯切趾函数（Gaussian apodization function）来抑制高频噪声。</p></blockquote></blockquote><h2 id="采集数据细节"><a href="#采集数据细节" class="headerlink" title="采集数据细节"></a>采集数据细节</h2><p>对于每种类型的标本和每种成像方式，从至少50个不同的ROI（regions-of-interest，感兴趣区域）获取原始数据集，其中35个ROI的数据集用于训练，而其他15个ROI的数据集用于生成评估矩阵。</p><blockquote><p>数据分布。</p></blockquote><p>对于每个感兴趣区，获得了9组N phase × M orientation的原始图像，曝光时间不变，但激发光强度增加，其中N和M分别为TIRF-SIM和GI-SIM的3组和非线性SIM的5组。</p><p>将每组N×M原始图像平均为衍射受限的WF图像，然后对其进行掩模（masked）以计算其每像素的平均光子计数。</p><p>将不同荧光水平的原始SIM图像和WF图像作为DLSR网络的输入LR图像。同时，将每组N×M的原始图像重建成与相应WF图像具有相同荧光水平的SIM图像，作为评价该荧光水平下DLSR图像质量的参考。</p><blockquote><p>DLSR的输入和输出。</p></blockquote><p>此外，在相同的ROI中，最终提高了激发强度和曝光时间（通常为120W 平方cm 10ms）以达到平均光子数&gt;1200的高荧光水平，并独立获得三组N×M原始图像。将得到的三幅超高信噪比的SIM图像平均为GT-SIM图像，保证了图像的高质量。</p><blockquote><p>在最高激发水平下，确保所有原始图像的SNR（signal-to-noise ratio，信噪比）足够高，以重建高质量的SIM图像。</p></blockquote><p>在转染后16-36小时将细胞固定以获得CCPs、MTs和F-肌动蛋白的数据。然而，发现目前的化学固定方法导致的内质网标记蛋白calnexin明显聚集，这显著改变了内质网的形态，因此从活细胞（live cells）中获得了内质网数据。</p><blockquote><p>内质网使用了live cells获取数据。</p></blockquote><p>仅使用肌动蛋白细胞骨架结构评估了3倍放大的DLSR成像性能。</p><blockquote><p>只有F-actin filaments做了3×，其他都是2×。</p></blockquote><h2 id="在BioSR上评估几种典型DLSR方法"><a href="#在BioSR上评估几种典型DLSR方法" class="headerlink" title="在BioSR上评估几种典型DLSR方法"></a>在BioSR上评估几种典型DLSR方法</h2><p>选择了四个具有代表性的DLSR模型，其中包括：<strong>SRCNN、EDSR、Pix2Pix、CM（cross-modality，跨模态）GAN。</strong></p><h3 id="不同荧光程度图像下SR方法的效果"><a href="#不同荧光程度图像下SR方法的效果" class="headerlink" title="不同荧光程度图像下SR方法的效果"></a>不同荧光程度图像下SR方法的效果</h3><p>对于所有的图像，文章以“荧光程度（fluorescence）”作为区分标准，将图像（或者一张图像中的不同部分）分为低荧光、中荧光和高荧光三种。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/2.png" title="Optional title"></p><blockquote><p>图1的a、b，分别是高、低荧光水平下的DLSR结果。</p></blockquote><p>文章计算了SISR图像和GT（SIM）的像素级绝对差（MAE，平均绝对误差）：<strong>中等或相对较高的荧光信号电平（这是活细胞SIM成像的典型值）足以允许常规SIM通常优于当前的DLSR方法。</strong>相比之下，在低荧光的成像条件下，SISR图像显示出比传统SIM图像更少的残余差异，因为数据驱动的DLSR方法通常有利于从生物结构中分离噪声。</p><blockquote><p>这里的意思就是说，在常规的活细胞SIM成像情况下，中等荧光水平以上的成像条件得到的图像已经足够用了，DLSR方法做出的结果并不特别接近GT。</p></blockquote><p>然而，如果期望的上缩放因子增加到3×，则SISR图像（图1a、b的底行）将包含太多错误或伪影，使得人们无法信任所推断的精细结构。</p><blockquote><p>3×的DLSR的结果会更差。</p></blockquote><h3 id="不同SR方法的性能"><a href="#不同SR方法的性能" class="headerlink" title="不同SR方法的性能"></a>不同SR方法的性能</h3><p>为了定量评估不同SR方法的性能，综合了标准化均方根误差（NRMSE）、多尺度结构相似性指数（MS-SSIM）和分辨率三个指标，以测量评估矩阵中每个成像条件下SR图像的质量。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/3.png" title="Optional title"></p><p>由于科学研究通常需要以牺牲荧光光子为代价的高真实性和可量化的SR图像，因此首先在评估矩阵的空间中确定了常规SIM的适用区域（图1c中用黑色虚线勾勒），其中常规SIM图像的三个矩阵与之接近GT。常规SIM的适用范围主要是中、高荧光区，符合活体细胞SIM成像实验的实际情况。</p><p>此外，利用这三个指标来评估四个DLSR模型，根据评估结果，确定了使用DLSR模型的优先区域，其中SISR图像的三种方法与传统SIM图像的方法相当或更好（图1c和补充说明4中以绿色列出）。显然，优先级区域越大，DLSR模型的性能越好。</p><p>然而，<strong>所有四种DLSR模型的优先区域都相对较小，并且集中在低荧光和低结构复杂性的区域，这些区域很少与常规SIM的适用区域重叠（图1c）</strong>。这些数据表明，硬件SR显微镜，例如SIM，比最新的DLSR模型更有效地利用增加的荧光来产生高保真SR信息，这可能阻碍DLSR模型在实际实验中的广泛应用。</p><h1 id="DFCAN和DFGAN"><a href="#DFCAN和DFGAN" class="headerlink" title="DFCAN和DFGAN"></a>DFCAN和DFGAN</h1><p>在学习了上述DLSR模型的特性之后，设计了DFCAN和DFGAN。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/4.png" title="Optional title"></p><h2 id="DFCAN"><a href="#DFCAN" class="headerlink" title="DFCAN"></a>DFCAN</h2><p>从卷积层和GELU开始。GELU的输出后面是四个相同的RG（residual groups，残差组），每个RG由四个FCAB（Fourier channel attention blocks傅立叶通道注意块）和一个跳跃连接组成。RG的操作表示为如下。x表示RG的输入特征映射。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/5.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/6.png" title="Optional title"></p><p>在每个FCAB中，特征图按如下方式按通道重新缩放：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/7.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/8.png" title="Optional title"></p><p>FFT（·）表示快速傅里叶变换，γ用于增强图像质量。</p><p>平均池化层，使得φ（y）的每个分量都可以解释为每个特征图的代表值。</p><p>WD和WU分别是下向和上向权值，这两种权值均由网络中的1×1卷积层实现。</p><p>f（·）和δ（·）分别是sigmoid激活函数和ReLU激活函数。它们共同产生了一种选通机制，可以自适应地计算最终的重缩放因子。</p><p>最后一个RG的输出被送入由GELU激活函数激活的卷积层。然后使用像素混洗层（Pixel Shuffle）、卷积层和sigmoid激活层将图像放大到与GT图像相同的大小（以适应推断的高频信息）。</p><p>输出为单色灰度SR图像。</p><p><strong>该网络的损失函数定义为MSE损耗和SSIM损耗的组合。</strong></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/9.png" title="Optional title"></p><p>其中，Y^定义为DFCAN的输出，Y定义为相应的GT，（w，h）定义为输出图像的像素大小。<strong>λ是用于平衡SSIM和MSE相对贡献的标量权重，文中大多数情况下设置为0.1。</strong></p><p><strong>MSE损失保证了像素级的精度，均衡了预测的动态范围，SSIM损失增强了输出的结构相似性。</strong></p><h2 id="DFGAN"><a href="#DFGAN" class="headerlink" title="DFGAN"></a>DFGAN</h2><p>DFGAN是基于cGAN（条件GAN）框架构建的。</p><p>在DFGAN中，更深层次的DFCAN充当G，G以低分辨率荧光图像作为输入，其输出是放大的SR图像。D基于传统的CNN架构，由12个卷积层组成，每个卷积层的输出由leaky因子α=0.1的LeakyReLU激活函数激活。</p><p>分别定义了G和D的损失函数。G的损失函数L_G/D由两项组成：SR误差，用于惩罚G输出和GT图像之间的差异；判别误差，与D计算的概率有关。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/10.png" title="Optional title"></p><p>其中X是输入的低分辨率图像，Y是SR目标图像。β、γ和λ是用来平衡相应项的标量加权因子，根据经验将其设置为β=0.1，γ=1，λ=0.1。</p><blockquote><p>最终结果对上面三个加权因子不是很敏感。</p></blockquote><p>D的损失函数定义为二元交叉熵：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/11.png" title="Optional title"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="FCA和SCA"><a href="#FCA和SCA" class="headerlink" title="FCA和SCA"></a>FCA和SCA</h3><p>DFCAN和DFGAN的设计利用了<strong>Fourier域中不同特征映射的功率谱特性。</strong></p><blockquote><p>何为“Fourier域中不同特征映射的功率谱特性”？</p></blockquote><p>在每个残差块中，FCA（Fourier channel attention，傅立叶信道注意）机制（图2a）使得网络能够根据其功率谱中包含的所有频率分量的综合贡献自适应地重新缩放每个特征图。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/12.png" title="Optional title"></p><p>相比之下，SCA（spatial channel attention，空间通道注意）机制仅利用特征映射的平均强度（相当于零频率（即直流分量））来计算重缩放因子。</p><blockquote><p>FCA的初衷是什么？凭空想出来的吗？还是有借鉴有曾今的论文做过这类理论？<br>为什么要和SCA进行比较？</p></blockquote><h4 id="FCA和SCA的性能比较"><a href="#FCA和SCA的性能比较" class="headerlink" title="FCA和SCA的性能比较"></a>FCA和SCA的性能比较</h4><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/13.png" title="Optional title"></p><p>为了比较FCA机制与SCA的性能，将DFCAN中的FCA替换为SCA，从而形成了深空间通道注意网络（DSCAN）。此外，通过去除DFCAN中的FCA，构造了一个改进的ResNet进行比较。</p><p><strong>结果显示，DFCAN比其他两个网络实现更快的收敛和更低的验证NRMSE。</strong></p><h4 id="FCA是否可以应用于其他网络结构？"><a href="#FCA是否可以应用于其他网络结构？" class="headerlink" title="FCA是否可以应用于其他网络结构？"></a>FCA是否可以应用于其他网络结构？</h4><p>为了进一步验证FCA机制是否可以普遍应用于其他类型的神经网络结构，在两个广泛使用的网络上实现了FCA：U-net18和DenseNet19。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/14.png" title="Optional title"></p><p>用三个下采样块和三个上采样块构造了U网络，并将两个机制引入到U网络中，生成的网络分别命名为U-net-FCA和U-net-SCA。</p><p>DenseNet由三个dense blocks和两个过渡层交织而成。每个dense blocks由8个密集连接的ReLU-Conv模块组成，每个过渡层由一个ReLU激活层和1×1卷积层组成。在每个dense blocks的末尾实现了FCA和SCA模块，分别生成DenseNet FCA和DenseNet SCA网络。</p><p>所有这些基于U-net或DenseNet的模型都是用管状结构的模拟数据进行训练的。<strong>结果显示，基于FCA的模型比基于SCA的相应模型更精确地推断出交错管状结构的精细结构。</strong></p><p>上述两部分实验的数据结果：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/15.png" title="Optional title"></p><h3 id="基于NRMSE、MS-SSIM和分辨率的DLSR模型性能评估"><a href="#基于NRMSE、MS-SSIM和分辨率的DLSR模型性能评估" class="headerlink" title="基于NRMSE、MS-SSIM和分辨率的DLSR模型性能评估"></a>基于NRMSE、MS-SSIM和分辨率的DLSR模型性能评估</h3><p>为了清楚地说明所有被评估的DLSR模型的性能差异，将它们分为两类：非GAN和GAN基模型。</p><p>对于每种类型的样品，分别绘制了同一类别模型的NRMSE、MS-SSIM和分辨率作为荧光强度的函数。<strong>结果表明，无论是DFCAN还是DFGAN的NRMSE都比同类的DLSR模型小。且在NRMSE和MS-SSIM指标方面，只有达到相对较高的荧光强度，传统的SIM才能超过DFCAN和DFGAN。</strong></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/16.png" title="Optional title"></p><blockquote><p>这里只放了NRMSE，其他指标的图示在补充图内。</p></blockquote><p>因此，相对于其他DLSR模型，DFCAN和DFGAN都提供了扩大的优先级区域。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/17.png" title="Optional title"></p><blockquote><p>这里是与前面的一张评估矩阵进行相比，绿色部分的面积扩大了。</p></blockquote><h3 id="DFCAN-DFGAN在SISR的2倍放大活细胞成像任务表现"><a href="#DFCAN-DFGAN在SISR的2倍放大活细胞成像任务表现" class="headerlink" title="DFCAN/DFGAN在SISR的2倍放大活细胞成像任务表现"></a>DFCAN/DFGAN在SISR的2倍放大活细胞成像任务表现</h3><p><strong>线粒体是高度动态的细胞器，线粒体动力学对于维持线粒体功能和细胞质量控制非常重要。</strong>然而，实现线粒体动力学的长时移SR成像仍然具有挑战性，当前SR成像技术需要高光照强度或长曝光时间来获取多个原始图像，这些原始图像容易引起线粒体的光毒性。</p><p>由于DFCAN采集单个WF图像所需的荧光比传统SR方法少得多，DFCAN-SISR允许长延时SR活细胞成像超过1000帧，与传统SIM或STED显微镜相比，成像持续时间延长了约10倍。</p><blockquote><p>生物相关，这部分实验与线粒体的内部结构有关。实验不放了，看不懂。</p></blockquote><h3 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h3><blockquote><p>直接从原文摘过来的。</p></blockquote><ul><li><p>图像预处理：对于每种类型的标本和每种成像方式，总共获得了约50组WF（512×512像素）和GT-SIM（1024×1024像素）或GT-NLSIM（1536×1536像素）图像。每一组都有9个不断上升的荧光水平。为了生成训练数据集，选取了35组原始数据（补充表5），采用随机裁剪、水平/垂直翻转和旋转变换等方法进一步丰富训练数据集，最终生成了20000对WF（128×128像素）和GT-SIM（256×256像素）图像，即2，每个荧光水平200对。对于每种类型的DLSR网络，用属于同一类型样品的所有荧光水平的数据训练一个专用模型。为了生成测试数据集，将剩余的15组数据扩充为WF（256×256像素）和GT-SIM（512×512像素）数据集，然后根据平均光子计数（即特定细胞的表达水平）将这些配对图像分为25到600的16个荧光水平。每一个荧光水平都保证有100多个图像。</p></li><li><p>训练：在一台计算机工作站上进行的，该工作站配备了3.20 GHz的Xeon（R）Gold 6134 CPU（Intel）和两块RTX 2080Ti图形处理卡（NVIDIA），其中包含python v.3.6、Tensorflow v.1.11.0和Keras v.2.2.4。在训练过程中，根据网络规模使用Adam优化器和2到6个批量大小。以CCPs的训练过程为例，对SRCNN、EDSR和DFCAN的非GAN方法，随机初始化网络，以1×10−4的典型起始学习率训练模型。<strong>最后的SRCNN、EDSR、DFCAN和RCAN模型分别经过约70000、150000、200000和500000次小批量迭代训练，时间分别为10、120、24和120h。对于基于GAN的Pix2Pix、CMGAN和DFGAN方法，还随机初始化了网络，训练了具有典型启动的生成模型和判别模型学习率分别为2×10−5和1×10−4。最后的Pix2Pix、CMGAN和DFGAN模型分别进行了约50000、90000和80000次小批量迭代，每次迭代时间分别为20、120和80h。在不同网络的训练过程期间的验证NRMSE的代表图示于补充图13中。通过迁移学习和混合精度训练可以缩短训练时间。一旦网络被训练，所有这些DLSR模型通常需要不到1s的时间来重建1024×1024像素的SR图像。</strong></p></li></ul><h3 id="统计和复现性"><a href="#统计和复现性" class="headerlink" title="统计和复现性"></a>统计和复现性</h3><blockquote><p>直接从原文摘过来的。</p></blockquote><p><strong>每个DLSR模型用相同的训练数据和超参数独立训练三次，然后采用NRMSE最低的模型进行进一步的评估。</strong></p><p>1a、b、3a、5a–d和扩展数据图2重复了120个测试图像，用于每种类型的样本和放大因子，都获得了相似的结果。</p><p>评估矩阵或曲线中的所有数据均来自100多幅图像的测试。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><ul><li><p>BioSR，它由广泛的LR–SR图像对组成，涵盖了广泛的信噪比水平、结构复杂性和放大因子。</p></li><li><p>非GAN模型更适用于低至中等荧光成像条件，以产生具有良好可量化性的SR图像。然而，如果GAN模型能够提供与传统SIM（例如DFGAN）相当的NRMSE和MS-SSIM，那么它将是首选的DLSR模型，特别是对于高结构复杂性的样本。</p></li><li><p>无论采用哪种DLSR模型，NRMSE和MS-SSIM对荧光强度的评估函数都很快接近渐近稳定，但即使接近无限荧光，也不能达到理想值。相比之下，随着荧光强度接近GT成像水平，传统SIM图像的度量越来越接近deal值。因此，这一局限性表明了用纯计算方法完全取代SR显微镜的巨大挑战。所以，将深度学习算法巧妙地集成到显微镜硬件开发中的整体设计可能是下一代SR显微镜的一种有前途的方法。</p></li></ul><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>文章：<a href="https://www.nature.com/articles/s41592-020-01048-5#data-availability">https://www.nature.com/articles/s41592-020-01048-5#data-availability</a></p></li><li><p>code：<a href="https://github.com/qc17-THU/DL-SR">https://github.com/qc17-THU/DL-SR</a></p></li><li><p>数据集（BioSR）：<a href="https://figshare.com/articles/dataset/BioSR/13264793">https://figshare.com/articles/dataset/BioSR/13264793</a></p></li></ul><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>文章内容太多了，一时半会都不能理解它到底在推什么东西，到底是数据集，还是算法，还是思想，还是硬件。但是非常有借鉴意义，得多读几遍。</p><hr><h1 id="GELU"><a href="#GELU" class="headerlink" title="GELU"></a>GELU</h1><blockquote><p>Gaussian Error Linear Units (GELUs)<br><a href="https://arxiv.org/abs/1606.08415">https://arxiv.org/abs/1606.08415</a></p></blockquote><p>高斯误差线性单元。</p><p>GELU非线性的实现是对神经网络的输入进行随机正则化的变化，为输入匹配一个或0或1的随机值。与ReLU的不同，GELU为其按照输入的magnitude（等级）为inputs加权值的；ReLUs是根据inputs的sign（正负）来gate（加门限）的。</p><p>论文实验证明GELU在多项计算机视觉，自然语言处理，语音任务上效果优于ReLU，ELU。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/18.png" title="Optional title"></p><h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><h2 id="MAE"><a href="#MAE" class="headerlink" title="MAE"></a>MAE</h2><p>MAE（Mean Square Error,平均绝对误差），所有单个观测值与算术平均值的偏差的绝对值的平均。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/19.png" title="Optional title"></p><h2 id="MSE"><a href="#MSE" class="headerlink" title="MSE"></a>MSE</h2><p>见《基础：图像超分辨率》。</p><h2 id="PSNR"><a href="#PSNR" class="headerlink" title="PSNR"></a>PSNR</h2><p>见《基础：图像超分辨率》。</p><h2 id="SSIM"><a href="#SSIM" class="headerlink" title="SSIM"></a>SSIM</h2><p>见《基础：图像超分辨率》。</p><h2 id="RMSE"><a href="#RMSE" class="headerlink" title="RMSE"></a>RMSE</h2><p>RMSE（Root Mean Square Error，均方根误差），观测值与真值偏差的平方和与观测次数m比值的平方根。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/20.png" title="Optional title"></p><blockquote><p>假如有2000次观测，即m=2000，对于某一次（第i次）观测来说，y值是真实值，而h(x)是观测值，对所有m次观测的的偏差取平方后相加，得到的值再除以m，然后再开根号，就得到RMSE了。</p></blockquote><p>RMSE对偏差做了一次平方（相比于MAE），这样，如果误差的离散度高，也就是说，如果最大偏差值大的话，RMSE就放大了。</p><h2 id="NRMSE"><a href="#NRMSE" class="headerlink" title="NRMSE"></a>NRMSE</h2><p>NRMSE（Normalized Root Mean Square Error，归一化均方根误差）</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/21.png" title="Optional title"></p><p>从数值角度，NRMSE就是将RMSE的值变成(0,1)之间。</p><h2 id="MS-SSIM"><a href="#MS-SSIM" class="headerlink" title="MS-SSIM"></a>MS-SSIM</h2><p>单尺度SSIM需要在特定的配置下才能表现良好，而MSSIM对不同分辨率的图像都能保持性能稳定。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/22.png" title="Optional title"></p><p>MSSIM的流程如上图所示。将参考图像和失真图像作为输入，然后分别依次迭代的使用低通滤波器和1/2降采样。假设原始图像为Scale 1，最高尺度为Scale M经过M-1次迭代得到。对于第j个尺度，只计算对比度c(x,y)和结构相似度s(x,y)。仅在Scale M计算亮度相似度l(x,y)。如上图所示。最终的SSIM是将各个尺度的结果连接起来：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-BioSR&DFCAN/23.png" title="Optional title"></p><p>SSIM不能很好的反应人类真实的视觉感受。</p><p>有人认为，这种方法之所以比SSIM好，是因为人类一般不是使用最清晰的图像，而是用低通分辨率处理之后的图像，所以把不同分辨率的图像纳入到评价参数中更符合人的使用习惯。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;2021，加油。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;2021.1.21 Nature&lt;/p&gt;
&lt;/bl</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Docker：配置、基础、应用栈、私有云</title>
    <link href="http://example.com/2020/12/23/Docker%EF%BC%9A%E9%85%8D%E7%BD%AE%E3%80%81%E5%9F%BA%E7%A1%80%E3%80%81%E5%BA%94%E7%94%A8%E6%A0%88%E3%80%81%E7%A7%81%E6%9C%89%E4%BA%91/"/>
    <id>http://example.com/2020/12/23/Docker%EF%BC%9A%E9%85%8D%E7%BD%AE%E3%80%81%E5%9F%BA%E7%A1%80%E3%80%81%E5%BA%94%E7%94%A8%E6%A0%88%E3%80%81%E7%A7%81%E6%9C%89%E4%BA%91/</id>
    <published>2020-12-23T03:52:20.000Z</published>
    <updated>2020-12-26T09:21:26.163Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h1><ul><li><p>Docker的安装：在Ubuntu、CentOS或者Windows上的安装Docker。</p></li><li><p>容器操作：启动容器、守护态运行、终止容器。</p></li><li><p>搭建一个Docker应用栈：获取镜像、应用栈容器节点互联、应用栈容器节点启动、应用栈容器节点配置。</p></li><li><p>实现私有云：启动Docker、获取镜像、实现sshd，在Base镜像基础上生成一个新镜像、分配容器、搭建自己的私有仓库。</p></li></ul><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>Docker是PaaS提供商dotCloud开源的一个基于LXC（Linux Containers，基于Linux的容器机制）的高级容器引擎，源代码托管在Github上，基于go语言并遵从Apache2.0协议开源的虚拟化技术。</p><p>可以把Docker近似地理解成是一个“轻量级的虚拟机”：<strong>只消耗较少的资源就能实现对进程的隔离保护</strong>。使用Docker可以把应用程序和它相关的各种依赖（如底层库、组件等）“打包”在一起，这就是Docker镜像（Docker image）。Docker镜像可以让应用程序不再顾虑环境的差异，在任意的系统中以容器的形式运行（当然必须要基于Docker环境），极大地增强了应用部署的灵活性和适应性。</p><p>在Docker的网站上提到了Docker的典型场景：</p><ul><li><p>Automating the packaging and deployment of applications（使应用的打包与部署自动化）</p></li><li><p>Creation of lightweight, private PAAS environments（创建轻量、私密的PAAS环境）</p></li><li><p>Automated testing and continuous integration/deployment（实现自动化测试和持续的集成/部署）</p></li><li><p>Deploying and scaling web apps, databases and backend services（部署与扩展webapp、数据库和后台服务）</p></li></ul><h1 id="Docker配置"><a href="#Docker配置" class="headerlink" title="Docker配置"></a>Docker配置</h1><p>在Mac OS上配置Docker。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>和安装Mac普通应用一样，下载.dmg安装包，运行安装包，把应用拖到到应用程序目录即可。</p><p>官方下载地址：<a href="https://download.docker.com/mac/stable/Docker.dmg">https://download.docker.com/mac/stable/Docker.dmg</a></p><p>可以使用国内地址（阿里云）进行下载：<a href="http://mirrors.aliyun.com/docker-toolbox/mac/docker-for-mac/stable/Docker.dmg">http://mirrors.aliyun.com/docker-toolbox/mac/docker-for-mac/stable/Docker.dmg</a></p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/1.png" title="Optional title"></p><p>初次打开会提示输入Mac的密码，输入后点击安装帮助程序即可。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/2.png" title="Optional title"></p><p>安装完之后，打开Docker桌面程序，可以看到如下用户界面。</p><p>教程。帮助建立一个Docker镜像和容器，最后一步是发布到自己的个人账号上。需要sign in。可以跳过。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/3.png" title="Optional title"></p><p>用户界面，左边栏分为Containers和Images，也是Docker的两个最重要的概念。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/4.png" title="Optional title"></p><p>可以在用户界面操作Docker，也可以直接使用terminal进行操作（实际上Docker可以直接在其用户界面使用terminal）。</p><h2 id="配置镜像加速"><a href="#配置镜像加速" class="headerlink" title="配置镜像加速"></a>配置镜像加速</h2><p>如果需要使用别人发布的镜像时，Docker默认从DockerHub拉取镜像。<strong>这个操作有时会遇到网络方面的困难</strong>，此时可以配置镜像加速器。</p><p>Docker官方在中国区提供了镜像地址，而且国内很多云服务商都提供了国内加速器服务，例如网易、阿里云等。</p><p>这里选择阿里云的镜像加速。登入阿里云网站：<a href="https://cr.console.aliyun.com/">https://cr.console.aliyun.com</a> ，左边栏-&gt;容器镜像服务-&gt;镜像中心-&gt;镜像加速器，默认会生成属于个人的一个加速器地址（需要注册阿里云账号）。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/5.png" title="Optional title"></p><p>得到上述加速器之后，打开Docker桌面程序-&gt;Preferences-&gt;Docker Engine，使用json格式进行配置。</p><p>原来的配置：</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/6.png" title="Optional title"></p><p>修改为：</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/7.png" title="Optional title"></p><h2 id="配置结果"><a href="#配置结果" class="headerlink" title="配置结果"></a>配置结果</h2><p>在terminal中，使用<code>docker info</code>可以查看Docker是否安装成功以及配置的结果。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/8.png" title="Optional title"></p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/9.png" title="Optional title"></p><h1 id="容器：基础操作"><a href="#容器：基础操作" class="headerlink" title="容器：基础操作"></a>容器：基础操作</h1><p><strong>创建一个Apache容器并进行一些基础操作</strong>。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/10.png" title="Optional title"></p><p><code>docker image ls</code>：用于列出所有镜像的属性。</p><p><code>docker run -p 80:80 httpd</code>：创建一个Apache容器。其中，<code>-p</code>用来指定端口映射关系：第一个为本地端口，第二个为容器端口；<code>httpd</code>是Apache超文本传输协议（HTTP）服务器的主程序，被设计为一个独立运行的后台进程，它会建立一个处理请求的子进程或线程的池。主要的命令是<code>docker run</code>，该命令会运行本地的镜像，如果该镜像不存在的话，会先从镜像仓库拉取该镜像。因此，这条命令的输出包含了两部分结果：第一部分是<code>docker pull</code>，用来从镜像仓库中拉取或者更新指定镜像，第二部分是运行该容器并输出log内容。</p><p>使用浏览器及<code>本地ip</code>访问80端口，会出现如下页面，说明Apache服务成功运行：</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/11.png" title="Optional title"></p><p>对运行的容器做如下操作：</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/12.png" title="Optional title"></p><p><code>docker ps -a</code>：列出所有的容器（不管是运行中的还是未运行的），可以看到刚刚建立的容器的一些状态。其中，<code>CONTAINER ID</code>给出了容器的ID，可以用这个ID指定该容器进行一些操作。<code>STATUS</code>中<code>Up</code>表示该容器正在运行。</p><p><code>docker logs 540cb924672b</code>：指定容器查看其log内容。可以看到这里的输出内容和上一图中的内容是一致的，也证明了服务成功运行。</p><p><code>docker stop 540cb924672b</code>：停止该容器。停止之后再使用ps命令查看状态，可以发现该容器的状态变成了<code>Exited (0)</code>。</p><p><code>docker run -d -p 80:80 httpd</code>：与上面一条run命令相比，加入了<code>-d</code>参数，<strong>表示容器运行于前台还是后台（即守护态）</strong>，默认为false。接着再使用ps命令查看状态，可以发现从该镜像新建了一个容器（不同的ID）。</p><p>在Docker桌面程序中也可以看到刚刚建立的两个容器。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/13.png" title="Optional title"></p><h1 id="应用栈"><a href="#应用栈" class="headerlink" title="应用栈"></a>应用栈</h1><p>Mac上做Docker应用栈，配置容器的时候会遇到一个极其让人难受的问题：</p><p><a href="https://forums.docker.com/t/var-lib-docker-does-not-exist-on-host/18314">https://forums.docker.com/t/var-lib-docker-does-not-exist-on-host/18314</a></p><p>本来是想用网上那个大家已经做烂了的例子去做一下的：</p><p><a href="https://blog.csdn.net/u012066426/article/details/52597991">https://blog.csdn.net/u012066426/article/details/52597991</a></p><blockquote><p>参考《Docker容器和容器云》 2.3.2章节应用栈搭建过程</p></blockquote><p><strong>这个问题本质上还是机器架构导致的软件环境的问题。应用栈搭建本身是比较简单的工作。</strong></p><p>因为这个问题，决定放弃了，因为真的很浪费时间，4天了没有找到什么解决方案。</p><h1 id="私有云"><a href="#私有云" class="headerlink" title="私有云"></a>私有云</h1><p>hub.docker.com（官方的Docker hub）上可以保存镜像，是一个用于管理公共镜像的地方，我们可以在上面找到我们想要的镜像，也可以把我们自己的镜像推送上去，但是网速相对较慢。修改了镜像地址可以加快网速，但是某些情况下，开发需要更加定制化和私人化的要求时，再或者个人的服务器无法访问互联网时，在内部环境中搭建一个私有的公共仓库是个更好的方案，这就是Docker私有云（私库）的意义。</p><p>过程如下：</p><h2 id="私有云工具：registry"><a href="#私有云工具：registry" class="headerlink" title="私有云工具：registry"></a>私有云工具：registry</h2><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/14.png" title="Optional title"></p><p><code>docker pull docker.io/registry</code>：利用官方提供的工具<strong>registry</strong>来配置私库。需要注意的是，这个工具是个本身就是个镜像，直接下载并使用该镜像启动容器就可以完成私库的搭建。</p><h2 id="搭建"><a href="#搭建" class="headerlink" title="搭建"></a>搭建</h2><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/15.png" title="Optional title"></p><p><code>docker run -d -p 5000:5000 --name registry_jfy --restart always -v /Users/qiwu/docker/registry:/var/lib/registry registry</code>：建立私库。其中，<code>-d</code>设置该私库为后台守护态运行；<code>-p</code>给出端口及其映射；<code>--name</code>设置该私库的名称为<code>registry_jfy</code>；<code>--restart</code>表示该容器总随着docker服务的开启而启动；<code>-v</code>把registry的镜像路径映射到本地。</p><p>使用<code>ps</code>命令查看容器，可看到刚刚新建立的私库的情况。使用 <a href="http://127.0.0.1:5000/v2/_catalog">http://127.0.0.1:5000/v2/_catalog</a> 测试刚刚搭建的私库的内容。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/16.png" title="Optional title"></p><blockquote><p>这里应该是空的repositores，busybox是之前用来做尝试而上传上去的镜像，此处无视之。</p></blockquote><p><code>docker tag ubuntu:latest 127.0.0.1:5000/ubuntu</code>：希望将本地已有的ubuntu镜像（本来是用来做应用栈的）作为私库的测试文件。标记本地镜像并指向目标仓库。<strong>这一步是必须的，如果直接将未tag的镜像上传，会提示此镜像不存在。</strong></p><p>tag操作之后，使用<code>ps</code>命令查看容器，发现多了一个刚刚标记过的镜像，大小和原本的ubuntu是一样的。</p><p>还是使用 <a href="http://127.0.0.1:5000/v2/_catalog">http://127.0.0.1:5000/v2/_catalog</a> 查看私库的内容，可以发现增添了刚才push的ubuntu。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/17.png" title="Optional title"></p><p><code>docker push 127.0.0.1:5000/ubuntu</code>：为了测试私库的镜像，先将本地已有的两个ubuntu镜像删除。</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/docker/18.png" title="Optional title"></p><p><code>docker pull 127.0.0.1:5000/ubuntu</code>：从本地仓库拉取到docker容器。</p><p>使用<code>ps</code>命令查看pull前后的镜像，发现多了名为<code>127.0.0.1:5000/ubuntu</code>的ubuntu镜像，说明成功的从本地仓库pull到了刚刚push上去的ubuntu。</p><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p><strong>实验本身比较简单</strong>。Docker的使用是比较友好的，尤其是在Mac上，还有桌面程序，即使对敲terminal命令不怎么熟悉的人，也有内置的一些教程帮助初步的使用。但实际上大部分的操作还是需要熟悉terminal命令。</p><p><strong>但是也有不怎么友好的地方，就是在搭应用栈的时候，因为操作系统导致的问题，上面已经说过，不再赘述，这里给差评。</strong></p><p>做完了这个实验之后，突然感觉虚拟化的东西真的已经不是一个纸面上的概念了，大家都在用这类应用做东西。尤其是在查找应用栈和私有云的资料的时候，会有很多的商业化广告，内容都是帮助公司等搭建Docker等的私有云。也非常惊诧居然有Docker这种轻量级的东西来做虚拟化，我个人的理解就是将“阉割”做到了标准化和规范化，以减少成本。这个概念以后也许不会再用到，但这种思路对于个人的学习是非常有帮助的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;目标&quot;&gt;&lt;a href=&quot;#目标&quot; class=&quot;headerlink&quot; title=&quot;目标&quot;&gt;&lt;/a&gt;目标&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Docke</summary>
      
    
    
    
    
    <category term="云计算与分布式系统" scheme="http://example.com/tags/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="Docker" scheme="http://example.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：SISR-DRN</title>
    <link href="http://example.com/2020/12/04/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASISR-DRN/"/>
    <id>http://example.com/2020/12/04/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASISR-DRN/</id>
    <published>2020-12-04T12:41:23.000Z</published>
    <updated>2020-12-05T01:50:53.489Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Closed-loop Matters: Dual Regression Networks for Single Image Super-Resolution<br>2020 cvpr</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>文章提出，目前超分辨率算法存在两个明显的问题：</p><ul><li>从LR图像到HR图像通常是一个ill-posed的反问题，<strong>存在无数可能的HR图像通过降采样得到同一张LR图像</strong>。解空间过大，从而很难去找到一个合适的解。</li></ul><blockquote><p>First, learning the mapping from LR to HR images is typ- ically an ill-posed problem since there exist infinitely many HR images that can be downscaled to obtain the same LR image [36]. Thus, the space of the possible functions that map LR to HR images becomes extremely large. As a result, the learning performance can be limited since learning a good solution in such a large space is very hard.</p></blockquote><ul><li>真实场景应用中，<strong>成对的LR-HR图像往往无法获得，因此对应图像降采样方式也往往未知</strong>。因此对于更普遍的情形，现有的SR模型经常会引起适应性问题，从而导致性能不佳。</li></ul><blockquote><p>Second, it is hard to obtain a promising SR model when the paired data are unavailable [43, 54]. Note that most SR methods rely on the paired training data, i.e., HR images with their Bicubic-degraded LR counterparts. However, the paired data may be unavailable and the unpaired data often dominate the real-world applications.</p></blockquote><p>论文针对这两个主要的问题进行改进，提出了对偶回归策略，通过引入对LR图像额外的约束，从而减小解空间的大小。</p><p>也就是说，模型除了学习LR到HR图像的映射外，还学习了额外的对偶回归映射，用于估计下采样内核并重建LR图像，从而形成一个闭环以提供额外的监督。特别地是，由于对偶回归策略并不依赖HR图像，因此可以直接从LR图像中进行学习。因此，可以很好地使得SR模型适应真实世界图像。</p><blockquote><p>作者的意思就是针对LR到HR解空间大的问题，作者通过设计一个反向的一个网络，实现HR到LR的映射，以此来制约和平衡主网络（也就是LR到HR映射的网络）的训练，怎么平衡的可以通过后面作者给的损失函数看出来。而为了解决HR和LR成对训练的依赖问题，作者通过在训练集中加入不成对的LR图像，那肯定有人要问了，那LR图像没有对应的HR图像那怎么训练呢？这个问题也是能通过后面作者给的损失函数来解决。</p></blockquote><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p>网络的整体结构如下，<strong>分为P和D两部分。</strong>：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/1.png" title="Optional title"></p><p>模型中黑色箭头所指部分，为DRN模型的Primary网络，而红色箭头所指部分，则对应Dual Regression 网络。Primary网络实现了从LR到HR映射，并且生成多尺度的SR图像：x1 SR，x2 SR。Dual Regression网络则是HR到LR映射，生成多尺度的LR图像：x2 LR，x1 LR。</p><blockquote><p>D网络中优化的损失函数不止一个，通过上图可以发现对于最后结果为4x的图像，反向进行下采样可以下采样成2x和1x的。而作者在P网络的设计中一开始Input图像（LR通过插值上采样后的）在输入时也经历了两个阶段就是下采样成2x和1x的，所以这就和D网络对应了起来。P网络的2x和D网络的2x图像形成一对，并进行损失函数优化。1x图像也是如此。如果最后的结果是8x的图像，就多一个4x的P网络和D网络的成对优化。</p></blockquote><h2 id="成对数据"><a href="#成对数据" class="headerlink" title="成对数据"></a>成对数据</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/2.png" title="Optional title"></p><p>针对配对的训练数据，将SR问题公式化为涉及两个回归任务的对偶回归模型。损失函数如下图所示，包含两部分，一个是P网络的损失，一个是D网络的损失：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/3.png" title="Optional title"></p><p>其中，lamda为对偶回归Loss的权重参数,推荐设置为0.1。每个loss均可以用L1或L2loss，本文中使用的是L1 loss。</p><h2 id="不成对数据"><a href="#不成对数据" class="headerlink" title="不成对数据"></a>不成对数据</h2><p>对于没有成对数据集的情况下，DRN采用了半监督学习，引入部分的成对数据集用于训练，DRN损失函数为：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/4.png" title="Optional title"></p><p>首先，为了保证SR的质量，这里需要添加一部分成对的合成数据（简单双三次三采样退化）。训练时选取m个无标签数据和n个合成数据。</p><p>其次，1sp表示数据为合成数据时取1，无标签数据时取0。通过这个参数来控制训练LR没有对应HR图像的情况下的训练损失函数，通过后面加上lamda权重的D网络损失函数，来平衡P网络的训练。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="有监督部分"><a href="#有监督部分" class="headerlink" title="有监督部分"></a>有监督部分</h2><p>有监督部分和目前大部分模型一样，使用的是常见的Data pair数据集DIV2K和Flick2K作为训练集，测试集为常见的SET5, SET14, BSDS100,URBAN100 和MANGA109。模型设置上，作者设置了两个不同参数量的模型，DRN-S（小模型）和DRN-L（大模型）。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/5.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/6.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/7.png" title="Optional title"></p><h2 id="半监督部分"><a href="#半监督部分" class="headerlink" title="半监督部分"></a>半监督部分</h2><p>半监督部分，data pair部分使用了DIV2K的训练集，而对于data unpiar部分，则划分成两个实验：</p><h3 id="Comparison-on-Unpaired-Synthetic-Data"><a href="#Comparison-on-Unpaired-Synthetic-Data" class="headerlink" title="Comparison on Unpaired Synthetic Data"></a>Comparison on Unpaired Synthetic Data</h3><p>data unpair则是在ImageNet中随机选取了3K张图像，并对图片使用Nearest和BD两种不同的方式采样出LR图像，将LR用于半监督训练。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/8.png" title="Optional title"></p><h3 id="Comparison-on-Unpaired-Real-world-Data"><a href="#Comparison-on-Unpaired-Real-world-Data" class="headerlink" title="Comparison on Unpaired Real-world Data"></a>Comparison on Unpaired Real-world Data</h3><p>data unpair则是来自于未知退化核的YouTube视频的3K张原始帧。由于没有GT，只能通过感性的方式进行不同方法的下恢复效果的观测。</p><h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><h3 id="对偶回归学习是否有效？lamba设置为多少合适？"><a href="#对偶回归学习是否有效？lamba设置为多少合适？" class="headerlink" title="对偶回归学习是否有效？lamba设置为多少合适？"></a>对偶回归学习是否有效？lamba设置为多少合适？</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/9.png" title="Optional title"></p><h3 id="半监督学习中，data-pair数据和unpair数据比例如何划分？"><a href="#半监督学习中，data-pair数据和unpair数据比例如何划分？" class="headerlink" title="半监督学习中，data pair数据和unpair数据比例如何划分？"></a>半监督学习中，data pair数据和unpair数据比例如何划分？</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/10.png" title="Optional title"></p><h1 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h1><p>来自原文叙述的主要贡献：</p><ul><li><p>We develop a dual regression scheme by introducing an additional constraint such that the mappings can form a closed-loop and LR images can be reconstructed to enhance the performance of SR models. Moreover, we also theoretically analyze the generalization ability of the proposed scheme, which further confirms its superiority to existing methods.</p></li><li><p>We study a <code>more general super-resolution case</code> where there is no corresponding HR data w.r.t. the real-world LR data. With the proposed dual regression scheme, deep models can be <code>easily adapted to real-world data</code>, e.g., raw video frames from YouTube.</p></li><li><p>Extensive experiments on both the SR tasks with paired training data and unpaired real-world data demonstrate the effectiveness of the <code>proposed dual regression scheme</code> in image super-resolution.</p></li></ul><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>文章：<br><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Closed-Loop_Matters_Dual_Regression_Networks_for_Single_Image_Super-Resolution_CVPR_2020_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Closed-Loop_Matters_Dual_Regression_Networks_for_Single_Image_Super-Resolution_CVPR_2020_paper.pdf</a></p><p>code：<a href="https://github.com/guoyongcs/DRN">https://github.com/guoyongcs/DRN</a></p><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><ul><li><p>思路非常的新颖且实用。最大的亮点，在于利用对偶任务的特点，将真实LR的半监督引入，为目前SISR普遍存在的泛化性问题提供了一种新的解决思路。即当前更多的工作是如何去盲估计出一个未知的退化核，这种盲估计通常是非常难的。而这种半监督的学习，则需要将真实的LR接纳到模型中来进行学习，因此这种半监督的方式，在真实场景下可能更多需要Online learning。</p></li><li><p>推测这种学习范式对其他的low level vision tasks应该都是同样适用的。</p></li></ul><hr><h1 id="对偶学习"><a href="#对偶学习" class="headerlink" title="对偶学习"></a>对偶学习</h1><blockquote><p>Dual Learning for Machine Translation<br>NIPS 2016</p></blockquote><blockquote><p><a href="https://www.msra.cn/zh-cn/news/features/dual-learning-20161207">https://www.msra.cn/zh-cn/news/features/dual-learning-20161207</a></p></blockquote><h2 id="立意-1"><a href="#立意-1" class="headerlink" title="立意"></a>立意</h2><p>深度学习之所以能够取得巨大的成功，一个非常重要的因素就是大数据，特别是大规模的带标签的数据。但在很多任务中，没办法收集到大规模的标注数据。为了使深度学习能够取得更广泛的成功，需要降低其对大规模标注数据的依赖性。</p><p><strong>为了解决这个问题，我们提出了一种新的学习范式，把它称作对偶学习。</strong> </p><h2 id="做法"><a href="#做法" class="headerlink" title="做法"></a>做法</h2><p>很多人工智能的应用涉及两个互为对偶的任务，例如机器翻译中从中文到英文翻译和从英文到中文的翻译互为对偶、语音处理中语音识别和语音合成互为对偶、图像理解中基于图像生成文本和基于文本生成图像互为对偶、问答系统中回答问题和生成问题互为对偶，以及在搜索引擎中给检索词查找相关的网页和给网页生成关键词互为对偶。这些互为对偶的人工智能任务可以形成一个闭环，使从没有标注的数据中进行学习成为可能。</p><p>对偶学习的最关键一点在于，给定一个原始任务模型，其对偶任务的模型可以给其提供反馈；同样的，给定一个对偶任务的模型，其原始任务的模型也可以给该对偶任务的模型提供反馈；从而这两个互为对偶的任务可以相互提供反馈，相互学习、相互提高。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DRN/11.png" title="Optional title"></p><p>考虑一个对偶翻译游戏，里面有两个玩家小明和爱丽丝。小明只能讲中文，爱丽丝只会讲英文，他们两个人一起希望能够提高英文到中文的翻译模型f和中文到英文的翻译模型g。给定一个英文的句子x，爱丽丝首先通过f把这个句子翻译成中文句子y1，然后把这个中文的句子发给小明。因为没有标注，所以小明不知道正确的翻译是什么，但是小明可以知道，这个中文的句子是不是语法正确、符不符合中文的语言模型，这些信息都能帮助小明大概判断翻译模型f是不是做的好。然后小明再把这个中文的句子y1通过翻译模型g翻译成一个新的英文句子x1，并发给爱丽丝。通过比较x和x1是不是相似，爱丽丝就能够知道翻译模型f和g是不是做得好，尽管x只是一个没有标注的句子。因此，通过这样一个对偶游戏的过程，我们能够从没有标注的数据上获得反馈，从而知道如何提高机器学习模型。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>认为对偶学习是一个全新的学习范式。</p><ul><li><p>监督学习（supervised learning）只能从标注的数据进行学习，只涉及一个学习任务；而对偶学习涉及至少两个学习任务，可以从未标注的数据进行学习。</p></li><li><p>半监督学习（semi-supervised learning）尽管可以对未标注的样本生成伪标签，但无法知道这些伪标签的好坏，而对偶学习通过对偶游戏生成的反馈（例如对偶翻译中x和x1的相似性）能知道中间过程产生的伪标签（y1）的好坏，因而可以更有效地利用未标注的数据。我们甚至可以说，对偶学习在某种程度上是把未标注的数据当作带标签的数据来使用。</p></li><li><p>对偶学习和多任务学习（multi-task learning）也不相同。尽管多任务学习也是同时学习多个任务共的模型，但这些任务必须共享相同的输入空间，而对偶学习对输入空间没有要求，只要这些任务能形成一个闭环系统即可。</p></li><li><p>对偶学习和迁移学习（transfer learning）也很不一样。迁移学习用一个或多个相关的任务来辅助主要任务的学习，而在对偶学习中，多个任务是相互帮助、相互提高，并没有主次之分。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Closed-loop Matters: Dual Regression Networks for Single Image Super</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：SISR数据增广的一种新思路</title>
    <link href="http://example.com/2020/12/04/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASISR%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%B9%BF%E7%9A%84%E4%B8%80%E7%A7%8D%E6%96%B0%E6%80%9D%E8%B7%AF/"/>
    <id>http://example.com/2020/12/04/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASISR%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%B9%BF%E7%9A%84%E4%B8%80%E7%A7%8D%E6%96%B0%E6%80%9D%E8%B7%AF/</id>
    <published>2020-12-04T12:40:23.000Z</published>
    <updated>2020-12-11T14:37:32.768Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Rethinking Data Augmentation for Image Super-resolution: A Comprehensive Analysis and a New Strategy<br>2020 cvpr</p></blockquote><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>数据扩充（DA）是在不增加计算成本的情况下提高模型性能的最实用的方法之一。<strong>但现在许多数据增强的方法也都是用于High-level的cv任务，并不适合图像超分辨率任务。</strong></p><p>像超分辨率这样的图像恢复研究，非常依赖于合成数据集。最常见的做法是通过模拟系统退化函数（bicubic），可以增加训练样本的数量。但这样的做法欠缺考虑：由于模拟数据分布与实际数据分布之间存在差距，在模拟数据集上训练的模型在实际环境中并没有表现出最佳性能。</p><p>这方面的相关工作很少有人做，[24]首先做了这个事情，但不过也就是通过旋转和翻转。也有部分论文提到过这个观点，但是要么就是给出的例子太少，要么就是使用的baseline比较老旧（SRCNN级别的这种）。总之这方面研究还比较稀少。</p><blockquote><p>[24]Seven ways to improve example-based single image super resolution.<br>2016cvpr</p></blockquote><p>为了更好地理解低水平视觉中的DA方法，对最初为高级视觉任务开发的各种DA方法的效果进行了全面分析（第2节）。首先根据方法的应用领域将现有的增强技术分为两类：像素域和特征域。当直接应用于SISR时，发现有些方法会损害图像恢复结果，甚至阻碍训练，特别是当一种方法在很大程度上导致相邻像素之间的空间信息丢失或混淆时。</p><p>在分析的基础上，提出了CutBlur。<strong>具体做法是：剪切模糊剪切并粘贴LR图像补丁到其对应的真实HR图像补丁，</strong>反之也一样。CurBlur的关键在于让模型不仅知道如何超分，同时也知道哪里需要超分。通过这样的方法，模型能够自适应地去决定图像多大程度上去应用超分而不是盲目地对所有像素进行超分。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DA-NewStrategy/1.png" title="Optional title"></p><h1 id="CurBlur"><a href="#CurBlur" class="headerlink" title="CurBlur"></a>CurBlur</h1><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>假设给定LR图像xlr(w<em>h</em>c)和xhr(sw<em>sh</em>c)图像块，其中s是放大倍数。由于 CurBlur需要匹配LR图像和HR图像的分辨率，所以会LR图像进行s倍的双三次插值。CurBlur然后就会生成成对的训练样本lr-&gt;hr和hr-&gt;lr ：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DA-NewStrategy/2.png" title="Optional title"></p><p>其中，M为二值Mask，用于决定哪里需要被替换掉。</p><p>源码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cutblur</span>(<span class="params">im1, im2, prob=<span class="number">1.0</span>, alpha=<span class="number">1.0</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> im1.size() != im2.size():</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;im1 and im2 have to be the same resolution.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> alpha &lt;= <span class="number">0</span> <span class="keyword">or</span> np.random.rand(<span class="number">1</span>) &gt;= prob:</span><br><span class="line">        <span class="keyword">return</span> im1, im2</span><br><span class="line"></span><br><span class="line">    cut_ratio = np.random.randn() * <span class="number">0.01</span> + alpha</span><br><span class="line"></span><br><span class="line">    h, w = im2.size(<span class="number">2</span>), im2.size(<span class="number">3</span>)</span><br><span class="line">    ch, cw = np.int(h*cut_ratio), np.int(w*cut_ratio)</span><br><span class="line">    cy = np.random.randint(<span class="number">0</span>, h-ch+<span class="number">1</span>)</span><br><span class="line">    cx = np.random.randint(<span class="number">0</span>, w-cw+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply CutBlur to inside or outside</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> np.random.random() &gt; <span class="number">0.5</span>:</span><br><span class="line">        im2[..., cy:cy+ch, cx:cx+cw] = im1[..., cy:cy+ch, cx:cx+cw]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        im2_aug = im1.clone()</span><br><span class="line">        im2_aug[..., cy:cy+ch, cx:cx+cw] = im2[..., cy:cy+ch, cx:cx+cw]</span><br><span class="line">        im2 = im2_aug</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> im1, im2</span><br></pre></td></tr></table></figure><h2 id="为什么CutBlur适用于SR？"><a href="#为什么CutBlur适用于SR？" class="headerlink" title="为什么CutBlur适用于SR？"></a>为什么CutBlur适用于SR？</h2><p>从之前不同DA方法对比，可以看到，图像内容信息的急剧变化，图像块的混叠，或者是丢失像素的相关性都能够损害SR的性能。因此，用于SR的良好的DA方法不应存在不符合实际的模式或信息丢失，并且应该为SR模型良好的正则。</p><p>CutBlur能够满足这样的条件：首先，它仅仅在HR和LR图像块之间进行裁剪和粘贴，因此能够最小化边界效应；其次，它可以利用整个图像信息，同时由于样本具有随机的HR比率和位置，CutBlur具有正则化效果。</p><blockquote><p>CutBlur satisfies these conditions because it performs cut-and-paste between the LR and HR image patches of the same content. By putting the LR (resp. HR) image region onto the corresponding HR (resp. LR) image region, it can minimize the boundary effect, which majorly comes from a mismatch between the image contents (e.g., Cutout and CutMix). Unlike Cutout, CutBlur can utilize the entire image information while it enjoys the regularization effect due to the varied samples of random HR ratios and locations.</p></blockquote><h2 id="模型从CutBlur中学到了什么？"><a href="#模型从CutBlur中学到了什么？" class="headerlink" title="模型从CutBlur中学到了什么？"></a>模型从CutBlur中学到了什么？</h2><p>与其他DA方法防止分类模型过分自信地做出决策的相似，<strong>CurBlur可以很好地避免SR模型过度锐化图像。</strong></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DA-NewStrategy/3.png" title="Optional title"></p><p>现在，模型必须同时学习“如何”和“何处”来超分辨率图像，这将导致模型学习“多少”它应该应用超分辨率，这为训练提供了有益的正则化效果。</p><blockquote><p>When the SR model takes HR images at the test phase, it commonly outputs over-sharpened predictions, especially where the edges are (Figure 2). CutBlur can resolve this issue by directly providing such examples to the model during the training phase. Not only does CutBlur mitigate the over-sharpening problem, but it enhances the SR performance on the other LR regions, thanks to the regularization effect (Figure 3). Note that the residual intensity has significantly decreased in the CutBlur model. We hypothesize that this enhancement comes from constraining the SR model to dis- criminatively apply super-resolution to the image. Now the model has to simultaneously learn both “how” and “where” to super-resolve an image, and this leads the model to learn “how much” it should apply super-resolution, which pro- vides a beneficial regularization effect to the training.</p></blockquote><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="SR模型和数据集的规模影响"><a href="#SR模型和数据集的规模影响" class="headerlink" title="SR模型和数据集的规模影响"></a>SR模型和数据集的规模影响</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DA-NewStrategy/4.png" title="Optional title"></p><h3 id="不同规模的SR模型"><a href="#不同规模的SR模型" class="headerlink" title="不同规模的SR模型"></a>不同规模的SR模型</h3><p><strong>众所周知，一个大模型比一个小模型受益更多。</strong>为了验证这在SR中是否正确，根据模型大小设置不同的扩展应用概率：对于小模型（SRCNN和CARN）p=0.2，对于大模型（RCAN和EDSR），p=1.0。</p><p>对于小模型，这个方法没有提供任何好处，或者只是略微提高了性能。这表明了小型模型的严重不拟合，导致DA的影响很小。EDSR规模尚可，因此有所提升。</p><h3 id="不同规模的数据集"><a href="#不同规模的数据集" class="headerlink" title="不同规模的数据集"></a>不同规模的数据集</h3><p>减少了用于训练的数据量。使用数据集的50%，10%，和15%。</p><p>SRCNN和CARN提升很小或者干脆没有提升。培训时的验证曲线也可以看出这一点（图4a和4b）。RCAN和EDSR提升很大带来了巨大的好处。</p><h2 id="不同数据集的比较"><a href="#不同数据集的比较" class="headerlink" title="不同数据集的比较"></a>不同数据集的比较</h2><p>当使用了 CutBlur 对模型进行训练，模型在测试集上的性能得到了明显的提升，尤其是在RealSR数据集上，所有模型至少得到了0.22dB 的提升。而 CARN则能够在 RealSR测试集上达到SOTA性能（RCAN basline），性能与LP-KPN近似，使用参数量仅为LP-KPN的22%。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DA-NewStrategy/5.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DA-NewStrategy/6.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DA-NewStrategy/7.png" title="Optional title"></p><h2 id="野外环境下的CutBlur"><a href="#野外环境下的CutBlur" class="headerlink" title="野外环境下的CutBlur"></a>野外环境下的CutBlur</h2><h2 id="其他的一些low-level的视觉任务"><a href="#其他的一些low-level的视觉任务" class="headerlink" title="其他的一些low-level的视觉任务"></a>其他的一些low-level的视觉任务</h2><h1 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h1><p>来自原文叙述的主要贡献：</p><ul><li><p>To the best of our knowledge, we are the <code>first to provide</code> comprehensive analysis of recent data augmentation methods when directly applied to the SISR task.</p></li><li><p>We propose a new DA method, CutBlur, which can reduce unrealistic distortions by regularizing a model to learn not only “how” but also “where” to apply the super-resolution to a given image.</p></li><li><p>Our mixed strategy shows consistent and significant improvements in the SR task, achieving <code>state-of-the-art (SOTA) performance in RealSR</code> [4].</p></li></ul><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>文章：<br><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yoo_Rethinking_Data_Augmentation_for_Image_Super-resolution_A_Comprehensive_Analysis_and_CVPR_2020_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2020/papers/Yoo_Rethinking_Data_Augmentation_for_Image_Super-resolution_A_Comprehensive_Analysis_and_CVPR_2020_paper.pdf</a></p><p>code：<a href="https://github.com/clovaai/cutblur">https://github.com/clovaai/cutblur</a></p><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><ul><li><p>思路非常的新颖，也许会对目前所做的工作有所启发。</p></li><li><p>比较简单。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Rethinking Data Augmentation for Image Super-resolution: A Comprehen</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Clang+OpenMP：初步</title>
    <link href="http://example.com/2020/11/25/Clang-OpenMP%EF%BC%9A%E5%88%9D%E6%AD%A5/"/>
    <id>http://example.com/2020/11/25/Clang-OpenMP%EF%BC%9A%E5%88%9D%E6%AD%A5/</id>
    <published>2020-11-25T02:45:29.000Z</published>
    <updated>2020-11-25T03:34:30.876Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h1><p>完成一个简单的OpenMP并行程序设计，体会多核并行程序的思想。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><h2 id="OpenMP"><a href="#OpenMP" class="headerlink" title="OpenMP"></a>OpenMP</h2><p>OpenMP是由OpenMP Architecture Review Board牵头提出的，并已被广泛接受，用于共享内存并行系统的多处理器程序设计的一套指导性编译处理方案（Compiler Directive）。</p><p>OpenMP支持的编程语言包括C、C++和Fortran；而支持OpenMp的编译器包括Sun Compiler，GNU Compiler和Intel Compiler等。</p><p>OpenMp提供了对并行算法的高层的抽象描述，程序员通过在源代码中加入专用的<strong>pragma</strong>来指明自己的意图，由此编译器可以自动将程序进行并行化，并在必要之处加入同步互斥以及通信。当选择忽略这些pragma，或者编译器不支持OpenMp时，程序又可退化为通常的程序（一般为串行），代码仍然可以正常运作，只是不能利用多线程来加速程序执行。</p><h2 id="Gcc"><a href="#Gcc" class="headerlink" title="Gcc"></a>Gcc</h2><p>GCC是以GPL许可证所发行的自由软件，也是GNU计划的关键部分。GCC的初衷是为GNU操作系统专门编写一款编译器，现已被大多数类Unix操作系统（如Linux、BSD、MacOS X等）采纳为标准的编译器，甚至在微软的Windows上也可以使用GCC。GCC支持多种计算机体系结构芯片，如x86、ARM、MIPS等，并已被移植到其他多种硬件平台。</p><h2 id="Clang"><a href="#Clang" class="headerlink" title="Clang"></a>Clang</h2><p>Clang是一个C++编写、基于LLVM、发布于LLVM BSD许可证下的C/C++/Objective-C/Objective-C++编译器。它与GNU C语言规范几乎完全兼容（当然，也有部分不兼容的内容，包括编译命令选项也会有点差异），并在此基础上增加了额外的语法特性，比如C函数重载（通过<strong>attribute</strong>((overloadable))来修饰函数），其目标（之一）就是超越GCC。</p><h2 id="Mac-OS下的Gcc-Clang"><a href="#Mac-OS下的Gcc-Clang" class="headerlink" title="Mac OS下的Gcc/Clang"></a>Mac OS下的Gcc/Clang</h2><p>有了GCC在前为何还有Clang的出现？</p><p>这是因为Apple使用LLVM在不支持全部OpenGL特性的GPU（Intel低端显卡）上生成代码JIT，令程序仍然能正常运行。之后LLVM与GCC的集成过程发生了一些不快，GCC系统庞大儿笨重，<strong>而Apple大量使用的Object-C在GCC中优先级很低</strong>。此外GCC作为一个纯粹的编译系统，与IDE配合很差。加上许可证方面的要求，<strong>Apple无法使用修改版GCC而闭源</strong>。于是Apple决定从0开始写C family的前端，也就是基于LLVM的Clang了。</p><p><strong>实际上，在Mac OS下，对于OpenMP，早期的Gcc并不支持（大概在4.2版本之前）。后期的Gcc也能做到支持，具体做法是直接下载<code>&lt;omp.h&gt;</code>。</strong></p><h1 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h1><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="Clang环境"><a href="#Clang环境" class="headerlink" title="Clang环境"></a>Clang环境</h3><p>选择使用Clang来完成OpenMP的简单程序。</p><p><code>clang -v</code>：检查本机的Clang环境。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/openmp/1.png" title="Optional title"></p><p>Mac OS通常来说会自带这一框架。</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>代码如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;omp.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[])</span> </span>&#123; </span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel for </span></span><br><span class="line">     <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++ ) &#123; </span><br><span class="line">         <span class="built_in">printf</span>(<span class="string">&quot;i = %d\n&quot;</span>, i); </span><br><span class="line">     &#125; </span><br><span class="line">     <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>显然，当上述代码正常运行时，会按序输出0到9，共10个数字。文件命名为<code>omp.c</code>。</p><p>这里需要注意的是：<code>#pragma omp parallel for</code>。这是因为，<strong>OpenMP编程需要通过编译制导指令制导并行化。</strong>编译制导指令以#pragma omp 开始，后边跟具体的功能指令，格式为：**#pragma omp 指令[子句[,子句] …]**。在上述指令中：</p><ul><li><p>parallel：用在一个结构块之前，<strong>表示这段代码将被多个线程并行执行</strong>。</p></li><li><p>for：用于for循环语句之前，表示将循环计算任务分配到多个线程中并行执行，以实现任务分担，必须由编程人员自己保证每次循环之间无数据相关性。</p></li><li><p>parallel for：parallel和for指令的结合，也是用在for循环语句之前，表示for循环体的代码将被多个线程并行执行，它同时具有并行域的产生和任务分担两个功能。</p></li></ul><blockquote><p>如果没有编译制导指令，在下面的步骤中使用同样的</p></blockquote><h2 id="代码编译与运行"><a href="#代码编译与运行" class="headerlink" title="代码编译与运行"></a>代码编译与运行</h2><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/openmp/2.png" title="Optional title"></p><p>去掉编译制导指令，作为对比，文件名为<code>no_omp.c</code>。会正常输出0到9。</p><p><code>clang -o omp.out omp.c -Xpreprocessor -fopenmp -lomp</code>：用来编译代码。其中，<code>-fopenmp</code>默认使用计算机最大核数进行运行，这个参数是可调的。</p><p>可以看到，在使用了OpenMP之后，变成了乱序输出。</p><p>再次运行代码，发现乱序输出也是随机的，表明是多线程并行处理。</p><hr><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><blockquote><p>百度百科：openmp： <a href="https://baike.baidu.com/item/openmp/3735430?fr=aladdin">https://baike.baidu.com/item/openmp/3735430?fr=aladdin</a></p></blockquote><blockquote><p>百度百科：gcc： <a href="https://baike.baidu.com/item/gcc/17570?fr=aladdin">https://baike.baidu.com/item/gcc/17570?fr=aladdin</a></p></blockquote><blockquote><p>百度百科：clang： <a href="https://baike.baidu.com/item/clang/3698345?fr=aladdin">https://baike.baidu.com/item/clang/3698345?fr=aladdin</a></p></blockquote><blockquote><p>Mac下使用OpenMP编写第一个多线程程序： <a href="https://blog.csdn.net/suki570/article/details/104272040">https://blog.csdn.net/suki570/article/details/104272040</a></p></blockquote><blockquote><p><a href="https://clang-omp.github.io/">https://clang-omp.github.io</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;目标&quot;&gt;&lt;a href=&quot;#目标&quot; class=&quot;headerlink&quot; title=&quot;目标&quot;&gt;&lt;/a&gt;目标&lt;/h1&gt;&lt;p&gt;完成一个简单的OpenMP并</summary>
      
    
    
    
    
    <category term="云计算与分布式系统" scheme="http://example.com/tags/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="OpenMP" scheme="http://example.com/tags/OpenMP/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04虚拟机+Apache：配置与初步使用</title>
    <link href="http://example.com/2020/11/24/Ubuntu16-04%E8%99%9A%E6%8B%9F%E6%9C%BA-Apache%EF%BC%9A%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%88%9D%E6%AD%A5%E4%BD%BF%E7%94%A8/"/>
    <id>http://example.com/2020/11/24/Ubuntu16-04%E8%99%9A%E6%8B%9F%E6%9C%BA-Apache%EF%BC%9A%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%88%9D%E6%AD%A5%E4%BD%BF%E7%94%A8/</id>
    <published>2020-11-24T14:45:50.000Z</published>
    <updated>2020-11-25T01:59:10.784Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h1><p>在VMWare Workstation之上安装CentOS或Ubuntu操作系统（或其他操作系统）。为虚拟机环境中运行的CentOS或Ubuntu配置网络使之可以访问互联网。在虚拟机中的CentOS或Ubuntu环境中安装Apache服务，并在宿主机中访问Apache提供的Web服务。为VMWare Workstation、CentOS或Ubuntu系统编写安装和配置手册，其中包括故障提示及使用到的一些Linux命令。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><h2 id="Apache"><a href="#Apache" class="headerlink" title="Apache"></a>Apache</h2><p>Apache HTTP Server（简称Apache）是Apache软件基金会的一个开放源码的网页服务器，可以在大多数计算机操作系统中运行，由于其多平台和安全性被广泛使用，是最流行的Web服务器端软件之一。它快速、可靠并且可通过简单的API扩展，将Perl/Python等解释器编译到服务器中。</p><p>它可以运行在几乎所有广泛使用的计算机平台上。</p><h2 id="Vmware-Workstation-Fusion"><a href="#Vmware-Workstation-Fusion" class="headerlink" title="Vmware Workstation/Fusion"></a>Vmware Workstation/Fusion</h2><p>VMware Workstation（中文名“威睿工作站”）是一款功能强大的桌面虚拟计算机软件，提供用户可在单一的桌面上同时运行不同的操作系统，和进行开发、测试 、部署新的应用程序的最佳解决方案。VMware Workstation可在一部实体机器上模拟完整的网络环境，以及可便于携带的虚拟机器，其更好的灵活性与先进的技术胜过了市面上其他的虚拟计算机软件。对于企业的 IT开发人员和系统管理员而言， VMware在虚拟网路，实时快照，拖曳共享文件夹，支持 PXE 等方面的特点使它成为必不可少的工具。</p><p><strong>VMware Fusion允许基于Intel的Mac在虚拟机上运行Microsoft Windows，Linux，NetWare或Solaris等操作系统。</strong></p><h1 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h1><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><h3 id="本地环境"><a href="#本地环境" class="headerlink" title="本地环境"></a>本地环境</h3><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/1.png" title="Optional title"></p><h3 id="VMware-Fusion"><a href="#VMware-Fusion" class="headerlink" title="VMware Fusion"></a>VMware Fusion</h3><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/2.png" title="Optional title"></p><h3 id="虚拟机环境"><a href="#虚拟机环境" class="headerlink" title="虚拟机环境"></a>虚拟机环境</h3><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/3.png" title="Optional title"></p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/4.png" title="Optional title"></p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/5.png" title="Optional title"></p><ul><li><p><code>uname -a</code>：查看内核/操作系统/CPU信息。 </p></li><li><p><code>head -n 1 /etc/issue</code>：查看操作系统版本。</p></li><li><p><code>cat /proc/cpuinfo</code>：查看CPU信息：四核，编号为0-3。</p></li></ul><h2 id="Apache框架"><a href="#Apache框架" class="headerlink" title="Apache框架"></a>Apache框架</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>Apache在Ubuntu的默认软件库中可用，因此将使用传统的软件包管理工具进行安装。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/6.png" title="Optional title"></p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/7.png" title="Optional title"></p><ul><li><p><code>sudo apt-get update</code>：更新本地包索引。</p></li><li><p><code>sudo apt-get install apache2</code>：安装apache2。</p></li></ul><p><strong>如果不进行本地包的更新，在安装Apache时可能会报各种各样的错误，一般都与依赖项有关。</strong></p><h3 id="调整防火墙"><a href="#调整防火墙" class="headerlink" title="调整防火墙"></a>调整防火墙</h3><p>在测试Apache之前，需要修改防火墙，允许外部访问默认的Web端口。 </p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/8.png" title="Optional title"></p><ul><li><p><code>sudo ufw app list</code>：获得防火墙允许的应用程序配置文件列表。可以看到，有三种可用于Apache的配置文件：（1）Apache ：此配置文件仅打开端口80（正常，未加密的Web流量）；（2）Apache Full：此配置文件打开端口80（正常，未加密的Web流量）和端口443（TLS/SSL加密流量）；（3）Apache Secure ：此配置文件仅打开端口443（TLS/SSL加密流量）。</p></li><li><p><code>sudo ufw allow &#39;Apache Full&#39;</code>：允许Apache Full配置文件的传入流量。</p></li><li><p><code>sudo ufw status</code>：验证防火墙的更改。</p></li></ul><blockquote><p>如果显示status:inactive，需要<code>sudo ufw enable</code>。</p></blockquote><h3 id="检查安装"><a href="#检查安装" class="headerlink" title="检查安装"></a>检查安装</h3><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/9.png" title="Optional title"></p><ul><li><code>sudo systemctl status apache2</code>：验证安装结果。在上述安装过程结束时，Apache会自动启动默认的Web服务器，所以直接运行该命令就能看到结果，无需使用<code>sudo systemctl start apache2</code>。</li></ul><p>看到绿色的<code>active(running)</code>证明Apache框架已经正常的安装并启动了。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/10.png" title="Optional title"></p><p>在虚拟机的浏览器（火狐）键入<code>127.0.0.1</code>（默认），会看到apache的初始网页。</p><h2 id="在Apache框架下配置新的站点"><a href="#在Apache框架下配置新的站点" class="headerlink" title="在Apache框架下配置新的站点"></a>在Apache框架下配置新的站点</h2><h3 id="新站点配置"><a href="#新站点配置" class="headerlink" title="新站点配置"></a>新站点配置</h3><ul><li><code>sudo /etc/init.d/apache2 restart</code>：停止当前的服务器之后再次启动。</li></ul><p>重启Apache服务器后，按照以下步骤配置一个新的网点：<strong>jfy_test</strong>。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/11.png" title="Optional title"></p><ul><li><p><code>sudo mkdir -p /var/www/jfy_test.com/public_html</code>：第一步是创建目录结构，其中包含将为访客提供的网站数据。document root（Apache寻找内容服务将以此作为根目录）将被设置为<code>/var/www</code>，命名为<code>jfy_test.com</code>。在这个目录下，再创建一个<code>public_html</code>，存放要提供的文件。</p></li><li><p><code>sudo chown -R $USER:$USER /var/www/jfy_test.com/public_html</code>：上述目录结构为当前虚拟机下root用户所拥有。如果希望普通用户也能够修改web目录中的文件，可以通过<code>chown</code>更改权限。<code>$USER</code>给出当前登录用户的id。</p></li><li><p><code>sudo chmod -R 755 /var/www</code>：修改当前用户在<code>/var/www</code>下的读取权限。</p></li></ul><blockquote><p>如果当前用户的权限足够，那么修改权限的这两条指令可以跳过。</p></blockquote><ul><li><code>nano /var/www/jfy_test.com/public_html/index.html</code>：使用nano在第一步给出的目录中创建一个将要展示的网页文件，如下所示：</li></ul><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/12.png" title="Optional title"></p><ul><li><p><code>sudo cp /etc/apache2/sites-available/000-default.conf /etc/apache2/sites-available/jfy_test.com.conf</code>：虚拟主机文件是指定虚拟主机的实际配置的文件，并指示Apache Web服务器如何响应各种域请求。Apache附带一个默认的虚拟主机文件：<code>000-default.conf</code>。这里，将这个默认的虚拟主机文件复制一份，以要新配置的站点为命名。这样做的目的是方便修改。</p></li><li><p><code>sudo nano /etc/apache2/sites-available/jfy_test.com.conf</code>：使用nano打开刚才新建的.conf虚拟主机文件，并做如下修改：</p></li></ul><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/13.png" title="Optional title"></p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/14.png" title="Optional title"></p><ul><li><p><code>sudo a2ensite jfy_test.com.conf</code>：使用<code>a2ensite</code>启用新的站点。</p></li><li><p><code>sudo a2dissite 000-default.conf</code>：使用<code>a2dissite</code>禁用定义的默认站点<code>000-default.conf</code>。</p></li></ul><blockquote><p>上面的两条关于虚拟主机文件指定的指令没有先后。</p></blockquote><p>完成上述操作后，再次重启服务器使上述命令生效。</p><h3 id="检查新配置"><a href="#检查新配置" class="headerlink" title="检查新配置"></a>检查新配置</h3><p>在虚拟机的浏览器中键入<code>127.0.0.1</code>，就可以看到新配置的站点。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/15.png" title="Optional title"></p><p>在宿主机中使用虚拟机的ip就可以访问到该站点。</p><ul><li><code>ifconfig</code>：查看ip地址。</li></ul><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/16.png" title="Optional title"></p><p>在宿主机的浏览器（Safari）中键入虚拟机ip<code>172.16.43.132</code>，可以看到新配置的站点。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/17.png" title="Optional title"></p><ul><li><code>sudo nano /etc/hosts</code>：设置本地主机文件，将请求jfy_test.com发送到<code>172.16.43.132</code>。 </li></ul><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/18.png" title="Optional title"></p><p>然后就可以使用<code>www.jfy_test.com</code>来访问站点。</p><p><img src="/images/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/apache/19.png" title="Optional title"></p><blockquote><p>设置本地主机文件不是必选项。</p></blockquote><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><ul><li><p>Apache这个框架在安装方面还是比较友好的，只要安装过程不报错，一条指令就解决。</p></li><li><p>Linux默认的系统权限并不足以完成上述工作，这一点比较烦。对于这类Unix系统的权限还需要进一步的学习。</p></li><li><p>可以在虚拟机环境下以预设好的网址来访问站点，但是并没有做到在宿主机上以这个网址做访问。这一步如果能做到的话就更完美了。</p></li></ul><hr><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><blockquote><p>百度百科：apache： <a href="https://baike.baidu.com/item/Apache/6265">https://baike.baidu.com/item/Apache/6265</a></p></blockquote><blockquote><p>百度百科：VMware Workstation： <a href="https://baike.baidu.com/item/VMware%20Workstation/9884359?fr=aladdin">https://baike.baidu.com/item/VMware%20Workstation/9884359?fr=aladdin</a></p></blockquote><blockquote><p>百度百科：VMware Fusion： <a href="https://baike.baidu.com/item/VMware%20Fusion/5661746?fr=aladdin">https://baike.baidu.com/item/VMware%20Fusion/5661746?fr=aladdin</a></p></blockquote><blockquote><p>如何在Ubuntu 16.04上安装Apache Web服务器： <a href="https://www.cnblogs.com/lfri/p/10522392.html">https://www.cnblogs.com/lfri/p/10522392.html</a></p></blockquote><blockquote><p>如何设置Ubuntu的16.04截至Apache的虚拟主机： <a href="https://www.howtoing.com/how-to-set-up-apache-virtual-hosts-on-ubuntu-16-04/">https://www.howtoing.com/how-to-set-up-apache-virtual-hosts-on-ubuntu-16-04/</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;目标&quot;&gt;&lt;a href=&quot;#目标&quot; class=&quot;headerlink&quot; title=&quot;目标&quot;&gt;&lt;/a&gt;目标&lt;/h1&gt;&lt;p&gt;在VMWare Workst</summary>
      
    
    
    
    
    <category term="Apache" scheme="http://example.com/tags/Apache/"/>
    
    <category term="云计算与分布式系统" scheme="http://example.com/tags/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：PolyFit</title>
    <link href="http://example.com/2020/11/10/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9APolyFit/"/>
    <id>http://example.com/2020/11/10/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9APolyFit/</id>
    <published>2020-11-10T07:00:05.000Z</published>
    <updated>2020-11-12T06:30:36.562Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><blockquote><p>感谢WCX、DHF等在本文写作中给出的建议。</p></blockquote><hr><blockquote><p> PolyFit: Perception-Aligned Vectorization of Raster Clip-Art via Intermediate Polygonal Fitting<br>通过中间多边形拟合实现光栅剪贴画的感知对齐矢量化</p></blockquote><p>来自SIGGRAPH 2020的一篇文章，做的是<strong>光栅图像矢量化</strong>。</p><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><blockquote><p>clip-art images是什么？为什么可以做矢量化？为什么要做矢量化？本篇的核心是什么？</p></blockquote><p>Clip-art images，中文译为剪贴画，通常用于数字媒体中。</p><p>光栅剪贴画图像由明显的彩色区域组成，这些区域由清晰的边界隔开，通常允许清晰的心理向量解释。</p><blockquote><p>Raster clip-art images, which consist of distinctly colored regions separated by sharp boundaries typically allow for a clear mental vector interpretation. </p></blockquote><p><strong>目前，由于各种原因，大量的剪贴画图像仍然以光栅格式创建和存储。但实际上，剪贴画图像可以以矢量形式紧凑无损地表示。</strong>图形矢量化后，在对图形进行调整大小等方面的操作时就会极大的提高效率和提升用户体验。这也是光栅图像矢量化算法开发的最直接驱动，即现代媒体的商业用途。</p><blockquote><p>Clip-art images can be compactly and losslessly represented in vector form; yet, for a variety of reasons, large numbers of clip-art images are still created and stored in raster format…… Vectorizing these images would enable resolution-free reuse of artwork created for legacy displays and facilitate a range of operations such as resizing or editing, which are easier to perform on vector rather than raster data. Vectorizing this data in a man- ner consistent with viewer expectations poses unique challenges, motivating the development of algorithms specifically designed for clip-art vectorization</p></blockquote><p>虽然先前的方法成功地将光栅剪贴画分割成符合观众期望的区域，但在满足观察者期望的同时，<strong>将这些区域之间的光栅边界矢量化仍然是一项挑战：现有的算法仍然经常无法产生与人类偏好一致的向量边界（图1b-c，2b-d）</strong>。我们提出了一种在光栅剪贴画图像中实现区域边界矢量化的新方法，该方法明显优于这些早期方法，产生的结果更符合观众的期望（图1e、2e）。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/1.png" title="Optional title"></p><blockquote><p>Notably, while prior methods [Kopf and Lischinski 2011] successfully segment raster clip-art into regions consistent with viewer expectations, vectorizing the raster boundaries between these regions while meeting observer expectations remains challenging; existing algorithms still often fail to produce vector boundaries consistent with human preferences (Figures 1b-c, 2b-d). We present a new approach for vectorization of region boundaries in raster clip-art images that significantly outperforms these earlier approaches, producing results much better aligned with viewer expectations (Figures 1e, 2e).</p></blockquote><p>基于上述情况，这篇文章提出了PolyFit，可以产生与人类偏好相一致的矢量化图像。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="带有人类主观偏向的矢量化过程的评价标准"><a href="#带有人类主观偏向的矢量化过程的评价标准" class="headerlink" title="带有人类主观偏向的矢量化过程的评价标准"></a>带有人类主观偏向的矢量化过程的评价标准</h2><blockquote><p>Sec. 3</p></blockquote><p>本文的思路更偏向人的主观意愿去做矢量化（本来就是做给人看的，主观评价很重要）,所以文章先叙述了带有人类主观偏向的矢量化过程的评价标准：2018H；Koffka 1955；Wagemans 等人，2012，确定的可能影响人类心理矢量化过程的主要标准有：准确性、简单性、规则性和连续性。</p><blockquote><p>accuracy, simplicity, regularity, and continuity</p></blockquote><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/2.png" title="Optional title"></p><ul><li><p>准确性：预示着人类所设想的矢量化在几何上接近输入栅格边界。</p></li><li><p>简单性：主张输出由<strong>最少数量的几何基元</strong>组成，主张优先考虑直线而不是曲线，主张优先选择曲率变化较小的曲线。</p></li><li><p>规则性：观察者希望更观察到的精确或近似的规律性，如栅格输入中存在的平行性、<strong>对称性</strong>。</p></li><li><p>连续性：观察者倾向于将栅格区域边界“臆想”为<strong>片状连续曲线</strong>。</p></li></ul><h2 id="方法概述"><a href="#方法概述" class="headerlink" title="方法概述"></a>方法概述</h2><blockquote><p>Sec. 3</p></blockquote><p>该方法的输入是光栅剪贴画图像，这些图像都具有一个特征：<strong>由许多具有清晰可辨的区域间边界的独特颜色区域组成。</strong></p><p>整个方法可分为两部分：首先使用[Kopf 和 Lischinski 2011]的框架将光栅图像分割成单色区域。然后将所得区域之间的栅格或片状轴对齐的边界转换为符合观众感知的片状平滑曲线。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/3.png" title="Optional title"></p><p>给定输入图像(a)，我们的第一个多边形拟合阶段(b-c)生成一个在给定精度阈值(b)内连接输入顶点的所有可能边缘的图，并计算该图上相对于感知激励成本函数的最短周期，以获得与观察者期望完全一致的多边形拟合(c)。我们的样条拟合（spline fitting）步骤(d-e)使用学习的分类器将最适合的基元组合拟合到每个多边形角(简单曲线、曲线-直线、直线-直线)(d)，并使用获得的基元集计算一个最适合的样条(e)。我们通过对多边形及其对应的样条(f)进行正则化，以获得最终的向量输出(g)，从而获得更规则的结果。</p><p><strong>上图的d-e、f是本文的核心内容，即边界的转化工作。</strong></p><h2 id="中间多边形近似"><a href="#中间多边形近似" class="headerlink" title="中间多边形近似"></a>中间多边形近似</h2><h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><p>文章将中间多边形近似的问题表述为：寻找一个环（cycle），该环是最小化定义在有向图G=(V,E)中的成对和三对边上（pairs and triplets of edges）的能量函数（energy function）。</p><blockquote><p>We formulate the extraction of an intermediate polygonal approximation as the problem of finding a cycle that minimizes an energy function defined on pairs and triplets of edges in a directed graph G = (V,E),wheretheverticesV representcandidatecornersandthe edges E represent candidate polygon segments (see Fig. 5b).</p></blockquote><p>为了方便高效的计算，原文通过只包括“有合理可能成为最终多边形一部分的顶点和边”来保持被操作图的小尺寸。然后，通过图的构造，以平衡准确性、简单性和连续性为标准计算在这个图上的最优周期。</p><p>最终，寻找在成对和三倍边上的能量函数的最优周期问题简化为一个经典的最短周期问题。</p><blockquote><p>and continuity (Sec. 4.2, Fig. 5c) via a graph construction that reduces the problem of finding the optimal cycle of an energy function defined on pairs and triplets of edges to a classical shortest-cycle problem.</p></blockquote><h3 id="图形结构"><a href="#图形结构" class="headerlink" title="图形结构"></a>图形结构</h3><p>使用想塑角到边缘的曼哈顿距离来定义大致的形状：丢弃所有违反曼哈顿距离标准的边缘，并要求所有错误分类的像素都位于直线的一侧，丢弃不满足这一属性的边缘。</p><p>简言之，这部分工作的目的是：<strong>给定大致范围</strong></p><h3 id="多边形近似"><a href="#多边形近似" class="headerlink" title="多边形近似"></a>多边形近似</h3><p>在评估多边形的最优性时，根据第3节中确定的三个感知标准（准确性、简单性和连续性）来评估。</p><h4 id="准确性"><a href="#准确性" class="headerlink" title="准确性"></a>准确性</h4><p>使用之前计算出的错分像素集Pij来衡量每个边缘相对于栅格边界的准确性。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/4.png" title="Optional title"></p><h4 id="连续性"><a href="#连续性" class="headerlink" title="连续性"></a>连续性</h4><p>用多边形角上的角Aijk=eijejk作为连续性的离散代表。Aijk是角处的内外角中较小的角。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/5.png" title="Optional title"></p><h3 id="简单性"><a href="#简单性" class="headerlink" title="简单性"></a>简单性</h3><p>将简单性表达为对最小曲率变化和较小多边形边缘数的偏好。</p><p>以连续角度之间的相似性来衡量曲率变化。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/6.png" title="Optional title"></p><p>为了最大限度地减少拐点的出现（与幅度无关），为每个拐点边缘增加一个绝对拐点惩罚，即binfl=0.1，并使用两个角度中较小的一个（较尖锐的一个）来评估拐点的幅度。这个幅度的饱和极限设置为linfl=90°。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/7.png" title="Optional title"></p><p>简单性也主张减少多边形边的数量。为此，我们给所有图边缘分配一个小的惩罚ε=1e-3，并进一步惩罚冗余的像素长边缘。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/8.png" title="Optional title"></p><h3 id="综合多边形成本"><a href="#综合多边形成本" class="headerlink" title="综合多边形成本"></a>综合多边形成本</h3><p>综合多边形成本C(P)是上述项在所有边和连续边的对和三倍体上的总和。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/9.png" title="Optional title"></p><h2 id="样条拟合"><a href="#样条拟合" class="headerlink" title="样条拟合"></a>样条拟合</h2><blockquote><p>Sec. 5</p></blockquote><p>算法的第二步是将封闭地、片状光滑地样条拟合到多边形上。</p><p>计算这种拟合需要求解几组变量,分别是：</p><ul><li><p>定义样条地基元序列（由其类型定义）；</p></li><li><p>基元与其拟合地栅格边界段之间地映射（特别是基元断点和栅格边界位置之间地对应关系）；</p></li><li><p>基元地形状参数（控制点的位置）。</p></li></ul><h3 id="样条基元"><a href="#样条基元" class="headerlink" title="样条基元"></a>样条基元</h3><p>根据以下设定：</p><ul><li><p>多边形和样条都要准确地近似于输入的栅格，中间多边形边缘的切线要与最终样条的<strong>切线</strong>密切相关；</p></li><li><p>希望样条将通过接近多边形边缘中点，并且在这些中点附近的样条切线与边缘切线相似</p></li><li><p>希望角样条部分至少和它们匹配的多边形角一样连续（即最多有一个C0不连续），并期望它们尊重多边形角所施加的基元数的上界（即两个）。</p></li></ul><p>确定了三种角跨基元配置，它们反映了简单性和连续性之间所有不同的平衡选择。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/10.png" title="Optional title"></p><p>总体目标是产生一个最佳平衡平滑性、简单性和输入栅格的准确性的养条。为了找到这样的养条，在分类过程中，使用优先考虑连续性来考虑配置类型。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/11.png" title="Optional title"></p><h3 id="训练和验证"><a href="#训练和验证" class="headerlink" title="训练和验证"></a>训练和验证</h3><p>对拥有100棵树的森林进行了训练，在23张不同分辨率的输入图像上手工标注多边形角，显示出各种各样的几何构型。50%用于训练，50%用于测试，在测试数据集上达到了99.3%的准确率。</p><p>二元决策使用的是置信度，阈值是0.75。</p><h2 id="规范化"><a href="#规范化" class="headerlink" title="规范化"></a>规范化</h2><blockquote><p>Sec. 6</p></blockquote><p>因为前面提到过，人类观察者能够识别输入数据中的规律性，并期望在拟合或矢量化过程中保留这些规律性，所以，<strong>这部分工作的目的是在最终输出中保留输入的正则性，关注的是对称性、平行性、连续性和轴对齐。</strong></p><ul><li><p>正交和轴对齐的边：通过不允许对两个入射多边形角使用（单一）曲线基元配置（从而迫使方法沿这些边缘使用曲线线或线型配置），使长度至少为各自边界框边范围50%的轴对齐边缘更加突出。</p></li><li><p>平行边缘：检测并强制执行存在于栅格输入中的突出的轴对齐平行边缘：如果边缘之间的距离不超过它们的重叠长度，才认为这些边缘是突出的。</p></li><li><p>对称性：如果两个栅格对称性发生冲突，会优先考虑非规则化多边形中先验存在的对称性，并优先考虑较长的对称边界路径。</p></li><li><p>（曲线）延续：在光栅层面(轴对齐)和多边形层面(任意方向)都检测并强制执行连续。</p></li></ul><h2 id="多色输入"><a href="#多色输入" class="headerlink" title="多色输入"></a>多色输入</h2><blockquote><p>Sec. 7</p></blockquote><p>上面叙述的都是一个由两个颜色均匀的区域组成的图像的情况。但现实生活中，图像通常包含多个彩色区域。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/12.png" title="Optional title"></p><p>对于这种输入，处理方法是：将两个相邻的区域都将其交界处分类为平滑配置，只有当分别对区域进行矢量化会产生一个以交界处为顶点的平滑配置类型的多边形，如果不是这种情况，最多只允许两个相邻区域中的一个区域被分类为光滑配置。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><blockquote><p>Sec. 8</p></blockquote><h2 id="与以往结果的对比"><a href="#与以往结果的对比" class="headerlink" title="与以往结果的对比"></a>与以往结果的对比</h2><p>文章将其结果与2018H等最近两个排名最高的方法在81个输入(Potrace[Selinger 2003]以及2018H)上产生的结果进行了比较。测试数据集包括37个单一区域边界的输入，28个低分辨率多色输入，以及16个中高分辨率的多色输入。对所有方法都使用默认参数。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/13.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/14.png" title="Optional title"></p><p>该部分实验主要着眼在区域分辨率为100以下的输入。对于较高的分辨率，拟合精度对于最终结果起决定性作用，所有方法产生的结果都相当相似。<strong>但在较低的分辨率下（16×及以下），原文方法有着明显的优势，并且在94%的情况下优于2018H的结果。</strong></p><h2 id="与手动矢量化的对比"><a href="#与手动矢量化的对比" class="headerlink" title="与手动矢量化的对比"></a>与手动矢量化的对比</h2><p>文章将其结果与艺术家进行的手动矢量化结果进行了对比。因为手动矢量化多色输入非常耗时（一个区域需要 30多分钟才能准确拟合)，所以这部分只针对2018H提供的15张矢量化二元图像进行了试验。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/15.png" title="Optional title"></p><p>在55%的结果中，参与者认为本文方法得出的结果比艺术家给出的结果更好。</p><p>当艺术家的结果被认为是更好的输入时（如图17c），猜测是由于识别–因为艺术家的输出反映了作者对所􏰑绘形状的知识。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文给出了PolyFit这种新的剪贴画矢量化方法，并给出了与现有的替代方法相比的结果。</p><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul><li><p>能产生更符合观察者意愿的结果（文章的核心立意的努力方向）。</p></li><li><p>在低分辨率数据上表现得特别好（结果提到了16×、32×和64×）。</p></li></ul><h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h2><blockquote><p>Sec. 8</p></blockquote><p>对于分辨率为32×和64×的单个区域，该方法分别需要0.5秒和1.2秒（中位数）来完成矢量化。大部分的运行时间是分布在初始花键拟合和后续的角反馈循环之间的时间大致相等。多边形􏰐取阶段平均占总运行时间的 10%到20%，并且随着输入中存在更多的正则性而增加，这就需要对多边形进行更多的正则化工作。</p><p>上述时间是在八代英特尔酷睿 i7、3.7GHz主频CPU下进行的实验结果。远快于2018H的时间（后者平均需要30到50倍的时间）。</p><h2 id="局限"><a href="#局限" class="headerlink" title="局限"></a>局限</h2><blockquote><p>Sec. 8</p></blockquote><p>该方法目前的实现只对单个边界进行规范化。在许多情况下，它默认实现了边界间的平行性。对这种平行性的明确检测和执行将是我们方法的一个重要的实际扩展。</p><blockquote><p>Our current implementation only regularizes individual boundaries. It achieves inter-boundary parallelism by default for many cases. Explicit detection and enforcement of such parallelism would be an important practical extension of our method. In the future, one can similarly extend our regularization step to address regularities between regions, e.g., using continuation detection, complementing our core method.</p></blockquote><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>本文的研究建立在样本点的逼近、笔触美化、自然图像矢量化、艺术家生成的图像矢量化这几个领域和解决剪贴画矢量化的一些商业软件包的基础上。其中本文主要引用了Hoshyari等在2018年发表的Perception-Driven Semi-Structured Boundary Vectorization，本文工作是在该文的基础上做的改进。此外，本文使用的中间多边形拟合方法受到Potrace[Selinger 2003]启发，在其基础上约束多边形以准确地保存输入细节，有更准确的最终拟合。</p><h2 id="Perception-Driven-Semi-Structured-Boundary-Vectorization"><a href="#Perception-Driven-Semi-Structured-Boundary-Vectorization" class="headerlink" title="Perception-Driven Semi-Structured Boundary Vectorization"></a>Perception-Driven Semi-Structured Boundary Vectorization</h2><blockquote><p>ACM Transaction on Graphics 37, 4 (2018)<br>https: //doi.org/10.1145/3197517.3201312</p></blockquote><p>这篇文章针对艺术家生成的光栅输入的可操作的矢量化算法。给出了一种算法，能够通过将学习到的分类器预测与关于角认知的见解相结合，从有限的训练数据中获得感知上一致的角分类。方法步骤如下：</p><ul><li><p>模型学习：使用随机森林从带有人工标注的角的训练栅格图像集合中学习推理局部的几何环境，并使用一个训练好的分类器计算局部角概率。</p></li><li><p>模型整合：将数据驱动的模型预测与后续的基于感知的角处理步骤相结合，逐渐修剪模型输出的角集，直到使用得到的角计算的边界矢量化符合标准。</p></li><li><p>模型优化：框架定位角的同时，并在它们之间拟合简单的G1连续样条曲线。角集最终确定后，继续进一步简化输出。</p></li></ul><p>框架包括三个主要步骤：(a)潜在角检测、(b)迭代角去除、(c)全局正则化。颜色区分不同的曲线类型。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/16.png" title="Optional title"></p><h2 id="Potrace-a-polygon-based-tracing-algorithm"><a href="#Potrace-a-polygon-based-tracing-algorithm" class="headerlink" title="Potrace: a polygon-based tracing algorithm"></a>Potrace: a polygon-based tracing algorithm</h2><blockquote><p><a href="http://potrace.sourceforge.net/">http://potrace.sourceforge.net</a></p></blockquote><p>将位图转换为矢量轮廓图，这种逆向过程称为跟踪。</p><p>这篇文章描述了一个简单，有效的跟踪算法：Potrace。该算法输出是由Bezier曲线构成的光滑轮廓，它使用多边形作为图像的中间表示。</p><ul><li><p>路径分解：位图被分解成许多路径，这些路径形成了黑白区域之间的边界。</p></li><li><p>最优多边形生成：用最优多边形逼近每条路径。</p></li><li><p>光滑轮廓：将每个多边形转化为光滑的轮廓。</p></li><li><p>曲线优化（可选）：通过在可能的地方连接连续的Bezier曲线片段来优化生成的曲线。</p></li><li><p>输出：以所需的格式生成输出。</p></li></ul><p>一个完整的示例：(a)原始位图；(b)路径分解和最优多边形；(c)顶点调整、角点分析和平滑；(d)曲线优化；(e)最终输出。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/PolyFit/17.png" title="Optional title"></p><hr><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="光栅图像与矢量图像"><a href="#光栅图像与矢量图像" class="headerlink" title="光栅图像与矢量图像"></a>光栅图像与矢量图像</h2><p>所有的电子艺术图像可被分为两种核心类型：光栅图像和矢量图像。</p><p><strong>简言之，光栅图像由像素点组成，矢量是由连接的线组成的图像。</strong></p><h3 id="光栅图像"><a href="#光栅图像" class="headerlink" title="光栅图像"></a>光栅图像</h3><h4 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h4><p>光栅图也称为<strong>位图</strong>、点阵图、像素图。</p><p>简言之，最小单位由像素构成的图，就被称为光栅图像。在光栅图像中，只包含像素点的信息，每个像素点有自己的颜色。</p><p>光栅图形最为人知晓的特征，就是<strong>它在放大时会失真</strong>。</p><p>这种格式的图适合存储图形不规则，而且颜色丰富没有规律的图，比如照相，扫描等。</p><h4 id="分辨率"><a href="#分辨率" class="headerlink" title="分辨率"></a>分辨率</h4><p>光栅图像或扫描图像的分辨率以DPI（Dots Per Inch，每英寸点数）表示。</p><p>DPI原来是印刷上的记量单位，意思是每英寸上，所能印刷的网点数（Dot Per Inch）。但随着数字输入，输出设备快速发展，大多数的人也将数字影像的解析度用DPI表示。</p><p>但较为严谨的情形下，印刷时计算的网点（Dot）和电脑显示器的显示像素（Pixel）并非相同，所以较专业的人士，会用PPI(Pixel Per Inch)表示数字影像的解析度，以区分二者。</p><h4 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h4><p>为了准确地记录光栅图像文件，图形软件必须跟踪大量信息，包括像素点集中每个像素的确切位置和颜色。这就导致光栅图像的文件很大。高DPI和更大的颜色深度也会产生更大的文件大小。典型的“2X3” 150 dpi黑白光栅图像徽标的文件大约为70K。保存为300 dpi 24位（百万种颜色）光栅图像文件时，大小可能会超过100倍。</p><p>常见的光栅图像格式包括BMP（Windows位图），PCX（画笔），TIFF（标签交错格式），JPEG（联合图像专家组），GIF（图形交换格式），PNG（便携式网络图形），PSD（Adobe PhotoShop）和CPT（Corel PhotoPAINT）。</p><h3 id="矢量图像"><a href="#矢量图像" class="headerlink" title="矢量图像"></a>矢量图像</h3><h4 id="特征-1"><a href="#特征-1" class="headerlink" title="特征"></a>特征</h4><p>矢量图像是生成对象的连接线和曲线的集合。在矢量插图程序中创建矢量图像时，会插入节点或绘图点，并且线条和曲线将注释连接在一起。这与“连接点”的原理相同。</p><h4 id="分辨率-1"><a href="#分辨率-1" class="headerlink" title="分辨率"></a>分辨率</h4><p><strong>矢量图像的分辨率由数学定义（不是像素），所以它们可以按比例放大或缩小而不会失真。</strong>当插图（绘图）程序向上或向下调整矢量图像的大小时，它只是将对象的数学描述乘以缩放因子。<strong>这也是矢量图形在剪贴画中特别受欢迎的一个重要原因。</strong></p><h4 id="文件-1"><a href="#文件-1" class="headerlink" title="文件"></a>文件</h4><p>矢量图像不需要跟踪图像中的每个像素，只需要数学描述。因此，矢量文件的文件大小非常小。矢量文件的主要内容就是数学的描述。<strong>所以，矢量文件非常适合通过网络传输。</strong></p><p>常见的矢量格式包括EPS（Encapsulated PostScript），WMF（Windows图元文件），AI（Adobe Illustrator），CDR（CorelDraw），DXF（AutoCAD），SVG（可缩放矢量图形）和PLT（Hewlett Packard图形语言图文件）。</p><h2 id="曼哈顿距离"><a href="#曼哈顿距离" class="headerlink" title="曼哈顿距离"></a>曼哈顿距离</h2><p>曼哈顿距离为：在平面直角坐标系中，两点在坐标轴方向上的距离之和，即d(i,j)=|xi-xj|+|yi-yj|。</p><p>对于一个具有正南正北、正东正西方向规则布局的城镇街道，从一点到达另一点的距离正是在南北方向上旅行的距离加上在东西方向上旅行的距离，因此，曼哈顿距离又称为出租车距离。</p><p>曼哈顿距离不是距离不变量，当坐标轴变动时，点间的距离就会不同。</p><p>曼哈顿距离示意图在早期的计算机图形学中，屏幕是由像素构成，是整数，点的坐标也一般是整数，原因是浮点运算很昂贵，很慢而且有误差，如果直接使用欧氏距离(欧几里德距离：在二维和三维空间中的欧氏距离的就是两点之间的距离），则必须要进行浮点运算，如果使用曼哈顿距离，则只要计算加减法即可，这就大大提高了运算速度，而且不管累计运算多少次，都不会有误差。</p><h2 id="多边形拟合"><a href="#多边形拟合" class="headerlink" title="多边形拟合"></a>多边形拟合</h2><p>这是一个专门的研究方向。不管是已有的进行多边形拟合的轮子还是使用多边形拟合做的图形学研究，都有这个概念。</p><blockquote><p>opencv里有多边形拟合的函数approxPolyDP，natlab里有名为polyfit的函数等。</p></blockquote><p>所介绍的这篇论文使用了这个概念去做矢量化。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;感谢WCX、DHF等在本文写作中给出的建议。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt; PolyFit: P</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="基础知识" scheme="http://example.com/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    <category term="计算机图形学" scheme="http://example.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：Image Super-Resolution as a Defense Against Adversarial Attacks</title>
    <link href="http://example.com/2020/10/12/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AImage-Super-Resolution-as-a-Defense-Against-Adversarial-Attacks/"/>
    <id>http://example.com/2020/10/12/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AImage-Super-Resolution-as-a-Defense-Against-Adversarial-Attacks/</id>
    <published>2020-10-12T06:40:56.000Z</published>
    <updated>2020-10-26T08:02:46.442Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><p>20年发在TIP上的一篇文章。</p><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>CNN在很多计算机视觉的任务都有好的表现，比如图像分类、目标检测、语义分割和视觉问答等。以及在现实生活中的场景也有好的应用，比如汽车的自动驾驶和疾病诊断模型。</p><p>这么多的应用，就要求这些基于CNN的模型具有良好的鲁棒性。但是有一些研究表明，CNN很容易被扭曲的自然图像和一些精心制作的、人类不可察觉的附加扰动欺骗。这些被称为对抗性例子的扭曲图像，已经被进一步证明可以在不同的体系结构之间传输。比如，从Inception v3模型生成的对抗性示例能够欺骗CNN的其他架构。</p><p>业界很早就注意到了这个问题，并且对这些对抗性的攻击做了防御方向的研究。这些研究可分为两类：</p><ul><li><p>模型特定：其目的是通过对抗训练或参数平滑调整特定模型的参数。这种方法通常需要计算量大的可微变换。此外，这些转换很容易受到进一步的攻击，因为对手可以利用可微模块来规避这些攻击。</p></li><li><p>模型不可知论：它们通过应用各种变换来减轻输入图像域中的对抗性扰动的影响。这类技术的例子包括JPEG压缩、基于中心凹的方法，这些方法裁剪图像背景、随机像素偏移和随机图像填充和重新调整大小。与可微模型特定方法相比，大多数模型不可知方法计算速度快，并且在输入域进行变换，使其更为有利。然而，这些方法大多在去除敌方噪声时丢失了关键的图像内容，导致对非攻击图像的分类性能较差。</p></li></ul><blockquote><p>We can broadly categorize these defenses along two directions: the first being model-specific mechanisms, which aim to regularize a specific model’s parameters through adversarial training or parameter smoothing [18], [20], [21], [28], [30]. Such methods often require differentiable transformations that are computationally demanding. Moreover these transformations are vulnerable to further attacks, as the adversaries can circumvent them by exploiting the differentiable modules. The second category of defenses are model-agnostic. They mitigate the effect of adversarial perturbations in the input image domain by applying various transformations. Examples of such techniques include JPEG compression [31], [32], foveation-based methods, which crop the image background [33], random pixel deflection [27] and random image padding &amp; re-sizing [19]. Compared with differentiable model-specific methods, most of the model-agnostic approaches are computationally faster and carry out transformations in input domain, making them more favorable. However, most of these approaches lose critical image content when removing adversarial noise, which results in poor classification performance on non-attacked images.</p></blockquote><p>基于上，这篇文章提出了一种基于模型不可知的防御机制，它可以抵抗最近提出的各种各样的对手攻击，并且不受信息损失的影响。</p><p>提出的防御是基于SR的，它选择性地向图像中添加高频分量，并消除对手添加的噪声干扰。我们把流形样本重新映射到图1上。通过小波域滤波进一步抑制了附加噪声的影响，并通过对高分辨率图像的全局合并操作将其固有地最小化。提出的图像超分辨率和基于小波滤波的防御方法形成了一个联合不可微模块，该模块可以有效地恢复敌方扰动图像的原始类别标签。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DAAA/1.png" title="Optional title"></p><h1 id="对抗性攻击"><a href="#对抗性攻击" class="headerlink" title="对抗性攻击"></a>对抗性攻击</h1><p>对输入样本故意添加一些人无法察觉的细微的干扰，导致模型以高置信度给出一个错误的输出，称为对抗样本。</p><p>以深度学习为主流的人工智能应用越来越广泛之后，陆续又出现了对于人工智能应用的攻击，主要分为两种：一是白盒测试，即深度学习的模型架构和参数都已经的情况下，这种场景的攻击一般可以进行参数的修改来达到攻击的效果；二是黑盒测试，即上述情况未知的情况下进行攻击，这时候采用的攻击手段主要是对抗样本。</p><p>对抗样本现在已经广泛应用于人脸识别、声纹识别等相关应用场景。</p><blockquote><p>2013：Intriguing properties of neural networks</p></blockquote><blockquote><p>2014：Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images</p></blockquote><p>原文中提到了6种对抗性攻击方法。</p><h2 id="FGSM"><a href="#FGSM" class="headerlink" title="FGSM"></a>FGSM</h2><blockquote><p>Explaining and Harnessing Adversarial Examples<br><a href="https://arxiv.org/abs/1412.6572">https://arxiv.org/abs/1412.6572</a></p></blockquote><p>在图像攻击算法中，FGSM（fast gradient sign method）是非常经典的一个算法。</p><p>在训练分类模型时，网络基于输入图像学习特征，然后经过softmax层得到分类概率，接着损失函数基于分类概率和真实标签计算损失值，回传损失值并计算梯度（也就是梯度反向传播），最后网络参数基于计算得到的梯度进行更新，网络参数的更新目的是使损失值越来越小，这样模型分类正确的概率也就越来越高。</p><p><strong>图像攻击的目的是不修改分类网络的参数，而是通过修改输入图像的像素值使得修改后的图像能够扰乱分类网络的分类。</strong></p><p><strong>常规的分类模型训练在更新参数时都是将参数减去计算得到的梯度，这样就能使得损失值越来越小，从而模型预测对的概率越来越大。</strong>既然无目标攻击是希望模型将输入图像错分类成正确类别以外的其他任何一个类别都算攻击成功，那么只需要损失值越来越大就可以达到这个目标，也就是模型预测的概率中对应于真实标签的概率越小越好，这和原来的参数更新目的正好相反。<strong>因此只需要在输入图像中加上计算得到的梯度方向，这样修改后的图像经过分类网络时的损失值就比修改前的图像经过分类网络时的损失值要大，换句话说，模型预测对的概率变小了。</strong></p><p>简言之，FGSM算法的主要内容，一方面是基于输入图像计算梯度，另一方面更新输入图像时是加上梯度，而不是减去梯度，这和常见的分类模型更新参数正好背道而驰。</p><p>根据一般的训练过程，可以将损失值回传到输入图像并计算梯度。接下来可以通过<code>sign</code>函数计算梯度的方向，sign函数是用来求数值符号的函数，比如对于大于0的输入，输出为1，对于小于0的输入，输出为-1，对于等于0的输入，输出为0。<strong>之所以采用梯度方向而不是采用梯度值是为了控制扰动的L∞距离</strong>，这是FGSM算法的评价指标。</p><h2 id="I-FGSM"><a href="#I-FGSM" class="headerlink" title="I-FGSM"></a>I-FGSM</h2><blockquote><p>Adversarial examples in the physical world<br><a href="https://arxiv.org/abs/1607.02533">https://arxiv.org/abs/1607.02533</a></p></blockquote><p>FGSM算法从梯度的角度做攻击，速度比较快，这是该算法比较创新的地方。但是FGSM算法只涉及单次梯度更新，有时候单次更新并不足以攻击成功，因此，在此基础上推出迭代式的FGSM，这就是I-FGSM（iterative FGSM）。</p><h2 id="MI-FGSM"><a href="#MI-FGSM" class="headerlink" title="MI-FGSM"></a>MI-FGSM</h2><blockquote><p>Boosting Adversarial Attacks with Momentum<br><a href="http://arxiv.org/abs/1710.06081">http://arxiv.org/abs/1710.06081</a></p></blockquote><p>基于动量迭代梯度的方法。</p><p>除了基于迭代梯度的方法用梯度来迭代扰动输入以使损失函数最大化以外，基于动量的方法还在迭代过程中沿损失函数的梯度方向累积速度矢量，为了稳定更新方向并避免出现不良的局部最大值。我们表明，由动量迭代方法生成的对抗样本在白盒和黑盒攻击中均具有较高的成功率。所提出的方法减轻了白盒攻击和可传递性之间的折衷，并且比单步方法和原始迭代方法具有更强的攻击算法。</p><h2 id="DeepFool"><a href="#DeepFool" class="headerlink" title="DeepFool"></a>DeepFool</h2><blockquote><p>DeepFool: a simple and accurate method to fool deep neural networks<br><a href="https://arxiv.org/abs/1511.04599">https://arxiv.org/abs/1511.04599</a></p></blockquote><p>基于迭代且线性近似的方案提出了一种计算对抗样本的方法DeepFool。大量的实验结果表明，这是一种非常精确且有效的对抗扰动计算方法，它可以提供一个有效的方式去评估分类器的鲁棒性并且可以通过恰当的微调改善分类器的表现。</p><h2 id="C-amp-W"><a href="#C-amp-W" class="headerlink" title="C&amp;W"></a>C&amp;W</h2><blockquote><p>Towards Evaluating the Robustness of Neural Networks<br><a href="https://arxiv.org/abs/1608.04644">https://arxiv.org/abs/1608.04644</a></p></blockquote><h2 id="DIFGSM-and-MDIFGSM"><a href="#DIFGSM-and-MDIFGSM" class="headerlink" title="DIFGSM and MDIFGSM"></a>DIFGSM and MDIFGSM</h2><blockquote><p>Improving transferability of adversarial examples with input diversity<br><a href="https://arxiv.org/abs/1803.06978">https://arxiv.org/abs/1803.06978</a></p></blockquote><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>文章给出的防御方法属于第二类防御机制。</p><p>整个方法可分为两个部分，它们共同构成一个不可微管道，很难绕过。首先应用小波去噪来抑制任何噪声模式。然后，核心部分是超分辨率操作，它在提高像素分辨率的同时同时消除对抗模式。</p><p>实验表明，仅图像超分辨率就足以恢复分类器对正确类别的信念；然而，第二步提供了额外的鲁棒性，因为它是一个不可微的去噪操作。</p><h2 id="Super-Resolution-as-a-Defense-Mechanism"><a href="#Super-Resolution-as-a-Defense-Mechanism" class="headerlink" title="Super Resolution as a Defense Mechanism"></a>Super Resolution as a Defense Mechanism</h2><h3 id="背景解释"><a href="#背景解释" class="headerlink" title="背景解释"></a>背景解释</h3><p>方法源于<code>manifold assumption</code>（流形假设），假设自然图像位于低维的manifolds上（这解释了为什么低维深层特征表示能够准确地捕捉真实数据集的结构），扰动图像位于自然图像的低维流形之外。</p><p><strong>尽管在视觉上看起来完全相同，但敌对和干净的数据并不是孪生的。</strong></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DAAA/2.png" title="Optional title"></p><p>上图显示了自然图像的低维流形。来自真实世界数据集（比如ImageNet）的数据点是从自然图像的分布中取样的，可以认为它们位于流形上。这类图像被称为<code>in-domain</code>。通过添加对抗性噪声来破坏这些域内图像，可以使图像脱离流形。一个学习从流形外的图像生成流形上的图像的模型可以在检测和防御敌方攻击方面发挥很大的作用。</p><p>那么解决的思路就是使用图像超分辨率作为一个映射函数，将多种对抗性样本重新映射到自然图像流形上。以这种方式，通过增强图像的视觉质量来实现对抗干扰的鲁棒性。这种方法比其他截断关键信息以获得健壮性的防御机制具有显著的优势。</p><h3 id="SR-Network"><a href="#SR-Network" class="headerlink" title="SR Network"></a>SR Network</h3><p>防御机制的一个必要特征是能够抑制对手添加的欺诈干扰。<strong>由于这些扰动通常是高频细节</strong>，使用一个超分辨率网络，它明确地使用剩余学习来关注这些细节。这些细节被添加到每个残差块的低分辨率输入中，最终生成高质量、超分辨率的图像。这项工作中考虑用到的是EDSR。</p><h3 id="对光谱分布的影响"><a href="#对光谱分布的影响" class="headerlink" title="对光谱分布的影响"></a>对光谱分布的影响</h3><p>对抗性图像包含高频模式，超分辨率操作进一步向恢复图像注入高频模式。</p><p>这有两个主要的好处：一是新增加的高频模式平滑了图像的频率响应（第5列）；二是超分辨率摧毁了试图愚弄模型的对抗模式。</p><blockquote><p>这里没看懂</p></blockquote><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DAAA/3.png" title="Optional title"></p><p>左下角示出了干净图像和恢复图像之间的确切区别，这说明了与原始图像相比，恢复图像相对更干净，但是具有更多的高频细节。对比原始噪声信号（图3中的左上角）和恢复图像中的剩余噪声（图3中的左下角），我们可以观察到SR网络丢弃了大部分的噪声扰动；但是仍然存在稀疏的噪声痕迹。此外，SR网络强化了图像中沿显著边界的高频变化（注意沿鸟类边界的响应）。</p><h2 id="小波降噪"><a href="#小波降噪" class="headerlink" title="小波降噪"></a>小波降噪</h2><p><strong>在空间或频域中进行图像去噪会导致纹理细节的丢失，这不利于这篇文章想要做到在去噪图像上实现像图像一样的干净性能的目标。</strong></p><p><code>wavelet shrinkage</code>（小波收缩）的主要原理是：真实世界信号的离散小波变换（DWT）本质上是稀疏的。<strong>与图像平滑去除图像中高频分量不同，真实世界图像的小波变换具有与重要图像特征对应的大系数，通过对较小系数应用阈值可以去除噪声。</strong></p><blockquote><p>这里也没看懂</p></blockquote><h3 id="阈值化"><a href="#阈值化" class="headerlink" title="阈值化"></a>阈值化</h3><p>阈值参数决定了如何有效地收缩小波系数和去除图像中的敌对噪声。</p><p>在实际应用中，有两种阈值方法：硬阈值和软阈值。硬阈值：<strong>完全保留大于t的系数</strong>，将小噪声系数降为零，然后进行小波逆变换，得到保留关键信息并抑制噪声的图像。</p><p>文章给出的模型选择了软阈值，因为它减少了在硬阈值中发生的突变。此外，硬阈值过度平滑图像，这会降低干净的非对抗性图像的分类精度。</p><p>在实验中，做了<code>visureshrink</code>和<code>bayeshrink</code>软阈值，发现后者的性能更好，提供了视觉上更好的去噪效果。</p><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DAAA/4.png" title="Optional title"></p><p>算法1给出了端到端防御方案的算法描述。</p><p>首先用软小波去噪来平滑对抗性噪声的影响。其次，采用超分辨率作为映射函数来提高图像的视觉质量。超分辨率图像将敌方示例映射到高分辨率空间中的自然图像流形，而在低分辨率空间中，自然图像流形位于流形之外。然后，恢复的图像通过与生成对抗性示例的相同的预先训练的模型。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>做了很多的对比实验，这里给出在ILSVRC验证集的5000幅图像上与最新防御机制的性能的比较。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DAAA/5.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DAAA/6.png" title="Optional title"></p><p>上标给出了5000个ILSVRC验证集图像上各种防御机制的<code>destruction rates</code>。破坏率定义为成功防御图像的比率，100%的破坏率意味着在应用防御机制后，所有图像都被正确分类。</p><p>“无防御”显示了模型对生成的对抗性图像的性能。“无防御”下的值较低表示攻击很强。结果表明，与单步攻击相比，迭代攻击具有更好的欺骗模型的能力。然而，迭代攻击是不可转移的，更容易防御。类似地，与非目标攻击相比，有针对性的攻击更容易防御，因为它们往往过度符合攻击模型。</p><p>对于迭代攻击（C&amp;W和DeepFool），Random resize+Padding和PD都获得了相似的性能，成功地恢复了大约90%的图像。在比较中，提出的基于超分辨率的防御可以恢复大约96%的图像。对于单步攻击类别，随机调整大小+填充无法防御。为了克服这一局限性，采用了对抗性增强的集成模型进行防御。</p><ul><li><p>与基于JPEG压缩的防御方法相比，该方法对FGSM（ε=10）的性能提高了31.1%。</p></li><li><p>在单步攻击类别（FGSM-10），该方法比随机调整+填充和PD分别高出26.7%和21.0%。</p></li><li><p>最近提出的强攻击（MDI2FGSM），所有的防御技术（JPEG压缩、随机大小调整+填充、绗缝+TVM和PD）都严重失败，仅恢复图像的1.3%、5.8%、1.7%和21.9%。相比之下，所提出的基于图像超分辨率的防御方法可以成功地恢复31.3%的图像。</p></li></ul><p>以及给出了为什么要用EDSR的理由（就是效果好）。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-DAAA/7.png" title="Optional title"></p><h1 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h1><p>来自原文的叙述：</p><ul><li><p>Through extensive empirical evaluations, we show image super-resolution to be an effective defense strategy against a wide range of recently proposed state-of-the-art attacks in the literature [15], [16], [34]–[37]. Using Class Activation Map visualizations, we demonstrate that super-resolution can successfully divert the attention of the classifier from random noisy patches to more distinctive regions of the attacked images (see Fig. 8 and 9).</p></li><li><p>Super-resolving an adversarial image projects it back to the natural image manifold learned by deep image classification networks.</p></li><li><p>Unlike existing image transformation based techniques, which introduce artifacts in the process of overcoming adversarial noise, the proposed scheme retains critical image content, and thus minimally impacts the classifier’s performance on clean, non-attacked images.</p></li><li><p>The proposed defense mechanism tackles adversarial attacks with no knowledge of the target model’s architecture or parameters. This can easily complement other existing model-specific defense methods.</p></li></ul><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>code：<a href="https://github.com/aamir-mustafa/super-resolution-adversarial-defense">https://github.com/aamir-mustafa/super-resolution-adversarial-defense</a></p><p>文章：<a href="http://arxiv.org/abs/1901.01677v1">http://arxiv.org/abs/1901.01677v1</a></p><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><ul><li><p>这篇文章做的是对抗样本的防御算法，整体思路是用小波+SR。并不是SISR的发展研究。</p></li><li><p>文章提到了算法的优点，其中有一点：“our purification approach is equally applicable to other computer vision tasks beyond classification”，这么看来泛用性还是很强。</p></li><li><p>说实话写的比较难，看不太懂很多细节的部分，尤其是小波那部分为什么要用，提到了“真实采样”的特点，确实没看懂。包括实验做的也很烦。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;20年发在TIP上的一篇文章。&lt;/p&gt;
&lt;h1 id=&quot;立意&quot;&gt;&lt;a href=&quot;#立意&quot; class=&quot;headerlink&quot; title=&quot;立意&quot;&gt;&lt;/a&gt;立</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：Wavelet-SRNet</title>
    <link href="http://example.com/2020/09/18/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AWavelet-SRNet/"/>
    <id>http://example.com/2020/09/18/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9AWavelet-SRNet/</id>
    <published>2020-09-18T14:09:13.000Z</published>
    <updated>2020-10-12T06:26:27.387Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Wavelet-SRNet: A Wavelet-based CNN for Multi-scale Face Super Resolution</p></blockquote><p>2017年的一篇文章，核心论点是小波变换+CNN，针对的问题是人脸的SR。</p><h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>文章提出，在对低分辨率图像做上采样的时候，传统CNN方法的问题：</p><ul><li><p>性能的降低；</p></li><li><p>输出过于平滑，导致了细节遗失（翻译成白话就是上采样的主观效果差）；</p></li></ul><blockquote><blockquote><p>When dealing with very low resolution (LR) images, the performance of these CNN based methods greatly degrades. Meanwhile, these methods tend to produce over-smoothed outputs and miss some textural details.</p></blockquote><blockquote><p>深度学习类的方法，导致过度平滑的原因：使用MSE作为loss。<br>Most of these convolutional methods use MSE loss to learn the map function of LR/HR image pairs, which leads to over-smooth outputs when the input resolution is very low and the magnification is large.</p></blockquote></blockquote><ul><li>低倍数下才有良好的效果。</li></ul><blockquote><p>Besides, they seem to only work well on limited up-scaling factors (2× or 4×) and degrades greatly when ultra-resolving a very small input (like 16 × 16 or smaller).</p></blockquote><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/1.png" title="Optional title"></p><p>HR图像在输入Wavelet-SRNet之前首先会做小波包分解：通过小波包分解将图像解析为一组具有相同大小的小波系数：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/2.png" title="Optional title"></p><p>Wavelet-SRNet选用最简单的小波：haar小波，此小波足以描述不同频率的人脸信息。另外，使用快速小波变换（2-D fast wavelet transform ，FWT）来计算haar小波：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/3.png" title="Optional title"></p><p>Wavelet-SRNet分为三个子网：</p><h2 id="embedding"><a href="#embedding" class="headerlink" title="embedding"></a>embedding</h2><p><strong>embedding网络的作用是将低分辨率输入表示为一组特征地图。</strong></p><p>嵌入网络以<code>3×h×w</code>的低分辨率图像为输入，将其表示为一组特征映射。所有卷积的大小都是相同的3×3，步长为1，pad为1，使得嵌入网络中的每个特征图与输入图像的大小相同。</p><p>通过embedding网络，将输入的LR图像映射成<code>Ne×h×w</code>的特征映射，不需要上采样或下采样，其中Ne是最后一层的信道尺寸。</p><h2 id="wavelet-prediction"><a href="#wavelet-prediction" class="headerlink" title="wavelet prediction"></a>wavelet prediction</h2><p><strong>经过embedding网络之后，利用小波预测网对相应的小波系数图像进行估计。</strong></p><p>将小波包分解到小波网络的每一个子网中，并将小波包分解成相应的小波网络。这里将所有3×3的卷积填充物设置为步长为1、pad为1的卷积（与embedding网络相似），使得每个推导出的小波系数与LR输入的大小相同，即<code>3×h×w</code>。</p><p>此外，由于Haar小波变换系数之间的高度独立性，<strong>每两个子网之间不允许任何信息交互，这使得我们的网络具有可扩展性。</strong></p><blockquote><p>可扩展性：wavelet prediction net中的子网数量Nw可根据需求进行调整。</p></blockquote><p>在预测网中，用不同的子网数目很容易实现不同的放大倍数。例如，Nw=16和Nw=64分别代表4倍和8倍的放大倍数。</p><h2 id="reconstruction-networks。"><a href="#reconstruction-networks。" class="headerlink" title="reconstruction networks。"></a>reconstruction networks。</h2><p><strong>最后，重建网络从这些系数图像重建出高分辨率图像。</strong></p><p>利用重构网络将总尺寸为<code>Nw×3×h×w</code>的小波图像变换成<code>3×（r×h）×（r×w）</code>的原始图像空间。由一个填充尺寸为r×r，步长为r的解卷积层（deconvolution）组成。</p><p>虽然解卷积层的大小取决于放大倍数r，但它可以由一个常数小波重构矩阵初始化并在训练中固定。因此，它对整个网络的可扩展性没有影响。</p><p>最后，三个子网络之间的输入输出映射关系：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/4.png" title="Optional title"></p><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>整个loss函数分为三部分：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/5.png" title="Optional title"></p><h2 id="full-image-loss（MSE）"><a href="#full-image-loss（MSE）" class="headerlink" title="full-image loss（MSE）"></a>full-image loss（MSE）</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/6.png" title="Optional title"></p><p>MSE是超分辨率方法中最常使用MSE损失函数。</p><p>MSE是在图像空间上的一个限制，<strong>因为MSE几乎不能获取到高频纹理细节信息。</strong>Wavelet-SRNet的full-image loss一方面是在图像空间上的限制，另一方面也能在平滑度和纹理细节上达到一个平衡。</p><blockquote><p>A traditional MSE loss in image space, which is called full-image loss, is also used to get a balance between s- moothness and textures. </p></blockquote><h2 id="wavelet-based-loss"><a href="#wavelet-based-loss" class="headerlink" title="wavelet-based loss"></a>wavelet-based loss</h2><p>这里同样分为两部分：</p><h3 id="wavelet-prediction-loss"><a href="#wavelet-prediction-loss" class="headerlink" title="wavelet prediction loss"></a>wavelet prediction loss</h3><p>wavelet prediction loss相当于在小波域上的加权MSE，定义如下：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/7.png" title="Optional title"></p><p>W=(λ1,λ2,⋅⋅⋅，λNw)，一个平衡不同组小波系数重要性的权重矩阵；C是真实值，C尖是小波系数；绝对值的项用来提取全局拓扑信息</p><p>这个loss函数应该更关注局部纹理信息，故高频系数的权重应该更大一点。</p><blockquote><p>The former one is a weight- ed version of MSE in wavelet domain…… More attention can be paid on local textures with bigger weights appointed to high-frequency coefficients.</p></blockquote><h3 id="texture-loss"><a href="#texture-loss" class="headerlink" title="texture loss"></a>texture loss</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/8.png" title="Optional title"></p><p>texture loss是为了避免高频小波系数收敛为0。α和ϵ是slack values，可以使高频小波系数不为0，因此避免了纹理细节的下降。</p><blockquote><p>The texture loss is designed to prevent high-frequency wavelet coefficients from converging to zero….. It keeps high-frequency wavelet coefficients non-zero and hence prevents the degradation of texture details.</p></blockquote><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h2><p>输入的LR图片做了一组对比：一组是双三次插值，一组是小波包变换的近似系数。</p><blockquote><p>Two types of low-resolution images are taken as input, one of which is down-sampled by bicubic interpolation and the other is the approximation coefficient of wavelet packet decomposition.</p></blockquote><p>模型可适应任意大小的输入，因为没有全连接层。</p><blockquote><p>Since our network is a fully convolutional architecture without fully-connected layers, it can be applied to the input of arbitrary size.</p></blockquote><p>给出了网络的设置。</p><blockquote><p>Our model is implemented with the Caffe frame- work [11]. The loss in (6) is minimized using SGD with a batch size of 256. For the hyper-parameters, we set em- piricallyλ1 = 0.01,λ2 = λ3 = ··· = λNw = 1,μ = 1,k=2,γk =γk+1 =···=γNw =1,ν=0.1. The learning rate is set to 0.01 initially and reduced by a factor of 10 each 3000 iterations. It takes about 14, 000 ∼ 16, 000 iterations for our network to converge.</p></blockquote><p>两个数据集做训练。</p><blockquote><p>Our experiments are implemented on two datasets: CelebA and Helen. There are 202,599 faces in CelebA and 2,230 faces in Helen. In the training phase, we use the large train set of CelebA(162,700 images) for training and the validation set(19,867 images) of CelebA for validation. In the testing phase, we evaluate with the 19,962-image test set of CelebA and the 330-image test set of Helen, assuring no over-lapped images appearing in both the training and testing phase. The images are cropped around the face with eyes aligned horizontally.</p></blockquote><p>为了保证对照训练的公平，所有的数据集都使用了眼睛对齐的人脸照片，且在下采样之前没有做其他额外的处理。</p><blockquote><p>For a fair comparison, we use the same set of eyes-aligned faces for all the methods with no extra preprocessing before down-sampling.</p></blockquote><p>使用的评价指标：SSIM和PSNR。</p><blockquote><p>We adopt PSNR(dB) and SSIM for quantitative metric, and calculate PSNR on the luminance channel, following by [35], and SSIM on the three channels of RGB.</p></blockquote><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="不同放大倍数的定性比较"><a href="#不同放大倍数的定性比较" class="headerlink" title="不同放大倍数的定性比较"></a>不同放大倍数的定性比较</h3><p>与双三次插值对比了不同放大倍数下的效果。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/9.png" title="Optional title"></p><h3 id="不同方法的定性比较"><a href="#不同方法的定性比较" class="headerlink" title="不同方法的定性比较"></a>不同方法的定性比较</h3><p>8倍放大下不同方法的对比。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/10.png" title="Optional title"></p><h3 id="鲁棒性讨论"><a href="#鲁棒性讨论" class="headerlink" title="鲁棒性讨论"></a>鲁棒性讨论</h3><p>讨论了模型在面对未知高斯模糊，姿态和遮挡的鲁棒性。</p><p>在图6中，低分辨率人脸由高斯模糊核生成，其步长为8，对应于8×下采样。高斯模糊核的σ从0增加到6，其中σ=0表示最近邻插值下采样。如图6所示，当σ&lt;4时，模型方法显示出一定的鲁棒性，当σ&gt;=4时生成更平滑的面。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/11.png" title="Optional title"></p><p>作为比较，CBN的结果与平均面更接近。对于姿态变化，如图7所示，CBN无法重建大姿态的可信人脸，可能是由于空间预测不准确。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/12.png" title="Optional title"></p><h3 id="不同放大倍数的定量比较"><a href="#不同放大倍数的定量比较" class="headerlink" title="不同放大倍数的定量比较"></a>不同放大倍数的定量比较</h3><p>四个放大倍数：(32×32,4×)，(16×16,8×)，(8×8,8×)以及(8×8,16×)，其中（m×m，n×）为m×m输入分辨率，放大倍数为n。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/13.png" title="Optional title"></p><h1 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h1><p>来自原文的叙述：</p><ul><li><ol><li>A novel wavelet-based approach is proposed for <code>CNN-based face SR problem</code>. To the best of our knowledge, this is <code>the first</code> attempt to transform single image SR to wavelet coefficients prediction task in deep learning framework - albeit many wavelet-based researches exist for SR.</li></ol></li><li><ol start="2"><li>A flexible and extensible fully convolutional neural network is presented to make the best use of wavelet transform. It can apply to different input-resolution faces with multiple upscaling factors.</li></ol></li><li><ol start="3"><li>We qualitatively and quantitatively explore multiscale face super-resolution, especially on <code>very low input resolutions</code>. Experimental results show that our proposed approach outperforms state-of-the-art face SR methods.</li></ol></li></ul><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>code：<a href="https://github.com/hhb072/WaveletSRNet">https://github.com/hhb072/WaveletSRNet</a></p></li><li><p>文章：<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper.pdf">http://openaccess.thecvf.com/content_ICCV_2017/papers/Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper.pdf</a></p></li></ul><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><ul><li><p>第一次用了小波包变换和深度的结合在这个问题上，有开创意义。</p></li><li><p>放大倍数方面，文章给出了32，对模型的描述是：可以接受任何大小的数据以及可拓展性。意思就是说放大的倍数也是比较有前景的（如果效果真的像叙述的一样有力的话）。</p></li><li><p>做的问题是有针对性的，就是正面、无任何遮挡或者偏移的人脸。</p></li><li><p>后面在做鲁棒性的讨论时，也凸显出了模型的局限性（实际上就是给出了大家都无法解决的问题）。</p></li></ul><hr><h1 id="小波变换"><a href="#小波变换" class="headerlink" title="小波变换"></a>小波变换</h1><p>一种进行信号时频分析和处理的方法。</p><blockquote><p>理论依据：傅里叶变换 -&gt; 短时傅里叶变换 -&gt; 小波变换<br>这三种变换的本质是将信号从时域转换为频域。</p><p>上面的论文里用到的：小波包变换，由小波变换而来</p></blockquote><h2 id="傅立叶变换（FT）"><a href="#傅立叶变换（FT）" class="headerlink" title="傅立叶变换（FT）"></a>傅立叶变换（FT）</h2><p>傅里叶变换的核心是<strong>从时域到频域的变换，而这种变换是通过一组特殊的正交基来实现的</strong>。</p><blockquote><p>如何理解傅里叶变换公式？ - 苗华栋的回答 - 知乎<br><a href="https://www.zhihu.com/question/19714540/answer/1119070975">https://www.zhihu.com/question/19714540/answer/1119070975</a></p></blockquote><p>傅立叶变换的三个核心：</p><ul><li><p>傅立叶的重要论断：<strong>任何连续周期信号都可以由一组适当的正弦曲线组合而成。</strong></p></li><li><p>为什么要<strong>变换</strong>？因为最容易理解的表现形式不一定是最方便做计算的。变换（计算空间）的意义是将原来空间中难以处理的问题变换到方便计算的空间去。</p></li><li><p>变换的<strong>空间</strong>是什么？时域和频域。</p></li></ul><h2 id="短时傅立叶变换（STFT）"><a href="#短时傅立叶变换（STFT）" class="headerlink" title="短时傅立叶变换（STFT）"></a>短时傅立叶变换（STFT）</h2><h3 id="傅立叶变换的缺陷"><a href="#傅立叶变换的缺陷" class="headerlink" title="傅立叶变换的缺陷"></a>傅立叶变换的缺陷</h3><p>在傅立叶变换的基础上提出新的讨论，是因为傅立叶变换是有缺陷的：<strong>对非平稳过程，傅里叶变换有局限性。</strong></p><p>再进一步的说明是，傅里叶变换处理<strong>非平稳信号</strong>有天生缺陷。它只能获取一段信号总体上包含哪些频率的成分，但是对各成分出现的时刻并无所知。因此时域相差很大的两个信号，可能频谱图一样。</p><p>然而平稳信号大多是人为制造出来的，自然界的大量信号几乎都是非平稳的。<strong>对于这样的非平稳信号，只知道包含哪些频率成分是不够的，我们还想知道各个成分出现的时间。</strong>理解信号频率随时间变化的情况，各个时刻的瞬时频率及其幅值，也就是时频分析。</p><h3 id="加窗"><a href="#加窗" class="headerlink" title="加窗"></a>加窗</h3><p>想到知道频率随时间变化的情况，一个简单的思想就是“加窗”：<strong>把整个时域过程分解成无数个等长的小过程，每个小过程近似平稳，再傅里叶变换，就知道在哪个时间点上出现了什么频率了。</strong></p><p>短时傅立叶变换的基本思想由此而来：将信号加滑动时间窗，并对窗内信号做傅立叶变换，得到信号的时变频谱。</p><h2 id="小波变换（WT）"><a href="#小波变换（WT）" class="headerlink" title="小波变换（WT）"></a>小波变换（WT）</h2><h3 id="短时傅立叶变换的缺陷"><a href="#短时傅立叶变换的缺陷" class="headerlink" title="短时傅立叶变换的缺陷"></a>短时傅立叶变换的缺陷</h3><p>同上，新的讨论一定是因为前者有缺陷：提到了加窗，那么问题就是，这个“窗函数”如何定义？窗太宽太窄都会出现问题：<strong>窗太窄，窗内的信号太短，会导致频率分析不够精准，频率分辨率差。窗太宽，时域上又不够精细，时间分辨率低。</strong></p><p>所以窄窗口时间分辨率高、频率分辨率低，宽窗口时间分辨率低、频率分辨率高。对于时变的非稳态信号，高频适合小窗口，低频适合大窗口。<strong>然而STFT的窗口是固定的，在一次STFT中宽度不会变化，所以STFT还是无法满足非稳态信号变化的频率的需求。</strong></p><h3 id="可变窗的STFT"><a href="#可变窗的STFT" class="headerlink" title="可变窗的STFT"></a>可变窗的STFT</h3><p>一种解决上述问题的方法是：让窗口大小变起来，多做几次STFT不就可以了吗？</p><p>小波变换就有着这样的思路，但事实上小波并不是这么做的。至于为什么不采用可变窗的STFT呢，有很多解释，公认的缺陷是这样做的话STFT做不到正交化。</p><blockquote><p>大部分的解释里有这样的说法：小波变换能够提供一个随频率改变的“时间-频率”窗口。这也是一种理解性的说法，通过改变基底做到了可变窗，重点应该是基底的改变。</p></blockquote><h3 id="改变基底"><a href="#改变基底" class="headerlink" title="改变基底"></a>改变基底</h3><p>小波变换的出发点和STFT还是不同的：直接把傅里叶变换的基底做了改变，<strong>将无限长的三角函数基换成了有限长的会衰减的小波基。</strong></p><p><strong>小波变换就是为了解决对非平稳信号的分解问题而产生的数学方法。</strong>相比于傅里叶变换使用一组无限长的三角函数基进行信号拟合，小波变换使用的是一组正交的、迅速衰减的小波函数基进行信号拟合。这种小波函数基可通过其尺度变量和平移变量，获得不同的频率和时间位置。因此在利用这种小波函数基对信号进行分解时，可以用较少的小波函数基就拟合出突变信号（稀疏编码特性），同时也能获得不同频率的信号分量在时域上的出现位置。</p><h3 id="WT的总体理解"><a href="#WT的总体理解" class="headerlink" title="WT的总体理解"></a>WT的总体理解</h3><p>小波变换是这样一个过程：首先将原始信号作为输入信号，通过一组正交的小波基分解成高频部分和低频部分，然后将得到的低频部分作为输入信号，又进行小波分解，得到下一级的高频部分和低频部分，以此类推。随着小波分解的级数增加，其在频域上的分辨率就越高。这就是多分辨率分析（MRA，MultiResolution Analysis）。</p><p>从数学的角度理解，<strong>在小波变换中，一个位于希尔伯特空间中的函数，可以分解成一个尺度函数和一个小波函数，其中尺度函数对应原始函数中的低频部分，小波函数对应原始函数中的高频部分。</strong>通过尺度函数可以构建对原始信号的低通滤波器，通过小波函数可以构建对原始信号的高通滤波器。</p><blockquote><p>希尔伯特空间：完备的内积空间。</p><p>Linear Space → Normal Linear Space → Banach Space</p><p>Normal Linear Space → Inner Product Space<br>→ Banach Space</p><p>Inner Product Space → Euclid Space<br>→ Hilbert Space</p></blockquote><p>从信号处理的角度理解，在小波变换中，信号可通过信号滤波器分解为高频分量（高频子带）和低频分量（低频子带），高频子带又称为细节（detailed）子带，低频子带又称为近似（approximate）子带。细节子带是由输入信号通过高通滤波器后再进行下采样得到的，近似子带是由输入信号通过低通滤波器后再进行下采样得到的。</p><blockquote><p>小波变换的内容很多，一下两下真的说不清楚（想说清楚我也看不懂），但基本原理就是这些。随着应用的深入在做学习比较靠谱。</p></blockquote><h3 id="二进小波变换（DWT）"><a href="#二进小波变换（DWT）" class="headerlink" title="二进小波变换（DWT）"></a>二进小波变换（DWT）</h3><p>在小波变换中，若对尺度按幂级数作离散化，同时对平移保持连续变化，则此类小波变换称为二进小波变换（Dyadic Wavelet Transform）。</p><h3 id="紧支撑小波基"><a href="#紧支撑小波基" class="headerlink" title="紧支撑小波基"></a>紧支撑小波基</h3><p>在小波变换中，紧支撑小波基是性质较好的一类小波基，紧支撑（Compact Support）函数是指这样的一类函数：其自变量仅在0附近的取值范围内能得到非零函数值，而在其他区间取值，则得到的函数值全为零。能得到非零函数值的自变量取值区间被称为该函数的支撑区间。</p><p>一个函数的支撑区间长度主要由其尺度参数决定。支撑区间越大，计算复杂度越高，边界拖尾效应越明显。不仅如此，支撑区间越大，会产生更多的高幅值小波系数，关于这个结论的解释，可参考傅里叶变换使用无限长（支撑区间大）的三角函数基进行信号拟合的情况，相比于使用信号迅速衰减（支撑区间小）的小波基，三角函数基拟合信号时需要更多的数量。因此在选择小波基时，以支撑长度较短的小波基为宜。</p><p>此外，小波基的正交性也是一类重要的性质，它确保了信号的分解没有冗余（最优分解）。</p><h2 id="小波包变换（WPT）"><a href="#小波包变换（WPT）" class="headerlink" title="小波包变换（WPT）"></a>小波包变换（WPT）</h2><h3 id="小波变换的缺陷"><a href="#小波变换的缺陷" class="headerlink" title="小波变换的缺陷"></a>小波变换的缺陷</h3><p>同上，小波变换仍存在着不足之处，<strong>由于正交小波变换只对信号的低频部分做进一步分解，而对高频部分也即信号的细节部分不再继续分解</strong>，所以小波变换能够很好地表征一大类以低频信息为主要成分的信号，但它不能很好地分解和表示包含大量细节信息（细小边缘或纹理）的信号，如非平稳机械振动信号、遥感图象、地震信号和生物医学信号等。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>小波包分解（Wavelet Packet Decomposition），又称为最优子带树结构（Optimal Subband Tree Structuring）是对小波变换的进一步优化。</p><p><strong>主要思想：在小波变换的基础上，在每一级信号分解时，除了对低频子带进行进一步分解，也对高频子带进行进一步分解。最后通过最小化一个代价函数，计算出最优的信号分解路径，并以此分解路径对原始信号进行分解。</strong></p><p>小波包变换可以对高频部分提供更精细的分解，而且这种分解既无冗余，也无疏漏，所以对包含大量中、高频信息的信号能够进行更好的时频局部化分析。</p><p>从函数理论的角度来看，小波包分解是将信号投影到小波包基函数张成的空间中。从信号处理的角度来看，它是让信号通过一系列中心频率不同但带宽相同的滤波器。</p><h2 id="小波函数基"><a href="#小波函数基" class="headerlink" title="小波函数基"></a>小波函数基</h2><p>无论是小波变换还是小波包变化，一个重要的环节是选择合适的小波函数基进行信号分解。</p><p>在选择合适的小波函数基时，需要考虑的因素</p><ul><li><p>小波的对称性：主要体现在保证信号重构时不会产生相位畸变，即是不会产生重构信号的相位失真。</p></li><li><p>小波的正则性：保证了信号的光滑和可微性，对于大部分小波而言（非全部），其与消失矩存在关系：小波的消失矩越大，正则性也就越大。</p></li><li><p>选择与输入信号的波形相似性高的小波：意义在于使数据压缩和降噪变得更容易（信号的拟合和分解都更容易）。</p></li></ul><p>几种常用的小波函数基：</p><h3 id="Haar小波"><a href="#Haar小波" class="headerlink" title="Haar小波"></a>Haar小波</h3><p>最简单的一个小波函数，其具有紧支撑性和正交性，函数图像为在支撑区间[0,1)上的单个矩形波。Haar小波在时域上不连续，作为基本小波时性能不是很好。</p><h3 id="Daubechies小波"><a href="#Daubechies小波" class="headerlink" title="Daubechies小波"></a>Daubechies小波</h3><h3 id="Biorthogonal小波"><a href="#Biorthogonal小波" class="headerlink" title="Biorthogonal小波"></a>Biorthogonal小波</h3><h3 id="Symlets小波"><a href="#Symlets小波" class="headerlink" title="Symlets小波"></a>Symlets小波</h3><h3 id="Mexican-Hat小波"><a href="#Mexican-Hat小波" class="headerlink" title="Mexican Hat小波"></a>Mexican Hat小波</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Wavelet-SRNet: A Wavelet-based CNN for Multi-scale Face Super Resolu</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>博客网站的建设问题集合</title>
    <link href="http://example.com/2020/09/11/%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99%E7%9A%84%E5%BB%BA%E8%AE%BE%E9%97%AE%E9%A2%98%E9%9B%86%E5%90%88/"/>
    <id>http://example.com/2020/09/11/%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99%E7%9A%84%E5%BB%BA%E8%AE%BE%E9%97%AE%E9%A2%98%E9%9B%86%E5%90%88/</id>
    <published>2020-09-11T12:47:24.000Z</published>
    <updated>2021-01-31T14:19:57.906Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>19年3月份开始了解并使用hexo环境下搭建的博客，当时的搭建过程是极其顺利的。所以在建设初期并没有想到要做这样的记录。</p><p>直到4天前，也就是9月7日，我在对上一篇，关于视频摘要的博客进行更新时，试图上传一个比较大的文件到git上时出现了问题，导致了我的博客无法再进行更新。</p><p>在我进行了一系列垂死挣扎的操作之后，选择了重装整个hexo环境以及按照原来的模样重新搭建博客。整个过程下来，我真的是心力憔悴，也遇到了许多新的问题。我突然觉得，有必要将这些问题做个记录。一是为了以后查自己找方便，二是能为别人做个参考，三是希望能从这些问题中，学习或者领悟到一些当程序员的必要的品质。毕竟对于程序员来说，最重要的部分就是查bug和调bug，其余的时间也说不定都是在写bug。</p><p><strong>这里将记录本博客建设过程中遇到的一切有意义的问题。</strong>遇到问题就会记录或对相应的部分做更新，没有什么上下文关联。</p><p><strong>时刻牢记：遇到bug，baidu、google以及Official documents才是正道，和大佬交流会有奇效。</strong></p><hr><h1 id="hexo博客的搭建和重新部署问题"><a href="#hexo博客的搭建和重新部署问题" class="headerlink" title="hexo博客的搭建和重新部署问题"></a>hexo博客的搭建和重新部署问题</h1><h2 id="从零开始搭建hexo博客"><a href="#从零开始搭建hexo博客" class="headerlink" title="从零开始搭建hexo博客"></a>从零开始搭建hexo博客</h2><p>这个话题在网上做一下搜索，写的人很多，不做赘述。</p><p>唯一需要注意的就是，不同操作系统环境下可能会有一些小的差别，但是总的步骤基本都是一致的。无非就是本地环境安装以及github的仓库设置，最后再学一下在terminal上敲hexo命令就齐活了。</p><p>我的本地环境是macOS，类UNIX的系统中下布置hexo环境还是十分友好的。<del>Win：？</del></p><blockquote><p>Mac 系统下搭建hexo个人博客：<a href="https://www.jianshu.com/p/77db3862595c">https://www.jianshu.com/p/77db3862595c</a></p></blockquote><h2 id="重新部署hexo博客"><a href="#重新部署hexo博客" class="headerlink" title="重新部署hexo博客"></a>重新部署hexo博客</h2><p>这里的“重新部署”，包括以下的情况：</p><ul><li><p>博客本身是没有问题的，只是想换一个新的环境去部署。比如说更换了个人PC等情况。</p></li><li><p>博客的建设方面出现了问题，包括但不限于本地环境的更改、提交新版本时的出现的不可逆的问题（基本上都是可逆的，不可逆都是因为个人能力问题<del>问就是菜</del>）等。</p></li></ul><p>对于第一种情况，略微做一下网络搜索，也有很多人写，而且基本和从零开始搭建一样，只是中间个别重复的步骤不用再执行一遍而已，比如git仓库的设置。</p><p>第二种情况就比较有说法了（我就是那个发现了bug之后又发现自己特别菜然后重新布置了整个博客的人）：</p><h3 id="Node-js和npm的版本"><a href="#Node-js和npm的版本" class="headerlink" title="Node.js和npm的版本"></a>Node.js和npm的版本</h3><blockquote><p>Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Coding上，是搭建博客的首选框架。</p></blockquote><p>hexo需要Node.js支持。布置hexo环境时需要首先安装Node.js。</p><p>如果是初次安装的话，直接Node.js安装最新版本即可。通常来说，nmp会跟随着Node.js一起安装。</p><p>但如果本地环境在hexo部署前已经有Node.js的环境，这时就要注意了：<strong>hexo会随着其依赖项的更新而更新。如果本地的Node.js版本太久远，一定要记得更新。</strong>截止到写作时，目前官方文档给出的建议是：Node.js 版本需不低于 10.13，建议使用 Node.js 12.0 及以上版本。</p><p>还需要注意的是，<strong>Node.js做更新时，需要手动对nmp单独做更新</strong>。</p><blockquote><p>node版本更新：<a href="https://www.jianshu.com/p/0a247218b486">https://www.jianshu.com/p/0a247218b486</a></p></blockquote><h3 id="启动页面显示extends-includes-layout-pug等"><a href="#启动页面显示extends-includes-layout-pug等" class="headerlink" title="启动页面显示extends includes/layout.pug等"></a>启动页面显示extends includes/layout.pug等</h3><p>重新布置完成后，启动界面显示如下信息：</p><pre><code>extends includes/layout.pug block content include includes/recent-posts.pug include includes/partial</code></pre><p>这也明显是依赖项问题，执行如下命令即可。</p><pre><code>npm install --save hexo-renderer-jade hexo-generator-feed hexo-generator-sitemap hexo-browsersync hexo-generator-archive</code></pre><p>上述命令会有各种各样的Warning，比较显眼的一个是：<strong>jade做了rename</strong>，请安装rename后的包。我是按照它的提示做了新的安装，但是并不清楚不安装这个新的包会有什么问题。</p><p>上述命令运行时，如果提示权限不足，命令中带上<code>sudo</code>；如果npm在安装包时提示<code>rollbackFailedOptional</code>，可以简单的理解为网络不好，改用淘宝的npm镜像即可：</p><pre><code>npm config set disturl https://npm.taobao.org/dist</code></pre><h3 id="用于提交git的ssh"><a href="#用于提交git的ssh" class="headerlink" title="用于提交git的ssh"></a>用于提交git的ssh</h3><p>如果之前使用过ssh的话，直接在本地文件中找到ssh的文件，复制内容到GitHub上即可。<strong>ssh可以重复使用，即使更换了新的仓库也无所谓。</strong> <del>不用ssh，不嫌麻烦的话每次更新输入账号密码也行</del></p><hr><h1 id="向博客中提交大文件"><a href="#向博客中提交大文件" class="headerlink" title="向博客中提交大文件"></a>向博客中提交大文件</h1><p>这次博客的奔溃就是因为这个原因。我在github上hexo项目的issue中有提问，叙述了这次经历。</p><blockquote><p>Error uploading large file #4523：<a href="https://github.com/hexojs/hexo/issues/4523">https://github.com/hexojs/hexo/issues/4523</a></p></blockquote><p>有一位大佬给出了一个解答，并且得到了开发者的赞同，然后关闭了问题：</p><p><img src="/images/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/1.png" title="Optional title"></p><p><img src="/images/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/2.png" title="Optional title"></p><p>hexo的命令中，<code>hexo d</code>的作用是<strong>将blog本地路径中生成的public提交到git的仓库中</strong>。一看这个解答，我也觉得很有道理：无非就是解决提交版本的问题嘛！</p><p><strong>但是我犯了一个致命的错误：提交git的路径。</strong>实际上我在第一天还在迷糊的时候就注意到了这一点，并隐隐觉得这是问题的根本：我要是在blog的子目录下进行这个工作，不就把blog的里整个内容提交进去了吗？但实际上git仓库里只有public中的文件啊！</p><p>于是，我找到了回答我问题大佬的主页，发现了他的blog地址，并在他的blog中寻找到了他的邮箱，然后发邮件进行询问。出现了以下的讨论：</p><p><img src="/images/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/3.png" title="Optional title"></p><p><img src="/images/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/4.png" title="Optional title"></p><p><img src="/images/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/5.png" title="Optional title"></p><p>是的，当我重新布置了整个环境之后大佬才给出了回复。属实无奈，我并没有办法实践这位大佬给出的方法<del>再作一遍死就行</del>，不过这应该就是终极的解决方法。非常感谢这位大佬。</p><p>实际上macOS会自动的隐藏一些配置类文件（我的为数不多的实践和记忆告诉我，大概Win10也会），如果不手动去显示隐藏文件的话，不会看到上述邮件中的提到的<code>.depoy.git</code>目录。实际上在此之前我也查看过隐藏文件，只是并没有察觉到是这里的问题。</p><p><code>cmd+shift+.</code>，以及<code>git</code>的一整套命令，这次牢记。</p><p><img src="/images/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/6.png" title="Optional title"></p><hr><h1 id="theme中的问题"><a href="#theme中的问题" class="headerlink" title="theme中的问题"></a>theme中的问题</h1><p>theme中的问题主要体现在网页的显示上。对网页效果展示有强迫症或者力求美观，这些问题会凸显出来。</p><h2 id="Cxo"><a href="#Cxo" class="headerlink" title="Cxo"></a>Cxo</h2><p>截止到本篇发布的时间，在用的theme是Cxo。在hexo.io的主题展示页面搜索Cxo就可找到。</p><blockquote><p><a href="https://github.com/Longlongyu/hexo-theme-Cxo">https://github.com/Longlongyu/hexo-theme-Cxo</a></p></blockquote><p>选择这个主题的原因是因为比较简单，符合审美。</p><p><del>因为头像是猫猫头，我永远喜欢猫猫头</del></p><p><strong>这个主题的作者2018年还有稀疏的commit，2019和2020完全没进行过更新，也就是说跑路了！由于hexo的各种依赖项是一直在更新的，所以现在还想用这个主题的话，会出现很多的问题。还想要用的同好们，慎选！</strong></p><p><del>独占猫猫头</del></p><h3 id="不蒜子访问计数显示"><a href="#不蒜子访问计数显示" class="headerlink" title="不蒜子访问计数显示"></a>不蒜子访问计数显示</h3><p>这个属于老生常谈，大部分19年以后不更新的主题都会遇到这个问题。具体原因是因为18年10月份以后不蒜子的域名更改了，导致script引用不了，从而无法进行统计。</p><p>不蒜子官网做了解释（注意下方小红字），以及新的写法：</p><p><img src="/images/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/7.png" title="Optional title"></p><p>具体做法：在下面的路径里按照官网提示修改就行。</p><pre><code>layout/includes/partial/head.pug</code></pre><h3 id="“翻页”箭头显示错误"><a href="#“翻页”箭头显示错误" class="headerlink" title="“翻页”箭头显示错误"></a>“翻页”箭头显示错误</h3><p>主页中换页处的下一页本该显示为“&gt;”，但是显示为“&amp;#62”。</p><p>更新本地环境后出现的问题，猜测也是由依赖项的更新所导致的。</p><p>已经有人提过issue且得到了可以解决问题的回复。实际上就是编码的问题。以及向前翻页的箭头的编码可以改成<code>&amp;lt</code>。</p><blockquote><p><a href="https://github.com/Longlongyu/hexo-theme-Cxo/issues/42">https://github.com/Longlongyu/hexo-theme-Cxo/issues/42</a></p></blockquote><h3 id="当前查看进度条不适用"><a href="#当前查看进度条不适用" class="headerlink" title="当前查看进度条不适用"></a>当前查看进度条不适用</h3><p>单篇博客写的太长的话，还没有拉到底部，进度条显示就已经满了。</p><p>19年就有这样的问题。</p><p>未解决。</p><h3 id="目录显示错误"><a href="#目录显示错误" class="headerlink" title="目录显示错误"></a>目录显示错误</h3><p>部分时候出现目录不随着显示的内容而变动，同时也有一些博客的目录是正常的。</p><p>更新本地环境后出现的问题，无法猜到到底是什么原因导致的。</p><p>未解决。</p><h3 id="内容在本地显示不全"><a href="#内容在本地显示不全" class="headerlink" title="内容在本地显示不全"></a>内容在本地显示不全</h3><blockquote><p>2021年1月31日更新</p></blockquote><p>如果.md文档写的太长的话（没有具体的数值，反正就是长了之后），会出现在localhost上显示内容不全的情况。</p><p>这个问题早就出现了，只是今天又写了博客，想起来了。猜测是theme的问题。</p><p>未解决。</p><hr><blockquote><p>2020年11月12日更新</p></blockquote><h1 id="更新时出现SSH失效"><a href="#更新时出现SSH失效" class="headerlink" title="更新时出现SSH失效"></a>更新时出现SSH失效</h1><p>使用<code>hexo g -d</code>进行更新时，出现如下提示。</p><p><img src="/images/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/8.png" title="Optional title"></p><p>根据报错，是SSH密钥的访问权限问题。</p><p>以<code>This private key will be ignored</code>为关键词进行搜索，进行如下操作后（任意目录均可）恢复正常。</p><p><img src="/images/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/9.png" title="Optional title"></p><p>之前因为在根目录敲了一些不该敲的命令，重装了电脑的系统。猜测是因为<code>重装系统</code>导致的这个问题。</p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;19年3月份开始了解并使用h</summary>
      
    
    
    
    
    <category term="闲谈" scheme="http://example.com/tags/%E9%97%B2%E8%B0%88/"/>
    
    <category term="hexo博客建设" scheme="http://example.com/tags/hexo%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/"/>
    
  </entry>
  
  <entry>
    <title>基础：视频摘要</title>
    <link href="http://example.com/2020/09/03/%E5%9F%BA%E7%A1%80%EF%BC%9A%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/"/>
    <id>http://example.com/2020/09/03/%E5%9F%BA%E7%A1%80%EF%BC%9A%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/</id>
    <published>2020-09-03T10:53:58.000Z</published>
    <updated>2020-09-09T11:55:58.472Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><h2 id="视频摘要概念简介"><a href="#视频摘要概念简介" class="headerlink" title="视频摘要概念简介"></a>视频摘要概念简介</h2><blockquote><p>什么是视频摘要？为什么要研究这项技术？</p></blockquote><p>近年来，随着人们对社会公共安全的需求不断增长，各大公共场所都安装了大量的监控摄像头，用于提高安防系统的侦查和报警水平。大量的监控摄像头实时录制了海量的视频数据，<strong>但这些视频数据的利用率极低</strong>。因为监控探头在每一天的24小时不间断记录，而往往人们所需要的信息只是很短的一部分，通常都是在成百上千个小时的监控视频中寻找个位数分钟的有价值部分。所以，如何在海量的数字视频中准确地找到感兴趣的视频片段已成为行业的迫切需求与巨大挑战。</p><p><strong>视频摘要是通过分析视频的结构与内容，从原视频中提取出有意义的信息，并重新组合有意义信息，浓缩成可以充分表现视频语义内容的概要。</strong></p><p>视频摘要是用一段静态图像或动态视频序列的长视频内容的缩略总结，供用户进行浏览和分析。视频摘要技术可以按照用户需求，将几十小时视频浓缩为十几分钟甚至更少，大幅度缩短视频观看浏览的时间，再利用视频目标特征检索技术，对视频底层特征进行语义分析，辅助用户迅速定位感兴趣目标。视频摘要与检索技术可以辅助用户充分挖掘海量视频监控录像中的有意义信息，提高海量监控视频录像分析与响应效率。</p><h2 id="视频摘要技术分类"><a href="#视频摘要技术分类" class="headerlink" title="视频摘要技术分类"></a>视频摘要技术分类</h2><p>视频摘要技术在近20年内已被国内外的学者广泛关注，并取得了一定的研究成果。</p><p>按照摘要结果表现形式的不同，视频摘要可分为静态视频摘要与动态视频摘要。</p><p><strong>目前，监控领域的视频摘要技术主要有两种形式：静态视频摘要中的基于关键帧的摘要和动态视频摘要中的基于对象的视频摘要。二者都可以大大缩短视频长度，方便对视频的观看、分析和检索。</strong></p><blockquote><p>基于关键帧的视频摘要的最小单位是“帧”，存储空间较小且方便传输，但不能完整表示每个目标的运动轨迹，不利于视频目标检索。基于对象的视频摘要的最小单位是“对象”，可以最大限度的减少时间冗余信息与空间冗余信息，且为视频检索等上层开发提供对象结构，能在监控安防中快速响应紧急事件，定位到事件相关“对象”，但存在处理复杂，摘要生成困难的问题。</p></blockquote><h3 id="静态视频摘要"><a href="#静态视频摘要" class="headerlink" title="静态视频摘要"></a>静态视频摘要</h3><p>静态的视频摘要是以一系列从原始视频流中抽取出的<strong>静态语义单元</strong>来表现视频的内容。静态语义单元是如关键帧、标题、 幻灯片等可以概括表示视频镜头内容的静态特征信息。</p><blockquote><p>目前静态视频摘要研究主要是基于关键帧选取方法开展的，关键帧又称故事板，是从原始视频中提取的反映镜头中主要信息内容的一帧或多帧图像。通过多个关键帧组合成视频摘要，允许用户通过少量的关键帧快速浏览原始视频的内容，并提供快速检索。经典的关键帧选取的算法主要利用颜色、运动矢量等视觉特征去区分帧间的差异性。但差异性的区分计算依赖阈值的选择，选择过程计算量较大，实时性困难。</p></blockquote><p>基于关键帧的视频摘要具有结果简单，观看方便的优点。但由于其以静态图像为结果的表达形式，很难准确地表达视频的内在语义，且对“对象”动态特性的描述不全面，所以仅仅适用于视频的精彩瞬间生成，无法适应需要进行“对象”特性分析的场合。</p><h3 id="动态视频摘要"><a href="#动态视频摘要" class="headerlink" title="动态视频摘要"></a>动态视频摘要</h3><p>基于对象的视频摘要，是一种近些年来提出的主要用于监控视频领域的动态视频摘要技术，通过提取用户感兴趣对象，然后重组感兴趣对象得到视频摘要。</p><blockquote><p>对象的提取主要使用背景建模技术、运动目标检测与跟踪技术及视觉分析技术，对象的重组主要考虑用户关注度、压缩比、多视频融合等因素。</p></blockquote><p>这种方法能有效地保持视频内容随时间动态变化的特征，同时最大限度地减少时间-空间冗余，但存在对象提取困难及很难解决复杂场景下的视频摘要生成的问题。</p><p>实际上，可将静态视频技术中的提取关键帧的技术融入到动态视频摘要中：即直接提取视频中的关键帧合成新的视频。此类方法虽然也可以缩短视频的时长，但是合成后视频给人一种快进看电影的感觉，而且实际使用较少。</p><h2 id="视频摘要技术评估方法"><a href="#视频摘要技术评估方法" class="headerlink" title="视频摘要技术评估方法"></a>视频摘要技术评估方法</h2><p>视频摘要技术中最常用的评估方法是基于用户对视频内容的主观理解，让多个观众对视频中重要内容进行标注打分，综合所有人员的标注结果作为参考标准。然后，将视频摘要方法得到的视频摘要结果与参考标准进行比较，得到准确度来衡量视频摘要结果好坏。</p><p>在已有视频摘要研究中，最常用的评估方法是计算Precision、Recall和F-measure值，在有些方法中也会用到mAP（mean Average Precision）。</p><h1 id="部分视频摘要方法复现及评估"><a href="#部分视频摘要方法复现及评估" class="headerlink" title="部分视频摘要方法复现及评估"></a>部分视频摘要方法复现及评估</h1><h2 id="SeDMi"><a href="#SeDMi" class="headerlink" title="SeDMi"></a>SeDMi</h2><ul><li>论文：Video Synopsis based on a Sequential Distortion Minimization Method</li></ul><blockquote><p>4个引用。</p></blockquote><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/1.png" title="Optional title"></p><blockquote><p>Fig. 1 illustrates a scheme of the proposed system architecture. The proposed<br>method can be divided into several steps. Initially, we estimate the CLD for each frame of the original video. Next, we performed shot detection (see Section 3.1). Based on the shot detection results and to the given parameter α we estimate the number of frames per shot that the video synopsis (see Section 3.1). Finally, the video distortion is sequentially minimized according to the proposed methods resulting to the video synopsis (see Section 3.2).</p></blockquote><ul><li>代码：官方提供的代码为matlab版本。</li></ul><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/2.png" title="Optional title"></p><ul><li>示例视频：foreman.avi。</li></ul><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/3.png" title="Optional title"></p><ul><li><p>实验：<strong>最主要的参数为“关键帧抽取百分比”</strong>，论文中给出的结果里，这项参数设置为百分之十。修改为0.2、0.05以及0.02分别运行。</p></li><li><p>结果：在上述4组关键帧抽取百分比下运行，代码运行的时常都不超过10s。可在此处下载结果：<a href="/download/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/SeDMi.zip">SeDMi在foreman下的四组结果视频</a> 。</p></li><li><p>观感：<strong>非常像快进</strong>，而且foreman.avi本身就很短，看不太出效果来。</p></li><li><p>注意：后期版本的Matlab（2018a）或者是Mac环境下，使用VideoReader函数读取视频文件时，不支持.avi格式，需要进行格式转换（大概率是Mac环境不兼容，上面的实验都将avi转为mp4后才不会报错）。</p></li></ul><h2 id="Web-based-video-synopsis-system-using-Apache-Spark"><a href="#Web-based-video-synopsis-system-using-Apache-Spark" class="headerlink" title="Web-based video synopsis system using Apache Spark"></a>Web-based video synopsis system using Apache Spark</h2><ul><li>简介：基于Apache Spark的视频概要系统，分为前端页面和后台算法处理。</li></ul><blockquote><p>The project proposes a system that can be divided into two parts: the frontend web pages and the backstage processing algorithm based on Apache Spark. In the web pages, users can upload as well as download video clip. This project designs and implements a synopsis algorithm in the backstage which uses background subtraction to grab all the frames containing moving targets in a video and then concatenate all the frames into a new shorter clip. The algorithm connects to Spark cluster. After users upload a video to be condensed on web page, the algorithm will process it and return success information to users after generating the new video. Then users can download the new video from the download page.</p></blockquote><ul><li>代码：<a href="https://github.com/question0914/VideoSynopsis">https://github.com/question0914/VideoSynopsis</a></li></ul><blockquote><p>5个星标，用的人不是很多。</p></blockquote><ul><li>运行：clone下来之后本地运行网页，给提交视频的按钮过不去：会提示无法找到h5代码中该处的form标签下的/video/upload，所以就无法验证运行结果。</li></ul><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/4.png" title="Optional title"></p><ul><li>其他：有前端的话感觉比较容易实现和看结果，这一项比较希望能复现。后端代码用的是java。</li></ul><h2 id="DimensionFour"><a href="#DimensionFour" class="headerlink" title="DimensionFour"></a>DimensionFour</h2><blockquote><p>这个模型没有打底的论文，不清楚为什么叫这个名字。</p></blockquote><ul><li>简介：一个git上的开源工具，用到了Keras YOLOv3做目标识别，用到了ImageAI（猜测是用来做遮挡物后的人的运动预测），然后做动态物体的标注，可能会有合成的操作（后面会根据运行的结果提到）。</li></ul><blockquote><p>YOLOv3：</p><blockquote><p><a href="https://github.com/qqwweee/keras-yolo3">https://github.com/qqwweee/keras-yolo3</a><br><a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf">https://pjreddie.com/media/files/papers/YOLOv3.pdf</a></p></blockquote></blockquote><ul><li>代码：<a href="https://github.com/kevinvincent/DimensionFour">https://github.com/kevinvincent/DimensionFour</a></li></ul><blockquote><p>8个星标，用的人不是很多。</p></blockquote><ul><li>示例视频：项目提供了dataset，共有三个视频，根据长度的不同命名为long、preferred以及short，都是街头监控探头拍下的画面，主要的对象为移动的人：</li></ul><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/5.png" title="Optional title"></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/6.png" title="Optional title"></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/7.png" title="Optional title"></p><ul><li><p>实验：先要对视频进行预处理，然后对提取出来的视频帧做集合和标注。</p></li><li><p>结果：在本机的cpu版本的tf下运行，最长的视频在1h内处理完成，最短的视频处理时间不到10min。但是经过处理后的视频的大小会有非常明显的变大。视频格式由.mp4转为.avi。以下给出的是long的展示：</p></li></ul><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/8.png" title="Optional title"></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/9.png" title="Optional title"></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/10.png" title="Optional title"></p><ul><li><p>观感：整体效果而言，对每个移动的人的标注做的很不错。视频有拼接的痕迹，不是很明显：移动到画面边缘的人会直接消失；在遮挡物背后的人的有些画面的处理会变得模糊。但是缩放比例不是特别出色，5min的long.mp4处理后也有4min，但各个视频的缩放比例确实不一样，推测是根据具体的场景来做的，不是按比例抽帧。</p></li><li><p>注意：tensorflow==1.7.0、keras==2.1.6、Opencv、h5py、ImageAI，以及下载需要yolo.h5（代码中会检测，没有这个模型文件的话会自动下载，需要科学上网，238m，早期版本的yolo文件不适用）。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;h2 id=&quot;视频摘要概念简介&quot;</summary>
      
    
    
    
    
    <category term="视频摘要" scheme="http://example.com/tags/%E8%A7%86%E9%A2%91%E6%91%98%E8%A6%81/"/>
    
    <category term="基础知识" scheme="http://example.com/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：SRGAN</title>
    <link href="http://example.com/2020/07/18/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASRGAN/"/>
    <id>http://example.com/2020/07/18/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASRGAN/</id>
    <published>2020-07-18T02:41:02.000Z</published>
    <updated>2020-09-09T11:58:05.509Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</p></blockquote><p><strong>这篇文章第一次将将生成对抗网络用在了解决超分辨率问题上。</strong></p><blockquote><p>生成对抗网络，Generative Adversarial Network，GAN；</p><blockquote><p>GAN = G + D + 其他；</p></blockquote><blockquote><p>G：以次充好；D：明辨是非；其他：具体问题具体分析；</p></blockquote></blockquote><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/1.png" title="Optional title"></p><h2 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h2><p>G：残差块 + 卷积层 + BN层 + PReLU</p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>常规的卷积层。</p><h3 id="BN层"><a href="#BN层" class="headerlink" title="BN层"></a>BN层</h3><blockquote><p>Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift</p></blockquote><p>在多数网络中，BN层的作用主要有以下四个：</p><ul><li>可以选择比较大的初始学习率，显著提高网络的训练速度</li></ul><blockquote><p>在此之前的深度网络的训练，需要慢慢的调整学习率，甚至在网络训练到中间的时候，还需要考虑对学习率进行渐进的调整，有了BN之后，可以采用初始很大的学习率，然后学习率的衰减速度也很大，因为这个算法收敛很快。当然这个算法即使在选择了较小的学习率，也比以前的收敛速度快，因为它具有快速训练收敛的特性。</p></blockquote><ul><li><p>无需再理会过拟合中drop out、L2正则项参数的选择问题。采用BN算法后，网络可以移除这两项了参数，或者可以选择更小的L2正则约束参数了，因为BN具有提高网络泛化能力的特性；</p></li><li><p>再也无需使用使用局部响应归一化层（局部响应归一化是Alexnet网络用到的方法），因为BN本身就是一个归一化网络层；</p></li><li><p>可以把训练数据彻底打乱。</p></li></ul><blockquote><p>防止每批训练的时候，某一个样本都经常被挑选到，原文说可以提高1%的精度。</p></blockquote><p>BN在SRGAN里并不是重点。<strong>ESRGAN有改进到这个点。</strong></p><h3 id="Residual-blocks"><a href="#Residual-blocks" class="headerlink" title="Residual blocks"></a>Residual blocks</h3><blockquote><p>为什么要使用残差块？意义如何？</p></blockquote><p>在生成器的前6层网络中，运用了残差块。</p><p>使用残差块的意义是：当损失函数从D开始反向传播回G的时候，实际上进过来很多层。<strong>众所周知，越深的网络隐藏参数越多，在反向传播的过程中也越容易梯度弥散。</strong>而且残差连接的方法，就有效的保证了梯度信息能够有效的传递而增强生成对抗网络的鲁棒性。</p><p><strong>很多的GAN类模型有这样的操作。</strong></p><p><strong>ESRGAN也针对残差做了改进。</strong></p><h2 id="为什么使用GAN来解决Image-SR的问题？"><a href="#为什么使用GAN来解决Image-SR的问题？" class="headerlink" title="为什么使用GAN来解决Image SR的问题？"></a>为什么使用GAN来解决Image SR的问题？</h2><p>传统插值方法可以看做把像素复制放大倍数后，用某种固定的卷积核去卷积。</p><p>类似于SRCNN这样的基于卷积神经网络的方案也容易理解：学习上述卷积核，根据构建出的超分图像与真实的高分辨率图像(Ground Truth)的差距去更新网络参数。这种方法的损失函数一般都采用均方误差MSE。</p><p>当前监督SR效果的算法（损失函数）的优化目标是使恢复后的HR图像与ground truth间的均方差（MSE）最小化。这样做的目的是：取得很高的PSNR。但是由于PSNR是基于像素级图像（pixel-wise image）的差异来定义的，因此PSNR捕捉到和人的感官非常密切的差异（纹理细节）的能力十分有限，<strong>因此最高的PSNR不一定能反映人感官上最好的结果。</strong>放大倍数越大，越不平滑的情况下PSNR越低。不过从视觉上说最为真实，因为过于平滑会使得图像内部物体的边缘看起来模糊。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/2.png" title="Optional title"></p><p>上图中的图三是SRGAN给出的结果。PSNR虽然比图二低了近0.1，但是图二的脸和手明显是模糊的，看起来很不真实，图三的各个细节都很清晰，真实度比图二更高。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/3.png" title="Optional title"></p><blockquote><p>while GAN drives the reconstruction towards the natural image manifold producing perceptually more convincing solutions.</p></blockquote><p>这张图也说明的是上述问题。一旦放大倍数超过4，那么基于MSE优化的网络产生出来的HR图像在纹理细节方面就会过于平滑，看起来就像是糊成一团，但就是这样PSNR还有很高的得分。</p><p><strong>换言之，SRGAN没有以PSNR/MSE为“最高指示”，它更注重人的主观的评价（SRGAN在PSNR的表现确实很一般）。</strong></p><p>为了证明“高PSNR并不能带来良好的感官效果”这个观点，原文给出了一个指标，称为Mean opinion score（MOS），是26位评判者的打分。在这26位评委眼中，SRGAN产生的图像更真实：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/4.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/5.png" title="Optional title"></p><h2 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h2><p>D：VGG19 + LeakyReLU + max-pooling</p><h3 id="VGG19"><a href="#VGG19" class="headerlink" title="VGG19"></a>VGG19</h3><blockquote><p>Very deep convolutional networks for large-scale image recognition</p></blockquote><p>上面提到了SRGAN以人的主观感受为主旨来进行网络的训练。</p><p>于是会出现一个无法避免的问题：即使在同一个领域内以及对评判人做过筛选，人的感官也是一定是极其主观的评判。<strong>换言之，人的主观感受是无法用数学的语言去确切表达的。</strong>那么该如何实现以感官的标准来指定监督算法（损失函数）呢？</p><p><strong>作者所选择的是基于VGG的内容损失。</strong>先基于预训练的19层VGG网络的ReLU激活层来定义损失函数。内容损失的实质就是从VGG19网络里提取的特征图之间欧式距离的损失函数，无论是超分辨率还是艺术风格的转移，效果都非常好。</p><h2 id="如何使用GAN解决Image-SR的问题？"><a href="#如何使用GAN解决Image-SR的问题？" class="headerlink" title="如何使用GAN解决Image SR的问题？"></a>如何使用GAN解决Image SR的问题？</h2><p>GAN的工作过程：给定一个低分辨率图片作为噪声z的输入，通过生成器的变换把噪声的概率分布空间尽可能的去拟合真实数据的分布空间。</p><p>而在SRGAN中生成器的输入不再是噪声，而是低分辨率图像；而判别器结构跟普通的GAN没有什么区别。</p><p>综上，SRGAN的功能叙述为：<strong>把LR看成是一个噪声z的输入，那么G的作用就是生成的是一个fake的HR，D的作用是要去分辨G生成的fakeHR与原始的HR之间的区别，给出判断。</strong></p><h1 id="min-max方程"><a href="#min-max方程" class="headerlink" title="min-max方程"></a>min-max方程</h1><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/6.png" title="Optional title"></p><p>对于判别器D，希望D最大,所以加号之前的log部分应该最大,意味着判别器D可以很好的识别出，真实的高分辨率图像是”true”。而要让log尽可能的大，加号后的这部分中的ΘD(ΘG(z))要尽可能的小，意味着生成模型复原的图片应该尽可能的被D视为”FALSE”。</p><p>对于生成器G，应该让G尽可能的小，加号前面的式子并没有G，加号后面的式子中要让ΘG尽可能地小,就要ΘD(ΘG(Z))尽可能的大，也就是说本来就一张低分辨率生成的图片，判别器却被迷惑了，以为是一张原始的高分辨率图片。</p><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="Perceptual-loss-function"><a href="#Perceptual-loss-function" class="headerlink" title="Perceptual loss function"></a>Perceptual loss function</h2><blockquote><p>Perceptual Losses for Real-Time Style Transfer and Super-Resolution and Super-Resolution</p></blockquote><p>最重要的就是本文特别定制的生成器D的损失函数，可以说就是为了这个损失函数才采用GAN的。这个特制的损失函数被称为感知损失，Perceptual loss function。结构如下：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/7.png" title="Optional title"></p><p>其中，加号左边是Content loss（内容损失），加号右边是Adversarial loss（对抗损失，包括系数）。</p><h3 id="Content-loss"><a href="#Content-loss" class="headerlink" title="Content loss"></a>Content loss</h3><p>与先前的基于深度学习的超分方法相比，SRGAN的D在Loss上只有一个明显的变化：Loss不再单是对构建出来图片与真实高分辨率图片求MSE，而是<strong>加上对构建出图片的特征图与真实高分辨率图片在VGG19下的特征图求MSE</strong>。</p><p>简言之，内容损失=原MSE+特征图MSE：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/8.png" title="Optional title"></p><blockquote><p>原文中没有上述公式。</p><p>原文中也并未明确地指出要加和原MSE，只是给出了VGG19特征图的MSE。</p></blockquote><p>加号左边是原MSE：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/9.png" title="Optional title"></p><p>其中，θσ是网络参数，ILR是低分辨率图像，减号后面的部分是重建出来的高分辨率图像，  减号之前的是真实的高分辨率图像，r、W、H分别是图片数量、图片宽和高，都可以看成常数。</p><p>加号右边是VGG19特征图的MSE，称为VGG loss：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/10.png" title="Optional title"></p><p>VGG loss与原MSE相比，多的部分是φij，指的是第i个maxpooling层前的第j个卷积的特征图。</p><blockquote><p>VGG Loss的权重，1e-6是个超参，是TensorLayer的设置，设置到这么小应该是因为用了所有的特征图。</p></blockquote><h3 id="Adversarial-loss"><a href="#Adversarial-loss" class="headerlink" title="Adversarial loss"></a>Adversarial loss</h3><p>除了内容损失以外，还要加上一个GAN原有的对抗损失：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/11.png" title="Optional title"></p><p>其中，log内的部分是判别器对于生成超分图片的输出，-log(x)在（0,1）上是个单调递减的函数。前面提到，生成器D希望log内部分的值越大越好，也就是-log(x)越小越好，因此梯度更新的时候需要最小化对抗损失。</p><h3 id="生成器D的Loss"><a href="#生成器D的Loss" class="headerlink" title="生成器D的Loss"></a>生成器D的Loss</h3><p>综上，生成器D的Loss</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/12.png" title="Optional title"></p><p>这一部分在Tensorflow平台上的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">g_gan_loss = <span class="number">1e-3</span> * tl.cost.sigmoid_cross_entropy(logits_fake, tf.ones_like(logits_fake))</span><br><span class="line">mse_loss = tl.cost.mean_squared_error(fake_patchs, hr_patchs, is_mean=<span class="literal">True</span>)</span><br><span class="line">vgg_loss = <span class="number">2e-6</span> * tl.cost.mean_squared_error(feature_fake, feature_real, is_mean=<span class="literal">True</span>)</span><br><span class="line">g_loss = mse_loss + vgg_loss + g_gan_loss</span><br></pre></td></tr></table></figure><p>代码中的logits_real和logits_fake分别判别器对是真实高分图片、GAN生成的高分图片的输出。fake_patchs, hr_patchs分别是生成器的输出、真实的高分图片。feature_fake、feature_real是构建的图片、真实图片在VGG网络中的特征图。</p><h2 id="判别器D的Loss"><a href="#判别器D的Loss" class="headerlink" title="判别器D的Loss"></a>判别器D的Loss</h2><p>D的Loss用到了sigmoid交叉熵。</p><p>这一部分在Tensorflow平台上的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d_loss1 = tl.cost.sigmoid_cross_entropy(logits_real, tf.ones_like(logits_real))</span><br><span class="line">d_loss2 = tl.cost.sigmoid_cross_entropy(logits_fake, tf.zeros_like(logits_fake))</span><br><span class="line">d_loss = d_loss1 + d_loss2</span><br></pre></td></tr></table></figure><blockquote><p>损失函数softmax_cross_entropy、binary_cross_entropy、sigmoid_cross_entropy之间的区别与联系：<a href="https://www.jianshu.com/p/47172eb86b39">https://www.jianshu.com/p/47172eb86b39</a></p></blockquote><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>实验部分给出的结论是</p><ul><li><p>未针对实时视频SR进行优化；</p></li><li><p>较浅的网络有可能提供非常有效的替代方案，但质量性能会有小幅度的降低。而更深层次的网络架构是也可以给出更高的结果。原文针对这一点给出的解释是：推测ResNet的设计对深层网络的性能有很大的影响：即使是更深层次的网络（B&gt;16）也可以进一步提高SRResNet的性能，但代价是需要更长的培训和测试时间。且由于高频伪影的出现，更深层次网络的SRGAN变体越来越难以训练。</p></li></ul><blockquote><p> In contrast to Dong et al. [10], we found deeper network architectures to be beneficial. We speculate that the ResNet design has a substantial impact on the performance of deeper networks. We found that even deeper networks (B &gt; 16) can further increase the performance of SRResNet, however, come at the cost of longer training and testing times (c.f. supple- mentary material). We further found SRGAN variants of deeper networks are increasingly difficult to train due to the appearance of high-frequency artifacts.</p></blockquote><p>对比实验做的是SRGAN在不同的卷积网络的深度下的效果，显然也是网络越深效果越好。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/13.png" title="Optional title"></p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SRGAN/14.png" title="Optional title"></p><p>VGG54取得了最好的视觉效果，原文给出的解释是：更深层的网络层在远离像素空间的情况下代表更高抽象的特征。</p><blockquote><p>In this work, we found lSR VGG/5.4 to yield the perceptually most convincing results, which we attribute to the potential of deeper network layers to represent features of higher abstraction [68, 65, 40] away from pixel space.</p></blockquote><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>文章：<a href="https://arxiv.org/pdf/1609.04802.pdf">https://arxiv.org/pdf/1609.04802.pdf</a></p></li><li><p>code：<a href="https://github.com/tensorlayer/srgan">https://github.com/tensorlayer/srgan</a> 、<a href="https://github.com/aitorzip/PyTorch-SRGAN">https://github.com/aitorzip/PyTorch-SRGAN</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Photo-Realistic Single Image Super-Resolution Using a Generative Adv</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：SRCNN</title>
    <link href="http://example.com/2020/07/18/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASRCNN/"/>
    <id>http://example.com/2020/07/18/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASRCNN/</id>
    <published>2020-07-18T02:38:15.000Z</published>
    <updated>2020-09-09T11:58:37.788Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><blockquote><p>Learning a Deep Convolutional Network for Image Super-Resolution</p></blockquote><p><strong>SRCNN是深度学习用在超分辨率重建上的开山之作。</strong></p><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p>SRCNN的网络结构非常简单，仅仅用了三个卷积层，网络结构如下所示：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/17.png" title="Optional title"></p><p>SRCNN网络包含三个模块：Patch extraction and representation（块析出与表示）、Non-linear mapping（非线性映射）、Reconstruction（重构）。<strong>这三个模块对应三个卷积操作。</strong></p><ul><li><p>第一层CNN：对输入图片的特征提取。（9x9x64卷积核）；</p></li><li><p>第二层CNN：对第一层提取的特征的非线性映射（1x1x32卷积核）；</p></li><li><p>第三层CNN：对映射后的特征进行重建，生成高分辨率图像（5x5x1卷积核）。</p></li></ul><p>在进行卷积操作之前，SRCNN对图像进行了一个预处理：将输入的低分辨率图像进行bicubic插值，将低分辨率图像放大成目标尺寸。接下来才是上面提到的三层卷积网络拟合非线性映射，最后输出高分辨率图像结果。</p><blockquote><p>插值后的图像依旧称为“低分辨率图像”，并用Y表示；将ground-truth（真实的高分辨率图像）用X表示；将网络记为映射函数F（·）；</p></blockquote><p>网络的总思路来源于稀疏编码，作者将上述三个过程表述为：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/18.png" title="Optional title"></p><h2 id="Patch-extraction-and-representation"><a href="#Patch-extraction-and-representation" class="headerlink" title="Patch extraction and representation"></a>Patch extraction and representation</h2><p>块析出和表示的目的是通过输入图像Y获得一系列特征图：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/19.png" title="Optional title"></p><p>其中W1和B1表示滤波器（卷积核）的权重和偏置，max操作对应ReLU激活函数。</p><blockquote><p>Here W1 is of a size c × f1 × f1 × n1, where c is the number of channels in the input image, f1 is the spatial size of a filter, and n1 is the number of filters.<br>B1 is an n1-dimensional vector, whose each element is associated with a filter.</p></blockquote><blockquote><p>卷积+激活操作。</p></blockquote><h2 id="Non-linear-mapping"><a href="#Non-linear-mapping" class="headerlink" title="Non-linear mapping"></a>Non-linear mapping</h2><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/20.png" title="Optional title"></p><p>其中W2和B2表示滤波器的权重和偏置，max操作依旧对应ReLU激活函数。</p><blockquote><p>第二层和第一层的思路完全一致，实际上这里想表述的是可以添加更多的层，但是这样的操作会显著增加网络的计算开销。</p><blockquote><p>It is possible to add more convolutional layers (whose spatial supports are 1× 1) to increase the non-linearity. But this can significantly increase the complexity of the model, and thus demands more training data and time. In this paper, we choose to use a single convolutional layer in this operation, because it has already provided compelling quality.</p></blockquote></blockquote><h2 id="Reconstruction"><a href="#Reconstruction" class="headerlink" title="Reconstruction"></a>Reconstruction</h2><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/21.png" title="Optional title"></p><blockquote><p>Here W3 is of a size n2 ×f3 ×f3 ×c, and B3 is a c-dimensional vector.<br>W3：线型滤波器</p><blockquote><p>we expect that W3 behaves like first projecting the coefficients onto the<br>image domain and then averaging. In either way, W3 is a set of linear filters.</p></blockquote></blockquote><p>重构依旧是卷积操作，但是这里没有激活函数了。</p><p><strong>根据上面的三个公式，SRCNN仅有6个需要学习的参数：W1、B1、W2、B2、W3、B3。</strong></p><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>损失的计算也仅仅需要网络的输出F(Y)与真实高分图像X，<strong>损失函数选择MSE损失：</strong></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/22.png" title="Optional title"></p><p>选择MSE作为Loss的原因：有利于提高PSNR。</p><blockquote><p>Using MSE as the loss function favors a high PSNR.</p></blockquote><p>实际上，卷积神经网络对损失函数的限制只有一个，即损失函数为可导函数。<strong>如果在训练过程中给出一个更好的感知激励指标，则网络可以灵活地适应该指标。</strong>但原文提到“会做研究但很难实现”。</p><blockquote><p>The PSNR is a widely-used metric for quantitatively evaluating image restoration quality, and is at least partially related to the perceptual quality. It is worth noticing that the convolu- tional neural networks do not preclude the usage of other kinds of loss functions, if only the loss functions are derivable. If a better perceptually motivated metric is given during training, it is flexible for the network to adapt to that metric. We will study this issue in the future. On the contrary, such a flexibility is in general difficult to achieve for traditional “hand-crafted” methods.</p></blockquote><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><h2 id="实验结论"><a href="#实验结论" class="headerlink" title="实验结论"></a>实验结论</h2><p>分别以Running Time和PSNR为指标并与多种方法进行了对比实验。</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/23.png" title="Optional title"></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/24.png" title="Optional title"></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/25.png" title="Optional title"></p><blockquote><p>SC：sparse coding，稀疏编码</p><blockquote><p>Image super-resolution via sparse representation, 2010</p></blockquote><p>K-SVD</p><blockquote><p>On single image scale-up using sparse representations, 2012</p></blockquote><p>NE+LLE：neighbour embedding + locally linear embedding，邻域嵌入+局部线性嵌入</p><blockquote><p>Super-resolution through neighbor embedding, 2004</p></blockquote><p>NE+NNLS：neighbour embedding + non-negative least squares，邻域嵌入+非负最小二乘法</p><blockquote><p>Low-complexity single image super-resolution based on nonnegative neighbor embedding, 2012</p></blockquote><p>ANR：Anchored Neighbourhood Regression，锚定邻域回归</p><blockquote><p>Anchored neighborhood regression for fast example-based super-resolution, 2013</p></blockquote></blockquote><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/26.png" title="Optional title"></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/27.png" title="Optional title"></p><p>给出了4个种类的图片的SR结果及对比。原文中提到，在“baby”的类别上PSNR并未达到最好的效果。以下给出两组。</p><blockquote><p>经典的“Butterfly”和原文中提到的PSNR不达标的“Baby”。</p><blockquote><p>In spite of the best average PSNR values, the proposed SRCNN does not achieve the highest PSNR on images “baby” and “head” from Set5. Nevertheless, our results are still visually appealing (see Figure 10).</p></blockquote></blockquote><h2 id="模型对数据量的敏感性"><a href="#模型对数据量的敏感性" class="headerlink" title="模型对数据量的敏感性"></a>模型对数据量的敏感性</h2><p>文章使用SRCNN分别在ImageNet和Timofte数据集这两个数据集上分别进行了实验。在ImageNet上获得的PSNR值明显高于在Timofte数据集上的结果。这里的结论是：<strong>在迭代次数相同的情况下，数据量的增加可能会提高网络的性能。</strong></p><h2 id="模型对卷积核数量的敏感性"><a href="#模型对卷积核数量的敏感性" class="headerlink" title="模型对卷积核数量的敏感性"></a>模型对卷积核数量的敏感性</h2><p>SRCNN中第一层包含n1=64个卷积核，第二层包含n2=32个卷积核。<strong>根据理论，增加网络的卷积核数量必然会提升模型的性能。</strong></p><p>作者尝试做了额外的增加以及减少卷积核数量两种情况：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/28.png" title="Optional title"></p><p>从上表可以看出，增加卷积核数量确实会获得更高的PSNR值，但是同时会增加计算时间。<strong>所以需要在时间和质量之间做一个权衡。</strong></p><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>训练数据集：Timofte数据集（包含91幅图像）和ImageNet数据集（数据量超大）；</p></li><li><p>文章：<a href="http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2014_deepresolution.pdf">http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2014_deepresolution.pdf</a> ；</p></li><li><p>code：<a href="http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html">http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html</a> ,有matlab版本和Caffe版本；</p></li><li><p>文章比较简单，内容都在字面上，比较好理解。方法超前但是借鉴的痕迹很重，因此没有那种“专事专办”的味道。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Learning a Deep Convolutional Network for Image Super-Resolution&lt;/p&gt;</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>基础：图像超分辨率</title>
    <link href="http://example.com/2020/07/18/%E5%9F%BA%E7%A1%80%EF%BC%9A%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    <id>http://example.com/2020/07/18/%E5%9F%BA%E7%A1%80%EF%BC%9A%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/</id>
    <published>2020-07-18T02:37:07.000Z</published>
    <updated>2020-09-09T11:58:54.104Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><ul><li><p>图像超分辨率，Image Super Resolution（SR）；</p></li><li><p>概念：SR是指通过<strong>软件或硬件</strong>的方法，从观测到的低分辨率（low resolution）图像重建出相应的高分辨率（high resolution）图像；</p></li></ul><blockquote><p>更直白的表述为：提高图像的分辨率，使被观察图像给出更清晰的图像表述。</p><blockquote><p>图像超分辨率重建关注的是恢复图像中丢失的细节，即高频信息。在大量的电子图像应用领域，人们经常期望得到高分辨率（简称HR）图像。但由于设备、传感器等原因，我们得到的图像往往是低分辨率图像（LR）。增加空间分辨率最直接的解决方法就是通过传感器制造技术减少像素尺寸（例如增加每单元面积的像素数量）；另外一个增加空间分辨率的方法是增加芯片的尺寸，从而增加图像的容量。因为很难提高大容量的偶合转换率，所以这种方法一般不认为是有效的，因此，引出了图像超分辨率技术。</p></blockquote><blockquote><p>图像分辨率：指图像中存储的信息量，是每英寸图像内有多少个像素点，分辨率的单位为PPI（Pixels Per Inch），通常叫做像素每英寸。一般情况下，图像分辨率越高，图像中包含的细节就越多，信息量也越大。图像分辨率分为空间分辨率和时间分辨率。通常，分辨率被表示成每一个方向上的像素数量，例如64*64的二维图像。但分辨率的高低其实并不等同于像素数量的多少，例如一个通过插值放大了5倍的图像并不表示它包含的细节增加了多少。</p></blockquote></blockquote><ul><li><p>应用：在监控设备、卫星图像遥感、数字高清、显微成像、视频编码通信、视频复原和医学影像等领域都有重要的应用价值。</p></li><li><p>分类：SR的应用方向大致可分为两种。一是Image SR。只参考当前低分辨率图像，不依赖其他相关图像，称之为单幅图像的超分辨率（single image super resolution，SISR）。Image SR的方法多种多样，从最基础的插值到现如今的深度学习，是一个非常热门的研究方向。另一是Video SR。参考多幅图像或多个视频帧的超分辨率技术，称之为多帧视频/多图的超分辨率（multi-frame super resolution）。</p></li></ul><hr><h1 id="传统的Image-SR技术"><a href="#传统的Image-SR技术" class="headerlink" title="传统的Image SR技术"></a>传统的Image SR技术</h1><h2 id="基于插值的Image-SR"><a href="#基于插值的Image-SR" class="headerlink" title="基于插值的Image SR"></a>基于插值的Image SR</h2><h3 id="何为“插值”？"><a href="#何为“插值”？" class="headerlink" title="何为“插值”？"></a>何为“插值”？</h3><p>通过某个点周围若干个已知点的值，以及周围点和此点的位置关系，根据一定的公式，算出此点的值，就是插值法。</p><p>例如，有如下2*2的图像，不同颜色代表不同的像素点值：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/1.png" title="Optional title"></p><p>使用插值算法给上面的这张图像做细节扩充，形成一张4*4大小的图像：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/2.png" title="Optional title"></p><p>由于已经得知原2*2图像中的4个像素点的值，那么在接下来的操作中，仅需要求剩余12个点（黑色）的值即可。通过某个点周围若干个已知点的值，以及周围点和此点的位置关系，根据一定的公式，算出此点的值，就是插值法。如何把原图像的点摆放在新图中（确定具体坐标）；未知的点计算时，需要周围多少个点参与，公式如何。对于上面的问题，<strong>不同的方案选择，就是不同的插值算法。</strong></p><p><strong>常用的插值算法有：最邻近元法,双线性内插法,双三次内插法等</strong>。</p><p>实际上，这类插值算法，提升的图像细节有限，所以使用较少。通常，通过多幅图像之间的插值算法来重建是一个手段。</p><blockquote><p>以2*2为例。放大2倍后，得到一个4*4的图片。其中(0,0)的灰度值与之前的(0,0)相同。(2,0) 与之前的 (1,0)的灰度值相同。那么，这个放大图像的 (1,0)的灰度值等于什么呢？等于原图像的（1/2,0）的灰度值。但是原图像并没有这个点，通过以下插值方法，可计算该点的像素值灰度。</p></blockquote><h3 id="最邻近插值"><a href="#最邻近插值" class="headerlink" title="最邻近插值"></a>最邻近插值</h3><p>这是最简单的插值算法。思路是：当图片放大时，缺少的像素通过直接使用与之最近原有颜色生成。</p><blockquote><p>可理解为“按比例放大”：放大之后缺少的内容，直接照搬最近的已知的内容。</p></blockquote><p>首先要确定：原图像的像素摆放在新图中的具体坐标。新坐标对应源图中的坐标可以由如下公式得出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">srcX &#x3D; dstX * ( srcWidth &#x2F; dstWidth )</span><br><span class="line">srcY &#x3D; dstY * ( srcHeight &#x2F; dstHeight )</span><br></pre></td></tr></table></figure><blockquote><p>这里依旧可以用“按比例放大”来理解：将srcWidth / dstWidth称为缩放系数K。</p></blockquote><blockquote><p>坐标必然是整数，但是上述公式不一定会得到整数（K不一定为整数）。当2*2放大到4*4，结果和2.2.1中相同。但如果将2*2放大到3*3，就会出现新的像素点坐标计算出来有小数点的情况。这里需要采用的策略是四舍五入的方法（也可直接舍掉小数位），把非整数坐标转换成整数。</p></blockquote><p>接下来，以图片的左上角建立坐标，并将待求象素的四已知邻象素中左上角的看作[i,j]，则待求像素的值依赖于以下四个已知坐标：[i,j]、[i,j+1]、[i+1,j]、[i+1,j+1]，对于每一个待求象素，将距离待求象素最近的邻灰度赋给待求象素。</p><blockquote><p>当待求像素距离四个已知邻象素的距离中的两个及以上都是最小时，给定一个统一的规则即可，例如：要么都取原坐标中的小值，要么都取大值等。</p></blockquote><blockquote><p>图像右侧和下侧的待求像素并不像图像中部的像素，拥有四个已知邻象素。它们最多拥有1个（图像右下角的区域）或2个邻象素，规则也是同样的。</p></blockquote><p>对于上面的2*2图像，扩大到3*3和4*4时，将得到：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/3.png" title="Optional title"></p><blockquote><p>由于2*2实在是太小，下面的两种插值法的到的3*3和4*4实际上和上面这个结果是一致的。</p></blockquote><p><strong>最邻近插值有着明显的缺陷，它会使结果图像产生明显可见的锯齿。</strong></p><h3 id="双线性插值"><a href="#双线性插值" class="headerlink" title="双线性插值"></a>双线性插值</h3><p>在数学上，双线性插值是有两个变量的插值函数的线形插值扩展，其核心思想是在两个方向分别进行一次线性插值。</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/4.png" title="Optional title"></p><p>假设已知上图中已知红色数据点的值，要求是通过双线性插值得到绿色数据点的值。将需求描述为数学方式：预得到未知函数f在点P(x,y)的值，并假设已知函数f有Q11、Q12、Q21、Q22四个坐标。</p><p>首先在x方向进行线性插值，得到：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/5_1.png" title="Optional title"><br><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/5_2.png" title="Optional title"></p><p>然后在y方向进行线性插值，得到：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/6.png" title="Optional title"></p><p>那么未知函数f就可以描述如下：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/7_1.png" title="Optional title"><br><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/7_2.png" title="Optional title"></p><p>如果选择一个坐标系统使得f的四个已知点坐标分别为 (0, 0)、(0, 1)、(1, 0) 和 (1, 1)，那么插值公式就可以化简为：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/8.png" title="Optional title"></p><p>用矩阵运算表示为：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/9.png" title="Optional title"></p><p>双线性内插法的结果通常不是线性的，线性插值的结果与插值的顺序无关。双线性内插法的计算比最邻近点法复杂，计算量较大但，没有灰度不连续的缺点。具有低通滤波性质，使高频分量受损，图像轮廓可能会有一点模糊。</p><blockquote><p>先进行y方向的插值，然后进行x方向的插值，所得到的结果同先x后y是一致的。</p></blockquote><p><strong>相对于最近邻插值简单的四舍五入，双线性插值的处理更为科学，优化了边缘保护。</strong></p><h3 id="双三次内插法"><a href="#双三次内插法" class="headerlink" title="双三次内插法"></a>双三次内插法</h3><blockquote><p>Cubic Convolution Interpolation for Digtial Image Processing</p></blockquote><p>双三次插值（bicubic）又称双立方插值。</p><p>在数值分析这个数学分支中，双三次插值是二维空间中最常用的插值方法。在这种方法中，函数f在点(x,y)的值可以通过矩形网格中最近的十六个采样点的加权平均得到，在这里需要使用两个多项式插值三次函数，每个方向使用一个。</p><p>双三次插值的本质就是用使用了两次Cubic Interpolation。</p><h4 id="Cubic-Interpolation"><a href="#Cubic-Interpolation" class="headerlink" title="Cubic Interpolation"></a>Cubic Interpolation</h4><h5 id="f-0-、f-1-已知"><a href="#f-0-、f-1-已知" class="headerlink" title="f(0)、f(1)已知"></a>f(0)、f(1)已知</h5><blockquote><p>假设已知f(0)，f(1)以及其导数一共四个值。用Cubic Interpolation计算f(0.5)的值。</p></blockquote><p>假设：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/10.png" title="Optional title"></p><p>因为已知f(0)、f(1)以及其导数四个值：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/11.png" title="Optional title"></p><p>可得：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/12.png" title="Optional title"></p><p>通过解出上述四个代数，可以算出f(0.5)。 </p><h5 id="f-0-、f-1-未知"><a href="#f-0-、f-1-未知" class="headerlink" title="f(0)、f(1)未知"></a>f(0)、f(1)未知</h5><blockquote><p>大多数情况下，并不知道f(0)、f(1)的导数。</p></blockquote><p>设f(-1)=P0、f(0)=P1、f(1)=P2、f(2)=P3。有：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/13.png" title="Optional title"></p><p>后两值一般使用相邻两个点的斜率代替。但在更多的情况下，并不知道边缘值的斜率（因为边缘点的相邻两个点知道不完全）。所以有如下两个方法去代替：</p><ul><li><p>Left: p0 = p1、Right: p3 = p2</p></li><li><p>Left: p0 = 2p1 - p2、Right: p3 = 2p2 - p1</p></li></ul><h4 id="两次Cubic-Interpolation"><a href="#两次Cubic-Interpolation" class="headerlink" title="两次Cubic Interpolation"></a>两次Cubic Interpolation</h4><blockquote><p>原来的一次导数，变成了偏导。</p></blockquote><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/14.png" title="Optional title"></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/15.png" title="Optional title"></p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/16.png" title="Optional title"></p><p><strong>这四个点的灰度值必然满足第一个方程。</strong>通过上述三组方程，求出所有未知数。从而计算f(0.5,0)。</p><h3 id="插值法的总结"><a href="#插值法的总结" class="headerlink" title="插值法的总结"></a>插值法的总结</h3><p>对于上述三种不同的插值技术：</p><p><strong>特性</strong>：最邻近插值，名称非常直白，核心是四舍五入选取最接近的整数。这样的做法就会导致像素的变化不连续，在图像中的体现就是会有锯齿。双线性插值则是利用与坐标轴平行的两条直线去把小数坐标分解到相邻的四个整数坐标点的和。双三次插值与双线性插值类似，只不过用了相邻的16个点。</p><p><strong>坐标权重</strong>：前两种方法能回保证两个方向的坐标权重和为1，但是双三次插值不能保证这点，所以又可能去出现像素值越界的情况，需要截断。</p><p><strong>效果与泛用性</strong>：双三次插值通常能产生效果最好、最精确的插补图形，但它速度也是最慢的。双线性插值的速度则要快一些，但没有前者精确。在商业性图像编辑软件中，经常采用的是速度最快，但也是最不准确的最近相邻插值。</p><h2 id="基于重建的Image-SR"><a href="#基于重建的Image-SR" class="headerlink" title="基于重建的Image SR"></a>基于重建的Image SR</h2><h3 id="何为“重建”？"><a href="#何为“重建”？" class="headerlink" title="何为“重建”？"></a>何为“重建”？</h3><p>基于重建的SR，其基础是均衡及非均衡采样定理。这类算法的思路大多都是通过多帧低分辨率的图像恢复出一幅高分辨率图像，利用低分辨率图像中的丰富信息来进行超分辨处理，从而获得比观测图像分辨率更高的图像，即它假设低分辨率的输入采样信号（图像）能很好地预估出原始的高分辨率信号（图像）。绝大多数超分辨率算法都属于这一类，进一步的分类则有频域法和空域法。</p><blockquote><p>Nyquist-Shannon采样定理：如果希望得到的采样型号有旋转和相位变化，那么采样周期要小于整数周期的1/2，采样频率应该大于原始频率的2倍。同理，对于模拟信号，如果希望得到信号的各种特性，采样频率应该大于原始模拟信号的最大频率的两倍，否则将发生混叠（相位/频率模糊）。</p></blockquote><blockquote><p>在统计、信号处理和相关领域中，混叠是指取样信号被还原成连续信号时产生彼此交叠而失真的现象。当混叠发生时，原始信号无法从取样信号还原。而混叠可能发生在时域上，称做时间混叠，或是发生在频域上，被称作空间混叠。</p></blockquote><h4 id="频率域方法"><a href="#频率域方法" class="headerlink" title="频率域方法"></a>频率域方法</h4><p>频率域方法是图像超分辨率重建中一类重要方法，其中最主要的是消混叠重建方法。</p><blockquote><p>消混叠重建方法是通过解混叠而改善图像的空间分辨率实现超分辨率复原。在原始场景信号带宽有限的假设下，利用离散傅立叶变换和连续傅立叶变换之间的平移、混叠性质，给出了一个由一系列欠采样观察图像数据复原高分辨率图像的公式。多幅观察图像经混频而得到的离散傅立叶变换系数与未知场景的连续傅立叶变换系数以方程组的形式联系起来，方程组的解就是原始图像的频率域系数，再对频率域系数进行傅立叶逆变换就可以实现原始图像的准确复原。</p></blockquote><h4 id="空间域方法"><a href="#空间域方法" class="headerlink" title="空间域方法"></a>空间域方法</h4><p>在空间域方法中，其线性空域观测模型涉及全局和局部运动、光学模糊、帧内运动模糊、空间可变点扩散函数、非理想采样等内容。空域方法具有很强的包含空域先验约束的能力，主要包括非均匀空间样本内插、迭代反投影方法（IBP）、凸集投影法（POCS）、最大后验概率、最优和自适应滤波方法、确定性重建方法等。</p><h3 id="非均匀空间样本内插"><a href="#非均匀空间样本内插" class="headerlink" title="非均匀空间样本内插"></a>非均匀空间样本内插</h3><p>非均匀空域样本内插法是最直观的超分辨率图像重建算法，一般包含三个基本步骤：（1）运动估计；（2）非线性内插得到高分辨率图像；（3）解除混叠。非均匀空间样本内插法首先对低分辨率视频序列进行运动补偿，继而采用内插的方法产生分辨率较高的合成图像，以这个合成图像中作为初始值，再用Landweber迭代法重建超分辨率图像，最后对超分辨率图像进行去模糊操作。</p><p>非均匀空间样本内插方法运算量较低，能适用于实时任务；但过于简单化，在重建时不能得到比低分辨率图像中更多地频率成分，退化模型受限制，只适用于模糊和嗓声特性对全部低分辨率图像都一样的情况，也没有使用先验约束。</p><h3 id="凸集投影法（POCS）"><a href="#凸集投影法（POCS）" class="headerlink" title="凸集投影法（POCS）"></a>凸集投影法（POCS）</h3><p>凸集投影算法是把未知图像假设为一个适宜的希尔伯特空间中的元素，关于未知图像的每一个先验知识或约束限制了希尔伯特空间中的一个封闭凸集的解，引入幅度边界的限制，导出求解未知图像的迭代公式,由初始估计迭代计算超分辨率图像。利用集合论方法来恢复超分辨率图像，它有效地利用了空间范围观察模型，同时允许包含先验信息。</p><p>综上，POCS算法是一个迭代过程，给定超分辨率图像空间上的任意一个点，来定位一个能满足所有凸集的点。</p><h3 id="迭代反投影法（IBP）"><a href="#迭代反投影法（IBP）" class="headerlink" title="迭代反投影法（IBP）"></a>迭代反投影法（IBP）</h3><p>顾名思义，该算法也是一个迭代过程。在迭代反投影法中，首先估计一个高分辨率图像作为初始解（通常采用的是单幅低分辨率图像的额差值结果），然后根据系统模型，计算其模拟低分辨率图像（1984年提出时，示例为线型模型）。如果初始解与与原始的高分辨率图像精确相等，并且模拟成像过程符合实际情况，则模拟低分辨率序列应与观察到的实际的低分辨率图像相等。当两者不同时，将它们之间的误差反向投影到初始解上，使其得到修正。当误差满足要求时，结束迭代过程，给出最终结果。</p><p><strong>IBP算法直观，计算简单，但是由于没有考虑嗓声的影响，对高频嗓声非常敏感，并且由于逆问题的病态性，该方法没有唯一解，选择系数也有一些难度。与POCS等一些空间域方法相比，IBP算法难以利用空间先验信息。</strong></p><h3 id="统计复原类方法"><a href="#统计复原类方法" class="headerlink" title="统计复原类方法"></a>统计复原类方法</h3><p>统计复原方法将SR看成是一个统计估计问题。它为求解病态的超分辨率问题加入必要的先验约束，为能得到满意解提供了可能。<strong>常用的统计复原方法包括最大后验概率（MAP）估算法和最大似然（ML）估算方法。</strong>最大后验概率就是在己知低分辨率序列的前提下，使出现高分辨率图像的后验概率达到最大，最大似然估算方法可认为是最大后验概率先脸模型下的特例。</p><blockquote><p>最大后验概率估计是后验概率分布的众数。利用最大后验概率估计可以获得对实验数据中无法直接观察到的量的点估计。</p></blockquote><blockquote><p>最大似然估计一种重要而普遍的求估计量的方法。最大似然法明确地使用概率模型，其目标是寻找能够以较高概率产生观察数据的系统发生树。</p></blockquote><h3 id="混合MAP-POCS方法"><a href="#混合MAP-POCS方法" class="headerlink" title="混合MAP/POCS方法"></a>混合MAP/POCS方法</h3><p>POCS和MAP是重建类SR算法中效果出众的两种，两者都很容易引入先验知识。其中POCS算法保持图像边缘和细节的能力很强，但收敛稳定性不高，没有唯一解，降嗓能力不强；MAP有唯一解且收效稳定性高，降嗓能力强，但边缘和细节保持能力不如POCS。混合MAP/POCS方法则结合两者的优势特征，得到HR图像的最佳估计。</p><p>混合MAP/POCS方法有效结合了全部先验知识，并且能确保唯一的最优解，是一种重建效果较好的算法。</p><h3 id="滤波类方法"><a href="#滤波类方法" class="headerlink" title="滤波类方法"></a>滤波类方法</h3><p><strong>IBP、POCS、MAP、MAP/POCS等几种超分辨率算法的运算量大，只能应用于实时性要求不高的图像SR情况，如遥感图像和医学图像的超分辨率处理。</strong>在实时性要求比较高的情况下，例如实时视频的超分辨率复原，要求算法既能提高图像的分辨率，运算复杂度又比较低。滤波的方法，如自适应滤波、Wiener滤波和Kalman滤波等方法运算复杂度低，适用于对运算速度要求比较高的场合。 </p><h2 id="基于学习的Image-SR"><a href="#基于学习的Image-SR" class="headerlink" title="基于学习的Image SR"></a>基于学习的Image SR</h2><p>在超分辨率重建的过程中，随着要求分辨率倍数的增加，低分辨率图像序列的冗余信息已经不能提供满足要求的高频细节。通过增加低分辨率图像序列帧数的方法得到的高频信息也是不能满足实际要求的。在这种情况下，利用神经网络的方法，通过学习训练来获得图像的先验信息，可以得到包含更多细节的高分辨率图像。</p><p><strong>从理论上讲，如果训练集合是通用的，就可以利用这个训练集合对各种类型的图像进行放大。</strong></p><p>机器学习领域（非深度学习邻域）的一些主流的Image SR方法如下：</p><ul><li><p>Example-based方法</p></li><li><p>邻域嵌入方法</p></li><li><p>支持向量回归方法</p></li><li><p>虚幻脸</p></li></ul><blockquote><p>很多人脸图像是被现场照相机获取的,由于环境限制或设备原因,这些图像经常存在分辨率较低的问题。在人脸分析识别领域,怎样恢复人脸图像已经成为一个重要的课题。这个问题在Baker和Kanade的先驱工作中第一次被定义为虚幻脸。</p></blockquote><ul><li>稀疏表示法</li></ul><hr><h1 id="基于深度学习的Image-SR技术"><a href="#基于深度学习的Image-SR技术" class="headerlink" title="基于深度学习的Image SR技术"></a>基于深度学习的Image SR技术</h1><p>基于深度学习的图像超分辨率重建的大值研究流程如下：</p><ul><li><p>给出初始图像输入Image_1；</p></li><li><p>然后将初始图像输入进行分辨率的降低，称为Image_2；</p></li><li><p>通过各种深度学习算法，将Image2重建为Image3，且Image3和Image1的分辨率一致（SR的工作部分），再由PSNR等方法比较Image1与Image3，验证SR重建的效果，并根据效果调节神经网络中的节点模型和参数；</p></li><li><p>迭代第三步直到得到满意的结果。</p></li></ul><p>接下来给出近年来较为有代表性的基于深度学习的Image SR方法：</p><blockquote><p>发展历程：SRCNN -&gt; FSRCNN -&gt; ESPCN -&gt; VDSR -&gt; SRGAN -&gt; ESRGAN -&gt; EDSR</p><p>额外的一篇：PULSE</p></blockquote><h2 id="SRCNN"><a href="#SRCNN" class="headerlink" title="SRCNN"></a>SRCNN</h2><p>见《论文研读：SRCNN》。</p><h2 id="FSRCNN"><a href="#FSRCNN" class="headerlink" title="FSRCNN"></a>FSRCNN</h2><blockquote><p>Accelerating the Super-Resolution Convolutional Neural Networks</p></blockquote><p>这篇文章指出，SRCNN在速度方面有着显著的限制。体现在：</p><ul><li><p>低分辨率图像需要上采样（双三次插值）；</p></li><li><p>非线性映射步骤，参数量依旧影响速度。</p></li></ul><p><strong>这篇文章主要的目的是对SRCNN进行加速。</strong>文章重新设计SRCNN结构，体现在以下三个方面：</p><ul><li><p><strong>在网络的最后新增添了一个解卷积层</strong>，作用是从没有插值的低分辨率图像直接映射到高分辨率图像；</p></li><li><p>重新改变输入特征维数；</p></li><li><p>使用了更小的卷积核但是使用了更多的映射层。</p></li></ul><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/29.png" title="Optional title"></p><p>对于上述第一个问题，采用解卷积层代替三次插值，针对第二个问题，添加萎缩层和扩张层，并将中间那一个大层用一些小层（卷积核大小是3*3）来代替。整个网络结构类似于漏斗的形状，中间细两端粗。这个网络不仅仅速度快，而且不需要更改参数（除新增添的在最后的解卷积层）。</p><p>损失函数：同SRCNN一致的MSE。</p><blockquote><p>Cost function: Following SRCNN, we adopt the mean square error (MSE) as the cost function.</p></blockquote><p>激活函数：改成了PReLU。</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/prelu.png" title="Optional title"></p><blockquote><p>PReLU（Parametric Rectified Linear Unit），带参数的ReLU。i表示不同的通道，如果ai=0，那么PReLU退化为ReLU；如果ai是一个很小的固定值（如ai=0.01），则PReLU退化为Leaky ReLU（LReLU）。有实验证明，与ReLU相比，LReLU对最终的结果几乎没什么影响。</p></blockquote><p>实验结果：</p><p>不同倍数、不同数据集、不同方法的对比：在SET5上的结果最好，放大2、3倍时的效果最好。</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/30.png" title="Optional title"></p><p>经典的lenna的结果，SET14在3倍下的模型。</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/31.png" title="Optional title"></p><p>三个贡献：</p><ul><li><p>设计漏斗结构的卷积网络，不需要预处理操作；</p></li><li><p><strong>速度提升；</strong></p></li><li><p>训练速度快。</p></li></ul><p>链接：<a href="https://arxiv.org/pdf/1608.00367v1.pdf">https://arxiv.org/pdf/1608.00367v1.pdf</a></p><p>代码：<a href="https://github.com/yifanw90/FSRCNN-TensorFlow">https://github.com/yifanw90/FSRCNN-TensorFlow</a></p><h2 id="ESPCN"><a href="#ESPCN" class="headerlink" title="ESPCN"></a>ESPCN</h2><blockquote><p>Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</p></blockquote><p>这篇文章指出，像SRCNN这一类的方法，需要将低分辨率图像通过上采样插值得到与高分辨率图像相同大小的尺寸，再输入到网络中，这意味着要在较高的分辨率上进行卷积操作，从而增加了计算复杂度。</p><p>然后这篇文章给出了一种直接在低分辨率图像尺寸上提取特征，计算得到高分辨率图像的高效方法，称为ESPCN。网络结构如下所示：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/32.png" title="Optional title"></p><p>最大贡献：亚像素卷积层/子像素卷积层/pixel shuffle</p><p>SR的流程里需要将一张低分辨率图像转换成一张高分辨率图像。如果直接用deconvolution作为upscale手段的话，通常会带入过多人工因素进来（有不少论文提到这个）。<strong>在反卷积里会存在大量补0的区域，这可能对结果有害。</strong>因此pixel shuffle通过亚像素卷积，实现从低分辨图到高分辨图的重构，通过将多通道feature上的单个像素组合成一个feature上的单位即可，每个feature上的像素就相当于新的feature上的亚像素。</p><p>所以在图像超分辨的任务多使用pixel shuffle的方式获得高分辨图像（ESPCN等很多论文都有）。</p><blockquote><p>亚像素：在相机成像的过程中，获得的图像数据是将图像进行了离散化的处理，由于感光元件本身的能力限制，到成像面上每个像素只代表附近的颜色。例如两个感官原件上的像素之间有4.5um的间距，宏观上它们是连在一起的，微观上它们之间还有无数微小的东西存在，这些存在于两个实际物理像素之间的像素，就被称为“亚像素”。亚像素实际上应该是存在的，只是缺少更小的传感器将其检测出来而已，因此只能在软件上将其近似计算出来。</p><blockquote><p>若输出是原来的 r * r 倍（如，r=2，200x200 变成 400x400），则输出的 channel 数是输入 channel 数除以 r * r （如200x200x40 变成 400x400x10）。</p></blockquote><blockquote><p>一文搞懂 deconvolution、transposed convolution、sub-­pixel or fractional convolution：<a href="https://www.cnblogs.com/shine-lee/p/11559825.html#convolution%E8%BF%87%E7%A8%8B">https://www.cnblogs.com/shine-lee/p/11559825.html#convolution过程</a></p></blockquote></blockquote><p>链接：<a href="https://arxiv.org/pdf/1609.05158.pdf">https://arxiv.org/pdf/1609.05158.pdf</a></p><p>code：<a href="https://github.com/JuheonYi/VESPCN-tensorflow">https://github.com/JuheonYi/VESPCN-tensorflow</a></p><h2 id="VDSR"><a href="#VDSR" class="headerlink" title="VDSR"></a>VDSR</h2><blockquote><p>Accurate Image Super-Resolution Using Very Deep Convolutional Networks</p></blockquote><p>本文提出，输入的低分辨率图像和输出的高分辨率图像在很大程度上是相似的，也就是指低分辨率图像携带的低频信息与高分辨率图像的低频信息相近，训练时带上这部分会多花费大量的时间，实际上只需要学习高分辨率图像和低分辨率图像之间的高频部分残差即可。残差网络结构的思想特别适合以这个思路来解决SR问题，可以说影响了之后的深度学习超分辨率方法。</p><blockquote><p>对SRCNN收敛速度的分析有点牵强，主要是提出了论文的基于残差建模。</p><blockquote><p>何恺明在2015年的时候提出了残差网络ResNet。ResNet的提出，解决了之前网络结构比较深时无法训练的问题，性能也得到了提升，ResNet也获得了CVPR2016的best paper。残差网络结构(residual network)被应用在了大量的工作中。</p></blockquote></blockquote><p>这篇文章依旧以SRCNN开启叙述，先提了SRCNN的优势，然后指出了SRCNN的三个缺点：</p><ul><li><p>依赖小图像区域的上下文；</p></li><li><p>收敛太慢；</p></li><li><p>只能做单个倍数的采样。</p></li></ul><blockquote><p>first, it relies on the context of small image regions; second, training converges too slowly; third, the network only works for a single scale</p></blockquote><p>然后针对上面三个问题来概括本篇创新点：</p><ul><li><p>Context（增大感受野）；</p></li><li><p>Convergence（残差学习和高学习率）；</p></li><li><p>Scale Factor（使用mutil-scale）。</p></li></ul><p>提出如下的VDSR：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/33.png" title="Optional title"></p><ul><li><p>20层的卷积核大小都为3*3*64，</p></li><li><p>使用插值将LR图片放大到期望的尺寸，再作为网络的输入</p></li><li><p>每经过一层，feature map将会变小，论文只用补0的方法来保持其尺寸不变。</p></li></ul><p>Loss依旧使用的是MSE：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/34.png" title="Optional title"></p><p>链接：<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Kim_Accurate_Image_Super-Resolution_CVPR_2016_paper.pdf">https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Kim_Accurate_Image_Super-Resolution_CVPR_2016_paper.pdf</a></p><p>code：<a href="https://github.com/huangzehao/caffe-vdsr">https://github.com/huangzehao/caffe-vdsr</a> 、<a href="https://github.com/Jongchan/tensorflow-vdsr">https://github.com/Jongchan/tensorflow-vdsr</a> 、<a href="https://github.com/twtygqyy/pytorch-vdsr">https://github.com/twtygqyy/pytorch-vdsr</a></p><h2 id="SRGAN"><a href="#SRGAN" class="headerlink" title="SRGAN"></a>SRGAN</h2><p>见《论文研读：SRGAN》。</p><h2 id="ESRGAN"><a href="#ESRGAN" class="headerlink" title="ESRGAN"></a>ESRGAN</h2><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/35.png" title="Optional title"></p><p>ESRGAN的整体框架和SRGAN保持一致。相比SRGAN，ESRGAN有5处改进：</p><h3 id="去除BN层"><a href="#去除BN层" class="headerlink" title="去除BN层"></a>去除BN层</h3><blockquote><p>为什么要去除BN层？</p><p>How does batch normalization help optimization</p></blockquote><p>对于有些像素级图片生成任务来说，BN效果不佳。对于图片分类等任务，只要能够找出关键特征，就能正确分类，这算是一种粗粒度的任务，在这种情形下通常BN是有积极效果的。但是对于有些输入输出都是图片的像素级别图片生成任务，比如图片风格转换等应用场景，使用BN会带来负面效果，这很可能是因为在Mini-Batch内多张无关的图片之间计算统计量，弱化了单张图片本身特有的一些细节信息。</p><p>以图像超分辨率来说，网络输出的图像在色彩、对比度、亮度上要求和输入一致，改变的仅仅是分辨率和一些细节。而BN，对图像来说类似于一种对比度的拉伸，任何图像经过BN后，其色彩的分布都会被归一化。也就是说，它破坏了图像原本的对比度信息，所以BN的加入反而影响了网络输出的质量。ResNet可以用BN，但也仅仅是在残差块当中使用。</p><h3 id="用Dense-Block替换Residual-Block"><a href="#用Dense-Block替换Residual-Block" class="headerlink" title="用Dense Block替换Residual Block"></a>用Dense Block替换Residual Block</h3><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/36.png" title="Optional title"></p><p>BN的作用是网络更容易优化，不容易陷入局部极小值。ESRGAN去掉了BN，可以猜想，如果保持原有的Residual Block结构，网络会变得非常难易训练，而且很容易陷入局部极小值导致结果不好。</p><p>而DenseNet的解空间非常平滑，换言之，DenseNet相比其他网络要容易训练的多，Dense Block和BN提升网络性能的原因是相同的。</p><p><strong>总的来说，去掉BN层是因为BN层有副作用，但是BN也有众多的优点且与Dense Block的作用相似，那么用Dense Block替换Residual Block是要弥补去掉BN带来的负面效果。</strong></p><h3 id="使用Relativistic-GAN改进对抗损失函数"><a href="#使用Relativistic-GAN改进对抗损失函数" class="headerlink" title="使用Relativistic GAN改进对抗损失函数"></a>使用Relativistic GAN改进对抗损失函数</h3><h3 id="使用relu激活前的特征图计算损失"><a href="#使用relu激活前的特征图计算损失" class="headerlink" title="使用relu激活前的特征图计算损失"></a>使用relu激活前的特征图计算损失</h3><p>原文给出的解释如下：</p><ul><li><p>激活后的特征图变的非常稀疏，丢失了很多信息；</p></li><li><p>使用激活后的特征图会造成重建图片在亮度上的不连续。</p></li></ul><p>生成器的损失函数为：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/37.png" title="Optional title"></p><blockquote><p>ESRGAN在生成器上的Loss几乎沿用了SRGAN，但是命名是有出入的。</p></blockquote><h3 id="使用network-interpolation平衡主客观指标"><a href="#使用network-interpolation平衡主客观指标" class="headerlink" title="使用network interpolation平衡主客观指标"></a>使用network interpolation平衡主客观指标</h3><p>基于GAN的方法有一个缺点，经常会生成奇怪的纹理，而非GAN的方法总是缺失细节，能不能把两种方法生成的图片加权相加呢？将这样的思路称为Network Interpolation，网络插值。</p><p>具体做法是，训练一个非GAN的网络，在这个网络的基础上fine-tuning出GAN的生成器，然后把两个网络的参数加权相加：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/38.png" title="Optional title"></p><p>链接：<a href="https://arxiv.org/pdf/1809.00219.pdf">https://arxiv.org/pdf/1809.00219.pdf</a></p><p>code：<a href="https://github.com/xinntao/ESRGAN%E2%80%8Bgithub.com">https://github.com/xinntao/ESRGAN​github.com</a></p><h2 id="EDSR和MDSR"><a href="#EDSR和MDSR" class="headerlink" title="EDSR和MDSR"></a>EDSR和MDSR</h2><blockquote><p>Enhanced Deep Residual Networks for Single Image Super-Resolution</p></blockquote><p>EDSR是NTIRE2017超分辨率挑战赛上获得冠军的方案。</p><p>EDSR的最大贡献是去除了SRResNet上的BN。</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/39.png" title="Optional title"></p><p>原文中提到，原始的ResNet最一开始是被提出来解决高层的计算机视觉问题，比如分类和检测，直接把ResNet的结构应用到像超分辨率这样的低层计算机视觉问题，显然不是最优的。由于批规范化层消耗了与它前面的卷积层相同大小的内存，在去掉这一步操作后，相同的计算资源下，EDSR就可以堆叠更多的网络层或者使每层提取更多的特征，从而得到更好的性能表现。EDSR用L1范数样式的损失函数来优化网络模型。在训练时先训练低倍数的上采样模型，接着用训练低倍数上采样模型得到的参数来初始化高倍数的上采样模型，这样能减少高倍数上采样模型的训练时间，同时训练结果也更好。</p><blockquote><p>训练结果给的是2、3、4倍，效果依次递减。也就是2倍的PSNR和SSIM效果是最好的。</p></blockquote><p>文中同时还给出了一个能同时进行不同上采样倍数的网络结构MDSR：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/40.png" title="Optional title"></p><p>链接：见4.2.2节。</p><p>code：<a href="https://github.com/jmiller656/EDSR-Tensorflow">https://github.com/jmiller656/EDSR-Tensorflow</a> 、<a href="https://github.com/thstkdgus35/EDSR-PyTorch">https://github.com/thstkdgus35/EDSR-PyTorch</a> 、<a href="https://github.com/LimBee/NTIRE2017">https://github.com/LimBee/NTIRE2017</a></p><h2 id="PULSE"><a href="#PULSE" class="headerlink" title="PULSE"></a>PULSE</h2><blockquote><p>PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models</p></blockquote><p>来自2020年CVPR，一篇非常amazing的文章：PULSE最多能将16*16放大至1024*1024，即64倍放大。</p><blockquote><p>Starting with a pre-trained GAN, our method operates only at test time, generating each image in about 5 seconds on a single GPU.</p></blockquote><p>传统方法处理此类问题时，一般拿到LR图像后，会“猜测”需要多少额外的像素，然后试着将此前处理过的HR图像中相应的像素，匹配给LR图像。而这种单纯匹配像素的结果是，像头发和皮肤的纹理这种区域，会出现像素匹配错位的现象。而且该方法还会忽略了HR图像中，感光性等感知细节。所以最终在平滑度、感光度上出现问题，结果依然会显得模糊或者不真实。</p><p>PULSE则给出了一个新的思路：在拿到一张LR图像后，PULSE系统不会慢慢添加新的细节，而是遍历GAN生成的HR图像，将这些HR图像对应的LR图像与原图对比，找到最接近的那张。实际上就是用LR图片做反推导：找到最相似的LR版本，那么再反推回去，这张LR图像所对应的HR图像，就是最终要输出的结果。</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/41.png" title="Optional title"></p><p>使用的基本模型是GAN。在复原的过程中，该网络会“想象”出一些原本不存在的特征，即使是原本LR照片中无法看到的细节，比如毛孔、细纹、睫毛、头发和胡茬等，经过其算法处理后，都能看得一清二楚。</p><p>实验结果：在著名的高分辨率人脸数据集CelebA HQ用64×，32×和8×的比例因子进行了这些实验。并要求40个人对通过PULSE和其他五种缩放方法生成的1440张图像进行MOS评分，PULSE的效果最佳，得分几乎与真实的高质量照片一样高。</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/42.png" title="Optional title"></p><p>对不同类型的人脸的复原Success rates也做了统计：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/43.png" title="Optional title"></p><p>比较在意的人像位置：头发、眼睛、嘴唇：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/44.png" title="Optional title"></p><p>该模型有如下局限性：</p><ul><li>只针对人脸（虚幻脸）；</li></ul><blockquote><p>We show extensive experimental results demonstrating the efficacy of our approach in the domain of face super-resolution (also known as face hallucination).</p></blockquote><blockquote><p>However, we also note significant limitations when evaluated on natural images past the standard benchmark.</p></blockquote><ul><li>不能用于识别身份：无法将安全摄像头拍摄的失焦、不能识别的照片，变成真人的清晰图像。</li></ul><blockquote><p>意思就是说仅会生成不存在但看上去很真实的新面孔。</p></blockquote><p>链接：<a href="https://arxiv.org/pdf/2003.03808.pdf">https://arxiv.org/pdf/2003.03808.pdf</a></p><p>code：<a href="https://github.com/adamian98/pulse">https://github.com/adamian98/pulse</a></p><hr><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="常用的Image-SR评价指标"><a href="#常用的Image-SR评价指标" class="headerlink" title="常用的Image SR评价指标"></a>常用的Image SR评价指标</h2><h3 id="PSNR"><a href="#PSNR" class="headerlink" title="PSNR"></a>PSNR</h3><p>􏱏􏱐􏰝􏱑􏱒􏰆峰值信噪比（Peak Signal to Noise Ratio，PSNR）是一种极其普遍且在各个领域进行图像处理工作时都广泛使用的评估图像质量的客观量测法。评价的指向为经过压缩、降噪等操作后的图像，相对于原图像的噪声强度。失真程度越大，该指标给出的值就越小。</p><p>给定不含噪声的图像I（大小为M*N）和带有噪声的图像K，将均方误差MSE定为：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/45.png" title="Optional title"></p><p>PSNR则定义为：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/46.png" title="Optional title"></p><p>作为一种客观的评价指标，众多的实验都指出，PSNR同人的眼睛所看到的图像的视觉品质的主观评价是由很大出入的，也就是说，PSNR评分高的图像，在人眼的主观评价中，反而可能不如PSNR低分图像。实际上该指标并未考虑视觉评价的感知特性。</p><h3 id="SSIM"><a href="#SSIM" class="headerlink" title="SSIM"></a>SSIM</h3><p>结构相似性（Structural Similarity index，SSIM）是对比两图相似程度的指标。SSIM分别使用协方差、均值和标准差作为图片的结构相似程度、亮度和对比度的评价指标，这三个因素共同组合成图片失真程度的评价。三个评价的范围都是从0到1，无单位。若进行评价的两张图像完全相同，SSIM就会等于1。</p><p>给定进行对比的两张图像x和y，SSIM定义为：</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/47.png" title="Optional title"></p><p>其中，α，β，γ&gt;0，l(x,y)是亮度的评价，c(x,y)是对比度的评价，s(x,y)是结构的评价。</p><p><img src="/images/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/48.png" title="Optional title"></p><p>c1，c2，c3均为非零常数，μ为均值，σ为方差和协方差。在实际工程计算中，一般设定α，β，γ=1，c3=c2/2。</p><p>SSIM具有对称性，即SSIM(x,y)=SSIM(y,x)。</p><p>SSIM是视频及图像质量评估的一种常见且效果良好的算法，不过它依久具有同PSNR相似的缺陷。</p><h2 id="NTIRE"><a href="#NTIRE" class="headerlink" title="NTIRE"></a>NTIRE</h2><p>NTIRE英文全称是New Trends in Image Restoration and Enhancement，也就是<strong>“图像恢复与增强的新趋势”</strong>，是近年来计算机图像修复领域最具影响力的一场赛事，每年都会吸引大量的关注者和参赛者。</p><p>该比赛自2017年开始第一届，到今年已经举办了四届。2017年该比赛共拥有三个赛道，而今年的比赛增加到了5个赛道（2020.6比赛结果已出），分别是：</p><h3 id="2020年赛道"><a href="#2020年赛道" class="headerlink" title="2020年赛道"></a>2020年赛道</h3><h4 id="Real-World-Image-Super-Resolution"><a href="#Real-World-Image-Super-Resolution" class="headerlink" title="Real-World Image Super-Resolution"></a>Real-World Image Super-Resolution</h4><p>经典赛道，自2017年开始就有的图像超分辨率。</p><h4 id="Image-Dehazing"><a href="#Image-Dehazing" class="headerlink" title="Image Dehazing"></a>Image Dehazing</h4><p>经典赛道，自2017年开始就有的图像去雾。</p><h4 id="Image-Demoireing"><a href="#Image-Demoireing" class="headerlink" title="Image Demoireing"></a>Image Demoireing</h4><p>图像去摩尔纹，是今年的新赛道。</p><blockquote><p>摩尔纹是一种在数码照相机或者扫描仪等设备上，感光元件出现的高频干扰的条纹，是一种会使图片出现彩色的高频率不规则的条纹。</p></blockquote><h4 id="Spectral-Reconstruction-from-an-RGB-Image"><a href="#Spectral-Reconstruction-from-an-RGB-Image" class="headerlink" title="Spectral Reconstruction from an RGB Image"></a>Spectral Reconstruction from an RGB Image</h4><p>该任务的目的是从RGB图像中重建全场景高光谱（HS）信息。</p><h4 id="Video-Quality-Mapping"><a href="#Video-Quality-Mapping" class="headerlink" title="Video Quality Mapping"></a>Video Quality Mapping</h4><p>该挑战拟解决从源视频域到目标视频域的质量映射问题。挑战包括两个子任务：监督轨道和弱监督轨道。 其中，轨道1提供了一个新的Internet视频基准数据集，要求算法以监督训练的方式学习从压缩程度更高的视频到压缩程度更低的视频间的映射关系。 在轨道2中，需要算法来学习从一个设备到另一个设备的质量映射关系。</p><h3 id="Image-SR赛道冠军论文"><a href="#Image-SR赛道冠军论文" class="headerlink" title="Image SR赛道冠军论文"></a>Image SR赛道冠军论文</h3><h4 id="2017"><a href="#2017" class="headerlink" title="2017"></a>2017</h4><blockquote><p>EDSR</p></blockquote><p><a href="https://arxiv.org/abs/1707.02921v1">Enhanced Deep Residual Networks for Single Image Super-Resolution</a></p><h4 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h4><blockquote><p>WDSR</p></blockquote><p><a href="https://arxiv.org/abs/1808.08718">Wide Activation for Efficient and Accurate Image Super-Resolution</a></p><h4 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h4><blockquote><p>UDSR</p></blockquote><h4 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h4><p><a href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w31/Ji_Real-World_Super-Resolution_via_Kernel_Estimation_and_Noise_Injection_CVPRW_2020_paper.html">Real-World Super-Resolution via Kernel Estimation and Noise Injection</a></p><h3 id="NTIRE中Image-SR赛道结果综述"><a href="#NTIRE中Image-SR赛道结果综述" class="headerlink" title="NTIRE中Image SR赛道结果综述"></a>NTIRE中Image SR赛道结果综述</h3><blockquote><p>Single Image SR -&gt; Real Image SR -&gt; Real-World Image SR<br>从最开始的模拟下采样（Bicubic）图像到现在的Real-World Image，可见现在的挑战任务越来越走向通用化和实用化，当然这也意味着难度的升级。</p></blockquote><p><a href="http://ieeexplore.ieee.org/document/8014883/">NTIRE 2017 Challenge on Single Image Super-Resolution: Methods and Results</a></p><p><a href="http://ieeexplore.ieee.org/document/8575282/">NTIRE 2018 Challenge on Single Image Super-Resolution: Methods and Results</a></p><p><a href="https://ieeexplore.ieee.org/xpl/conhome/8972688/proceeding">NTIRE 2019 Challenge on Real Image Super-Resolution: Methods and Results</a></p><p><a href="http://ieeexplore.ieee.org/document/9022354/">NTIRE 2020 Challenge on Real-World Image Super-Resolution: Methods and Results</a></p><h2 id="Image-SR相关资源（持续更新）"><a href="#Image-SR相关资源（持续更新）" class="headerlink" title="Image SR相关资源（持续更新）"></a>Image SR相关资源（持续更新）</h2><ul><li><p>2000-2020的Image SR文献总结：<a href="https://github.com/YapengTian/Single-Image-Super-Resolution">https://github.com/YapengTian/Single-Image-Super-Resolution</a></p></li><li><p>CVPR2020｜图像重建(超分辨率，图像恢复，去雨，去雾，去模糊，去噪等)相关论文汇总：<a href="https://blog.csdn.net/Kobaayyy/article/details/106815083">https://blog.csdn.net/Kobaayyy/article/details/106815083</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;图像超分辨</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="基础知识" scheme="http://example.com/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    <category term="图像超分辨率" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>论文研读：Single Image Deraining: From Model-Based to Data-Driven and Beyond</title>
    <link href="http://example.com/2020/03/30/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASingle-Image-Deraining-From-Model-Based-to-Data-Driven-and-Beyond/"/>
    <id>http://example.com/2020/03/30/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB%EF%BC%9ASingle-Image-Deraining-From-Model-Based-to-Data-Driven-and-Beyond/</id>
    <published>2020-03-30T07:06:06.000Z</published>
    <updated>2020-09-09T12:26:58.700Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创文章，转载、引用请注明出处！</p></blockquote><hr><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>这是一篇关于单图像去雨和视频去雨方法的综述性论文。作者在文中总结了过去20年内的一些去雨方法，并对它们进行分类，对它们的结果做了定性和定量分析以及结果之间的对比。</p><blockquote><blockquote><p>The goal of single-image deraining is to restore the rain-free background scenes of an image degraded by rain streaks and rain accumulation. The early single-image deraining methods employ a cost function, where various priors are developed to represent the properties of rain and background layers. Since 2017, single-image deraining methods step into a deep-learning era, and exploit various types of networks, i.e. convolutional neural networks, recurrent neural networks, generative adversarial networks, etc., demonstrating impressive performance. </p></blockquote><blockquote><p>Given the current rapid development, in this paper, we provide a comprehensive survey of deraining methods over the last decade. We summarize the rain appearance models, and discuss two categories of deraining approaches: model-based and data-driven approaches. For the former, we organize the literature based on their basic models and priors. For the latter, we discuss developed ideas related to architectures, constraints, loss functions, and training datasets. </p></blockquote><blockquote><p>We present milestones of single-image deraining methods, review a broad selection of previous works in different categories, and provide insights on the historical development route from the model-based to data-driven methods. We also summarize performance comparisons quantitatively and qualitatively. Beyond discussing the technicality of deraining methods, we also discuss the future directions.</p></blockquote></blockquote><blockquote><blockquote><p>单幅图像排空的目标是恢复因雨水条纹和雨水堆积而退化的图像的无雨背景场景。早期的单图像排空方法采用成本函数，其中开发了各种先验以表示雨层和背景层的属性。自2017年以来，单图像排空方法进入了深度学习时代，并利用了各种类型的网络，即卷积神经网络，递归神经网络，生成对抗网络等，展示了令人印象深刻的性能。</p></blockquote><blockquote><p>鉴于当前的快速发展，本文对过去十年中的去雨方法进行了全面的调查。我们总结了降雨外观模型，并讨论了两种减雨方法：基于模型的方法和基于数据驱动的方法。对于前者，我们根据其基本模型和先验知识来整理文献。对于后者，我们讨论与体系结构，约束，损失函数和训练数据集有关的已开发思想。</p></blockquote><blockquote><p>我们介绍了单图像排空方法的里程碑，回顾了不同类别的大量先前作品，并提供了从基于模型的方法到数据驱动方法的历史发展路线的见解。我们还定量和定性总结了性能比较。除了讨论去雨方法的技术性之外，我们还讨论了未来的发展方向。</p></blockquote></blockquote><h1 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h1><blockquote><p>An early study of video deraining was started in 2004 by Garg and Nayar [3]. They analyze rain dynamic appearances, and develop an approach to remove rain streaks from videos. Kang et al. [4] was a pioneer in the single image deraining by publishing a method in 2012. The method extracts the high-frequency layer of a rain image, and decomposes the layer further into rain and non-rain components using dictionary learning and sparse coding. Starting from 2017, by the publications of [1, 18], data-driven deep-learning methods that learn features automatically become dominant in the literature.</p></blockquote><blockquote><blockquote><p>Garg和Nayar [3]于2004年开始了对视频去雨的早期研究。他们分析了雨水的动态外观，并开发了一种方法来消除视频中的雨水条纹。 </p></blockquote><blockquote><p>Kang等。 [4]是通过在2012年发布一种方法来进行单幅图像去雨的先驱。该方法提取了降雨图像的高频层，并使用字典学习和稀疏编码将其进一步分解为降雨和非降雨成分。 </p></blockquote><blockquote><p>从[1，18]的出版物开始，从2017年开始，学习特征的数据驱动的深度学习方法自动成为文献中的主导。</p></blockquote></blockquote><ul><li>单一图像去雨研究的历史：在2017年之前，典型的方法是基于模型的方法（或非深度学习方法）。基于模型的方法的主要发展受到以下观念的推动：图像分解（2012年），稀疏编码（2015年）和基于先验的高斯混合模型（2016年）。自2017年以来，单图像清除方法进入了数据驱动方法（或深度学习方法）时期。</li></ul><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SID/11.png" title="Optional title"></p><blockquote><p>单一图像排空方法的里程碑：图像分解，稀疏编码，高斯混合模型，深度卷积网络，生成对抗网络以及半/无监督学习。在2017年之前，典型的方法是基于模型的方法（或非深度学习方法）。自2017年以来，单图像去雨方法进入了数据驱动方法（或深度学习方法）时期。</p></blockquote><ul><li>基于模型的方法及相关论文。</li></ul><blockquote><blockquote><p>Model-based methods rely more on the statistical analysis of rain streaks and background scenes. The methods enforce handcrafted priors on both rain and background layers, then build a cost function and optimize it. The priors are extracted from various ways: Luo et al. [5] learn dictionaries for both rain streak and background layers, Li et al. [6] build Gaussian mixture models from clean images to model background scenes, and from rain patches of the input image to model rain streaks, Zhu et al. [7] enforce a certain rain direction based on rain-dominated regions so that the background textures can be differentiated from rain streaks.</p></blockquote><blockquote><p>基于模型的方法更多地依赖于雨条纹和背景场景的统计分析。该方法在雨层和背景层都执行手工制作的先验，然后构建成本函数并对其进行优化。先验是通过各种方式提取的：Luo等。 [5] Li等人学习了有关雨条纹和背景层的词典。［6］建立了高斯混合模型，从干净的图像到背景场景，从输入图像的雨斑到雨条纹，朱等人。 [7]基于降雨为主的地区实施一定的降雨方向，以便可以将背景纹理与降雨条纹区分开。</p></blockquote></blockquote><ul><li>基于数据驱动的相关方法和论文：数据驱动方法的主要发展：深度卷积网络（2017），生成对抗网络（2019）和半/无监督方法（2019）。在2017-2019年间，关于这种深度学习方法的论文超过30篇，大大超过了2017年之前的枯竭论文数量。</li></ul><blockquote><blockquote><p>In recent years, the popularity of data-driven methods has overtaken model-based methods. These methods exploits deep net- works to automatically extract hierarchical features, enabling them to model more complicated mappings from rain images to clean ones. Some rain-related constraints are usually injected into the networks to learn more effective features, such as rain masks [1], background features [8], etc. Architecture wise, some methods utilize recurrent network [1], or recursive network [9] to remove rain progressively. There are also a series of works focusing on the hierarchical information of deep features, e.g. [10, 11].<br>While deep networks lead to a rapid progress in deraining performance, many of these deep-learning deraining methods train the networks in a fully supervised way. This can cause a problem, since to obtain paired images of rain and rain-free images is intractable. The simplest solution is to utilize synthetic images. Yet, there are domain gaps between synthetic rain and real rain images, which can make the deraining performance not optimum. To overcome the problem, unsupervised/semi-supervised methods thatexploitrealrainimages [12]and [13]areintroduced.</p></blockquote><blockquote><p>近年来，数据驱动方法的普及已经取代了基于模型的方法。这些方法利用深层网络自动提取层次结构特征，从而使它们能够建模从降雨图像到干净图像的更复杂的映射。通常会向网络中注入一些与雨水相关的约束条件，以学习更有效的功能，例如防雨罩[1]，背景特征[8]等。在架构上，某些方法利用循环网络[1]或递归网络[9] ]逐步去除雨水。还有一系列针对深度特征的分层信息的作品，例如[10，11]。<br>虽然深层网络在去雨性能方面取得了迅速的进步，但许多深度学习去雨方法都以完全受监督的方式训练网络。这可能会引起问题，因为难以获得雨水和无雨水图像的配对图像。最简单的解决方案是利用合成图像。然而，合成降雨和真实降雨图像之间存在域间隙，这可能会使去雨性能不是最佳的。为了克服这个问题，引入了无监督/半监督的方法来利用真实的雨像[12]和[13]。</p></blockquote></blockquote><h1 id="RAINDROP-APPEARANCE-MODELS"><a href="#RAINDROP-APPEARANCE-MODELS" class="headerlink" title="RAINDROP APPEARANCE MODELS"></a>RAINDROP APPEARANCE MODELS</h1><p>此段结合雨滴的物理特性，给出合成雨模型的合成方法。</p><blockquote><blockquote><p>The shape of a raindrop is usually approximated by a spherical shape [14]. </p></blockquote><blockquote><p>雨滴的形状通常近似为球形[14]。</p></blockquote></blockquote><blockquote><blockquote><p>As a result, in most rain synthetic models, rain streaks are assumed to be superimposed on the background image.</p></blockquote><blockquote><p>最终，在大多数降雨合成模型中，降雨条纹是假定叠加在背景图像上。</p></blockquote></blockquote><h1 id="LITERATURE-SURVEY"><a href="#LITERATURE-SURVEY" class="headerlink" title="LITERATURE SURVEY"></a>LITERATURE SURVEY</h1><h2 id="Synthetic-Rain-Models"><a href="#Synthetic-Rain-Models" class="headerlink" title="Synthetic Rain Models"></a>Synthetic Rain Models</h2><p>六种合成雨模型。</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SID/33.png" title="Optional title"></p><blockquote><blockquote><p>However, as we mentioned in the beginning of this section that all these models are heuristic; implying that they might not entirely correct physically. Nevertheless, as shown in the literature, they can be effective, at least to some extent, for image deraining.</p></blockquote><blockquote><p>但是，正如我们在本节开始时提到的那样，所有这些模型都是启发式的。暗示它们可能在物理上不完全正确。 然而，如文献所示，它们可以至少在一定程度上有效地消除图像。</p></blockquote></blockquote><h3 id="Additive-Composite-Model-（ACM）"><a href="#Additive-Composite-Model-（ACM）" class="headerlink" title="Additive Composite Model （ACM）"></a>Additive Composite Model （ACM）</h3><p>现有研究中使用的最简单和流行的降雨模型是加法复合模型[4，6]，它遵循方程式：O = B + S。</p><blockquote><p>其中B表示背景层，S表示雨条纹层。 O是由于雨条纹而劣化的图像。在此，该模型假设降雨条纹的外观仅与背景重叠，并且在降雨退化图像中没有降雨积聚。</p></blockquote><h3 id="Screen-Blend-Model-（SBM）"><a href="#Screen-Blend-Model-（SBM）" class="headerlink" title="Screen Blend Model （SBM）"></a>Screen Blend Model （SBM）</h3><p>罗等[5]提出了一种非线性复合模型，称为背景混合模型：</p><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SID/22.png" title="Optional title"></p><blockquote><p>其中◦表示逐点乘法运算。不同于加性复合模型，背景层和雨层相互影响。 罗等[5]声称，背景混合模型可以对真实降雨图像的某些视觉属性（例如内部反射的效果）进行建模，从而生成视觉上更真实的降雨图像。雨层和背景层的组合取决于信号。这意味着，当背景昏暗时，雨层将主导雨图像的外观；并且，当背景明亮时，背景层将主导图像。</p></blockquote><h3 id="Heavy-Rain-Model-（HRM）"><a href="#Heavy-Rain-Model-（HRM）" class="headerlink" title="Heavy Rain Model （HRM）"></a>Heavy Rain Model （HRM）</h3><blockquote><p>杨等[1]提出了一个包括降雨条纹和降雨积聚的降雨模型。这是去雨文献中包含两个降雨现象的第一个模型。雨水积聚或遮雨效果是大气中的水颗粒和无法单独看到的远距离雨水条纹的结果。雨水积累的视觉效果类似于雾气或雾气，导致对比度低。 考虑到降雨的两个主要方面：Koschmieder模型（用于近似浑浊介质中场景的视觉外观）以及方向和形状不同的重叠降雨条纹，引入了一种新颖的降雨模型.</p></blockquote><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SID/55.png" title="Optional title"></p><blockquote><p>其中，St表示条纹方向相同的雨条纹层。t标记雨条纹层，而s是最大雨条纹层数。A是整体大气光，α是大气透射率。</p></blockquote><h3 id="Rain-Model-with-Occlusion-（ROM）"><a href="#Rain-Model-with-Occlusion-（ROM）" class="headerlink" title="Rain Model with Occlusion （ROM）"></a>Rain Model with Occlusion （ROM）</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SID/44.png" title="Optional title"></p><blockquote><p>刘等[15]将大雨模型扩展为可识别视频中雨的遮挡意识雨模型。该模型将雨条纹分为两种类型：添加到背景层的透明雨条纹和完全遮盖背景层的不透明雨条纹。这些不透明雨条纹的位置由称为信赖图的地图指示。</p></blockquote><h3 id="Comprehensive-Rain-Model-CRM"><a href="#Comprehensive-Rain-Model-CRM" class="headerlink" title="Comprehensive Rain Model (CRM)"></a>Comprehensive Rain Model (CRM)</h3><blockquote><p>杨等[2]结合以上所有内容在综合降雨模型中提到了退化因素用于在视频中模拟雨的外观。它考虑了时间雨景的属性，尤其是快速变化的降雨通常会引起闪烁的杂音。这种可见强度发生变化沿时间维度的时间称为降雨积累流。此外，它还考虑了其他因素，包括降雨条纹，降雨积聚和降雨遮挡。</p></blockquote><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SID/66.png" title="Optional title"></p><h3 id="Depth-Aware-Rain-Model-（DARM）"><a href="#Depth-Aware-Rain-Model-（DARM）" class="headerlink" title="Depth-Aware Rain Model （DARM）"></a>Depth-Aware Rain Model （DARM）</h3><blockquote><p>Hu等[16]进一步将α连接到场景深度d，以创建深度感知雨模型.</p></blockquote><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SID/77.png" title="Optional title"></p><h2 id="Deraining-Challenges"><a href="#Deraining-Challenges" class="headerlink" title="Deraining Challenges"></a>Deraining Challenges</h2><ul><li>建模雨水图像的困难在现实世界中，雨水可能以多种不同的方式在视觉上出现。雨条纹的大小，形状，比例，密度，方向等可能会有所不同。同样，雨的积累取决于各种水颗粒和大气条件。此外，下雨的外观也极大地依赖于背景场景的纹理和深度。所有这些都导致对降雨的外观进行建模的困难，因此导致渲染物理上正确的降雨图像是一项复杂的任务。</li><li>去雨问题的不适性即使使用仅考虑雨条纹的简单降雨模型，从退化的图像估计背景场景也是不适性的问题。原因是我们只有由带有雨和背景场景的融合信息的光产生的像素强度值。更糟的是，在某些情况下，背景信息可能会完全被雨条纹或浓雨积累或两者同时遮挡。</li><li>难以找到合适的先验信息。由于特征空间中的雨水和背景信息可能会重叠，因此将它们分开并不容易。 背景纹理可能被错误地认为是雨，导致不正确的排水。 因此，有必要对背景纹理和雨水有很强的先验性。 但是，要找到这些先验是困难的，因为背景纹理是多种多样的，并且某些与雨条纹或雨水堆积的外观相似。</li><li>真正的配对真相。大多数深度学习方法都依赖配对的雨水和干净的背景图像来训练其网络。但是，要获得真实的降雨图像及其确切的干净背景图像对是很棘手的。 即使对于静态背景，照明条件也总是会变化。这个困难不仅影响深度学习方法，而且影响评估任何方法的有效性。当前，为了进行定性评估，所有方法都依赖于人类主观判断，以确定恢复后的图像是否良好。 对于定量评估，所有当前方法都依赖于合成图像。不幸的是，到目前为止，合成图像和真实图像之间还存在很大差距。</li></ul><h2 id="Single-image-Deraining-Methods"><a href="#Single-image-Deraining-Methods" class="headerlink" title="Single-image Deraining Methods"></a>Single-image Deraining Methods</h2><h3 id="Model-based-Methods"><a href="#Model-based-Methods" class="headerlink" title="Model-based Methods"></a>Model-based Methods</h3><h4 id="Sparse-Coding-Methods"><a href="#Sparse-Coding-Methods" class="headerlink" title="Sparse Coding Methods"></a>Sparse Coding Methods</h4><p>[56]将输入向量表示为基本向量的稀疏线性组合。这些基向量的集合称为字典，其用于重建特定类型的信号，例如信号。排水问题中有雨条纹和背景信号。</p><p>Lin等[4]首次尝试使用形态学成分分析通过图像分解进行单图像排水。通过字典学习和稀疏编码，将最初提取的雨图像的高频分量进一步分解为雨分量和非雨分量。这项先锋工作成功地消除了稀疏的小雨条纹。但是，它极大地依赖于双边过滤器的预处理，因此会产生模糊的背景细节。</p><p>在后续工作中，Luo等人[5]增强了降雨的稀疏性，并将互斥性引入了区分性稀疏编码（DSC）中，以帮助准确地将降雨/背景层与其非线性复合物分离。由于具有互斥性，DSC保留了干净的纹理细节；但是，它在输出中显示出一些残留的降雨条纹，特别是对于大而密集的降雨条纹。</p><p>为了进一步提高建模能力，Zhu等人[7]构建了一个迭代的层分离过程，以使用背景特定的先验技术从背景层中去除雨水条纹，并从雨层中去除背景的纹理细节。从数量上讲，该方法在某些合成数据集上可获得与同期发布的基于深度学习的方法（即JORDER [1]和DDN [18]）相当的性能。但是，从质量上说，在真实图像上，该方法在处理大雨的情况下往往会失败，因为大雨可能会在不同的方向上移动。</p><p>为了模拟雨条纹的方向和稀疏度，邓等人[17]制定了方向群稀疏模型（DGSM），其中包括三个稀疏项，代表着雨纹的内在方向和结构知识。它可以有效消除模糊的雨纹，但不能消除锐利的雨纹。</p><h4 id="Gaussian-Mixture-Model"><a href="#Gaussian-Mixture-Model" class="headerlink" title="Gaussian Mixture Model"></a>Gaussian Mixture Model</h4><p>Li等[6]应用高斯混合模型（GMM）来模拟雨层和背景层。背景层的GMM是从具有不同背景场景的真实图像中离线获取的。建议从输入图像中选择不具有背景纹理的雨斑来训练雨层的GMM。利用总变化量来消除小火花雨。该方法能够有效地去除小规模和中等规模的雨斑，但是不能处理大而尖的雨斑。</p><h3 id="Deep-Learning-Based-Methods"><a href="#Deep-Learning-Based-Methods" class="headerlink" title="Deep Learning Based Methods"></a>Deep Learning Based Methods</h3><h4 id="Deep-CNNs"><a href="#Deep-CNNs" class="headerlink" title="Deep CNNs"></a>Deep CNNs</h4><p>[1]构建一个联合降雨检测和清除网络。它可以处理大雨，重叠的雨条纹和雨水堆积。该网络可以通过预测二元防雨罩来检测雨水的位置，并采用递归框架来去除雨水条纹并逐步清除雨水积聚。该方法在下大雨的情况下取得了良好的效果。但是，它可能会错误地去除垂直纹理并生成曝光不足的照明。</p><p>同年，傅等人[18，19]尝试通过深层细节网络（DetailNet）去除雨水条纹。该网络仅将高频细节作为输入，并预测雨水残留和清晰的图像。该论文表明，删除网络输入中的背景信息是有益的，因为这样做可以使训练更容易，更稳定。但是，该方法仍然不能处理大而尖的雨纹。</p><p>继杨等[1]和傅等[18，19]，提出了许多基于CNN的方法[8，10，20-23]。这些方法采用了更先进的网络架构，并注入了与降雨相关的新先验。他们在数量和质量上都取得了更好的结果。但是，由于它们受完全监督的学习范式的局限性（即使用合成降雨图像），它们在处理训练中从未见过的真实降雨条件时往往会失败。</p><h4 id="Generative-Adversarial-Networks"><a href="#Generative-Adversarial-Networks" class="headerlink" title="Generative Adversarial Networks"></a>Generative Adversarial Networks</h4><p>生成对抗网络为了捕获无法建模和合成的某些视觉降雨属性，引入对抗学习以减少生成的结果与真实清晰图像之间的领域差距。典型的网络体系结构由两部分组成：生成器和鉴别器，其中鉴别器试图评估所生成的结果是真实的还是伪造的，这提供了额外的反馈以使生成器正规化，以产生更令人愉悦的结果。</p><p>张等[24]直接将条件生成对抗网络（CGAN）用于单图像除雨任务。CGAN能够捕获超出信号保真度的视觉属性，并以更好的照明，颜色和对比度分布呈现结果。但是，当测试降雨图像的背景与训练集中的背景不同时，CGAN有时可能会生成视觉伪像。</p><p>Li等[13]提出了一种结合物理驱动网络和对抗学习精炼网络的单幅图像去雨方法。第一阶段从合成数据中学习并估算与物理相关的成分，即降雨条纹，透射率和大气光。在第二细化阶段，提出了深度引导GAN，以补偿丢失的细节并在第一阶段抑制引入的伪影。从真实的降雨数据中学习，通过这些方法得出的结果在视觉上具有显着性改进，即彻底清除雨水堆积，并实现更加平衡的亮度分布。然而，因为基于GAN的方法并不擅长捕获细粒度细节信号，这些方法也无法正确模拟真实的雨水条纹的外观。</p><h1 id="代码和数据"><a href="#代码和数据" class="headerlink" title="代码和数据"></a>代码和数据</h1><p>作者在github发布了一个存储库，包括74篇雨水去除论文的直接链接，9种视频雨水去除方法和20种单图像雨水去除方法的源代码，19个相关项目页面，6个合成数据集和4个真实数据集，以及4个常用的图像质量度量。</p><p>&lt;hongwang01/Video-and-Single-Image-Deraining&gt;</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>按照原文的序号列出。</p><p>之前进行过阅读的论文序号：</p><ul><li>[1]：基于模型的去雨方法。</li><li>[19]：基于深度CNN网络的去雨方法，称为DerainNet。</li><li>[22]：提出一种密度感知多路稠密连接神经网络算法，DID-MDN。</li><li>[24、28]：GAN相关。</li><li>[29]：基于循环旋转CNN的不确定性多尺度残差学习。</li></ul><p>[1] W. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo, and S. Yan, “Deep joint rain detection and removal from a single image,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, July 2017.<br>[2] W. Yang, J. Liu, and J. Feng, “Frame-consistent recurrent video deraining with dual-level flow,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2019.<br>[3] K. Garg and S. K. Nayar, “Detection and removal of rain from videos,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, vol. 1, 2004, pp. I–528.<br>[4] L. W. Kang, C. W. Lin, and Y. H. Fu, “Automatic single- image-based rain streaks removal via image decomposition,” IEEE Trans. on Image Processing, vol. 21, no. 4, pp. 1742– 1755, April 2012.<br>[5] Y. Luo, Y. Xu, and H. Ji, “Removing rain from a single image via discriminative sparse coding,” in Proc. IEEE Int’l Conf. Computer Vision, 2015, pp. 3397–3405.<br>[6] Y. Li, R. T. Tan, X. Guo, J. Lu, and M. S. Brown, “Rain streak removal using layer priors,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, 2016, pp. 2736–2744.<br>[7] L. Zhu, C. Fu, D. Lischinski, and P. Heng, “Joint bi- layer optimization for single-image rain streak removal,” in Proc. IEEE Int’l Conf. Computer Vision, Oct 2017, pp. 2545– 2553.<br>[8] Z. Fan, H. Wu, X. Fu, Y. Huang, and X. Ding, “Residual- guide network for single image deraining,” in ACM Trans. Multimedia, 2018, pp. 1751–1759.<br>[9] D. Ren, W. Zuo, Q. Hu, P. Zhu, and D. Meng, “Progressive image deraining networks: A better and simpler baseline,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2019.<br>[10] G. Li, X. He, W. Zhang, H. Chang, L. Dong, and L. Lin, “Non-locally enhanced encoder-decoder network for single image de-raining,” in ACM Trans. Multimedia. ACM, 2018, pp. 1056–1064.<br>[11] X. Fu, B. Liang, Y. Huang, X. Ding, and J. Paisley, “Lightweight pyramid networks for image deraining,” IEEE Trans. on Neural Networks and Learning Systems, pp. 1–14, 2019.<br>[12] X.Jin,Z.Chen,J.Lin,Z.Chen,andW.Zhou,“Unsupervised single image deraining with self-supervised constraints,” in Proc. IEEE Int’l Conf. Image Processing, Sep. 2019, pp. 2761–2765.<br>[13] R. Li, L.-F. Cheong, and R. T. Tan, “Heavy rain image restoration: Integrating physics model and conditional adver- sarial learning,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2019.<br>[14] K. Garg and S. K. Nayar, “Vision and rain,” Int. J. Comput. Vision, vol. 75, no. 1, pp. 3–27, October 2007.<br>[15] J. Liu, W. Yang, S. Yang, and Z. Guo, “Erase or fill? deep joint recurrent rain removal and reconstruction in videos,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2018, pp. 3233–3242.<br>[16] X. Hu, C.-W. Fu, L. Zhu, and P.-A. Heng, “Depth-attentional features for single-image rain removal,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2019.<br>[17] L.-J. Deng, T.-Z. Huang, X.-L. Zhao, and T.-X. Jiang, “A di- rectional global sparse model for single image rain removal,” Applied Mathematical Modelling, vol. 59, pp. 662 – 679, 2018.<br>[18] X. Fu, J. Huang, X. Ding, Y. Liao, and J. Paisley, “Re- moving rain from single images via a deep detail network,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, Honolulu, Hawaii, USA, July 2017.<br>[19] ——, “Clearing the skies: A deep network architecture for single-image rain removal,” IEEE Trans. on Image Process- ing, vol. 26, no. 6, pp. 2944–2956, June 2017.<br>[20] R. Li, L.-F. Cheong, and R. T. Tan, “Single Image Deraining using Scale-Aware Multi-Stage Recurrent Network,” arXiv e-prints, p. arXiv:1712.06830, Dec 2017.<br>[21] X. Li, J. Wu, Z. Lin, H. Liu, and H. Zha, “Recurrent squeeze-and-excitation context aggregation net for single image deraining,” in Proc. IEEE European Conf. Computer Vision, 2018, pp. 262–277.<br>[22] H. Zhang and V. M. Patel, “Density-aware single image de- raining using a multi-stream dense network,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2018.<br>[23] J. Pan, S. Liu, D. Sun, J. Zhang, Y. Liu, J. Ren, Z. Li, J. Tang, H. Lu, Y.-W. Tai, and M.-H. Yang, “Learning dual convolu-tional neural networks for low-level vision,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2018.<br>[24] H. Zhang, V. Sindagi, and V. M. Patel, “Image De-raining Using a Conditional Generative Adversarial Network,” arXiv e-prints, p. arXiv:1701.05957, Jan 2017.<br>[25] W. Wei, D. Meng, Q. Zhao, Z. Xu, and Y. Wu, “Semi- supervised transfer learning for image rain removal,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recog- nition, June 2019.<br>[26] S. Li, I. B. Araujo, W. Ren, Z. Wang, E. K. Tokuda, R. H. Junior, R. Cesar-Junior, J. Zhang, X. Guo, and X. Cao, “Single image deraining: A comprehensive benchmark anal- ysis,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2019.<br>[27] D. Eigen, D. Krishnan, and R. Fergus, “Restoring an image taken through a window covered with dirt or rain,” in Proc. IEEE Int’l Conf. Computer Vision, December 2013.<br>[28] R. Qian, R. T. Tan, W. Yang, J. Su, and J. Liu, “Attentive generative adversarial network for raindrop removal from a single image,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2018.<br>[29] R. Yasarla and V. M. Patel, “Uncertainty guided multi-scale residual learning-using a cycle spinning cnn for single image de-raining,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2019.<br>[30] W. Yang, J. Liu, S. Yang, and Z. Guo, “Scale-free single image deraining via visibility-enhanced recurrent wavelet learning,” IEEE Trans. on Image Processing, vol. 28, no. 6, pp. 2948–2961, June 2019.<br>[31] X. Li, J. Wu, Z. Lin, H. Liu, and H. Zha, “Rescan: Re- current squeeze-and-excitation context aggregation net,” in Proc. IEEE European Conf. Computer Vision, Oct. 2018.<br>[32] Y. Wang, S. Liu, C. Chen, and B. Zeng, “A hierarchical approach for rain or snow removing in a single color image,” IEEE Trans. on Image Processing, vol. 26, no. 8, pp. 3936– 3950, Aug 2017.<br>[33] W. Yang, R. T. Tan, J. Feng, J. Liu, S. Yan, and Z. Guo, “Joint rain detection and removal from a single image with contex- tualized deep networks,” IEEE Trans. on Pattern Analysis and Machine Intelligence, pp. 1–1, 2019.<br>[34] T. Wang, X. Yang, K. Xu, S. Chen, Q. Zhang, and R. W. Lau, “Spatial attentive single-image deraining with a high quality real rain dataset,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2019.<br>[35] R. Li, L.-F. Cheong, and R. T. Tan, “Single Image Deraining using Scale-Aware Multi-Stage Recurrent Network,” ArXiv e-prints, December 2017.<br>[36] M. S. Gerald Schaefer, “Ucid: an uncompressed color image database,” 2003.<br>[37] P. K. Nathan Silberman, Derek Hoiem and R. Fergus, “In- door segmentation and support inference from rgbd images,” in Proc. IEEE European Conf. Computer Vision, 2012.<br>[38] C. Godard, O. Mac Aodha, and G. J. Brostow, “Unsupervised monocular depth estimation with left-right consistency,” 2017.<br>[39] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban scene understanding,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, 2016.<br>[40] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik, “Contour detection and hierarchical image segmentation,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 5, pp. 898–916, May 2011.<br>[41] Zhou Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality assessment: from error visibility to structural similarity,” IEEE Trans. on Image Processing, vol. 13, no. 4, pp. 600–612, April 2004.<br>[42] S. Gu, D. Meng, W. Zuo, and L. Zhang, “Joint convolutional analysis and synthesis sparse representation for single image layer separation,” in Proc. IEEE Int’l Conf. Computer Vision, Oct 2017, pp. 1717–1725.<br>[43] A. C. Brooks, X. Zhao, and S. Member, “Structural similarity quality metrics in a coding context: Exploring the space of realistic distortions,” IEEE Trans. on Image Processing, pp. 1261–1273, 2008.<br>[44] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a completely blind image quality analyzer,” IEEE Signal Processing Letters, vol. 20, no. 3, pp. 209–212, March 2013.<br>[45] N. Venkatanath, D. Praneeth, B. M. Chandrasekhar, S. S. Channappayya, and S. S. Medasani, “Blind image quality evaluation using perception based features,” in Proc. IEEE National Conf. Communications, 2008.<br>[46] A. Mittal, A. K. Moorthy, and A. C. Bovik, “Blind/referenceless image spatial quality evaluator,” in Conf. Record of Asilomar Conf. on Signals, Systems and Computers, Nov 2011, pp. 723–727.<br>[47] L. Zhang, L. Zhang, and A. C. Bovik, “A feature-enriched completely blind image quality evaluator,” IEEE Trans. on Image Processing, vol. 24, no. 8, pp. 2579–2591, Aug 2015.<br>[48] L. Liu, B. Liu, H. Huang, and A. C. Bovik, “No-reference image quality assessment based on spatial and spectral en- tropies,” Signal Processing: Image Communication, vol. 29, no. 8, pp. 856 – 863, 2014.<br>[49] C. Ma, C.-Y. Yang, X. Yang, and M.-H. Yang, “Learn- ing a no-reference quality metric for single-image super- resolution,” Comput. Vis. Image Underst., vol. 158, pp. 1–16, May 2017.<br>[50] X. Chen, Q. Zhang, M. Lin, G. Yang, and C. He, “No- reference color image quality assessment: from entropy to perceptual quality,” EURASIP Journal on Image and Video Processing, vol. 2019, no. 1, p. 77, Sep 2019. [Online]. Available: <a href="https://doi.org/10.1186/s13640-019-0479-7">https://doi.org/10.1186/s13640-019-0479-7</a><br>[51] S. Gabarda and G. Cristo ́bal, “Blind image quality assess- ment through anisotropy,” J. Opt. Soc. Am. A, vol. 24, no. 12, pp. B42–B51, Dec 2007.<br>[52] L. Zhang, L. Zhang, and A. C. Bovik, “A feature-enriched completely blind image quality evaluator,” IEEE Trans. on Image Processing, vol. 24, no. 8, pp. 2579–2591, Aug 2015.<br>[53] M. A. Saad, A. C. Bovik, and C. Charrier, “A dct statistics- based blind image quality index,” IEEE Signal Processing Letters, vol. 17, no. 6, pp. 583–586, June 2010.<br>[54] H. Wang, Y. Wu, M. Li, Q. Zhao, and D. Meng, “A Survey on Rain Removal from Video and Single Image,” arXiv e- prints, p. arXiv:1909.08326, Sep 2019.<br>[55] R. A. Bradley and M. E. Terry, “Rank analysis of incom- plete block designs: The method of paired comparisons,” Biometrika, vol. 39, no. 3-4, pp. 324–345, 12 1952.<br>[56] M. Elad and M. Aharon, “Image denoising via learned dictionaries and sparse representation,” in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, June 2006,pp. 895–900.<br>[57] A.Yamashita,Y.Tanaka,andT.Kaneko,“Removalofadher-<br>ent waterdrops from images acquired with stereo camera,” in IEEE/RSJ Int’l Conf. on Intelligent Robots and Systems, Aug 2005, pp. 400–405.<br>[58] A. Yamashita, I. Fukuchi, and T. Kaneko, “Noises removal from image sequences acquired with moving camera by esti- mating camera motion from spatio-temporal information,” in IEEE/RSJ Int’l Conf. on Intelligent Robots and Systems, Oct 2009, pp. 3794–3801.<br>[59] S. You, R. T. Tan, R. Kawakami, Y. Mukaigawa, and K. Ikeuchi, “Adherent raindrop modeling, detectionand re- moval in video,” IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 38, no. 9, pp. 1721–1733, Sep. 2016.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;原创文章，转载、引用请注明出处！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h1&gt;&lt;p&gt;这是一篇关于单图像去雨和视频</summary>
      
    
    
    
    
    <category term="论文研读" scheme="http://example.com/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
    
    <category term="图像去雨" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%A8/"/>
    
  </entry>
  
</feed>
