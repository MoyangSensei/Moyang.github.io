<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><meta name="author" content="Fy J"><meta name="renderer" content="webkit"><meta name="copyright" content="Fy J"><meta name="keywords" content="MoyangSensei"><meta name="description" content="null"><meta name="Cache-Control" content="no-cache"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>论文研读：Wavelet-SRNet · MoYang</title><link rel="stylesheet" href="/css/style.css?v=2018.7.9"><link rel="stylesheet" href="/css/animation.css?v=2018.7.9"><link rel="icon" href="/img/assets/Moyangico.ico"><link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.6"><!-- scripts--><script>(function( w ){
  "use strict";
  // rel=preload support test
  if( !w.loadCSS ){
    w.loadCSS = function(){};
  }
  // define on the loadCSS obj
  var rp = loadCSS.relpreload = {};
  // rel=preload feature support test
  // runs once and returns a function for compat purposes
  rp.support = (function(){
    var ret;
    try {
      ret = w.document.createElement( "link" ).relList.supports( "preload" );
    } catch (e) {
      ret = false;
    }
    return function(){
      return ret;
    };
  })();

  // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
  // then change that media back to its intended value on load
  rp.bindMediaToggle = function( link ){
    // remember existing media attr for ultimate state, or default to 'all'
    var finalMedia = link.media || "all";

    function enableStylesheet(){
      link.media = finalMedia;
    }

    // bind load handlers to enable media
    if( link.addEventListener ){
      link.addEventListener( "load", enableStylesheet );
    } else if( link.attachEvent ){
      link.attachEvent( "onload", enableStylesheet );
    }

    // Set rel and non-applicable media type to start an async request
    // note: timeout allows this to happen async to let rendering continue in IE
    setTimeout(function(){
      link.rel = "stylesheet";
      link.media = "only x";
    });
    // also enable media after 3 seconds,
    // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
    setTimeout( enableStylesheet, 3000 );
  };

  // loop through link elements in DOM
  rp.poly = function(){
    // double check this to prevent external calls from running
    if( rp.support() ){
      return;
    }
    var links = w.document.getElementsByTagName( "link" );
    for( var i = 0; i < links.length; i++ ){
      var link = links[ i ];
      // qualify links to those with rel=preload and as=style attrs
      if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
        // prevent rerunning on link
        link.setAttribute( "data-loadcss", true );
        // bind listeners to toggle media back
        rp.bindMediaToggle( link );
      }
    }
  };

  // if unsupported, run the polyfill
  if( !rp.support() ){
    // run once at least
    rp.poly();

    // rerun poly on an interval until onload
    var run = w.setInterval( rp.poly, 500 );
    if( w.addEventListener ){
      w.addEventListener( "load", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    } else if( w.attachEvent ){
      w.attachEvent( "onload", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    }
  }


  // commonjs
  if( typeof exports !== "undefined" ){
    exports.loadCSS = loadCSS;
  }
  else {
    w.loadCSS = loadCSS;
  }
}( typeof global !== "undefined" ? global : this ) );</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" defer></script><script src="/js/main.js?v=2018.7.9" defer></script><!-- fancybox--><link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script><!-- busuanzi--><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="MoyangSensei" type="application/atom+xml">
</head><body><section class="profile-close" id="cxo-profile"><div class="profile-avatar"><i class="fa fa-caret-left"></i><img src="/img/assets/cat.png"></div><!--.profile-saying
  i.fa.fa-comment
  .saying--><div class="cxo-profile-inner"><div class="profile-name">Fy J</div><div class="profile-signature">CS专业扫雷学深造学者互联网冲浪一级选手</div><div class="friends"><div>FRIENDS</div><span><a href="//hnjia00.github.io" target="_black">jhn</a></span></div><div class="read-progress"></div></div></section><header id="cxo-intro" style="height: 70vh;background-image: url(/img/intro/index-bg.png);"><nav id="cxo-intro-nav"><section><div class="intro-nav-title"><a href="/">MoYang's Blog</a></div><div class="intro-nav-label-box"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div><i class="fa fa-bars intro-nav-menu"><div class="intro-nav-drop"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div></i><div class="clear"></div></section></nav><h1 class="post-title">论文研读：Wavelet-SRNet</h1><div class="post-intros"><div class="post-intro-meta"><span class="post-intro-time"><i class="post-intro-calendar fa fa-calendar"></i><span>09-18-2020 22:09:13</span></span><span class="post-intro-tags"><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="论文研读"> 论文研读</a><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="图像超分辨率"> 图像超分辨率</a></span></div><div class="post-intro-read"><span> Word count: <span class="post-count">5k</span> | Reading time: <span class="post-count">19</span>min</span></div></div></header><article class="cxo-up" id="cxo-content-outer"><section id="cxo-content-inner"><article class="article-entry" id="post"><blockquote>
<p>原创文章，转载、引用请注明出处！</p>
</blockquote>
<hr>
<blockquote>
<p>Wavelet-SRNet: A Wavelet-based CNN for Multi-scale Face Super Resolution</p>
</blockquote>
<p>2017年的一篇文章，核心论点是小波变换+CNN，针对的问题是人脸的SR。</p>
<h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>文章提出，在对低分辨率图像做上采样的时候，传统CNN方法的问题：</p>
<ul>
<li><p>性能的降低；</p>
</li>
<li><p>输出过于平滑，导致了细节遗失（翻译成白话就是上采样的主观效果差）；</p>
</li>
</ul>
<blockquote>
<blockquote>
<p>When dealing with very low resolution (LR) images, the performance of these CNN based methods greatly degrades. Meanwhile, these methods tend to produce over-smoothed outputs and miss some textural details.</p>
</blockquote>
<blockquote>
<p>深度学习类的方法，导致过度平滑的原因：使用MSE作为loss。<br>Most of these convolutional methods use MSE loss to learn the map function of LR/HR image pairs, which leads to over-smooth outputs when the input resolution is very low and the magnification is large.</p>
</blockquote>
</blockquote>
<ul>
<li>低倍数下才有良好的效果。</li>
</ul>
<blockquote>
<p>Besides, they seem to only work well on limited up-scaling factors (2× or 4×) and degrades greatly when ultra-resolving a very small input (like 16 × 16 or smaller).</p>
</blockquote>
<h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/1.png" title="Optional title"></p>
<p>HR图像在输入Wavelet-SRNet之前首先会做小波包分解：通过小波包分解将图像解析为一组具有相同大小的小波系数：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/2.png" title="Optional title"></p>
<p>Wavelet-SRNet选用最简单的小波：haar小波，此小波足以描述不同频率的人脸信息。另外，使用快速小波变换（2-D fast wavelet transform ，FWT）来计算haar小波：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/3.png" title="Optional title"></p>
<p>Wavelet-SRNet分为三个子网：</p>
<h2 id="embedding"><a href="#embedding" class="headerlink" title="embedding"></a>embedding</h2><p><strong>embedding网络的作用是将低分辨率输入表示为一组特征地图。</strong></p>
<p>嵌入网络以<code>3×h×w</code>的低分辨率图像为输入，将其表示为一组特征映射。所有卷积的大小都是相同的3×3，步长为1，pad为1，使得嵌入网络中的每个特征图与输入图像的大小相同。</p>
<p>通过embedding网络，将输入的LR图像映射成<code>Ne×h×w</code>的特征映射，不需要上采样或下采样，其中Ne是最后一层的信道尺寸。</p>
<h2 id="wavelet-prediction"><a href="#wavelet-prediction" class="headerlink" title="wavelet prediction"></a>wavelet prediction</h2><p><strong>经过embedding网络之后，利用小波预测网对相应的小波系数图像进行估计。</strong></p>
<p>将小波包分解到小波网络的每一个子网中，并将小波包分解成相应的小波网络。这里将所有3×3的卷积填充物设置为步长为1、pad为1的卷积（与embedding网络相似），使得每个推导出的小波系数与LR输入的大小相同，即<code>3×h×w</code>。</p>
<p>此外，由于Haar小波变换系数之间的高度独立性，<strong>每两个子网之间不允许任何信息交互，这使得我们的网络具有可扩展性。</strong></p>
<blockquote>
<p>可扩展性：wavelet prediction net中的子网数量Nw可根据需求进行调整。</p>
</blockquote>
<p>在预测网中，用不同的子网数目很容易实现不同的放大倍数。例如，Nw=16和Nw=64分别代表4倍和8倍的放大倍数。</p>
<h2 id="reconstruction-networks。"><a href="#reconstruction-networks。" class="headerlink" title="reconstruction networks。"></a>reconstruction networks。</h2><p><strong>最后，重建网络从这些系数图像重建出高分辨率图像。</strong></p>
<p>利用重构网络将总尺寸为<code>Nw×3×h×w</code>的小波图像变换成<code>3×（r×h）×（r×w）</code>的原始图像空间。由一个填充尺寸为r×r，步长为r的解卷积层（deconvolution）组成。</p>
<p>虽然解卷积层的大小取决于放大倍数r，但它可以由一个常数小波重构矩阵初始化并在训练中固定。因此，它对整个网络的可扩展性没有影响。</p>
<p>最后，三个子网络之间的输入输出映射关系：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/4.png" title="Optional title"></p>
<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>整个loss函数分为三部分：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/5.png" title="Optional title"></p>
<h2 id="full-image-loss（MSE）"><a href="#full-image-loss（MSE）" class="headerlink" title="full-image loss（MSE）"></a>full-image loss（MSE）</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/6.png" title="Optional title"></p>
<p>MSE是超分辨率方法中最常使用MSE损失函数。</p>
<p>MSE是在图像空间上的一个限制，<strong>因为MSE几乎不能获取到高频纹理细节信息。</strong>Wavelet-SRNet的full-image loss一方面是在图像空间上的限制，另一方面也能在平滑度和纹理细节上达到一个平衡。</p>
<blockquote>
<p>A traditional MSE loss in image space, which is called full-image loss, is also used to get a balance between s- moothness and textures. </p>
</blockquote>
<h2 id="wavelet-based-loss"><a href="#wavelet-based-loss" class="headerlink" title="wavelet-based loss"></a>wavelet-based loss</h2><p>这里同样分为两部分：</p>
<h3 id="wavelet-prediction-loss"><a href="#wavelet-prediction-loss" class="headerlink" title="wavelet prediction loss"></a>wavelet prediction loss</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/7.png" title="Optional title"></p>
<p>wavelet prediction loss相当于在小波域上的加权MSE，定义如下：</p>
<p>W=(λ1,λ2,⋅⋅⋅，λNw)，一个平衡不同组小波系数重要性的权重矩阵；C是真实值，C尖是小波系数；绝对值的项用来提取全局拓扑信息</p>
<p>这个loss函数应该更关注局部纹理信息，故高频系数的权重应该更大一点。</p>
<blockquote>
<p>The former one is a weight- ed version of MSE in wavelet domain…… More attention can be paid on local textures with bigger weights appointed to high-frequency coefficients.</p>
</blockquote>
<h3 id="texture-loss"><a href="#texture-loss" class="headerlink" title="texture loss"></a>texture loss</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/8.png" title="Optional title"></p>
<p>texture loss是为了避免高频小波系数收敛为0。α和ϵ是slack values，可以使高频小波系数不为0，因此避免了纹理细节的下降。</p>
<blockquote>
<p>The texture loss is designed to prevent high-frequency wavelet coefficients from converging to zero….. It keeps high-frequency wavelet coefficients non-zero and hence prevents the degradation of texture details.</p>
</blockquote>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h2><p>输入的LR图片做了一组对比：一组是双三次插值，一组是小波包变换的近似系数。</p>
<blockquote>
<p>Two types of low-resolution images are taken as input, one of which is down-sampled by bicubic interpolation and the other is the approximation coefficient of wavelet packet decomposition.</p>
</blockquote>
<p>模型可适应任意大小的输入，因为没有全连接层。</p>
<blockquote>
<p>Since our network is a fully convolutional architecture without fully-connected layers, it can be applied to the input of arbitrary size.</p>
</blockquote>
<p>给出了网络的设置。</p>
<blockquote>
<p>Our model is implemented with the Caffe frame- work [11]. The loss in (6) is minimized using SGD with a batch size of 256. For the hyper-parameters, we set em- piricallyλ1 = 0.01,λ2 = λ3 = ··· = λNw = 1,μ = 1,k=2,γk =γk+1 =···=γNw =1,ν=0.1. The learning rate is set to 0.01 initially and reduced by a factor of 10 each 3000 iterations. It takes about 14, 000 ∼ 16, 000 iterations for our network to converge.</p>
</blockquote>
<p>两个数据集做训练。</p>
<blockquote>
<p>Our experiments are implemented on two datasets: CelebA and Helen. There are 202,599 faces in CelebA and 2,230 faces in Helen. In the training phase, we use the large train set of CelebA(162,700 images) for training and the validation set(19,867 images) of CelebA for validation. In the testing phase, we evaluate with the 19,962-image test set of CelebA and the 330-image test set of Helen, assuring no over-lapped images appearing in both the training and testing phase. The images are cropped around the face with eyes aligned horizontally.</p>
</blockquote>
<p>为了保证对照训练的公平，所有的数据集都使用了眼睛对齐的人脸照片，且在下采样之前没有做其他额外的处理。</p>
<blockquote>
<p>For a fair comparison, we use the same set of eyes-aligned faces for all the methods with no extra preprocessing before down-sampling.</p>
</blockquote>
<p>使用的评价指标：SSIM和PSNR。</p>
<blockquote>
<p>We adopt PSNR(dB) and SSIM for quantitative metric, and calculate PSNR on the luminance channel, following by [35], and SSIM on the three channels of RGB.</p>
</blockquote>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="不同放大倍数的定性比较"><a href="#不同放大倍数的定性比较" class="headerlink" title="不同放大倍数的定性比较"></a>不同放大倍数的定性比较</h3><p>与双三次插值对比了不同放大倍数下的效果。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/9.png" title="Optional title"></p>
<h3 id="不同方法的定性比较"><a href="#不同方法的定性比较" class="headerlink" title="不同方法的定性比较"></a>不同方法的定性比较</h3><p>8倍放大下不同方法的对比。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/10.png" title="Optional title"></p>
<h3 id="鲁棒性讨论"><a href="#鲁棒性讨论" class="headerlink" title="鲁棒性讨论"></a>鲁棒性讨论</h3><p>讨论了模型在面对未知高斯模糊，姿态和遮挡的鲁棒性。</p>
<p>在图6中，低分辨率人脸由高斯模糊核生成，其步长为8，对应于8×下采样。高斯模糊核的σ从0增加到6，其中σ=0表示最近邻插值下采样。如图6所示，当σ&lt;4时，模型方法显示出一定的鲁棒性，当σ&gt;=4时生成更平滑的面。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/11.png" title="Optional title"></p>
<p>作为比较，CBN的结果与平均面更接近。对于姿态变化，如图7所示，CBN无法重建大姿态的可信人脸，可能是由于空间预测不准确。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/12.png" title="Optional title"></p>
<h3 id="不同放大倍数的定量比较"><a href="#不同放大倍数的定量比较" class="headerlink" title="不同放大倍数的定量比较"></a>不同放大倍数的定量比较</h3><p>四个放大倍数：(32×32,4×)，(16×16,8×)，(8×8,8×)以及(8×8,16×)，其中（m×m，n×）为m×m输入分辨率，放大倍数为n。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Wavelet-SRNet/13.png" title="Optional title"></p>
<h1 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h1><p>来自原文的叙述：</p>
<ul>
<li><ol>
<li>A novel wavelet-based approach is proposed for <code>CNN-based face SR problem</code>. To the best of our knowledge, this is <code>the first</code> attempt to transform single image SR to wavelet coefficients prediction task in deep learning framework - albeit many wavelet-based researches exist for SR.</li>
</ol>
</li>
<li><ol start="2">
<li>A flexible and extensible fully convolutional neural network is presented to make the best use of wavelet transform. It can apply to different input-resolution faces with multiple upscaling factors.</li>
</ol>
</li>
<li><ol start="3">
<li>We qualitatively and quantitatively explore multiscale face super-resolution, especially on <code>very low input resolutions</code>. Experimental results show that our proposed approach outperforms state-of-the-art face SR methods.</li>
</ol>
</li>
</ul>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul>
<li><p>code：<a target="_blank" rel="noopener" href="https://github.com/hhb072/WaveletSRNet">https://github.com/hhb072/WaveletSRNet</a></p>
</li>
<li><p>文章：<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper.pdf">http://openaccess.thecvf.com/content_ICCV_2017/papers/Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper.pdf</a></p>
</li>
</ul>
<h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><ul>
<li><p>第一次用了小波包变换和深度的结合在这个问题上，有开创意义。</p>
</li>
<li><p>放大倍数方面，文章给出了32，对模型的描述是：可以接受任何大小的数据以及可拓展性。意思就是说放大的倍数也是比较有前景的（如果效果真的像叙述的一样有力的话）。</p>
</li>
<li><p>做的问题是有针对性的，就是正面、无任何遮挡或者偏移的人脸。</p>
</li>
<li><p>后面在做鲁棒性的讨论时，也凸显出了模型的局限性（实际上就是给出了大家都无法解决的问题）。</p>
</li>
</ul>
<hr>
<h1 id="小波变换"><a href="#小波变换" class="headerlink" title="小波变换"></a>小波变换</h1><p>一种进行信号时频分析和处理的方法。</p>
<blockquote>
<p>理论依据：傅里叶变换 -&gt; 短时傅里叶变换 -&gt; 小波变换<br>这三种变换的本质是将信号从时域转换为频域。</p>
<p>上面的论文里用到的：小波包变换，由小波变换而来</p>
</blockquote>
<h2 id="傅立叶变换（FT）"><a href="#傅立叶变换（FT）" class="headerlink" title="傅立叶变换（FT）"></a>傅立叶变换（FT）</h2><p>傅里叶变换的核心是<strong>从时域到频域的变换，而这种变换是通过一组特殊的正交基来实现的</strong>。</p>
<blockquote>
<p>如何理解傅里叶变换公式？ - 苗华栋的回答 - 知乎<br><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/19714540/answer/1119070975">https://www.zhihu.com/question/19714540/answer/1119070975</a></p>
</blockquote>
<p>傅立叶变换的三个核心：</p>
<ul>
<li><p>傅立叶的重要论断：<strong>任何连续周期信号都可以由一组适当的正弦曲线组合而成。</strong></p>
</li>
<li><p>为什么要<strong>变换</strong>？因为最容易理解的表现形式不一定是最方便做计算的。变换（计算空间）的意义是将原来空间中难以处理的问题变换到方便计算的空间去。</p>
</li>
<li><p>变换的<strong>空间</strong>是什么？时域和频域。</p>
</li>
</ul>
<h2 id="短时傅立叶变换（STFT）"><a href="#短时傅立叶变换（STFT）" class="headerlink" title="短时傅立叶变换（STFT）"></a>短时傅立叶变换（STFT）</h2><h3 id="傅立叶变换的缺陷"><a href="#傅立叶变换的缺陷" class="headerlink" title="傅立叶变换的缺陷"></a>傅立叶变换的缺陷</h3><p>在傅立叶变换的基础上提出新的讨论，是因为傅立叶变换是有缺陷的：<strong>对非平稳过程，傅里叶变换有局限性。</strong></p>
<p>再进一步的说明是，傅里叶变换处理<strong>非平稳信号</strong>有天生缺陷。它只能获取一段信号总体上包含哪些频率的成分，但是对各成分出现的时刻并无所知。因此时域相差很大的两个信号，可能频谱图一样。</p>
<p>然而平稳信号大多是人为制造出来的，自然界的大量信号几乎都是非平稳的。<strong>对于这样的非平稳信号，只知道包含哪些频率成分是不够的，我们还想知道各个成分出现的时间。</strong>理解信号频率随时间变化的情况，各个时刻的瞬时频率及其幅值，也就是时频分析。</p>
<h3 id="加窗"><a href="#加窗" class="headerlink" title="加窗"></a>加窗</h3><p>想到知道频率随时间变化的情况，一个简单的思想就是“加窗”：<strong>把整个时域过程分解成无数个等长的小过程，每个小过程近似平稳，再傅里叶变换，就知道在哪个时间点上出现了什么频率了。</strong></p>
<p>短时傅立叶变换的基本思想由此而来：将信号加滑动时间窗，并对窗内信号做傅立叶变换，得到信号的时变频谱。</p>
<h2 id="小波变换（WT）"><a href="#小波变换（WT）" class="headerlink" title="小波变换（WT）"></a>小波变换（WT）</h2><h3 id="短时傅立叶变换的缺陷"><a href="#短时傅立叶变换的缺陷" class="headerlink" title="短时傅立叶变换的缺陷"></a>短时傅立叶变换的缺陷</h3><p>同上，新的讨论一定是因为前者有缺陷：提到了加窗，那么问题就是，这个“窗函数”如何定义？窗太宽太窄都会出现问题：<strong>窗太窄，窗内的信号太短，会导致频率分析不够精准，频率分辨率差。窗太宽，时域上又不够精细，时间分辨率低。</strong></p>
<p>所以窄窗口时间分辨率高、频率分辨率低，宽窗口时间分辨率低、频率分辨率高。对于时变的非稳态信号，高频适合小窗口，低频适合大窗口。<strong>然而STFT的窗口是固定的，在一次STFT中宽度不会变化，所以STFT还是无法满足非稳态信号变化的频率的需求。</strong></p>
<h3 id="可变窗的STFT"><a href="#可变窗的STFT" class="headerlink" title="可变窗的STFT"></a>可变窗的STFT</h3><p>一种解决上述问题的方法是：让窗口大小变起来，多做几次STFT不就可以了吗？</p>
<p>小波变换就有着这样的思路，但事实上小波并不是这么做的。至于为什么不采用可变窗的STFT呢，有很多解释，公认的缺陷是这样做的话STFT做不到正交化。</p>
<blockquote>
<p>大部分的解释里有这样的说法：小波变换能够提供一个随频率改变的“时间-频率”窗口。这也是一种理解性的说法，通过改变基底做到了可变窗，重点应该是基底的改变。</p>
</blockquote>
<h3 id="改变基底"><a href="#改变基底" class="headerlink" title="改变基底"></a>改变基底</h3><p>小波变换的出发点和STFT还是不同的：直接把傅里叶变换的基底做了改变，<strong>将无限长的三角函数基换成了有限长的会衰减的小波基。</strong></p>
<p><strong>小波变换就是为了解决对非平稳信号的分解问题而产生的数学方法。</strong>相比于傅里叶变换使用一组无限长的三角函数基进行信号拟合，小波变换使用的是一组正交的、迅速衰减的小波函数基进行信号拟合。这种小波函数基可通过其尺度变量和平移变量，获得不同的频率和时间位置。因此在利用这种小波函数基对信号进行分解时，可以用较少的小波函数基就拟合出突变信号（稀疏编码特性），同时也能获得不同频率的信号分量在时域上的出现位置。</p>
<h3 id="WT的总体理解"><a href="#WT的总体理解" class="headerlink" title="WT的总体理解"></a>WT的总体理解</h3><p>小波变换是这样一个过程：首先将原始信号作为输入信号，通过一组正交的小波基分解成高频部分和低频部分，然后将得到的低频部分作为输入信号，又进行小波分解，得到下一级的高频部分和低频部分，以此类推。随着小波分解的级数增加，其在频域上的分辨率就越高。这就是多分辨率分析（MRA，MultiResolution Analysis）。</p>
<p>从数学的角度理解，<strong>在小波变换中，一个位于希尔伯特空间中的函数，可以分解成一个尺度函数和一个小波函数，其中尺度函数对应原始函数中的低频部分，小波函数对应原始函数中的高频部分。</strong>通过尺度函数可以构建对原始信号的低通滤波器，通过小波函数可以构建对原始信号的高通滤波器。</p>
<blockquote>
<p>希尔伯特空间：完备的内积空间。</p>
<p>Linear Space → Normal Linear Space → Banach Space</p>
<p>Normal Linear Space → Inner Product Space<br>→ Banach Space</p>
<p>Inner Product Space → Euclid Space<br>→ Hilbert Space</p>
</blockquote>
<p>从信号处理的角度理解，在小波变换中，信号可通过信号滤波器分解为高频分量（高频子带）和低频分量（低频子带），高频子带又称为细节（detailed）子带，低频子带又称为近似（approximate）子带。细节子带是由输入信号通过高通滤波器后再进行下采样得到的，近似子带是由输入信号通过低通滤波器后再进行下采样得到的。</p>
<blockquote>
<p>小波变换的内容很多，一下两下真的说不清楚（想说清楚我也看不懂），但基本原理就是这些。随着应用的深入在做学习比较靠谱。</p>
</blockquote>
<h3 id="二进小波变换（DWT）"><a href="#二进小波变换（DWT）" class="headerlink" title="二进小波变换（DWT）"></a>二进小波变换（DWT）</h3><p>在小波变换中，若对尺度按幂级数作离散化，同时对平移保持连续变化，则此类小波变换称为二进小波变换（Dyadic Wavelet Transform）。</p>
<h3 id="紧支撑小波基"><a href="#紧支撑小波基" class="headerlink" title="紧支撑小波基"></a>紧支撑小波基</h3><p>在小波变换中，紧支撑小波基是性质较好的一类小波基，紧支撑（Compact Support）函数是指这样的一类函数：其自变量仅在0附近的取值范围内能得到非零函数值，而在其他区间取值，则得到的函数值全为零。能得到非零函数值的自变量取值区间被称为该函数的支撑区间。</p>
<p>一个函数的支撑区间长度主要由其尺度参数决定。支撑区间越大，计算复杂度越高，边界拖尾效应越明显。不仅如此，支撑区间越大，会产生更多的高幅值小波系数，关于这个结论的解释，可参考傅里叶变换使用无限长（支撑区间大）的三角函数基进行信号拟合的情况，相比于使用信号迅速衰减（支撑区间小）的小波基，三角函数基拟合信号时需要更多的数量。因此在选择小波基时，以支撑长度较短的小波基为宜。</p>
<p>此外，小波基的正交性也是一类重要的性质，它确保了信号的分解没有冗余（最优分解）。</p>
<h2 id="小波包变换（WPT）"><a href="#小波包变换（WPT）" class="headerlink" title="小波包变换（WPT）"></a>小波包变换（WPT）</h2><h3 id="小波变换的缺陷"><a href="#小波变换的缺陷" class="headerlink" title="小波变换的缺陷"></a>小波变换的缺陷</h3><p>同上，小波变换仍存在着不足之处，<strong>由于正交小波变换只对信号的低频部分做进一步分解，而对高频部分也即信号的细节部分不再继续分解</strong>，所以小波变换能够很好地表征一大类以低频信息为主要成分的信号，但它不能很好地分解和表示包含大量细节信息（细小边缘或纹理）的信号，如非平稳机械振动信号、遥感图象、地震信号和生物医学信号等。</p>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>小波包分解（Wavelet Packet Decomposition），又称为最优子带树结构（Optimal Subband Tree Structuring）是对小波变换的进一步优化。</p>
<p><strong>主要思想：在小波变换的基础上，在每一级信号分解时，除了对低频子带进行进一步分解，也对高频子带进行进一步分解。最后通过最小化一个代价函数，计算出最优的信号分解路径，并以此分解路径对原始信号进行分解。</strong></p>
<p>小波包变换可以对高频部分提供更精细的分解，而且这种分解既无冗余，也无疏漏，所以对包含大量中、高频信息的信号能够进行更好的时频局部化分析。</p>
<p>从函数理论的角度来看，小波包分解是将信号投影到小波包基函数张成的空间中。从信号处理的角度来看，它是让信号通过一系列中心频率不同但带宽相同的滤波器。</p>
<h2 id="小波函数基"><a href="#小波函数基" class="headerlink" title="小波函数基"></a>小波函数基</h2><p>无论是小波变换还是小波包变化，一个重要的环节是选择合适的小波函数基进行信号分解。</p>
<p>在选择合适的小波函数基时，需要考虑的因素</p>
<ul>
<li><p>小波的对称性：主要体现在保证信号重构时不会产生相位畸变，即是不会产生重构信号的相位失真。</p>
</li>
<li><p>小波的正则性：保证了信号的光滑和可微性，对于大部分小波而言（非全部），其与消失矩存在关系：小波的消失矩越大，正则性也就越大。</p>
</li>
<li><p>选择与输入信号的波形相似性高的小波：意义在于使数据压缩和降噪变得更容易（信号的拟合和分解都更容易）。</p>
</li>
</ul>
<p>几种常用的小波函数基：</p>
<h3 id="Haar小波"><a href="#Haar小波" class="headerlink" title="Haar小波"></a>Haar小波</h3><p>最简单的一个小波函数，其具有紧支撑性和正交性，函数图像为在支撑区间[0,1)上的单个矩形波。Haar小波在时域上不连续，作为基本小波时性能不是很好。</p>
<h3 id="Daubechies小波"><a href="#Daubechies小波" class="headerlink" title="Daubechies小波"></a>Daubechies小波</h3><h3 id="Biorthogonal小波"><a href="#Biorthogonal小波" class="headerlink" title="Biorthogonal小波"></a>Biorthogonal小波</h3><h3 id="Symlets小波"><a href="#Symlets小波" class="headerlink" title="Symlets小波"></a>Symlets小波</h3><h3 id="Mexican-Hat小波"><a href="#Mexican-Hat小波" class="headerlink" title="Mexican Hat小波"></a>Mexican Hat小波</h3></article><!-- lincense--><div class="post-paginator"><a class="nextSlogan" href="/2020/09/11/%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99%E7%9A%84%E5%BB%BA%E8%AE%BE%E9%97%AE%E9%A2%98%E9%9B%86%E5%90%88/" title="博客网站的建设问题集合"><span>NextPost ></span><br><span class="nextTitle">博客网站的建设问题集合</span></a><div class="clear"></div></div><div id="comment"></div></section></article><footer id="cxo-footer-outer"><div id="cxo-footer-inner"><p class="footer-container"><span>Site by </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span> | theme </span><a target="_blank" rel="noopener" href="https://github.com/Longlongyu/hexo-theme-Cxo"><span>Cxo</span></a></p><i class="fa fa-user"> </i><span id="busuanzi_value_site_uv"></span><span> | </span><i class="fa fa-eye"> </i><span id="busuanzi_value_site_pv"></span></div></footer><!-- catelog--><div class="toc-wrapper" style="top: 70vh;"><div class="toc-catalog"><i class="fa fa-list"> </i><span>CATALOG</span></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AB%8B%E6%84%8F"><span class="toc-number">1.</span> <span class="toc-text">立意</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">2.</span> <span class="toc-text">网络结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#embedding"><span class="toc-number">2.1.</span> <span class="toc-text">embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#wavelet-prediction"><span class="toc-number">2.2.</span> <span class="toc-text">wavelet prediction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reconstruction-networks%E3%80%82"><span class="toc-number">2.3.</span> <span class="toc-text">reconstruction networks。</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">3.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#full-image-loss%EF%BC%88MSE%EF%BC%89"><span class="toc-number">3.1.</span> <span class="toc-text">full-image loss（MSE）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#wavelet-based-loss"><span class="toc-number">3.2.</span> <span class="toc-text">wavelet-based loss</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#wavelet-prediction-loss"><span class="toc-number">3.2.1.</span> <span class="toc-text">wavelet prediction loss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#texture-loss"><span class="toc-number">3.2.2.</span> <span class="toc-text">texture loss</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">4.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%86%E8%8A%82"><span class="toc-number">4.1.</span> <span class="toc-text">实验细节</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">4.2.</span> <span class="toc-text">实验结果</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E6%94%BE%E5%A4%A7%E5%80%8D%E6%95%B0%E7%9A%84%E5%AE%9A%E6%80%A7%E6%AF%94%E8%BE%83"><span class="toc-number">4.2.1.</span> <span class="toc-text">不同放大倍数的定性比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E6%96%B9%E6%B3%95%E7%9A%84%E5%AE%9A%E6%80%A7%E6%AF%94%E8%BE%83"><span class="toc-number">4.2.2.</span> <span class="toc-text">不同方法的定性比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%B2%81%E6%A3%92%E6%80%A7%E8%AE%A8%E8%AE%BA"><span class="toc-number">4.2.3.</span> <span class="toc-text">鲁棒性讨论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E6%94%BE%E5%A4%A7%E5%80%8D%E6%95%B0%E7%9A%84%E5%AE%9A%E9%87%8F%E6%AF%94%E8%BE%83"><span class="toc-number">4.2.4.</span> <span class="toc-text">不同放大倍数的定量比较</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE"><span class="toc-number">5.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">6.</span> <span class="toc-text">其他</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%84%9F%E6%83%B3"><span class="toc-number">7.</span> <span class="toc-text">感想</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2"><span class="toc-number">8.</span> <span class="toc-text">小波变换</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%82%85%E7%AB%8B%E5%8F%B6%E5%8F%98%E6%8D%A2%EF%BC%88FT%EF%BC%89"><span class="toc-number">8.1.</span> <span class="toc-text">傅立叶变换（FT）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%AD%E6%97%B6%E5%82%85%E7%AB%8B%E5%8F%B6%E5%8F%98%E6%8D%A2%EF%BC%88STFT%EF%BC%89"><span class="toc-number">8.2.</span> <span class="toc-text">短时傅立叶变换（STFT）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%82%85%E7%AB%8B%E5%8F%B6%E5%8F%98%E6%8D%A2%E7%9A%84%E7%BC%BA%E9%99%B7"><span class="toc-number">8.2.1.</span> <span class="toc-text">傅立叶变换的缺陷</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E7%AA%97"><span class="toc-number">8.2.2.</span> <span class="toc-text">加窗</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2%EF%BC%88WT%EF%BC%89"><span class="toc-number">8.3.</span> <span class="toc-text">小波变换（WT）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%AD%E6%97%B6%E5%82%85%E7%AB%8B%E5%8F%B6%E5%8F%98%E6%8D%A2%E7%9A%84%E7%BC%BA%E9%99%B7"><span class="toc-number">8.3.1.</span> <span class="toc-text">短时傅立叶变换的缺陷</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AF%E5%8F%98%E7%AA%97%E7%9A%84STFT"><span class="toc-number">8.3.2.</span> <span class="toc-text">可变窗的STFT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%94%B9%E5%8F%98%E5%9F%BA%E5%BA%95"><span class="toc-number">8.3.3.</span> <span class="toc-text">改变基底</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#WT%E7%9A%84%E6%80%BB%E4%BD%93%E7%90%86%E8%A7%A3"><span class="toc-number">8.3.4.</span> <span class="toc-text">WT的总体理解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E8%BF%9B%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2%EF%BC%88DWT%EF%BC%89"><span class="toc-number">8.3.5.</span> <span class="toc-text">二进小波变换（DWT）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B4%A7%E6%94%AF%E6%92%91%E5%B0%8F%E6%B3%A2%E5%9F%BA"><span class="toc-number">8.3.6.</span> <span class="toc-text">紧支撑小波基</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E6%B3%A2%E5%8C%85%E5%8F%98%E6%8D%A2%EF%BC%88WPT%EF%BC%89"><span class="toc-number">8.4.</span> <span class="toc-text">小波包变换（WPT）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2%E7%9A%84%E7%BC%BA%E9%99%B7"><span class="toc-number">8.4.1.</span> <span class="toc-text">小波变换的缺陷</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-number">8.4.2.</span> <span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E6%B3%A2%E5%87%BD%E6%95%B0%E5%9F%BA"><span class="toc-number">8.5.</span> <span class="toc-text">小波函数基</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Haar%E5%B0%8F%E6%B3%A2"><span class="toc-number">8.5.1.</span> <span class="toc-text">Haar小波</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Daubechies%E5%B0%8F%E6%B3%A2"><span class="toc-number">8.5.2.</span> <span class="toc-text">Daubechies小波</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Biorthogonal%E5%B0%8F%E6%B3%A2"><span class="toc-number">8.5.3.</span> <span class="toc-text">Biorthogonal小波</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Symlets%E5%B0%8F%E6%B3%A2"><span class="toc-number">8.5.4.</span> <span class="toc-text">Symlets小波</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mexican-Hat%E5%B0%8F%E6%B3%A2"><span class="toc-number">8.5.5.</span> <span class="toc-text">Mexican Hat小波</span></a></li></ol></li></ol></li></ol></div><!-- top--><i class="fa fa-arrow-up close" id="go-up" aria-hidden="true"></i></body></html>