<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><meta name="author" content="Fy J"><meta name="renderer" content="webkit"><meta name="copyright" content="Fy J"><meta name="keywords" content="MoyangSensei"><meta name="description" content="null"><meta name="Cache-Control" content="no-cache"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>论文研读：TTSR · MoYang</title><link rel="stylesheet" href="/css/style.css?v=2018.7.9"><link rel="stylesheet" href="/css/animation.css?v=2018.7.9"><link rel="icon" href="/img/assets/Moyangico.ico"><link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.6"><!-- scripts--><script>(function( w ){
  "use strict";
  // rel=preload support test
  if( !w.loadCSS ){
    w.loadCSS = function(){};
  }
  // define on the loadCSS obj
  var rp = loadCSS.relpreload = {};
  // rel=preload feature support test
  // runs once and returns a function for compat purposes
  rp.support = (function(){
    var ret;
    try {
      ret = w.document.createElement( "link" ).relList.supports( "preload" );
    } catch (e) {
      ret = false;
    }
    return function(){
      return ret;
    };
  })();

  // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
  // then change that media back to its intended value on load
  rp.bindMediaToggle = function( link ){
    // remember existing media attr for ultimate state, or default to 'all'
    var finalMedia = link.media || "all";

    function enableStylesheet(){
      link.media = finalMedia;
    }

    // bind load handlers to enable media
    if( link.addEventListener ){
      link.addEventListener( "load", enableStylesheet );
    } else if( link.attachEvent ){
      link.attachEvent( "onload", enableStylesheet );
    }

    // Set rel and non-applicable media type to start an async request
    // note: timeout allows this to happen async to let rendering continue in IE
    setTimeout(function(){
      link.rel = "stylesheet";
      link.media = "only x";
    });
    // also enable media after 3 seconds,
    // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
    setTimeout( enableStylesheet, 3000 );
  };

  // loop through link elements in DOM
  rp.poly = function(){
    // double check this to prevent external calls from running
    if( rp.support() ){
      return;
    }
    var links = w.document.getElementsByTagName( "link" );
    for( var i = 0; i < links.length; i++ ){
      var link = links[ i ];
      // qualify links to those with rel=preload and as=style attrs
      if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
        // prevent rerunning on link
        link.setAttribute( "data-loadcss", true );
        // bind listeners to toggle media back
        rp.bindMediaToggle( link );
      }
    }
  };

  // if unsupported, run the polyfill
  if( !rp.support() ){
    // run once at least
    rp.poly();

    // rerun poly on an interval until onload
    var run = w.setInterval( rp.poly, 500 );
    if( w.addEventListener ){
      w.addEventListener( "load", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    } else if( w.attachEvent ){
      w.attachEvent( "onload", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    }
  }


  // commonjs
  if( typeof exports !== "undefined" ){
    exports.loadCSS = loadCSS;
  }
  else {
    w.loadCSS = loadCSS;
  }
}( typeof global !== "undefined" ? global : this ) );</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" defer></script><script src="/js/main.js?v=2018.7.9" defer></script><!-- fancybox--><link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script><!-- busuanzi--><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="MoyangSensei" type="application/atom+xml">
</head><body><section class="profile-close" id="cxo-profile"><div class="profile-avatar"><i class="fa fa-caret-left"></i><img src="/img/assets/cat.png"></div><!--.profile-saying
  i.fa.fa-comment
  .saying--><div class="cxo-profile-inner"><div class="profile-name">Fy J</div><div class="profile-signature">CS专业扫雷学深造学者互联网冲浪一级选手</div><div class="friends"><div>FRIENDS</div><span><a href="//hnjia00.github.io" target="_black">jhn</a></span></div><div class="read-progress"></div></div></section><header id="cxo-intro" style="height: 70vh;background-image: url(/img/intro/index-bg.png);"><nav id="cxo-intro-nav"><section><div class="intro-nav-title"><a href="/">MoYang's Blog</a></div><div class="intro-nav-label-box"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div><i class="fa fa-bars intro-nav-menu"><div class="intro-nav-drop"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div></i><div class="clear"></div></section></nav><h1 class="post-title">论文研读：TTSR</h1><div class="post-intros"><div class="post-intro-meta"><span class="post-intro-time"><i class="post-intro-calendar fa fa-calendar"></i><span>06-15-2022 16:53:15</span></span><span class="post-intro-tags"><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="论文研读"> 论文研读</a><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="图像超分辨率"> 图像超分辨率</a></span></div><div class="post-intro-read"><span> Word count: <span class="post-count">2k</span> | Reading time: <span class="post-count">7</span>min</span></div></div></header><article class="cxo-up" id="cxo-content-outer"><section id="cxo-content-inner"><article class="article-entry" id="post"><blockquote>
<p>原创文章，转载、引用请注明出处！</p>
</blockquote>
<hr>
<blockquote>
<p>Learning Texture Transformer Network for Image Super-Resolution</p>
</blockquote>
<blockquote>
<p>CVPR 2020</p>
</blockquote>
<h1 id="立意"><a href="#立意" class="headerlink" title="立意"></a>立意</h1><p>文章指出，现有的超分辨率方法忽略了使用注意力机制从Ref图像传输高分辨率的纹理。</p>
<blockquote>
<p>使用高分辨率图像作为参考，把这个称为Ref，同SISR是两种不同的范式，abstract和introduction里提到。</p>
<blockquote>
<p>We study on image super-resolution (SR), which aims to recover realistic textures from a low-resolution (LR) image. Recent progress has been made by taking high-resolution images as references (Ref), so that relevant textures can be transferred to LR images.<br>The research on image SR is usually conducted on two paradigms, including single image super-resolution (SISR), and reference-based image super-resolution (RefSR).</p>
</blockquote>
</blockquote>
<blockquote>
<p>However, existing SR approaches neglect to use attention mechanisms to transfer high-resolution (HR) textures from Ref images, which limits these approaches in challenging cases.</p>
</blockquote>
<p>解决思路：TTSR，其中有4个模块。<strong>最大的贡献：最早将transformer体系结构引入图像生成任务。</strong></p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="Texture-Transformer"><a href="#Texture-Transformer" class="headerlink" title="Texture Transformer"></a>Texture Transformer</h2><p>用于纹理变换的transformer，也是本文的核心。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/1.png" title="Optional title"></p>
<h3 id="Learnable-Texture-Extractor-LTE"><a href="#Learnable-Texture-Extractor-LTE" class="headerlink" title="Learnable Texture Extractor (LTE)"></a>Learnable Texture Extractor (LTE)</h3><p><strong>过去通常使用预训练好的VGG网络提取特征</strong>，LTE是被设计用来进行特征提取的提取器，这里可以自设训练，在超分过程中训练从而更好地提取跨LR和Ref的联合特征。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/2.png" title="Optional title"></p>
<h3 id="Relevance-Embedding-RE"><a href="#Relevance-Embedding-RE" class="headerlink" title="Relevance Embedding (RE)"></a>Relevance Embedding (RE)</h3><p>RE的目的是通过估计Q和K之间的相似度来估计LR和REF图像之间的相关性。</p>
<p>LTE提取的Q和K被分成多个patch，其中每个patch可以分别表示为:</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/3.png" title="Optional title"></p>
<p>计算q和k的归一化内积，作为相似度：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/4.png" title="Optional title"></p>
<blockquote>
<p>R是没有出现在图上的，R是RE的结果，H和S是通过它得到的。</p>
</blockquote>
<h3 id="Hard-Attention-HA"><a href="#Hard-Attention-HA" class="headerlink" title="Hard-Attention (HA)"></a>Hard-Attention (HA)</h3><p>传统的注意力机制会直接计算qi与V的加权和，作者认为这样会造成模糊。具体原因是<strong>由于Ref图像和LR图像之间的domain gap比较大导致直接这样模型会缺乏传递HR纹理特征的能力</strong>。HA则用来传递Ref图像中纹理特征以解决这个问题。</p>
<p>计算HA map：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/5.png" title="Optional title"></p>
<p>hi是一个索引值，表示Ref图像中和LR图像中第i个patch最相关的patch的索引。根据这个值去V中索引得到最相关的纹理特征T。</p>
<h3 id="Soft-Attention-SA"><a href="#Soft-Attention-SA" class="headerlink" title="Soft-Attention (SA)"></a>Soft-Attention (SA)</h3><p>HA是负责找出Ref特征中和LR图像特征相关较强的部分从而建立对应关系，SA是负责将两个特征通过加权的方式更好地融合。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/6.png" title="Optional title"></p>
<blockquote>
<p>跳过的公式7就是从HA map中找到最大的（Max）那个。</p>
</blockquote>
<h2 id="Cross-Scale-Feature-Integration-CSFI"><a href="#Cross-Scale-Feature-Integration-CSFI" class="headerlink" title="Cross-Scale Feature Integration (CSFI)"></a>Cross-Scale Feature Integration (CSFI)</h2><p>通过堆叠Texture Transformer可以实现不同倍数的放大，作者提出了一个跨尺度特征集成模块（CSFI）在不同尺度的特征之间交换信息。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/7.png" title="Optional title"></p>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>线性组合Loss，三部分组成：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/8.png" title="Optional title"></p>
<p>Reconstruction loss：<strong>就是L1。大家都认为这个比L2强。</strong></p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/9.png" title="Optional title"></p>
<p>Adversarial loss：参照WGAN-GP。提出用梯度范数的惩罚来代替权重裁剪加粗样式，使得训练更加稳定，性能更好。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/10.png" title="Optional title"></p>
<p>Perceptual loss：SRGAN里已经详细提到过了。感知损失的关键思想是增强预测图像与目标图像在特征空间上的相似性，这里包含两部分，第一部分是传统的基于VGG19的特征。第二部分是转移感知损失（transferal perceptual loss），对应LTE提取的特征，<strong>这种传递感知损失限制了预测的SR图像与传递的纹理特征T具有相似的纹理特征，这使得作者的方法更有效地传递Ref纹理。</strong></p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/11.png" title="Optional title"></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><ul>
<li><p>可学习纹理提取器包含5个卷积层和2个池层，它们以三种不同的尺度输出纹理特征。为了减少时间和GPU内存的消耗，相关性嵌入只应用于最小尺度，并进一步传播到其他尺度；</p>
</li>
<li><p>鉴别器G：我们采用了SRNTT中使用的相同网络，并删除了所有BN层；</p>
</li>
<li><p>数据增广：随机水平和垂直翻转，然后随机旋转90，180和270；每个minibatch包含9块大小为40×40的LR patch，以及9块大小为160×160的HR和Ref patch；</p>
</li>
<li><p>组合Loss的权重：Lrec、Ladv和Lper的权重系数分别为1、1e-3和1e-2；</p>
</li>
<li><p>优化器：使用β1=0.9、β2=0.999和ǫ=1e-8的Adam opti-mizer；</p>
</li>
<li><p>学习率：1e-4；</p>
</li>
<li><p>训练策略：前2个epoch只用Lrec，之后将所有的Loss都用上后训练50个epoch；</p>
</li>
</ul>
<blockquote>
<p>这部分写在原文的method的最后，也就是3.4，没放在4.1去写。</p>
</blockquote>
<ul>
<li><p>数据集：CUFED5【41】，训练集包含11871对，每对由输入图像和Ref图像组成；测试集共有126幅测试图像，每幅图像由4幅相似程度不同的Ref图像组成；</p>
</li>
<li><p>额外数据集1：Sun80【26】包含80幅自然图像，每幅图像都有多幅Ref图像；</p>
</li>
<li><p>额外数据集2：Urban100【11】，该数据集没有Ref图像，将其LR图像视为Ref图像。由于Urban100都是具有强自相似性的建筑图像，因此这种设计可以实现显式的自相似搜索和传输过程；</p>
</li>
<li><p>额外数据集3：Manga109【20】，同样缺少Ref图像，随机抽取该数据集中的HR图像作为Ref图像；</p>
</li>
<li><p>评价指标：在YCbCr空间的Y通道上，对SR结果进行了PSNR和SSIM评估。</p>
</li>
</ul>
<h2 id="定量实验"><a href="#定量实验" class="headerlink" title="定量实验"></a>定量实验</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/12.png" title="Optional title"></p>
<h2 id="定性实验"><a href="#定性实验" class="headerlink" title="定性实验"></a>定性实验</h2><p>可视化：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/13.png" title="Optional title"></p>
<p>用户调研：对RCAN【39】、RSR-GAN【38】、CrossNet【43】和SRNTT【41】，10名受试者，在CUFED5测试集上收集了2520张选票。对于每个比较过程，我们为用户提供两个图像，其中包括一个TTSR图像。要求用户选择视觉质量更高的。其中Y轴上的值表示倾向于TTSR而非其他方法的用户的年龄百分比。超过90%的用户投票支持的TTSR。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/14.png" title="Optional title"></p>
<blockquote>
<p>用户调研这里很像LPIPS中的用户调研，都是同时给你两张图让你选更清楚的。</p>
</blockquote>
<h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><h3 id="Texture-Transformer中模块的影响"><a href="#Texture-Transformer中模块的影响" class="headerlink" title="Texture Transformer中模块的影响"></a>Texture Transformer中模块的影响</h3><p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/15.png" title="Optional title"></p>
<h3 id="CSFI的影响"><a href="#CSFI的影响" class="headerlink" title="CSFI的影响"></a>CSFI的影响</h3><p>第一行显示了仅使用TT时我们模型的性能，而第二行证明了CSFI的有效性，它使PSNR指标增加了0.17。</p>
<p>为了验证性能的改善不是由参数大小的增加带来的，将“Base+TT”模型的信道数增加到80和96。“Base+TT（C80）”几乎没有增长，其参数数几乎与“Base+TT+CSFI”相同。“Base+TT（C96）”将参数数量增加到9.10M也没有增长。<strong>CSFI可以以相对较小的参数大小有效地利用参考纹理信息。</strong></p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/16.png" title="Optional title"></p>
<h3 id="Adv-loss和Transferal-per-loss的影响"><a href="#Adv-loss和Transferal-per-loss的影响" class="headerlink" title="Adv loss和Transferal per loss的影响"></a>Adv loss和Transferal per loss的影响</h3><p>两幅图，不重要。</p>
<h3 id="不同的Ref图像相关性的影响"><a href="#不同的Ref图像相关性的影响" class="headerlink" title="不同的Ref图像相关性的影响"></a>不同的Ref图像相关性的影响</h3><p>为了研究LR和Ref图像之间的相关性如何影响TTSR的结果，在CUFED5测试集上进行了实验。其中，“L1”至“L4”表示CUFED5测试集提供的参考图像，其中L1是最相关的级别，而L4是最不相关的级别。“LR”是指使用输入图像本身作为参考图像。使用L1作为参考图像的TTSR实现了最佳性能。当使用LR作为参考图像时，TTSR的性能仍然优于以前最先进的RefSR方法。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/17.png" title="Optional title"></p>
<blockquote>
<p>一张图像的LR难道不应该是最相关的？还是说这个数据集的组成并不是依靠两个客观指标的计算来定义参考图像是否相关？？？</p>
</blockquote>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul>
<li><p>文章：<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Learning_Texture_Transformer_Network_for_Image_Super-Resolution_CVPR_2020_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Learning_Texture_Transformer_Network_for_Image_Super-Resolution_CVPR_2020_paper.pdf</a></p>
</li>
<li><p>code：<a target="_blank" rel="noopener" href="https://github.com/researchmm/TTSR">https://github.com/researchmm/TTSR</a></p>
</li>
</ul>
<h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>Transformer能够用来做很多事情，在看这篇文章之前，以为是一transformer的领域迁移为核心的创新是最大亮点，确实是，但占比重没有绝对的高，文章还设计了自己的方法流程用来解决超分辨率问题，就是CSFI。</p>
<p>对比实验做的非常的细，该论证到的内容都论证到了。没有提供太高倍数的模型估计是因为计算量的问题，毕竟CSFI是级联的结构，想一想就知道高倍数需要的参数量估计很大。</p>
<hr>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>懒得写字，直接放PPT。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/18.png" title="Optional title"></p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/19.png" title="Optional title"></p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/20.png" title="Optional title"></p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/21.png" title="Optional title"></p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/22.png" title="Optional title"></p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/23.png" title="Optional title"></p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/24.png" title="Optional title"></p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/25.png" title="Optional title"></p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/26.png" title="Optional title"></p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SISR-TTSR/27.png" title="Optional title"></p>
</article><!-- lincense--><div class="post-paginator"><a class="nextSlogan" href="/2022/06/08/%E9%9A%8F%E8%AE%B0%E3%80%82/" title="随记。"><span>NextPost ></span><br><span class="nextTitle">随记。</span></a><div class="clear"></div></div><div id="comment"></div></section></article><footer id="cxo-footer-outer"><div id="cxo-footer-inner"><p class="footer-container"><span>Site by </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span> | theme </span><a target="_blank" rel="noopener" href="https://github.com/Longlongyu/hexo-theme-Cxo"><span>Cxo</span></a></p><i class="fa fa-user"> </i><span id="busuanzi_value_site_uv"></span><span> | </span><i class="fa fa-eye"> </i><span id="busuanzi_value_site_pv"></span></div></footer><!-- catelog--><div class="toc-wrapper" style="top: 70vh;"><div class="toc-catalog"><i class="fa fa-list"> </i><span>CATALOG</span></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AB%8B%E6%84%8F"><span class="toc-number">1.</span> <span class="toc-text">立意</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">2.</span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Texture-Transformer"><span class="toc-number">2.1.</span> <span class="toc-text">Texture Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Learnable-Texture-Extractor-LTE"><span class="toc-number">2.1.1.</span> <span class="toc-text">Learnable Texture Extractor (LTE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Relevance-Embedding-RE"><span class="toc-number">2.1.2.</span> <span class="toc-text">Relevance Embedding (RE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hard-Attention-HA"><span class="toc-number">2.1.3.</span> <span class="toc-text">Hard-Attention (HA)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Soft-Attention-SA"><span class="toc-number">2.1.4.</span> <span class="toc-text">Soft-Attention (SA)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cross-Scale-Feature-Integration-CSFI"><span class="toc-number">2.2.</span> <span class="toc-text">Cross-Scale Feature Integration (CSFI)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Loss-Function"><span class="toc-number">2.3.</span> <span class="toc-text">Loss Function</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">3.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E8%AE%BE%E7%BD%AE"><span class="toc-number">3.1.</span> <span class="toc-text">实验设置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E9%87%8F%E5%AE%9E%E9%AA%8C"><span class="toc-number">3.2.</span> <span class="toc-text">定量实验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E6%80%A7%E5%AE%9E%E9%AA%8C"><span class="toc-number">3.3.</span> <span class="toc-text">定性实验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="toc-number">3.4.</span> <span class="toc-text">消融实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Texture-Transformer%E4%B8%AD%E6%A8%A1%E5%9D%97%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">3.4.1.</span> <span class="toc-text">Texture Transformer中模块的影响</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CSFI%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">3.4.2.</span> <span class="toc-text">CSFI的影响</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adv-loss%E5%92%8CTransferal-per-loss%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">3.4.3.</span> <span class="toc-text">Adv loss和Transferal per loss的影响</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E7%9A%84Ref%E5%9B%BE%E5%83%8F%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">3.4.4.</span> <span class="toc-text">不同的Ref图像相关性的影响</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">4.</span> <span class="toc-text">其他</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%84%9F%E6%83%B3"><span class="toc-number">5.</span> <span class="toc-text">感想</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer"><span class="toc-number">6.</span> <span class="toc-text">Transformer</span></a></li></ol></div><!-- top--><i class="fa fa-arrow-up close" id="go-up" aria-hidden="true"></i></body></html>